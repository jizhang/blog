<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>How to Achieve Exactly-Once Semantics in Spark Streaming | Ji ZHANG&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Exactly-once semantics is one of the advanced topics of stream processing. To process every message once and only once, in spite of system or network failure, not only the stream processing framework">
<meta name="keywords" content="spark,spark streaming,kafka,stream processing,scala">
<meta property="og:type" content="article">
<meta property="og:title" content="How to Achieve Exactly-Once Semantics in Spark Streaming">
<meta property="og:url" content="http://shzhangji.com/blog/2017/07/31/how-to-achieve-exactly-once-semantics-in-spark-streaming/index.html">
<meta property="og:site_name" content="Ji ZHANG&#39;s Blog">
<meta property="og:description" content="Exactly-once semantics is one of the advanced topics of stream processing. To process every message once and only once, in spite of system or network failure, not only the stream processing framework">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://spark.apache.org/docs/latest/img/streaming-arch.png">
<meta property="og:updated_time" content="2017-09-04T13:26:37.432Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="How to Achieve Exactly-Once Semantics in Spark Streaming">
<meta name="twitter:description" content="Exactly-once semantics is one of the advanced topics of stream processing. To process every message once and only once, in spite of system or network failure, not only the stream processing framework">
<meta name="twitter:image" content="http://spark.apache.org/docs/latest/img/streaming-arch.png">
<meta name="twitter:creator" content="@zjerryj">
<link rel="publisher" href="zhangji87@gmail.com">
  
    <link rel="alternate" href="/atom.xml" title="Ji ZHANG&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link rel="stylesheet" href="/css/source-code-pro.css">
  
  <link rel="stylesheet" href="/css/style.css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-37223379-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Ji ZHANG&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">If I rest, I rust.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/categories/Big-Data">Big Data</a>
        
          <a class="main-nav-link" href="/categories/Programming">Programming</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
        <a class="main-nav-link" href="http://shzhangji.com/cnblogs"><img src="/images/cnblogs.png"></a>
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://shzhangji.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-how-to-achieve-exactly-once-semantics-in-spark-streaming" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2017/07/31/how-to-achieve-exactly-once-semantics-in-spark-streaming/" class="article-date">
  <time datetime="2017-07-31T14:56:07.000Z" itemprop="datePublished">2017-07-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Big-Data/">Big Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      How to Achieve Exactly-Once Semantics in Spark Streaming
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Exactly-once semantics is one of the advanced topics of stream processing. To process every message once and only once, in spite of system or network failure, not only the stream processing framework needs to provide such functionality, but also the message delivery system, the output data store, as well as how we implement the processing procedure, altogether can we ensure the exactly-once semantics. In this article, I’ll demonstrate how to use Spark Streaming, with Kafka as data source and MySQL the output storage, to achieve exactly-once stream processing.</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt="Spark Streaming"></p>
<h2 id="An-Introductory-Example"><a href="#An-Introductory-Example" class="headerlink" title="An Introductory Example"></a>An Introductory Example</h2><p>First let’s implement a simple yet complete stream processing application that receive access logs from Kafka, parse and count the errors, then write the errors per minute metric into MySQL database.</p>
<p>Sample access logs:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2017-07-30 14:09:08 ERROR some message</span><br><span class="line">2017-07-30 14:09:20 INFO  some message</span><br><span class="line">2017-07-30 14:10:50 ERROR some message</span><br></pre></td></tr></table></figure>
<p>Output table, where <code>log_time</code> should be truncated to minutes:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> error_log (</span><br><span class="line">  log_time datetime primary <span class="keyword">key</span>,</span><br><span class="line">  log_count <span class="built_in">int</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="number">0</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>Scala projects are usually managed by <code>sbt</code> tool. Let’s add the following dependencies into <code>build.sbt</code> file. We’re using Spark 2.2 with Kafka 0.10. The choice of database library is ScalikeJDBC 3.0.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scalaVersion := <span class="string">"2.11.11"</span></span><br><span class="line"></span><br><span class="line">libraryDependencies ++= <span class="type">Seq</span>(</span><br><span class="line">  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-streaming"</span> % <span class="string">"2.2.0"</span> % <span class="string">"provided"</span>,</span><br><span class="line">  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-streaming-kafka-0-10"</span> % <span class="string">"2.2.0"</span>,</span><br><span class="line">  <span class="string">"org.scalikejdbc"</span> %% <span class="string">"scalikejdbc"</span> % <span class="string">"3.0.1"</span>,</span><br><span class="line">  <span class="string">"mysql"</span> % <span class="string">"mysql-connector-java"</span> % <span class="string">"5.1.43"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>The complete code can be found on GitHub (<a href="https://github.com/jizhang/spark-sandbox/blob/master/src/main/scala/ExactlyOnce.scala" target="_blank" rel="noopener">link</a>), so here only shows the major parts of the application:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// initialize database connection</span></span><br><span class="line"><span class="type">ConnectionPool</span>.singleton(<span class="string">"jdbc:mysql://localhost:3306/spark"</span>, <span class="string">"root"</span>, <span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// create Spark streaming context</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ExactlyOnce"</span>).setIfMissing(<span class="string">"spark.master"</span>, <span class="string">"local[2]"</span>)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// create Kafka DStream with Direct API</span></span><br><span class="line"><span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">   <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">   <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Seq</span>(<span class="string">"alog"</span>), kafkaParams))</span><br><span class="line"></span><br><span class="line">messages.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="comment">// do transformation</span></span><br><span class="line">  <span class="keyword">val</span> result = rdd.map(_.value)</span><br><span class="line">    .flatMap(parseLog) <span class="comment">// utility function to parse log line into case class</span></span><br><span class="line">    .filter(_.level == <span class="string">"ERROR"</span>)</span><br><span class="line">    .map(log =&gt; log.time.truncatedTo(<span class="type">ChronoUnit</span>.<span class="type">MINUTES</span>) -&gt; <span class="number">1</span>)</span><br><span class="line">    .reduceByKey(_ + _)</span><br><span class="line">    .collect()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// store result into database</span></span><br><span class="line">  <span class="type">DB</span>.autoCommit &#123; <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">    result.foreach &#123; <span class="keyword">case</span> (time, count) =&gt;</span><br><span class="line">      <span class="string">sql""</span><span class="string">"</span></span><br><span class="line"><span class="string">      insert into error_log (log_time, log_count)</span></span><br><span class="line"><span class="string">      value ($&#123;time&#125;, $&#123;count&#125;)</span></span><br><span class="line"><span class="string">      on duplicate key update log_count = log_count + values(log_count)</span></span><br><span class="line"><span class="string">      "</span><span class="string">""</span>.update.apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Stream-Processing-Semantics"><a href="#Stream-Processing-Semantics" class="headerlink" title="Stream Processing Semantics"></a>Stream Processing Semantics</h2><p>There’re three semantics in stream processing, namely at-most-once, at-least-once, and exactly-once. In a typical Spark Streaming application, there’re three processing phases: receive data, do transformation, and push outputs. Each phase takes different efforts to achieve different semantics.</p>
<p>For <strong>receiving data</strong>, it largely depends on the data source. For instance, reading files from a fault-tolerant file system like HDFS, gives us exactly-once semantics. For upstream queues that support acknowledgement, e.g. RabbitMQ, we can combine it with Spark’s write ahead logs to achieve at-least-once semantics. For unreliable receivers like <code>socketTextStream</code>, there might be data loss due to worker/driver failure and gives us undefined semantics. Kafka, on the other hand, is offset based, and its direct API can give us exactly-once semantics.</p>
<p>When <strong>transforming data</strong> with Spark’s RDD, we automatically get exactly-once semantics, for RDD is itself immutable, fault-tolerant and deterministically re-computable. As long as the source data is available, and there’s no side effects during transformation, the result will always be the same.</p>
<p><strong>Output operation</strong> by default has at-least-once semantics. The <code>foreachRDD</code> function will execute more than once if there’s worker failure, thus writing same data to external storage multiple times. There’re two approaches to solve this issue, idempotent updates, and transactional updates. They are further discussed in the following sections.</p>
<h2 id="Exactly-once-with-Idempotent-Writes"><a href="#Exactly-once-with-Idempotent-Writes" class="headerlink" title="Exactly-once with Idempotent Writes"></a>Exactly-once with Idempotent Writes</h2><p>If multiple writes produce the same data, then this output operation is idempotent. <code>saveAsTextFile</code> is a typical idempotent update; messages with unique keys can be written to database without duplication. This approach will give us the equivalent exactly-once semantics. Note though it’s usually for map-only procedures, and it requires some setup on Kafka DStream.</p>
<ul>
<li>Set <code>enable.auto.commit</code> to <code>false</code>. By default, Kafka DStream will commit the consumer offsets right after it receives the data. We want to postpone this action unitl the batch is fully processed.</li>
<li>Turn on Spark Streaming’s checkpointing to store Kafka offsets. But if the application code changes, checkpointed data is not reusable. This leads to a second option:</li>
<li>Commit Kafka offsets after outputs. Kafka provides a <code>commitAsync</code> API, and the <code>HasOffsetRanges</code> class can be used to extract offsets from the initial RDD:</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">messages.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">  rdd.foreachPartition &#123; iter =&gt;</span><br><span class="line">    <span class="comment">// output to database</span></span><br><span class="line">  &#125;</span><br><span class="line">  messages.asInstanceOf[<span class="type">CanCommitOffsets</span>].commitAsync(offsetRanges)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Exactly-once-with-Transactional-Writes"><a href="#Exactly-once-with-Transactional-Writes" class="headerlink" title="Exactly-once with Transactional Writes"></a>Exactly-once with Transactional Writes</h2><p>Transactional updates require a unique identifier. One can generate from batch time, partition id, or Kafka offsets, and then write the result along with the identifier into external storage within a single transaction. This atomic operation gives us exactly-once semantics, and can be applied to both map-only and aggregation procedures.</p>
<p>Usually writing to database should happen in <code>foreachPartition</code>, i.e. in worker nodes. It is true for map-only procedure, because Kafka RDD’s partition is correspondent to Kafka partition, so we can extract each partition’s offset like this:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">messages.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">  rdd.foreachPartition &#123; iter =&gt;</span><br><span class="line">    <span class="keyword">val</span> offsetRange = offsetRanges(<span class="type">TaskContext</span>.get.partitionId)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>But for shuffled operations like the error log count example, we need to first collect the result back into driver and then perform the transaction.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">messages.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">  <span class="keyword">val</span> result = processLogs(rdd).collect() <span class="comment">// parse log and count error</span></span><br><span class="line">  <span class="type">DB</span>.localTx &#123; <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">    result.foreach &#123; <span class="keyword">case</span> (time, count) =&gt;</span><br><span class="line">      <span class="comment">// save to error_log table</span></span><br><span class="line">    &#125;</span><br><span class="line">    offsetRanges.foreach &#123; offsetRange =&gt;</span><br><span class="line">      <span class="keyword">val</span> affectedRows = <span class="string">sql""</span><span class="string">"</span></span><br><span class="line"><span class="string">      update kafka_offset set offset = $&#123;offsetRange.untilOffset&#125;</span></span><br><span class="line"><span class="string">      where topic = $&#123;topic&#125; and `partition` = $&#123;offsetRange.partition&#125;</span></span><br><span class="line"><span class="string">      and offset = $&#123;offsetRange.fromOffset&#125;</span></span><br><span class="line"><span class="string">      "</span><span class="string">""</span>.update.apply()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (affectedRows != <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"fail to update offset"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>If the offsets fail to update, or there’s a duplicate offset range detected by <code>offset != $fromOffset</code>, the whole transaction will rollback, which guarantees the exactly-once semantics.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Exactly-once is a very strong semantics in stream processing, and will inevitably bring some overhead to your application and impact the throughput. It’s also not applicable to <a href="https://github.com/koeninger/kafka-exactly-once/blob/master/src/main/scala/example/Windowed.scala" target="_blank" rel="noopener">windowed</a> operations. So you need to decide whether it’s necessary to spend such efforts, or weaker semantics even with few data loss will suffice. But surely knowing how to achieve exactly-once is a good chance of learning, and it’s a great fun.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/" target="_blank" rel="noopener">http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/</a></li>
<li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html</a></li>
<li><a href="http://kafka.apache.org/documentation.html#semantics" target="_blank" rel="noopener">http://kafka.apache.org/documentation.html#semantics</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://shzhangji.com/blog/2017/07/31/how-to-achieve-exactly-once-semantics-in-spark-streaming/" data-id="cjfp0rbs3000o88r4p46led22" class="article-share-link">Share</a>
      
        <a href="http://shzhangji.com/blog/2017/07/31/how-to-achieve-exactly-once-semantics-in-spark-streaming/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scala/">scala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark-streaming/">spark streaming</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/stream-processing/">stream processing</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2017/08/05/how-to-extract-event-time-in-apache-flume/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          How to Extract Event Time in Apache Flume
        
      </div>
    </a>
  
  
    <a href="/blog/2017/07/23/learn-pandas-from-a-sql-perspective/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Learn Pandas from a SQL Perspective</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/algorithm/" style="font-size: 10px;">algorithm</a> <a href="/tags/analytics/" style="font-size: 17.5px;">analytics</a> <a href="/tags/apache-beam/" style="font-size: 10px;">apache beam</a> <a href="/tags/canal/" style="font-size: 10px;">canal</a> <a href="/tags/clojure/" style="font-size: 10px;">clojure</a> <a href="/tags/crossfilter/" style="font-size: 10px;">crossfilter</a> <a href="/tags/dc-js/" style="font-size: 10px;">dc.js</a> <a href="/tags/elasticsearch/" style="font-size: 10px;">elasticsearch</a> <a href="/tags/es6/" style="font-size: 10px;">es6</a> <a href="/tags/etl/" style="font-size: 12.5px;">etl</a> <a href="/tags/flume/" style="font-size: 12.5px;">flume</a> <a href="/tags/frontend/" style="font-size: 15px;">frontend</a> <a href="/tags/functional-programming/" style="font-size: 10px;">functional programming</a> <a href="/tags/hive/" style="font-size: 10px;">hive</a> <a href="/tags/java/" style="font-size: 17.5px;">java</a> <a href="/tags/javascript/" style="font-size: 15px;">javascript</a> <a href="/tags/kafka/" style="font-size: 10px;">kafka</a> <a href="/tags/lodash/" style="font-size: 12.5px;">lodash</a> <a href="/tags/mapreduce/" style="font-size: 10px;">mapreduce</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/ops/" style="font-size: 10px;">ops</a> <a href="/tags/pandas/" style="font-size: 12.5px;">pandas</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/scala/" style="font-size: 12.5px;">scala</a> <a href="/tags/scalatra/" style="font-size: 10px;">scalatra</a> <a href="/tags/source-code/" style="font-size: 10px;">source code</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/spark-streaming/" style="font-size: 10px;">spark streaming</a> <a href="/tags/sql/" style="font-size: 12.5px;">sql</a> <a href="/tags/stream-processing/" style="font-size: 15px;">stream processing</a> <a href="/tags/webjars/" style="font-size: 10px;">webjars</a> <a href="/tags/websocket/" style="font-size: 10px;">websocket</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/05/">May 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/10/">October 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/04/">April 2013</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2018/04/07/error-handling-in-restful-api/">Error Handling in RESTful API</a>
          </li>
        
          <li>
            <a href="/blog/2017/10/23/flume-source-code-component-lifecycle/">Flume Source Code: Component Lifecycle</a>
          </li>
        
          <li>
            <a href="/blog/2017/09/30/pandas-and-tidy-data/">Pandas and Tidy Data</a>
          </li>
        
          <li>
            <a href="/blog/2017/09/12/apache-beam-quick-start-with-python/">Apache Beam Quick Start with Python</a>
          </li>
        
          <li>
            <a href="/blog/2017/09/04/hive-window-and-analytical-functions/">Hive Window and Analytical Functions</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://mirrors.creativecommons.org/presskit/buttons/80x15/svg/by-nc-sa.svg"></a>
      <br>
      &copy; 2018 Ji ZHANG<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/categories/Big-Data" class="mobile-nav-link">Big Data</a>
  
    <a href="/categories/Programming" class="mobile-nav-link">Programming</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
  <a href="http://shzhangji.com/cnblogs" class="mobile-nav-link">中文</a>
</nav>

    
<script>
  var disqus_shortname = 'jizhang';
  
  var disqus_url = 'http://shzhangji.com/blog/2017/07/31/how-to-achieve-exactly-once-semantics-in-spark-streaming/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>