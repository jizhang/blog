<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tutorial | Ji ZHANG's Blog]]></title>
  <link href="http://shzhangji.com/blog/categories/tutorial/atom.xml" rel="self"/>
  <link href="http://shzhangji.com/"/>
  <updated>2015-01-15T12:30:37+08:00</updated>
  <id>http://shzhangji.com/</id>
  <author>
    <name><![CDATA[Ji ZHANG]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[数据挖掘指南[8]聚类]]></title>
    <link href="http://shzhangji.com/blog/2015/01/15/guidetodatamining-8/"/>
    <updated>2015-01-15T12:24:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/01/15/guidetodatamining-8</id>
    <content type="html"><![CDATA[<p>原文：<a href="http://guidetodatamining.com/chapter-8/">http://guidetodatamining.com/chapter-8/</a></p>

<p>前几章我们学习了如何构建分类系统，使用的是已经标记好类别的数据集进行训练：</p>

<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-8/chapter-8-1.png" alt="" /></p>

<p>训练完成后我们就可以用来预测了：这个人看起来像是篮球运动员，那个人可能是练体操的；这个人三年内不会患有糖尿病。</p>

<p>可以看到，分类器在训练阶段就已经知道各个类别的名称了。那如果我们不知道呢？如何构建一个能够自动对数据进行分组的系统？比如有1000人，每人有20个特征，我想把这些人分为若干个组。</p>

<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-8/chapter-8-2.png" alt="" /></p>

<p>这个过程叫做聚类：通过物品特征来计算距离，并自动分类到不同的群集或组中。有两种聚类算法比较常用：</p>

<p><strong>k-means聚类算法</strong></p>

<p>我们会事先告诉这个算法要将数据分成几个组，比如“请把这1000个人分成5个组”，“将这些网页分成15个组”。这种方法就叫k-means，我们会在后面的章节讨论。</p>

<h2>层次聚类法</h2>

<p>对于层次聚类法，我们不需要预先指定分类的数量，这个算方法会将每条数据都当作是一个分类，每次迭代的时候合并距离最近的两个分类，直到剩下一个分类为止。因此聚类的结果是：顶层有一个大分类，这个分类下有两个子分类，每个子分类下又有两个子分类，依此类推，层次聚类也因此得命。</p>

<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-8/chapter-8-3.png" alt="" /></p>

<p>在合并的时候我们会计算两个分类之间的距离，可以采用不同的方法。如下图中的A、B、C三个分类，我们应该将哪两个分类合并起来呢？</p>

<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-8/chapter-8-4.png" alt="" /></p>

<p><a href="https://github.com/jizhang/guidetodatamining/blob/master/chapter-8.md">前往GitHub阅读全文</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据挖掘指南[7]朴素贝叶斯和文本数据]]></title>
    <link href="http://shzhangji.com/blog/2014/12/30/guidetodatamining-7/"/>
    <updated>2014-12-30T11:40:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/12/30/guidetodatamining-7</id>
    <content type="html"><![CDATA[<p>原文：<a href="http://guidetodatamining.com/chapter-7/">http://guidetodatamining.com/chapter-7/</a></p>

<h2>非结构化文本的分类算法</h2>

<p>在前几个章节中，我们学习了如何使用人们对物品的评价（五星、顶和踩）来进行推荐；还使用了他们的隐式评价——买过什么，点击过什么；我们利用特征来进行分类，如身高、体重、对法案的投票等。这些数据有一个共性——能用表格来展现：</p>

<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-7/chapter-7-1.png" alt="" /></p>

<p>因此这类数据我们称为“结构化数据”——数据集中的每条数据（上表中的一行）由多个特征进行描述（上表中的列）。而非结构化的数据指的是诸如电子邮件文本、推特信息、博客、新闻等。这些数据至少第一眼看起来是无法用一张表格来展现的。</p>

<p>举个例子，我们想从推特信息中获取用户对各种电影的评价：</p>

<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-7/chapter-7-2.png" alt="" /></p>

<p>可以看到，Andy Gavin喜欢看地心引力，因为他的消息中有“不寒而栗”、“演的太棒了”之类的文本。而Debra Murphy则不太喜欢这部电影，因为她说“还是省下看这部电影的钱吧”。如果有人说“我太想看这部电影了，都兴奋坏了！”，我们可以看出她是喜欢这部电影的，即使信息中有“坏”这个字。</p>

<p>我在逛超市时看到一种叫Chobani的酸奶，名字挺有趣的，但真的好吃吗？于是我掏出iPhone，谷歌了一把，看到一篇名为“女人不能只吃面包”的博客：</p>

<blockquote><p><strong>无糖酸奶品评</strong></p>

<p>你喝过Chobani酸奶吗？如果没有，就赶紧拿起钥匙出门去买吧！虽然它是脱脂原味的，但喝起来和酸奶的口感很像，致使我每次喝都有负罪感，因为这分明就是在喝全脂酸奶啊！原味的感觉很酸很够味，你也可以尝试一下蜂蜜口味的。我承认，虽然我在减肥期间不该吃蜂蜜的，但如果我有一天心情很糟想吃甜食，我就会在原味酸奶里舀一勺蜂蜜，太值得了！至于那些水果味的，应该都有糖分在里面，但其实酸奶本身就已经很美味了，水果只是点缀。如果你家附近没有Chobani，也可以试试Fage，同样好吃。</p>

<p>虽然需要花上一美元不到，而且还会增加20卡路里，但还是很值得的，毕竟我已经一下午没吃东西了！</p>

<p>来源：<a href="http://womandoesnotliveonbreadalone.blogspot.com/2009/03/sugar-free-yogurt-reviews.html">http://womandoesnotliveonbreadalone.blogspot.com/2009/03/sugar-free-yogurt-reviews.html</a></p></blockquote>

<p>这是一篇正面评价吗？从第二句就可以看出，作者非常鼓励我去买。她还用了“够味”、“美味”等词汇，这些都是正面的评价。所以，让我先去吃会儿……</p>

<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-7/chapter-7-3.png" alt="" /></p>

<p><a href="https://github.com/jizhang/guidetodatamining/blob/master/chapter-7.md">前往GitHub阅读全文</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据挖掘指南[6]概率和朴素贝叶斯]]></title>
    <link href="http://shzhangji.com/blog/2014/12/22/guidetodatamining-6/"/>
    <updated>2014-12-22T10:33:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/12/22/guidetodatamining-6</id>
    <content type="html"><![CDATA[<p>原文：<a href="http://guidetodatamining.com/chapter-6">http://guidetodatamining.com/chapter-6</a></p>

<h2>朴素贝叶斯</h2>

<p>还是让我们回到运动员的例子。如果我问你Brittney Griner的运动项目是什么，她有6尺8寸高，207磅重，你会说“篮球”；我再问你对此分类的准确度有多少信心，你会回答“非常有信心”。</p>

<p>我再问你Heather Zurich，6尺1寸高，重176磅，你可能就不能确定地说她是打篮球的了，至少不会像之前判定Brittney那样肯定。因为从Heather的身高体重来看她也有可能是跑马拉松的。</p>

<p>最后，我再问你Yumiko Hara的运动项目，她5尺4寸高，95磅重，你也许会说她是跳体操的，但也不太敢肯定，因为有些马拉松运动员也是类似的身高体重。</p>

<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-6/chapter-6-1.png" alt="" /></p>

<p>使用近邻算法时，我们很难对分类结果的置信度进行量化。但如果使用的是基于概率的分类算法——贝叶斯算法——那就可以给出分类结果的可能性了：这名运动员有80%的几率是篮球运动员；这位病人有40%的几率患有糖尿病；拉斯克鲁塞斯24小时内有雨的概率是10%。</p>

<p>近邻算法又称为<strong>被动学习</strong>算法。这种算法只是将训练集的数据保存起来，在收到测试数据时才会进行计算。如果我们有10万首音乐，那每进行一次分类，都需要遍历这10万条记录才行。</p>

<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-6/chapter-6-2.png" alt="" /></p>

<p>贝叶斯算法则是一种<strong>主动学习</strong>算法。它会根据训练集构建起一个模型，并用这个模型来对新的记录进行分类，因此速度会快很多。</p>

<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-6/chapter-6-3.png" alt="" /></p>

<p>所以说，贝叶斯算法的两个优点即：能够给出分类结果的置信度；以及它是一种主动学习算法。</p>

<p><a href="https://github.com/jizhang/guidetodatamining/blob/master/chapter-6.md">前往GitHub阅读全文</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark快速入门]]></title>
    <link href="http://shzhangji.com/blog/2014/12/16/spark-quick-start/"/>
    <updated>2014-12-16T15:59:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/12/16/spark-quick-start</id>
    <content type="html"><![CDATA[<p><img src="http://spark.apache.org/images/spark-logo.png" alt="" /></p>

<p><a href="http://spark.apache.org">Apache Spark</a>是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：</p>

<ul>
<li><strong>通用计算引擎</strong> 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；</li>
<li><strong>基于内存</strong> 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；</li>
<li><strong>与Hadoop集成</strong> 能够直接读写HDFS中的数据，并能运行在YARN之上。</li>
</ul>


<p>Spark是用<a href="http://www.scala-lang.org/">Scala语言</a>编写的，所提供的API也很好地利用了这门语言的特性。它也可以使用Java和Python编写应用。本文将用Scala进行讲解。</p>

<h2>安装Spark和SBT</h2>

<ul>
<li>从<a href="http://spark.apache.org/downloads.html">官网</a>上下载编译好的压缩包，解压到一个文件夹中。下载时需注意对应的Hadoop版本，如要读写CDH4 HDFS中的数据，则应下载Pre-built for CDH4这个版本。</li>
<li>为了方便起见，可以将spark/bin添加到$PATH环境变量中：</li>
</ul>


<p><code>bash
export SPARK_HOME=/path/to/spark
export PATH=$PATH:$SPARK_HOME/bin
</code></p>

<ul>
<li>在练习例子时，我们还会用到<a href="http://www.scala-sbt.org/">SBT</a>这个工具，它是用来编译打包Scala项目的。Linux下的安装过程比较简单：

<ul>
<li>下载<a href="https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.7/sbt-launch.jar">sbt-launch.jar</a>到$HOME/bin目录；</li>
<li>新建$HOME/bin/sbt文件，权限设置为755，内容如下：</li>
</ul>
</li>
</ul>


<p><code>bash
SBT_OPTS="-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M"
java $SBT_OPTS -jar `dirname $0`/sbt-launch.jar "$@"
</code></p>

<!-- more -->


<h2>日志分析示例</h2>

<p>假设我们有如下格式的日志文件，保存在/tmp/logs.txt文件中：</p>

<p><code>text
2014-12-11 18:33:52 INFO    Java    some message
2014-12-11 18:34:33 INFO    MySQL   some message
2014-12-11 18:34:54 WARN    Java    some message
2014-12-11 18:35:25 WARN    Nginx   some message
2014-12-11 18:36:09 INFO    Java    some message
</code></p>

<p>每条记录有四个字段，即时间、级别、应用、信息，使用制表符分隔。</p>

<p>Spark提供了一个交互式的命令行工具，可以直接执行Spark查询：</p>

<p>```
$ spark-shell
Welcome to</p>

<pre><code>  ____              __
 / __/__  ___ _____/ /__
_\ \/ _ \/ _ `/ __/  '_/
</code></pre>

<p>   /<em><strong>/ .</strong>/_,</em>/<em>/ /</em>/_\   version 1.1.0</p>

<pre><code>  /_/
</code></pre>

<p>Spark context available as sc.
scala>
```</p>

<h3>加载并预览数据</h3>

<p>```scala
scala> val lines = sc.textFile(&ldquo;/tmp/logs.txt&rdquo;)
lines: org.apache.spark.rdd.RDD[String] = /tmp/logs.txt MappedRDD[1] at textFile at <console>:12</p>

<p>scala> lines.first()
res0: String = 2014-12-11 18:33:52  INFO    Java    some message
```</p>

<ul>
<li>sc是一个SparkContext类型的变量，可以认为是Spark的入口，这个对象在spark-shell中已经自动创建了。</li>
<li>sc.textFile()用于生成一个RDD，并声明该RDD指向的是/tmp/logs.txt文件。RDD可以暂时认为是一个列表，列表中的元素是一行行日志（因此是String类型）。这里的路径也可以是HDFS上的文件，如hdfs://127.0.0.1:8020/user/hadoop/logs.txt。</li>
<li>lines.first()表示调用RDD提供的一个方法：first()，返回第一行数据。</li>
</ul>


<h3>解析日志</h3>

<p>为了能对日志进行筛选，如只处理级别为ERROR的日志，我们需要将每行日志按制表符进行分割：</p>

<p>```scala
scala> val logs = lines.map(line => line.split(&ldquo;\t&rdquo;))
logs: org.apache.spark.rdd.RDD[Array[String]] = MappedRDD[2] at map at <console>:14</p>

<p>scala> logs.first()
res1: Array[String] = Array(2014-12-11 18:33:52, INFO, Java, some message)
```</p>

<ul>
<li>lines.map(f)表示对RDD中的每一个元素使用f函数来处理，并返回一个新的RDD。</li>
<li>line => line.split(&ldquo;\t&rdquo;)是一个匿名函数，又称为Lambda表达式、闭包等。它的作用和普通的函数是一样的，如这个匿名函数的参数是line（String类型），返回值是Array数组类型，因为String.split()函数返回的是数组。</li>
<li>同样使用first()方法来看这个RDD的首条记录，可以发现日志已经被拆分成四个元素了。</li>
</ul>


<h3>过滤并计数</h3>

<p>我们想要统计错误日志的数量：</p>

<p>```scala
scala> val errors = logs.filter(log => log(1) == &ldquo;ERROR&rdquo;)
errors: org.apache.spark.rdd.RDD[Array[String]] = FilteredRDD[3] at filter at <console>:16</p>

<p>scala> errors.first()
res2: Array[String] = Array(2014-12-11 18:39:42, ERROR, Java, some message)</p>

<p>scala> errors.count()
res3: Long = 158
```</p>

<ul>
<li>logs.filter(f)表示筛选出满足函数f的记录，其中函数f需要返回一个布尔值。</li>
<li>log(1) == &ldquo;ERROR"表示获取每行日志的第二个元素（即日志级别），并判断是否等于ERROR。</li>
<li>errors.count()用于返回该RDD中的记录。</li>
</ul>


<h3>缓存</h3>

<p>由于我们还会对错误日志做一些处理，为了加快速度，可以将错误日志缓存到内存中，从而省去解析和过滤的过程：</p>

<p><code>scala
scala&gt; errors.cache()
</code></p>

<p>errors.cache()函数会告知Spark计算完成后将结果保存在内存中。所以说Spark是否缓存结果是需要用户手动触发的。在实际应用中，我们需要迭代处理的往往只是一部分数据，因此很适合放到内存里。</p>

<p>需要注意的是，cache函数并不会立刻执行缓存操作，事实上map、filter等函数都不会立刻执行，而是在用户执行了一些特定操作后才会触发，比如first、count、reduce等。这两类操作分别称为Transformations和Actions。</p>

<h3>显示前10条记录</h3>

<p>```scala
scala> val firstTenErrors = errors.take(10)
firstTenErrors: Array[Array[String]] = Array(Array(2014-12-11 18:39:42, ERROR, Java, some message), Array(2014-12-11 18:40:23, ERROR, Nginx, some message), &hellip;)</p>

<p>scala> firstTenErrors.map(log => log.mkString(&ldquo;\t&rdquo;)).foreach(line => println(line))
2014-12-11 18:39:42 ERROR   Java    some message
2014-12-11 18:40:23 ERROR   Nginx   some message
&hellip;
```</p>

<p>errors.take(n)方法可用于返回RDD前N条记录，它的返回值是一个数组。之后对firstTenErrors的处理使用的是Scala集合类库中的方法，如map、foreach，和RDD提供的接口基本一致。所以说用Scala编写Spark程序是最自然的。</p>

<h3>按应用进行统计</h3>

<p>我们想要知道错误日志中有几条Java、几条Nginx，这和常见的Wordcount思路是一样的。</p>

<p>```scala
scala> val apps = errors.map(log => (log(2), 1))
apps: org.apache.spark.rdd.RDD[(String, Int)] = MappedRDD[15] at map at <console>:18</p>

<p>scala> apps.first()
res20: (String, Int) = (Java,1)</p>

<p>scala> val counts = apps.reduceByKey((a, b) => a + b)
counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[17] at reduceByKey at <console>:20</p>

<p>scala> counts.foreach(t => println(t))
(Java,58)
(Nginx,53)
(MySQL,47)
```</p>

<p>errors.map(log => (log(2), 1))用于将每条日志转换为键值对，键是应用（Java、Nginx等），值是1，如<code>("Java", 1)</code>，这种数据结构在Scala中称为元组（Tuple），这里它有两个元素，因此称为二元组。</p>

<p>对于数据类型是二元组的RDD，Spark提供了额外的方法，reduceByKey(f)就是其中之一。它的作用是按键进行分组，然后对同一个键下的所有值使用f函数进行归约（reduce）。归约的过程是：使用列表中第一、第二个元素进行计算，然后用结果和第三元素进行计算，直至列表耗尽。如：</p>

<p><code>scala
scala&gt; Array(1, 2, 3, 4).reduce((a, b) =&gt; a + b)
res23: Int = 10
</code></p>

<p>上述代码的计算过程即<code>((1 + 2) + 3) + 4</code>。</p>

<p>counts.foreach(f)表示遍历RDD中的每条记录，并应用f函数。这里的f函数是一条打印语句（println）。</p>

<h2>打包应用程序</h2>

<p>为了让我们的日志分析程序能够在集群上运行，我们需要创建一个Scala项目。项目的大致结构是：</p>

<p>```
spark-sandbox
├── build.sbt
├── project
│   ├── build.properties
│   └── plugins.sbt
└── src</p>

<pre><code>└── main
    └── scala
        └── LogMining.scala
</code></pre>

<p>```</p>

<p>你可以直接使用<a href="https://github.com/jizhang/spark-sandbox">这个项目</a>作为模板。下面说明一些关键部分：</p>

<h3>配置依赖</h3>

<p><code>build.sbt</code></p>

<p><code>scala
libraryDependencies += "org.apache.spark" %% "spark-core" % "1.1.1"
</code></p>

<h3>程序内容</h3>

<p><code>src/main/scala/LogMining.scala</code></p>

<p>```scala
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf</p>

<p>object LogMining extends App {
  val conf = new SparkConf().setAppName(&ldquo;LogMining&rdquo;)
  val sc = new SparkContext(conf)
  val inputFile = args(0)
  val lines = sc.textFile(inputFile)
  // 解析日志
  val logs = lines.map(<em>.split(&ldquo;\t&rdquo;))
  val errors = logs.filter(</em>(1) == &ldquo;ERROR&rdquo;)
  // 缓存错误日志
  errors.cache()
  // 统计错误日志记录数
  println(errors.count())
  // 获取前10条MySQL的错误日志
  val mysqlErrors = errors.filter(<em>(2) == &ldquo;MySQL&rdquo;)
  mysqlErrors.take(10).map(</em> mkString &ldquo;\t&rdquo;).foreach(println)
  // 统计每个应用的错误日志数
  val errorApps = errors.map(_(2) &ndash;> 1)
  errorApps.countByKey().foreach(println)
}
```</p>

<h3>打包运行</h3>

<p><code>bash
$ cd spark-sandbox
$ sbt package
$ spark-submit --class LogMining --master local target/scala-2.10/spark-sandbox_2.10-0.1.0.jar data/logs.txt
</code></p>

<h2>参考资料</h2>

<ul>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html">Spark Programming Guide</a></li>
<li><a href="http://www.slideshare.net/cloudera/spark-devwebinarslides-final">Introduction to Spark Developer Training</a></li>
<li><a href="http://www.slideshare.net/liancheng/dtcc-14-spark-runtime-internals">Spark Runtime Internals</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据挖掘指南[5]进一步探索分类]]></title>
    <link href="http://shzhangji.com/blog/2014/11/27/guidetodatamining-5/"/>
    <updated>2014-11-27T12:00:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/11/27/guidetodatamining-5</id>
    <content type="html"><![CDATA[<p>原文：<a href="http://guidetodatamining.com/chapter-5">http://guidetodatamining.com/chapter-5</a></p>

<h2>效果评估算法和kNN</h2>

<p>让我们回到上一章中运动项目的例子。</p>

<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-5/chapter-5-1.png" alt="" /></p>

<p>在那个例子中，我们编写了一个分类器程序，通过运动员的身高和体重来判断她参与的运动项目——体操、田径、篮球等。</p>

<p>上图中的Marissa Coleman，身高6尺1寸，重160磅，我们的分类器可以正确的进行预测：</p>

<p>```python</p>

<blockquote><blockquote><blockquote><p>cl = Classifier(&lsquo;athletesTrainingSet.txt&rsquo;)
cl.classify([73, 160])
&lsquo;Basketball&rsquo;
```</p></blockquote></blockquote></blockquote>

<p>对于身高4尺9寸，90磅重的人：</p>

<p>```python</p>

<blockquote><blockquote><blockquote><p>cl.classify([59, 90])
&lsquo;Gymnastics&rsquo;
```</p></blockquote></blockquote></blockquote>

<p>当我们构建完一个分类器后，应该问以下问题：</p>

<ul>
<li>分类器的准确度如何？</li>
<li>结果理想吗？</li>
<li>如何与其它分类器做比较？</li>
</ul>


<p><img src="https://github.com/jizhang/guidetodatamining/raw/master/img/chapter-5/chapter-5-2.png" alt="" /></p>

<p><a href="https://github.com/jizhang/guidetodatamining/blob/master/chapter-5.md">前往GitHub阅读全文</a></p>
]]></content>
  </entry>
  
</feed>
