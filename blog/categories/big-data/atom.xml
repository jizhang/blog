<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: big data | Ji ZHANG's Blog]]></title>
  <link href="http://shzhangji.com/blog/categories/big-data/atom.xml" rel="self"/>
  <link href="http://shzhangji.com/"/>
  <updated>2015-09-20T17:35:23+08:00</updated>
  <id>http://shzhangji.com/</id>
  <author>
    <name><![CDATA[Ji ZHANG]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[View Spark Source in Eclipse]]></title>
    <link href="http://shzhangji.com/blog/2015/09/01/view-spark-source-in-eclipse/"/>
    <updated>2015-09-01T18:38:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/09/01/view-spark-source-in-eclipse</id>
    <content type="html"><![CDATA[<p>Reading source code is a great way to learn opensource projects. I used to read Java projects' source code on <a href="http://grepcode.com/">GrepCode</a> for it is online and has very nice cross reference features. As for Scala projects such as <a href="http://spark.apache.org">Apache Spark</a>, though its source code can be found on <a href="https://github.com/apache/spark/">GitHub</a>, it&rsquo;s quite necessary to setup an IDE to view the code more efficiently. Here&rsquo;s a howto of viewing Spark source code in Eclipse.</p>

<h2>Install Eclipse and Scala IDE Plugin</h2>

<p>One can download Eclipse from <a href="http://www.eclipse.org/downloads/">here</a>. I recommend the &ldquo;Eclipse IDE for Java EE Developers&rdquo;, which contains a lot of daily-used features.</p>

<p><img src="/images/scala-ide.png" alt="" /></p>

<p>Then go to Scala IDE&rsquo;s <a href="http://scala-ide.org/download/current.html">official site</a> and install the plugin through update site or zip archive.</p>

<h2>Generate Project File with Maven</h2>

<p>Spark is mainly built with Maven, so make sure you have Maven installed on your box, and download the latest Spark source code from <a href="http://spark.apache.org/downloads.html">here</a>, unarchive it, and execute the following command:</p>

<pre><code class="bash">$ mvn -am -pl core dependency:resolve eclipse:eclipse
</code></pre>

<!-- more -->


<p>This command does a bunch of things. First, it indicates what modules should be built. Spark is a large project with multiple modules. Currently we&rsquo;re only interested in its core module, so <code>-pl</code> or <code>--projects</code> is used. <code>-am</code> or <code>--also-make</code> tells Maven to build core module&rsquo;s dependencies as well. We can see the module list in output:</p>

<pre><code class="text">[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO]
[INFO] Spark Project Parent POM
[INFO] Spark Launcher Project
[INFO] Spark Project Networking
[INFO] Spark Project Shuffle Streaming Service
[INFO] Spark Project Unsafe
[INFO] Spark Project Core
</code></pre>

<p><code>dependency:resolve</code> tells Maven to download all dependencies. <code>eclipse:eclipse</code> will generate the <code>.project</code> and <code>.classpath</code> files for Eclipse. But the result is not perfect, both files need some fixes.</p>

<p>Edit <code>core/.classpath</code>, change the following two lines:</p>

<pre><code class="xml">&lt;classpathentry kind="src" path="src/main/scala" including="**/*.java"/&gt;
&lt;classpathentry kind="src" path="src/test/scala" output="target/scala-2.10/test-classes" including="**/*.java"/&gt;
</code></pre>

<p>to</p>

<pre><code class="xml">&lt;classpathentry kind="src" path="src/main/scala" including="**/*.java|**/*.scala"/&gt;
&lt;classpathentry kind="src" path="src/test/scala" output="target/scala-2.10/test-classes" including="**/*.java|**/*.scala"/&gt;
</code></pre>

<p>Edit <code>core/.project</code>, make it looks like this:</p>

<pre><code class="xml">&lt;buildSpec&gt;
  &lt;buildCommand&gt;
    &lt;name&gt;org.scala-ide.sdt.core.scalabuilder&lt;/name&gt;
  &lt;/buildCommand&gt;
&lt;/buildSpec&gt;
&lt;natures&gt;
  &lt;nature&gt;org.scala-ide.sdt.core.scalanature&lt;/nature&gt;
  &lt;nature&gt;org.eclipse.jdt.core.javanature&lt;/nature&gt;
&lt;/natures&gt;
</code></pre>

<p>Now you can import &ldquo;Existing Projects into Workspace&rdquo;, including <code>core</code>, <code>launcher</code>, <code>network</code>, and <code>unsafe</code>.</p>

<h2>Miscellaneous</h2>

<h3>Access restriction: The type &lsquo;Unsafe&rsquo; is not API</h3>

<p>For module <code>spark-unsafe</code>, Eclipse will report an error &ldquo;Access restriction: The type &lsquo;Unsafe&rsquo; is not API (restriction on required library /path/to/jre/lib/rt.jar&rdquo;. To fix this, right click the &ldquo;JRE System Library&rdquo; entry in Package Explorer, change it to &ldquo;Workspace default JRE&rdquo;.</p>

<h3>Download Sources and Javadocs</h3>

<p>Add the following entry into pom&rsquo;s project / build / plugins:</p>

<pre><code class="xml">&lt;plugin&gt;
    &lt;artifactId&gt;maven-eclipse-plugin&lt;/artifactId&gt;
    &lt;configuration&gt;
        &lt;downloadSources&gt;true&lt;/downloadSources&gt;
        &lt;downloadJavadocs&gt;true&lt;/downloadJavadocs&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;
</code></pre>

<h3>build-helper-maven-plugin</h3>

<p>Since Spark is a mixture of Java and Scala code, and the maven-eclipse-plugin only knows about Java source files, so we need to use build-helper-maven-plugin to include the Scala sources, as is described <a href="http://docs.scala-lang.org/tutorials/scala-with-maven.html#integration-with-eclipse-scala-ide24">here</a>. Fortunately, Spark&rsquo;s pom.xml has already included this setting.</p>

<h2>References</h2>

<ul>
<li><a href="http://docs.scala-lang.org/tutorials/scala-with-maven.html">http://docs.scala-lang.org/tutorials/scala-with-maven.html</a></li>
<li><a href="https://wiki.scala-lang.org/display/SIW/ScalaEclipseMaven">https://wiki.scala-lang.org/display/SIW/ScalaEclipseMaven</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools">https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Streaming Logging Configuration]]></title>
    <link href="http://shzhangji.com/blog/2015/05/31/spark-streaming-logging-configuration/"/>
    <updated>2015-05-31T18:18:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/05/31/spark-streaming-logging-configuration</id>
    <content type="html"><![CDATA[<p>Spark Streaming applications tend to run forever, so their log files should be properly handled, to avoid exploding server hard drives. This article will give some practical advices of dealing with these log files, on both Spark on YARN and standalone mode.</p>

<h2>Log4j&rsquo;s RollingFileAppender</h2>

<p>Spark uses log4j as logging facility. The default configuraiton is to write all logs into standard error, which is fine for batch jobs. But for streaming jobs, we&rsquo;d better use rolling-file appender, to cut log files by size and keep only several recent files. Here&rsquo;s an example:</p>

<pre><code class="properties">log4j.rootLogger=INFO, rolling

log4j.appender.rolling=org.apache.log4j.RollingFileAppender
log4j.appender.rolling.layout=org.apache.log4j.PatternLayout
log4j.appender.rolling.layout.conversionPattern=[%d] %p %m (%c)%n
log4j.appender.rolling.maxFileSize=50MB
log4j.appender.rolling.maxBackupIndex=5
log4j.appender.rolling.file=/var/log/spark/${dm.logging.name}.log
log4j.appender.rolling.encoding=UTF-8

log4j.logger.org.apache.spark=WARN
log4j.logger.org.eclipse.jetty=WARN

log4j.logger.com.anjuke.dm=${dm.logging.level}
</code></pre>

<p>This means log4j will roll the log file by 50MB and keep only 5 recent files. These files are saved in <code>/var/log/spark</code> directory, with filename picked from system property <code>dm.logging.name</code>. We also set the logging level of our package <code>com.anjuke.dm</code> according to <code>dm.logging.level</code> property. Another thing to mention is that we set <code>org.apache.spark</code> to level <code>WARN</code>, so as to ignore verbose logs from spark.</p>

<!-- more -->


<h2>Standalone Mode</h2>

<p>In standalone mode, Spark Streaming driver is running on the machine where you submit the job, and each Spark worker node will run an executor for this job. So you need to setup log4j for both driver and executor.</p>

<p>For driver, since it&rsquo;s a long-running application, we tend to use some process management tools like <a href="http://supervisord.org/">supervisor</a> to monitor it. And supervisor itself provides the facility of rolling log files, so we can safely write all logs into standard output when setting up driver&rsquo;s log4j.</p>

<p>For executor, there&rsquo;re two approaches. One is using <code>spark.executor.logs.rolling.strategy</code> provided by Spark 1.1 and above. It has both time-based and size-based rolling methods. These log files are stored in Spark&rsquo;s work directory. You can find more details in the <a href="https://spark.apache.org/docs/1.1.0/configuration.html">documentation</a>.</p>

<p>The other approach is to setup log4j manually, when you&rsquo;re using a legacy version, or want to gain more control on the logging process. Here are the steps:</p>

<ol>
<li>Make sure the logging directory exists on all worker nodes. You can use some provisioning tools like <a href="https://github.com/ansible/ansible">ansbile</a> to create them.</li>
<li>Create driver&rsquo;s and executor&rsquo;s log4j configuration files, and distribute the executor&rsquo;s to all worker nodes.</li>
<li>Use the above two files in <code>spark-submit</code> command:</li>
</ol>


<pre><code>spark-submit
  --master spark://127.0.0.1:7077
  --driver-java-options "-Dlog4j.configuration=file:/path/to/log4j-driver.properties -Ddm.logging.level=DEBUG"
  --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/path/to/log4j-executor.properties -Ddm.logging.name=myapp -Ddm.logging.level=DEBUG"
  ...
</code></pre>

<h2>Spark on YARN</h2>

<p><a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/index.html">YARN</a> is a <strong>resource manager</strong> introduced by Hadoop2. Now we can run differenct computational frameworks on the same cluster, like MapReduce, Spark, Storm, etc. The basic unit of YARN is called container, which represents a certain amount of resource (currently memory and virtual CPU cores). Every container has its working directory, and all related files such as application command (jars) and log files are stored in this directory.</p>

<p>When running Spark on YARN, there is a system property <code>spark.yarn.app.container.log.dir</code> indicating the container&rsquo;s log directory. We only need to replace one line of the above log4j config:</p>

<pre><code class="properties">log4j.appender.rolling.file=${spark.yarn.app.container.log.dir}/spark.log
</code></pre>

<p>And these log files can be viewed on YARN&rsquo;s web UI:</p>

<p><img src="/images/spark/yarn-logs.png" alt="" /></p>

<p>The <code>spark-submit</code> command is as following:</p>

<pre><code>spark-submit
  --master yarn-cluster
  --files /path/to/log4j-spark.properties
  --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j-spark.properties"
  --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=log4j-spark.properties"
  ...
</code></pre>

<p>As you can see, both driver and executor use the same configuration file. That is because in <code>yarn-cluster</code> mode, driver is also run as a container in YARN. In fact, the <code>spark-submit</code> command will just quit after job submission.</p>

<p>If YARN&rsquo;s <a href="http://zh.hortonworks.com/blog/simplifying-user-logs-management-and-access-in-yarn/">log aggregation</a> is enabled, application logs will be saved in HDFS after the job is done. One can use <code>yarn logs</code> command to view the files or browse directly into HDFS directory indicated by <code>yarn.nodemanager.log-dirs</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache HBase的适用场景]]></title>
    <link href="http://shzhangji.com/blog/2015/03/08/hbase-dos-and-donts/"/>
    <updated>2015-03-08T08:03:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/03/08/hbase-dos-and-donts</id>
    <content type="html"><![CDATA[<p>原文：<a href="http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/">http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/</a></p>

<p>最近我在<a href="http://www.meetup.com/LA-HUG/">洛杉矶Hadoop用户组</a>做了一次关于<a href="http://www.meetup.com/LA-HUG/pages/Video_from_April_13th_HBASE_DO%27S_and_DON%27TS/">HBase适用场景</a>的分享。在场的听众水平都很高，给到了我很多值得深思的反馈。主办方是来自Shopzilla的Jody，我非常感谢他能给我一个在60多位Hadoop使用者面前演讲的机会。可能一些朋友没有机会来洛杉矶参加这次会议，我将分享中的主要内容做了一个整理。如果你没有时间阅读全文，以下是一些摘要：</p>

<ul>
<li>HBase很棒，但不是关系型数据库或HDFS的替代者；</li>
<li>配置得当才能运行良好；</li>
<li>监控，监控，监控，重要的事情要说三遍。</li>
</ul>


<p>Cloudera是HBase的铁杆粉丝。我们热爱这项技术，热爱这个社区，发现它能适用于非常多的应用场景。HBase如今已经有很多<a href="#use-cases">成功案例</a>，所以很多公司也在考虑如何将其应用到自己的架构中。我做这次分享以及写这篇文章的动因就是希望能列举出HBase的适用场景，并提醒各位哪些场景是不适用的，以及如何做好HBase的部署。</p>

<!-- more -->


<h2>何时使用HBase</h2>

<p>虽然HBase是一种绝佳的工具，但我们一定要记住，它并非银弹。HBase并不擅长传统的事务处理程序或关联分析，它也不能完全替代MapReduce过程中使用到的HDFS。从文末的<a href="#use-cases">成功案例</a>中你可以大致了解HBase适用于怎样的应用场景。如果你还有疑问，可以到<a href="http://www.cloudera.com/community/">社区</a>中提问，我说过这是一个非常棒的社区。</p>

<p>除去上述限制之外，你为何要选择HBase呢？如果你的应用程序中，数据表每一行的结构是有差别的，那就可以考虑使用HBase，比如在标准化建模的过程中使用它；如果你需要经常追加字段，且大部分字段是NULL值的，那可以考虑HBase；如果你的数据（包括元数据、消息、二进制数据等）都有着同一个主键，那就可以使用HBase；如果你需要通过键来访问和修改数据，使用HBase吧。</p>

<h2>后台服务</h2>

<p>如果你已决定尝试一下HBase，那以下是一些部署过程中的提示。HBase会用到一些后台服务，这些服务非常关键。如果你之前没有了解过ZooKeeper，那现在是个好时候。HBase使用ZooKeeper作为它的分布式协调服务，用于选举Master等。随着HBase的发展，ZooKeeper发挥的作用越来越重要。另外，你需要搭建合适的网络基础设施，如NTP和DNS。HBase要求集群内的所有服务器时间一致，并且能正确地访问其它服务器。正确配置NTP和DNS可以杜绝一些奇怪的问题，如服务器A认为当前是明天，B认为当前是昨天；再如Master要求服务器C开启新的Region，而C不知道自己的机器名，从而无法响应。NTP和DNS服务器可以让你减少很多麻烦。</p>

<p>我前面提到过，在考虑是否使用HBase时，需要针对你自己的应用场景来进行判别。而在真正使用HBase时，监控则成了第一要务。和大多数分布式服务一样，HBase服务器宕机会有多米诺骨牌效应。如果一台服务器因内存不足开始swap数据，它会失去和Master的联系，这时Master会命令其他服务器接过这部分请求，可能会导致第二台服务器也发生宕机。所以，你需要密切监控服务器的CPU、I/O以及网络延迟，确保每台HBase服务器都在良好地工作。监控对于维护HBase集群的健康至关重要。</p>

<h2>HBase架构最佳实践</h2>

<p>当你找到了适用场景，并搭建起一个健康的HBase集群后，我们来看一些使用过程中的最佳实践。键的前缀要有良好的分布性。如果你使用时间戳或其他类似的递增量作为前缀，那就会让单个Region承载所有请求，而不是分布到各个Region上。此外，你需要根据Memstore和内存的大小来控制Region的数量。RegionServer的JVM堆内存应该控制在12G以内，从而避免过长的GC停顿。举个例子，在一台内存为36G的服务器上部署RegionServer，同时还运行着DataNode，那大约可以提供100个48M大小的Region。这样的配置对HDFS、HBase、以及Linux本身的文件缓存都是有利的。</p>

<p>其他一些设置包括禁用自动合并机制（默认的合并操作会在HBase启动后每隔24小时进行），改为手动的方式在低峰期间执行。你还应该配置数据文件压缩（如LZO），并将正确的配置文件加入HBase的CLASSPATH中。</p>

<h2>非适用场景</h2>

<p>上文讲述了HBase的适用场景和最佳实践，以下则是一些需要规避的问题。比如，不要期许HBase可以完全替代关系型数据库——虽然它在许多方面都表现优秀。它不支持SQL，也没有优化器，更不能支持跨越多条记录的事务或关联查询。如果你用不到这些特性，那HBase将是你的不二选择。</p>

<p>在复用HBase的服务器时有一些注意事项。如果你需要保证HBase的服务器质量，同时又想在HBase上运行批处理脚本（如使用Pig从HBase中获取数据进行处理），建议还是另搭一套集群。HBase在处理大量顺序I/O操作时（如MapReduce），其CPU和内存资源将会十分紧张。将这两类应用放置在同一集群上会造成不可预估的服务延迟。此外，共享集群时还需要调低任务槽（task slot）的数量，至少要留一半的CPU核数给HBase。密切关注内存，因为一旦发生swap，HBase很可能会停止心跳，从而被集群判为无效，最终产生一系列宕机。</p>

<h2>总结</h2>

<p>最后要提的一点是，在加载数据到HBase时，应该使用MapReduce+HFileOutputFormat来实现。如果仅使用客户端API，不仅速度慢，也没有充分利用HBase的分布式特性。</p>

<p>用一句话概述，HBase可以让你用键来存储和搜索数据，且无需定义表结构。</p>

<h2><a id="use-cases"></a>使用案例</h2>

<ul>
<li>Apache HBase: <a href="http://wiki.apache.org/hadoop/Hbase/PoweredBy">Powered By HBase Wiki</a></li>
<li>Mozilla: <a href="http://blog.mozilla.com/webdev/2010/07/26/moving-socorro-to-hbase/">Moving Socorro to HBase</a></li>
<li>Facebook: <a href="http://highscalability.com/blog/2010/11/16/facebooks-new-real-time-messaging-system-hbase-to-store-135.html">Facebook’s New Real-Time Messaging System: HBase</a></li>
<li>StumbleUpon: <a href="http://www.stumbleupon.com/devblog/hbase_at_stumbleupon/">HBase at StumbleUpon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深入理解Reduce-side Join]]></title>
    <link href="http://shzhangji.com/blog/2015/01/13/understand-reduce-side-join/"/>
    <updated>2015-01-13T14:20:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/01/13/understand-reduce-side-join</id>
    <content type="html"><![CDATA[<p>在《<a href="http://www.amazon.com/MapReduce-Design-Patterns-Effective-Algorithms/dp/1449327176">MapReduce Design Patterns</a>》一书中，作者给出了Reduce-side Join的实现方法，大致步骤如下：</p>

<p><img src="/images/reduce-side-join/reduce-side-join.png" alt="" /></p>

<ol>
<li>使用<a href="https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/MultipleInputs.html">MultipleInputs</a>指定不同的来源表和相应的Mapper类；</li>
<li>Mapper输出的Key为Join的字段内容，Value为打了来源表标签的记录；</li>
<li>Reducer在接收到同一个Key的记录后，执行以下两步：

<ol>
<li>遍历Values，根据标签将来源表的记录分别放到两个List中；</li>
<li>遍历两个List，输出Join结果。</li>
</ol>
</li>
</ol>


<p>具体实现可以参考<a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/InnerJoinJob.java">这段代码</a>。但是这种实现方法有一个问题：如果同一个Key的记录数过多，存放在List中就会占用很多内存，严重的会造成内存溢出（Out of Memory, OOM）。这种方法在一对一的情况下没有问题，而一对多、多对多的情况就会有隐患。那么，Hive在做Reduce-side Join时是如何避免OOM的呢？两个关键点：</p>

<ol>
<li>Reducer在遍历Values时，会将前面的表缓存在内存中，对于最后一张表则边扫描边输出；</li>
<li>如果前面几张表内存中放不下，就写入磁盘。</li>
</ol>


<!-- more -->


<p>按照我们的实现，Mapper输出的Key是<code>product_id</code>，Values是打了标签的产品表（Product）和订单表（Order）的记录。从数据量来看，应该缓存产品表，扫描订单表。这就要求两表记录到达Reducer时是有序的，产品表在前，边扫描边放入内存；订单表在后，边扫描边结合产品表的记录进行输出。要让Hadoop在Shuffle&amp;Sort阶段先按<code>product_id</code>排序、再按表的标签排序，就需要用到二次排序。</p>

<p>二次排序的概念很简单，将Mapper输出的Key由单一的<code>product_id</code>修改为<code>product_id+tag</code>的复合Key就可以了，但需通过以下几步实现：</p>

<h3>自定义Key类型</h3>

<p>原来<code>product_id</code>是Text类型，我们的复合Key则要包含<code>product_id</code>和<code>tag</code>两个数据，并实现<code>WritableComparable</code>接口：</p>

<pre><code class="java">public class TaggedKey implements WritableComparable&lt;TaggedKey&gt; {

    private Text joinKey = new Text();
    private IntWritable tag = new IntWritable();

    @Override
    public int compareTo(TaggedKey taggedKey) {
        int compareValue = joinKey.compareTo(taggedKey.getJoinKey());
        if (compareValue == 0) {
            compareValue = tag.compareTo(taggedKey.getTag());
        }
        return compareValue;
    }

    // 此处省略部分代码
}
</code></pre>

<p>可以看到，在比较两个TaggedKey时，会先比较joinKey（即<code>product_id</code>），再比较<code>tag</code>。</p>

<h3>自定义分区方法</h3>

<p>默认情况下，Hadoop会对Key进行哈希，以保证相同的Key会分配到同一个Reducer中。由于我们改变了Key的结构，因此需要重新编 写分区函数：</p>

<pre><code class="java">public class TaggedJoiningPartitioner extends Partitioner&lt;TaggedKey, Text&gt; {

    @Override
    public int getPartition(TaggedKey taggedKey, Text text, int numPartitions) {
        return taggedKey.getJoinKey().hashCode() % numPartitions;
    }

}
</code></pre>

<h3>自定义分组方法</h3>

<p>同理，调用reduce函数需要传入同一个Key的所有记录，这就需要重新定义分组函数：</p>

<pre><code class="java">public class TaggedJoiningGroupingComparator extends WritableComparator {

    public TaggedJoiningGroupingComparator() {
        super(TaggedKey.class, true);
    }

    @SuppressWarnings("rawtypes")
    @Override
    public int compare(WritableComparable a, WritableComparable b) {
        TaggedKey taggedKey1 = (TaggedKey) a;
        TaggedKey taggedKey2 = (TaggedKey) b;
        return taggedKey1.getJoinKey().compareTo(taggedKey2.getJoinKey());
    }

}
</code></pre>

<h3>配置Job</h3>

<pre><code class="java">job.setMapOutputKeyClass(TaggedKey.class);
job.setMapOutputValueClass(Text.class);

job.setPartitionerClass(TaggedJoiningPartitioner.class);
job.setGroupingComparatorClass(TaggedJoiningGroupingComparator.class);
</code></pre>

<h3>MapReduce过程</h3>

<p>最后，我们在Mapper阶段使用TaggedKey，在Reducer阶段按照tag进行不同的操作就可以了：</p>

<pre><code class="java">@Override
protected void reduce(TaggedKey key, Iterable&lt;Text&gt; values, Context context)
        throws IOException, InterruptedException {

    List&lt;String&gt; products = new ArrayList&lt;String&gt;();

    for (Text value : values) {
        switch (key.getTag().get()) {
        case 1: // Product
            products.add(value.toString());
            break;

        case 2: // Order
            String[] order = value.toString().split(",");
            for (String productString : products) {
                String[] product = productString.split(",");
                List&lt;String&gt; output = new ArrayList&lt;String&gt;();
                output.add(order[0]);
                // ...
                context.write(NullWritable.get(), new Text(StringUtils.join(output, ",")));
            }
            break;

        default:
            assert false;
        }
    }
}
</code></pre>

<p>遍历values时，开始都是tag=1的记录，之后都是tag=2的记录。以上代码可以<a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/ReduceSideJoinJob.java">在这里</a>查看。</p>

<p>对于第二个问题，超过缓存大小的记录（默认25000条）就会存入临时文件，由Hive的RowContainer类实现，具体可以看<a href="http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hive/hive-exec/0.10.0-cdh4.5.0/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java#RowContainer.add%28java.util.List%29">这个链接</a>。</p>

<p>需要注意的是，Hive默认是按SQL中表的书写顺序来决定排序的，因此应该将大表放在最后。如果要人工改变顺序，可以使用STREAMTABLE配置：</p>

<pre><code class="sql">SELECT /*+ STREAMTABLE(a) */ a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)
</code></pre>

<p>但不要将这点和Map-side Join混淆，在配置了<code>hive.auto.convert.join=true</code>后，是不需要注意表的顺序的，Hive会自动将小表缓存在Mapper的内存中。</p>

<h2>参考资料</h2>

<ol>
<li><a href="http://codingjunkie.net/mapreduce-reduce-joins/">http://codingjunkie.net/mapreduce-reduce-joins/</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark快速入门]]></title>
    <link href="http://shzhangji.com/blog/2014/12/16/spark-quick-start/"/>
    <updated>2014-12-16T15:59:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/12/16/spark-quick-start</id>
    <content type="html"><![CDATA[<p><img src="http://spark.apache.org/images/spark-logo.png" alt="" /></p>

<p><a href="http://spark.apache.org">Apache Spark</a>是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：</p>

<ul>
<li><strong>通用计算引擎</strong> 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；</li>
<li><strong>基于内存</strong> 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；</li>
<li><strong>与Hadoop集成</strong> 能够直接读写HDFS中的数据，并能运行在YARN之上。</li>
</ul>


<p>Spark是用<a href="http://www.scala-lang.org/">Scala语言</a>编写的，所提供的API也很好地利用了这门语言的特性。它也可以使用Java和Python编写应用。本文将用Scala进行讲解。</p>

<h2>安装Spark和SBT</h2>

<ul>
<li>从<a href="http://spark.apache.org/downloads.html">官网</a>上下载编译好的压缩包，解压到一个文件夹中。下载时需注意对应的Hadoop版本，如要读写CDH4 HDFS中的数据，则应下载Pre-built for CDH4这个版本。</li>
<li>为了方便起见，可以将spark/bin添加到$PATH环境变量中：</li>
</ul>


<pre><code class="bash">export SPARK_HOME=/path/to/spark
export PATH=$PATH:$SPARK_HOME/bin
</code></pre>

<ul>
<li>在练习例子时，我们还会用到<a href="http://www.scala-sbt.org/">SBT</a>这个工具，它是用来编译打包Scala项目的。Linux下的安装过程比较简单：

<ul>
<li>下载<a href="https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.7/sbt-launch.jar">sbt-launch.jar</a>到$HOME/bin目录；</li>
<li>新建$HOME/bin/sbt文件，权限设置为755，内容如下：</li>
</ul>
</li>
</ul>


<pre><code class="bash">SBT_OPTS="-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M"
java $SBT_OPTS -jar `dirname $0`/sbt-launch.jar "$@"
</code></pre>

<!-- more -->


<h2>日志分析示例</h2>

<p>假设我们有如下格式的日志文件，保存在/tmp/logs.txt文件中：</p>

<pre><code class="text">2014-12-11 18:33:52 INFO    Java    some message
2014-12-11 18:34:33 INFO    MySQL   some message
2014-12-11 18:34:54 WARN    Java    some message
2014-12-11 18:35:25 WARN    Nginx   some message
2014-12-11 18:36:09 INFO    Java    some message
</code></pre>

<p>每条记录有四个字段，即时间、级别、应用、信息，使用制表符分隔。</p>

<p>Spark提供了一个交互式的命令行工具，可以直接执行Spark查询：</p>

<pre><code>$ spark-shell
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0
      /_/
Spark context available as sc.
scala&gt;
</code></pre>

<h3>加载并预览数据</h3>

<pre><code class="scala">scala&gt; val lines = sc.textFile("/tmp/logs.txt")
lines: org.apache.spark.rdd.RDD[String] = /tmp/logs.txt MappedRDD[1] at textFile at &lt;console&gt;:12

scala&gt; lines.first()
res0: String = 2014-12-11 18:33:52  INFO    Java    some message
</code></pre>

<ul>
<li>sc是一个SparkContext类型的变量，可以认为是Spark的入口，这个对象在spark-shell中已经自动创建了。</li>
<li>sc.textFile()用于生成一个RDD，并声明该RDD指向的是/tmp/logs.txt文件。RDD可以暂时认为是一个列表，列表中的元素是一行行日志（因此是String类型）。这里的路径也可以是HDFS上的文件，如hdfs://127.0.0.1:8020/user/hadoop/logs.txt。</li>
<li>lines.first()表示调用RDD提供的一个方法：first()，返回第一行数据。</li>
</ul>


<h3>解析日志</h3>

<p>为了能对日志进行筛选，如只处理级别为ERROR的日志，我们需要将每行日志按制表符进行分割：</p>

<pre><code class="scala">scala&gt; val logs = lines.map(line =&gt; line.split("\t"))
logs: org.apache.spark.rdd.RDD[Array[String]] = MappedRDD[2] at map at &lt;console&gt;:14

scala&gt; logs.first()
res1: Array[String] = Array(2014-12-11 18:33:52, INFO, Java, some message)
</code></pre>

<ul>
<li>lines.map(f)表示对RDD中的每一个元素使用f函数来处理，并返回一个新的RDD。</li>
<li>line => line.split(&ldquo;\t&rdquo;)是一个匿名函数，又称为Lambda表达式、闭包等。它的作用和普通的函数是一样的，如这个匿名函数的参数是line（String类型），返回值是Array数组类型，因为String.split()函数返回的是数组。</li>
<li>同样使用first()方法来看这个RDD的首条记录，可以发现日志已经被拆分成四个元素了。</li>
</ul>


<h3>过滤并计数</h3>

<p>我们想要统计错误日志的数量：</p>

<pre><code class="scala">scala&gt; val errors = logs.filter(log =&gt; log(1) == "ERROR")
errors: org.apache.spark.rdd.RDD[Array[String]] = FilteredRDD[3] at filter at &lt;console&gt;:16

scala&gt; errors.first()
res2: Array[String] = Array(2014-12-11 18:39:42, ERROR, Java, some message)

scala&gt; errors.count()
res3: Long = 158
</code></pre>

<ul>
<li>logs.filter(f)表示筛选出满足函数f的记录，其中函数f需要返回一个布尔值。</li>
<li>log(1) == &ldquo;ERROR"表示获取每行日志的第二个元素（即日志级别），并判断是否等于ERROR。</li>
<li>errors.count()用于返回该RDD中的记录。</li>
</ul>


<h3>缓存</h3>

<p>由于我们还会对错误日志做一些处理，为了加快速度，可以将错误日志缓存到内存中，从而省去解析和过滤的过程：</p>

<pre><code class="scala">scala&gt; errors.cache()
</code></pre>

<p>errors.cache()函数会告知Spark计算完成后将结果保存在内存中。所以说Spark是否缓存结果是需要用户手动触发的。在实际应用中，我们需要迭代处理的往往只是一部分数据，因此很适合放到内存里。</p>

<p>需要注意的是，cache函数并不会立刻执行缓存操作，事实上map、filter等函数都不会立刻执行，而是在用户执行了一些特定操作后才会触发，比如first、count、reduce等。这两类操作分别称为Transformations和Actions。</p>

<h3>显示前10条记录</h3>

<pre><code class="scala">scala&gt; val firstTenErrors = errors.take(10)
firstTenErrors: Array[Array[String]] = Array(Array(2014-12-11 18:39:42, ERROR, Java, some message), Array(2014-12-11 18:40:23, ERROR, Nginx, some message), ...)

scala&gt; firstTenErrors.map(log =&gt; log.mkString("\t")).foreach(line =&gt; println(line))
2014-12-11 18:39:42 ERROR   Java    some message
2014-12-11 18:40:23 ERROR   Nginx   some message
...
</code></pre>

<p>errors.take(n)方法可用于返回RDD前N条记录，它的返回值是一个数组。之后对firstTenErrors的处理使用的是Scala集合类库中的方法，如map、foreach，和RDD提供的接口基本一致。所以说用Scala编写Spark程序是最自然的。</p>

<h3>按应用进行统计</h3>

<p>我们想要知道错误日志中有几条Java、几条Nginx，这和常见的Wordcount思路是一样的。</p>

<pre><code class="scala">scala&gt; val apps = errors.map(log =&gt; (log(2), 1))
apps: org.apache.spark.rdd.RDD[(String, Int)] = MappedRDD[15] at map at &lt;console&gt;:18

scala&gt; apps.first()
res20: (String, Int) = (Java,1)

scala&gt; val counts = apps.reduceByKey((a, b) =&gt; a + b)
counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[17] at reduceByKey at &lt;console&gt;:20

scala&gt; counts.foreach(t =&gt; println(t))
(Java,58)
(Nginx,53)
(MySQL,47)
</code></pre>

<p>errors.map(log => (log(2), 1))用于将每条日志转换为键值对，键是应用（Java、Nginx等），值是1，如<code>("Java", 1)</code>，这种数据结构在Scala中称为元组（Tuple），这里它有两个元素，因此称为二元组。</p>

<p>对于数据类型是二元组的RDD，Spark提供了额外的方法，reduceByKey(f)就是其中之一。它的作用是按键进行分组，然后对同一个键下的所有值使用f函数进行归约（reduce）。归约的过程是：使用列表中第一、第二个元素进行计算，然后用结果和第三元素进行计算，直至列表耗尽。如：</p>

<pre><code class="scala">scala&gt; Array(1, 2, 3, 4).reduce((a, b) =&gt; a + b)
res23: Int = 10
</code></pre>

<p>上述代码的计算过程即<code>((1 + 2) + 3) + 4</code>。</p>

<p>counts.foreach(f)表示遍历RDD中的每条记录，并应用f函数。这里的f函数是一条打印语句（println）。</p>

<h2>打包应用程序</h2>

<p>为了让我们的日志分析程序能够在集群上运行，我们需要创建一个Scala项目。项目的大致结构是：</p>

<pre><code>spark-sandbox
├── build.sbt
├── project
│   ├── build.properties
│   └── plugins.sbt
└── src
    └── main
        └── scala
            └── LogMining.scala
</code></pre>

<p>你可以直接使用<a href="https://github.com/jizhang/spark-sandbox">这个项目</a>作为模板。下面说明一些关键部分：</p>

<h3>配置依赖</h3>

<p><code>build.sbt</code></p>

<pre><code class="scala">libraryDependencies += "org.apache.spark" %% "spark-core" % "1.1.1"
</code></pre>

<h3>程序内容</h3>

<p><code>src/main/scala/LogMining.scala</code></p>

<pre><code class="scala">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object LogMining extends App {
  val conf = new SparkConf().setAppName("LogMining")
  val sc = new SparkContext(conf)
  val inputFile = args(0)
  val lines = sc.textFile(inputFile)
  // 解析日志
  val logs = lines.map(_.split("\t"))
  val errors = logs.filter(_(1) == "ERROR")
  // 缓存错误日志
  errors.cache()
  // 统计错误日志记录数
  println(errors.count())
  // 获取前10条MySQL的错误日志
  val mysqlErrors = errors.filter(_(2) == "MySQL")
  mysqlErrors.take(10).map(_ mkString "\t").foreach(println)
  // 统计每个应用的错误日志数
  val errorApps = errors.map(_(2) -&gt; 1)
  errorApps.countByKey().foreach(println)
}
</code></pre>

<h3>打包运行</h3>

<pre><code class="bash">$ cd spark-sandbox
$ sbt package
$ spark-submit --class LogMining --master local target/scala-2.10/spark-sandbox_2.10-0.1.0.jar data/logs.txt
</code></pre>

<h2>参考资料</h2>

<ul>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html">Spark Programming Guide</a></li>
<li><a href="http://www.slideshare.net/cloudera/spark-devwebinarslides-final">Introduction to Spark Developer Training</a></li>
<li><a href="http://www.slideshare.net/liancheng/dtcc-14-spark-runtime-internals">Spark Runtime Internals</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
