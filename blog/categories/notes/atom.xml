<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Notes | Ji ZHANG's Blog]]></title>
  <link href="http://shzhangji.com/blog/categories/notes/atom.xml" rel="self"/>
  <link href="http://shzhangji.com/"/>
  <updated>2014-05-27T19:02:54+08:00</updated>
  <id>http://shzhangji.com/</id>
  <author>
    <name><![CDATA[Ji ZHANG]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Use WebJars in Scalatra Project]]></title>
    <link href="http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project/"/>
    <updated>2014-05-27T17:44:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project</id>
    <content type="html"><![CDATA[<p>As I&rsquo;m working with my first <a href="http://www.scalatra.org/">Scalatra</a> project, I automatically think of using <a href="http://www.webjars.org/">WebJars</a> to manage Javascript library dependencies, since it&rsquo;s more convenient and seems like a good practice. Though there&rsquo;s no <a href="http://www.webjars.org/documentation">official support</a> for Scalatra framework, the installation process is not very complex. But this doesn&rsquo;t mean I didn&rsquo;t spend much time on this. I&rsquo;m still a newbie to Scala, and there&rsquo;s only a few materials on this subject.</p>

<h2>Add WebJars Dependency in SBT Build File</h2>

<p>Scalatra uses <code>.scala</code> configuration file instead of <code>.sbt</code>, so let&rsquo;s add dependency into <code>project/build.scala</code>. Take <a href="http://dojotoolkit.org/">Dojo</a> for example.</p>

<p>```scala
object DwExplorerBuild extends Build {
  &hellip;
  lazy val project = Project (</p>

<pre><code>...
settings = Defaults.defaultSettings ++ ScalatraPlugin.scalatraWithJRebel ++ scalateSettings ++ Seq(
  ...
  libraryDependencies ++= Seq(
    ...
    "org.webjars" % "dojo" % "1.9.3"
  ),
  ...
)
</code></pre>

<p>  )
}
```</p>

<p>To view this dependency in Eclipse, you need to run <code>sbt eclipse</code> again. In the <em>Referenced Libraries</em> section, you can see a <code>dojo-1.9.3.jar</code>, and the library lies in <code>META-INF/resources/webjars/</code>.</p>

<!-- more -->


<h2>Add a Route for WebJars Resources</h2>

<p>Find the <code>ProjectNameStack.scala</code> file and add the following lines at the bottom of the trait:</p>

<p>```scala
trait ProjectNameStack extends ScalatraServlet with ScalateSupport {
  &hellip;
  get(&ldquo;/webjars/*&rdquo;) {</p>

<pre><code>val resourcePath = "/META-INF/resources/webjars/" + params("splat")
Option(getClass.getResourceAsStream(resourcePath)) match {
  case Some(inputStream) =&gt; {
    contentType = servletContext.getMimeType(resourcePath)
    IOUtil.loadBytes(inputStream)
  }
  case None =&gt; resourceNotFound()
}
</code></pre>

<p>  }
}
```</p>

<p><strong>That&rsquo;s it!</strong> Now you can refer to the WebJars resources in views, like this:</p>

<p>```ssp</p>

<h1>set (title)</h1>

<p>Hello, Dojo!</p>

<h1>end</h1>

<div id="greeting"></div>




<script type="text/javascript" src="${uri("/webjars/dojo/1.9.3/dojo/dojo.js")}" data-dojo-config="async: true"></script>


<script type="text/javascript">
require([
    'dojo/dom',
    'dojo/dom-construct'
], function (dom, domConstruct) {
    var greetingNode = dom.byId('greeting');
    domConstruct.place('<i>Dojo!</i>', greetingNode);
});
</script>


<p>```</p>

<h3>Some Explanations on This Route</h3>

<ul>
<li><code>/webjars/*</code> is a <a href="http://www.scalatra.org/2.2/guides/http/routes.html#toc_233">Wildcards</a> and <code>params("splat")</code> is to extract the asterisk part.</li>
<li><code>resourcePath</code> points to the WebJars resources in the jar file, as we saw in Eclipse. It is then fetched as an <code>InputStream</code> with <code>getResourceAsStream()</code>.</li>
<li><code>servletContext.getMimeType()</code> is a handy method to determine the content type of the requested resource, instead of parsing it by ourselves. I find this in SpringMVC&rsquo;s <a href="http://grepcode.com/file/repo1.maven.org/maven2/org.springframework/spring-webmvc/3.2.7.RELEASE/org/springframework/web/servlet/resource/ResourceHttpRequestHandler.java#ResourceHttpRequestHandler.handleRequest%28javax.servlet.http.HttpServletRequest%2Cjavax.servlet.http.HttpServletResponse%29">ResourceHttpRequestHandler</a>.</li>
<li><code>IOUtil</code> is a utiliy class that comes with <a href="http://scalate.fusesource.org/">Scalate</a>, so don&rsquo;t forget to import it first.</li>
</ul>


<p>At first I tried to figure out whether Scalatra provides a conveniet way to serve static files in classpath, I failed. So I decided to serve them by my own, and <a href="https://gist.github.com/laurilehmijoki/4483113">this gist</a> was very helpful.</p>

<p>Anyway, I&rsquo;ve spent more than half a day to solve this problem, and it turned out to be a very challenging yet interesting way to learn a new language, new framework, and new tools. Keep moving!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Generate Auto-increment Id in Map-reduce Job]]></title>
    <link href="http://shzhangji.com/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job/"/>
    <updated>2013-10-31T09:35:00+08:00</updated>
    <id>http://shzhangji.com/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job</id>
    <content type="html"><![CDATA[<p>In DBMS world, it&rsquo;s easy to generate a unique, auto-increment id, using MySQL&rsquo;s <a href="http://dev.mysql.com/doc/refman/5.1/en/example-auto-increment.html">AUTO_INCREMENT attribute</a> on a primary key or MongoDB&rsquo;s <a href="http://docs.mongodb.org/manual/tutorial/create-an-auto-incrementing-field/">Counters Collection</a> pattern. But when it comes to a distributed, parallel processing framework, like Hadoop Map-reduce, it is not that straight forward. The best solution to identify every record in such framework is to use UUID. But when an integer id is required, it&rsquo;ll take some steps.</p>

<h2>Solution A: Single Reducer</h2>

<p>This is the most obvious and simple one, just use the following code to specify reducer numbers to 1:</p>

<p><code>java
job.setNumReduceTasks(1);
</code></p>

<p>And also obvious, there are several demerits:</p>

<ol>
<li>All mappers output will be copied to one task tracker.</li>
<li>Only one process is working on shuffel &amp; sort.</li>
<li>When producing output, there&rsquo;s also only one process.</li>
</ol>


<p>The above is not a problem for small data sets, or at least small mapper outputs. And it is also the approach that Pig and Hive use when they need to perform a total sort. But when hitting a certain threshold, the sort and copy phase will become very slow and unacceptable.</p>

<!-- more -->


<h2>Solution B: Increment by Number of Tasks</h2>

<p>Inspired by a <a href="http://mail-archives.apache.org/mod_mbox/hadoop-common-user/200904.mbox/%3C49E13557.7090504@domaintools.com%3E">mailing list</a> that is quite hard to find, which is inspired by MySQL master-master setup (with auto_increment_increment and auto_increment_offset), there&rsquo;s a brilliant way to generate a globally unique integer id across mappers or reducers. Let&rsquo;s take mapper for example:</p>

<p>```java
public static class JobMapper extends Mapper&lt;LongWritable, Text, LongWritable, Text> {</p>

<pre><code>private long id;
private int increment;

@Override
protected void setup(Context context) throws IOException,
        InterruptedException {

    super.setup(context);

    id = context.getTaskAttemptID().getTaskID().getId();
    increment = context.getConfiguration().getInt("mapred.map.tasks", 0);
    if (increment == 0) {
        throw new IllegalArgumentException("mapred.map.tasks is zero");
    }
}

@Override
protected void map(LongWritable key, Text value, Context context)
        throws IOException, InterruptedException {

    id += increment;
    context.write(new LongWritable(id),
            new Text(String.format("%d, %s", key.get(), value.toString())));
}
</code></pre>

<p>}
```</p>

<p>The basic idea is simple:</p>

<ol>
<li>Set the initial id to current tasks&rsquo;s id.</li>
<li>When mapping each row, increment the id by the number of tasks.</li>
</ol>


<p>It&rsquo;s also applicable to reducers.</p>

<h2>Solution C: Sorted Auto-increment Id</h2>

<p>Here&rsquo;s a real senario: we have several log files pulled from different machines, and we want to identify each row by an auto-increment id, and they should be in time sequence order.</p>

<p>We know Hadoop has a sort phase, so we can use timestamp as the mapper output key, and the framework will do the trick. But the sorting thing happends in one reducer (partition, in fact), so when using multiple reducer tasks, the result is not in total order. To achieve this, we can use the <a href="http://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.html">TotalOrderPartitioner</a>.</p>

<p>How about the incremental id? Even though the outputs are in total order, Solution B is not applicable here. So we take another approach: seperate the job in two phases, use the reducer to do sorting <em>and</em> counting, then use the second mapper to generate the id.</p>

<p>Here&rsquo;s what we gonna do:</p>

<ol>
<li>Use TotalOrderPartitioner, and generate the partition file.</li>
<li>Parse logs in mapper A, use time as the output key.</li>
<li>Let the framework do partitioning and sorting.</li>
<li>Count records in reducer, write it with <a href="http://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.html">MultipleOutput</a>.</li>
<li>In mapper B, use count as offset, and increment by 1.</li>
</ol>


<p>To simplify the situation, we assume to have the following inputs and outputs:</p>

<p>```text
 Input       Output</p>

<p>11:00 a     1 11:00 a
12:00 b     2 11:01 aa
13:00 c     3 11:02 aaa</p>

<p>11:01 aa    4 12:00 b
12:01 bb    5 12:01 bb
13:01 cc    6 12:02 bbb</p>

<p>11:02 aaa   7 13:00 c
12:02 bbb   8 13:01 cc
13:02 ccc   9 13:02 ccc
```</p>

<h3>Generate Partition File</h3>

<p>To use TotalOrderpartitioner, we need a partition file (i.e. boundaries) to tell the partitioner how to partition the mapper outputs. Usually we&rsquo;ll use <a href="https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapreduce/lib/partition/InputSampler.RandomSampler.html">InputSampler.RandomSampler</a> class, but this time let&rsquo;s use a manual partition file.</p>

<p>```java
SequenceFile.Writer writer = new SequenceFile.Writer(fs, getConf(), partition,</p>

<pre><code>    Text.class, NullWritable.class);
</code></pre>

<p>Text key = new Text();
NullWritable value = NullWritable.get();
key.set(&ldquo;12:00&rdquo;);
writer.append(key, value);
key.set(&ldquo;13:00&rdquo;);
writer.append(key, value);
writer.close();
```</p>

<p>So basically, the partitioner will partition the mapper outputs into three parts, the first part will be less than &ldquo;12:00&rdquo;, seceond part [&ldquo;12:00&rdquo;, &ldquo;13:00&rdquo;), thrid [&ldquo;13:00&rdquo;, ).</p>

<p>And then, indicate the job to use this partition file:</p>

<p>```java
job.setPartitionerClass(TotalOrderPartitioner.class);
otalOrderPartitioner.setPartitionFile(job.getConfiguration(), partition);</p>

<p>// The number of reducers should equal the number of partitions.
job.setNumReduceTasks(3);
```</p>

<h3>Use MutipleOutputs</h3>

<p>In the reducer, we need to note down the row count of this partition, to do that, we&rsquo;ll need the MultipleOutputs class, which let use output multiple result files apart from the default &ldquo;part-r-xxxxx&rdquo;. The reducer&rsquo;s code is as following:</p>

<p>```java
public static class JobReducer extends Reducer&lt;Text, Text, NullWritable, Text> {</p>

<pre><code>private MultipleOutputs&lt;NullWritable, Text&gt; mos;
private long count;

@Override
protected void setup(Context context)
        throws IOException, InterruptedException {

    super.setup(context);
    mos = new MultipleOutputs&lt;NullWritable, Text&gt;(context);
    count = 0;
}

@Override
protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
        throws IOException, InterruptedException {

    for (Text value : values) {
        context.write(NullWritable.get(), value);
        ++count;
    }
}

@Override
protected void cleanup(Context context)
        throws IOException, InterruptedException {

    super.cleanup(context);
    mos.write("count", NullWritable.get(), new LongWritable(count));
    mos.close();
}
</code></pre>

<p>}
```</p>

<p>There&rsquo;re several things to pay attention to:</p>

<ol>
<li>MultipleOutputs is declared as class member, defined in Reducer#setup method, and must be closed at Reducer#cleanup (otherwise the file will be empty).</li>
<li>When instantiating MultipleOutputs class, the generic type needs to be the same as reducer&rsquo;s output key/value class.</li>
<li>In order to use a different output key/value class, additional setup needs to be done at job definition:</li>
</ol>


<p>```java
Job job = new Job(getConf());
MultipleOutputs.addNamedOutput(job, &ldquo;count&rdquo;, SequenceFileOutputFormat.class,</p>

<pre><code>NullWritable.class, LongWritable.class);
</code></pre>

<p>```</p>

<p>For example, if the output folder is &ldquo;/tmp/total-sort/&rdquo;, there&rsquo;ll be the following files when job is done:</p>

<p><code>text
/tmp/total-sort/count-r-00001
/tmp/total-sort/count-r-00002
/tmp/total-sort/count-r-00003
/tmp/total-sort/part-r-00001
/tmp/total-sort/part-r-00002
/tmp/total-sort/part-r-00003
</code></p>

<h3>Pass Start Ids to Mapper</h3>

<p>When the second mapper processes the inputs, we want them to know the initial id of its partition, which can be calculated from the &ldquo;count-*&rdquo; files we produce before. To pass this information, we can use the job&rsquo;s Configuration object.</p>

<p>```java
// Read and calculate the start id from those row-count files.
Map&lt;String, Long> startIds = new HashMap&lt;String, Long>();
long startId = 1;
FileSystem fs = FileSystem.get(getConf());
for (FileStatus file : fs.listStatus(countPath)) {</p>

<pre><code>Path path = file.getPath();
String name = path.getName();
if (!name.startsWith("count-")) {
    continue;
}

startIds.put(name.substring(name.length() - 5), startId);

SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, getConf());
NullWritable key = NullWritable.get();
LongWritable value = new LongWritable();
if (!reader.next(key, value)) {
    continue;
}
startId += value.get();
reader.close();
</code></pre>

<p>}</p>

<p>// Serialize the map and pass it to Configuration.
job.getConfiguration().set(&ldquo;startIds&rdquo;, Base64.encodeBase64String(</p>

<pre><code>    SerializationUtils.serialize((Serializable) startIds)));
</code></pre>

<p>// Recieve it in Mapper#setup
public static class JobMapperB extends Mapper&lt;NullWritable, Text, LongWritable, Text> {</p>

<pre><code>private Map&lt;String, Long&gt; startIds;
private long startId;

@SuppressWarnings("unchecked")
@Override
protected void setup(Context context)
        throws IOException, InterruptedException {

    super.setup(context);
    startIds = (Map&lt;String, Long&gt;) SerializationUtils.deserialize(
            Base64.decodeBase64(context.getConfiguration().get("startIds")));
    String name = ((FileSplit) context.getInputSplit()).getPath().getName();
    startId = startIds.get(name.substring(name.length() - 5));
}

@Override
protected void map(NullWritable key, Text value, Context context)
        throws IOException, InterruptedException {

    context.write(new LongWritable(startId++), value);
}
</code></pre>

<p>}
```</p>

<h3>Set the Input Non-splitable</h3>

<p>When the file is bigger than a block or so (depending on some configuration entries), Hadoop will split it, which is not good for us. So let&rsquo;s define a new InputFormat class to disable the splitting behaviour:</p>

<p>```java
public static class NonSplitableSequence extends SequenceFileInputFormat&lt;NullWritable, Text> {</p>

<pre><code>@Override
protected boolean isSplitable(JobContext context, Path filename) {
    return false;
}
</code></pre>

<p>}</p>

<p>// use it
job.setInputFormatClass(NonSplitableSequence.class);
```</p>

<p>And that&rsquo;s it, we are able to generate a unique, auto-increment id for a sorted collection, with Hadoop&rsquo;s parallel computing capability. The process is rather complicated, which requires several techniques about Hadoop. It&rsquo;s worthwhile to dig.</p>

<p>A workable example can be found in my <a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/AutoIncrementId2Job.java">Github repository</a>. If you have some more straight-forward approach, please do let me know.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hive并发情况下报DELETEME表不存在的异常]]></title>
    <link href="http://shzhangji.com/blog/2013/09/06/hive-deleteme-error/"/>
    <updated>2013-09-06T11:20:00+08:00</updated>
    <id>http://shzhangji.com/blog/2013/09/06/hive-deleteme-error</id>
    <content type="html"><![CDATA[<p>在每天运行的Hive脚本中，偶尔会抛出以下错误：</p>

<p>```
2013-09-03 01:39:00,973 ERROR parse.SemanticAnalyzer (SemanticAnalyzer.java:getMetaData(1128)) &ndash; org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table dw_xxx_xxx</p>

<pre><code>    at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:896)
    ...
</code></pre>

<p>Caused by: javax.jdo.JDODataStoreException: Exception thrown obtaining schema column information from datastore
NestedThrowables:
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table &lsquo;hive.DELETEME1378143540925&rsquo; doesn&rsquo;t exist</p>

<pre><code>    at org.datanucleus.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:313)
    ...
</code></pre>

<p>```</p>

<p>查阅了网上的资料，是DataNucleus的问题。</p>

<p>背景1：我们知道MySQL中的库表信息是存放在information_schema库中的，Hive也有类似的机制，它会将库表信息存放在一个第三方的RDBMS中，目前我们线上配置的是本机MySQL，即：</p>

<p>$ mysql -uhive -ppassword hive</p>

<p><img src="/images/hive-deleteme-error/1.png" alt="1.png" /></p>

<!--more-->


<p>背景2：Hive使用的是DataNuclues ORM库来操作数据库的，而基本上所有的ORM框架（对象关系映射）都会提供自动建表的功能，即开发者只需编写Java对象，ORM会自动生成DDL。DataNuclues也有这一功能，而且它在初始化时会通过生成临时表的方式来获取数据库的Catalog和Schema，也就是 DELETEME表：</p>

<p><img src="/images/hive-deleteme-error/2.png" alt="2.png" /></p>

<p>这样就有一个问题：在并发量大的情况下，DELETEME表名中的毫秒数可能相同，那在pt.drop(conn)的时候就会产生找不到表的报错。</p>

<p>解决办法已经可以在代码中看到了：将datanucleus.fixedDataStore选项置为true，即告知DataNuclues该数据库的表结构是既定的，不允许执行DDL操作。</p>

<p>这样配置会有什么问题？让我们回忆一下Hive的安装步骤：</p>

<ol>
<li>解压hive-xyz.tar.gz；</li>
<li>在conf/hive-site.xml中配置Hadoop以及用于存放库表信息的第三方数据库；</li>
<li>执行bin/hive -e &ldquo;&hellip;"即可使用。DataNucleus会按需创建上述的DBS等表。</li>
</ol>


<p>这对新手来说很有用，因为不需要手动去执行建表语句，但对生产环境来说，普通帐号是没有DDL权限的，我们公司建表也都是提DB-RT给DBA操作。同理，线上Hive数据库也应该采用手工创建的方式，导入scripts/metastore/upgrade/mysql/hive-schema-0.9.0.mysql.sql文件即可。这样一来，就可以放心地配置datanucleus.fixedDataStore以及 datanecleus.autoCreateSchema两个选项了。</p>

<p>这里我们也明确了一个问题：设置datanucleus.fixedDataStore=true不会影响Hive建库建表，因为Hive中的库表只是DBS、TBLS表中的一条记录而已。</p>

<p>建议的操作：</p>

<ol>
<li>在线上导入hive-schema-0.9.0.mysql.sql，将尚未创建的表创建好（比如我们没有用过Hive的权限管理，所以DataNucleus没有自动创建DB_PRIVS表）；</li>
<li>在hive-site.xml中配置 datanucleus.fixedDataStore=true；datanecleus.autoCreateSchema=false。</li>
</ol>


<p>这样就可以彻底解决这个异常了。</p>

<p>为什么HWI没有遇到类似问题？因为它是常驻内存的，DELETEME表只会在启动的时候创建，后续的查询不会创建。而我们这里每次调用hive命令行都会去创建，所以才有这样的问题。</p>

<p>参考链接：</p>

<ul>
<li><a href="http://www.cnblogs.com/ggjucheng/archive/2012/07/25/2608633.html">http://www.cnblogs.com/ggjucheng/archive/2012/07/25/2608633.html</a></li>
<li><a href="https://github.com/dianping/cosmos-hive/issues/10">https://github.com/dianping/cosmos-hive/issues/10</a></li>
<li><a href="https://issues.apache.org/jira/browse/HIVE-1841">https://issues.apache.org/jira/browse/HIVE-1841</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Manage Leiningen Project Configuration]]></title>
    <link href="http://shzhangji.com/blog/2013/04/30/manage-leiningen-project-configuration/"/>
    <updated>2013-04-30T16:16:00+08:00</updated>
    <id>http://shzhangji.com/blog/2013/04/30/manage-leiningen-project-configuration</id>
    <content type="html"><![CDATA[<p>In Maven projects, we tend to use <code>.properties</code> files to store various configurations, and use Maven profiles to switch between development and production environments. Like the following example:</p>

<p>```text</p>

<h1>database.properties</h1>

<p>mydb.jdbcUrl=${mydb.jdbcUrl}
```</p>

<p>```xml</p>

<!-- pom.xml -->


<p><profiles></p>

<pre><code>&lt;profile&gt;
    &lt;id&gt;development&lt;/id&gt;
    &lt;activation&gt;&lt;activeByDefault&gt;true&lt;/activeByDefault&gt;&lt;/activation&gt;
    &lt;properties&gt;
        &lt;mydb.jdbcUrl&gt;jdbc:mysql://127.0.0.1:3306/mydb&lt;/mydb.jdbcUrl&gt;
    &lt;/properties&gt;
&lt;/profile&gt;
&lt;profile&gt;
    &lt;id&gt;production&lt;/id&gt;
    &lt;!-- This profile could be moved to ~/.m2/settings.xml to increase security. --&gt;
    &lt;properties&gt;
        &lt;mydb.jdbcUrl&gt;jdbc:mysql://10.0.2.15:3306/mydb&lt;/mydb.jdbcUrl&gt;
    &lt;/properties&gt;
&lt;/profile&gt;
</code></pre>

<p></profiles>
```</p>

<p>As for Leiningen projects, there&rsquo;s no variable substitution in profile facility, and although in profiles we could use <code>:resources</code> to compact production-wise files into Jar, these files are actually replacing the original ones, instead of being merged. One solution is to strictly seperate environment specific configs from the others, so the replacement will be ok. But here I take another approach, to manually load files from difference locations, and then merge them.</p>

<!-- more -->


<h2>Read Configuration from <code>.clj</code> Files</h2>

<p>Instead of using <code>.properties</code>, we&rsquo;ll use <code>.clj</code> files directly, since it&rsquo;s more expressive and Clojure makes it very easy to utilize them.</p>

<p>```clojure
(defn read-config [section]
  (let [read (fn [res-path]</p>

<pre><code>           (if-let [res (clojure.java.io/resource res-path)]
             (read-string (slurp res))
             {}))
    default-name (str (name section) ".clj")
    default (read default-name)
    override (read (str "override/" default-name))]
(merge default override)))
</code></pre>

<p>```</p>

<p>This function assumes the following directory layout:</p>

<p>```text
test-project/
├── README.md
├── project.clj
├── resources
│   ├── database.clj
│   └── override
│       └── database.clj
└── src</p>

<pre><code>└── test_project
    └── core.clj
</code></pre>

<p>```</p>

<p>And the <code>database.clj</code>s are like:</p>

<p>```clojure
; resources/database.clj
{:charset &ldquo;utf-8&rdquo;
 :mydb {:host &ldquo;127.0.0.1&rdquo;}}</p>

<p>; resources/override/database.clj
{:mydb {:host &ldquo;10.0.2.15&rdquo;}}
```</p>

<p>The <code>.clj</code> files simply contains a <code>map</code> object, and we use <code>read-string</code> facility to parse the map. Since the latter map is merged into the former one, we can include some default settings without worrying about whether they&rsquo;ll be available.</p>

<h2>Places to Put &lsquo;Outter Configuration&rsquo;</h2>

<p>Here I use the word &lsquo;outter&rsquo;, which means those configs are related to environments, and will override the default settings. In this section, I&rsquo;ll introduce some typical places to put these outter configs and how to use them.</p>

<h3>A &lsquo;resources/override/&rsquo; Directory</h3>

<p>First of all, this directory should be removed from version control, such as <code>.gitignore</code>:</p>

<p>```text
/.project
/.settings</p>

<p>/resources/override
```</p>

<p>And then, developers can put production or local configuration files in this directory.</p>

<p>In production, there&rsquo;s typically a &lsquo;compiling server&rsquo;, which can be used to store production configs. After compiling, the Jar file will include the proper configs and are ready to be deployed.</p>

<h3>A Dedicated Directory on Every Server</h3>

<p>We could simply replace the <code>override</code> directory with an absolute path, such as <code>/home/www/config</code>. The pros are that we don&rsquo;t need to recompile the jar files when config changes, and some of the configs could be shared between different projects.</p>

<p>But in such approach, you&rsquo;ll need a provisioning tool like Puppet to manage those configs and notify the applications to restart. For something like Hadoop MapReduce job, it&rsquo;s probably not practical to have such a directory on every compute node.</p>

<p>Another thing I want to mention in this approach is that, I suggest using an environment variable to indicate the path to config directory, not hard-coded in application. As a matter of fact, you could even place all configs into env vars, as suggested by <a href="http://www.12factor.net/config">12-factor apps</a>.</p>

<h3>A Central Configuration Server</h3>

<p>As for really big corporations, a central configuration server is necessary. One popular option is to use ZooKeeper. Or your companies have some service-discovery mechanism. These are really advanced topics, and I&rsquo;ll leave them to the readers.</p>

<h2>Manage Configs in Application</h2>

<p>Lastly, I&rsquo;ll share a snippet that&rsquo;ll manage the configs, it&rsquo;s actually quite easy:</p>

<p>```clojure
(def ^:private config (atom {}))</p>

<p>(defn get-config</p>

<p>  ([section]</p>

<pre><code>(if-let [config-section (get @config section)]
  config-section
  (let [config-section (read-config section)]
    (swap! config assoc section config-section)
    config-section)))
</code></pre>

<p>  ([section item]</p>

<pre><code>(get (get-config section) item)))
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[fork()与僵尸进程]]></title>
    <link href="http://shzhangji.com/blog/2013/03/27/fork-and-zombie-process/"/>
    <updated>2013-03-27T20:18:00+08:00</updated>
    <id>http://shzhangji.com/blog/2013/03/27/fork-and-zombie-process</id>
    <content type="html"><![CDATA[<p>使用fork()函数派生出多个子进程来并行执行程序的不同代码块，是一种常用的编程泛型。特别是在网络编程中，父进程初始化后派生出指定数量的子进程，共同监听网络端口并处理请求，从而达到扩容的目的。</p>

<p>但是，在使用fork()函数时若处理不当，很容易产生僵尸进程。根据UNIX系统的定义，僵尸进程是指子进程退出后，它的父进程没有“等待”该子进程，这样的子进程就会成为僵尸进程。何谓“等待”？僵尸进程的危害是什么？以及要如何避免？这就是本文将要阐述的内容。</p>

<h2>fork()函数</h2>

<p>下面这段C语言代码展示了fork()函数的使用方法：</p>

<p>```c
// myfork.c</p>

<h1>include &lt;unistd.h></h1>

<h1>include &lt;stdio.h></h1>

<p>int main(int argc, char **argv) {</p>

<pre><code>while (1) {
    pid_t pid = fork();
    if (pid &gt; 0) {
        // 主进程
        sleep(5);
    } else if (pid == 0) {
        // 子进程
        return 0;
    } else {
        fprintf(stderr, "fork error\n");
        return 2;
    }
}
</code></pre>

<p>}
```</p>

<!-- more -->


<p>调用fork()函数后，系统会将当前进程的绝大部分资源拷贝一份（其中的copy-on-write技术这里不详述），该函数的返回值有三种情况，分别是：</p>

<ul>
<li>大于0，表示当前进程为父进程，返回值是子进程号；</li>
<li>等于0，表示当前进程是子进程；</li>
<li>小于0（确切地说是等于-1），表示fork()调用失败。</li>
</ul>


<p>让我们编译执行这段程序，并查看进程表：</p>

<p><code>bash
$ gcc myfork.c -o myfork &amp;&amp; ./myfork
$ ps -ef | grep fork
vagrant  14860  2748  0 06:09 pts/0    00:00:00 ./myfork
vagrant  14861 14860  0 06:09 pts/0    00:00:00 [myfork] &lt;defunct&gt;
vagrant  14864 14860  0 06:09 pts/0    00:00:00 [myfork] &lt;defunct&gt;
vagrant  14877 14860  0 06:09 pts/0    00:00:00 [myfork] &lt;defunct&gt;
vagrant  14879  2784  0 06:09 pts/1    00:00:00 grep fork
</code></p>

<p>可以看到子进程创建成功了，进程号也有对应关系。但是每个子进程后面都跟有“defunct”标识，即表示该进程是一个僵尸进程。</p>

<p>这段程序会每五秒创建一个新的子进程，如果不加以回收，那就会占满进程表，使得系统无法再创建进程。这也是僵尸进程最大的危害。</p>

<h2>wait()函数</h2>

<p>我们对上面这段程序稍加修改：</p>

<p>```c
pid_t pid = fork();
if (pid > 0) {</p>

<pre><code>// parent process
wait(NULL);
sleep(5);
</code></pre>

<p>} else &hellip;
```</p>

<p>编译执行后会发现进程表中不再出现defunct进程了，即子进程已被完全回收。因此上文中的“等待”指的是主进程等待子进程结束，获取子进程的结束状态信息，这时内核才会回收子进程。</p>

<p>除了通过“等待”来回收子进程，主进程退出也会回收子进程。这是因为主进程退出后，init进程（PID=1）会接管这些僵尸进程，该进程一定会调用wait()函数（或其他类似函数），从而保证僵尸进程得以回收。</p>

<h2>SIGCHLD信号</h2>

<p>通常，父进程不会始终处于等待状态，它还需要执行其它代码，因此“等待”的工作会使用信号机制来完成。</p>

<p>在子进程终止时，内核会发送SIGCHLD信号给父进程，因此父进程可以添加信号处理函数，并在该函数中调用wait()函数，以防止僵尸进程的产生。</p>

<p>```c
// myfork2.c</p>

<h1>include &lt;unistd.h></h1>

<h1>include &lt;stdio.h></h1>

<h1>include &lt;signal.h></h1>

<h1>include &lt;time.h></h1>

<h1>include &lt;sys/wait.h></h1>

<p>void signal_handler(int signo) {</p>

<pre><code>if (signo == SIGCHLD) {
    pid_t pid;
    while ((pid = waitpid(-1, NULL, WNOHANG)) &gt; 0) {
        printf("SIGCHLD pid %d\n", pid);
    }
}
</code></pre>

<p>}</p>

<p>void mysleep(int sec) {</p>

<pre><code>time_t start = time(NULL), elapsed = 0;
while (elapsed &lt; sec) {
    sleep(sec - elapsed);
    elapsed = time(NULL) - start;
}
</code></pre>

<p>}</p>

<p>int main(int argc, char **argv) {</p>

<pre><code>signal(SIGCHLD, signal_handler);

while (1) {
    pid_t pid = fork();
    if (pid &gt; 0) {
        // parent process
        mysleep(5);
    } else if (pid == 0) {
        // child process
        printf("child pid %d\n", getpid());
        return 0;
    } else {
        fprintf(stderr, "fork error\n");
        return 2;
    }
}
</code></pre>

<p>}
```</p>

<p>代码执行结果：</p>

<p><code>bash
$ gcc myfork2.c -o myfork2 &amp;&amp; ./myfork2
child pid 17422
SIGCHLD pid 17422
child pid 17423
SIGCHLD pid 17423
</code></p>

<p>其中，signal()用于注册信号处理函数，该处理函数接收一个signo参数，用来标识信号的类型。</p>

<p>waitpid()的功能和wait()类似，但提供了额外的选项（<code>wait(NULL)</code>等价于<code>waitpid(-1, NULL, 0)</code>）。如，wait()函数是阻塞的，而waitpid()提供了WNOHANG选项，调用后会立刻返回，可根据返回值判断等待结果。</p>

<p>此外，我们在信号处理中使用了一个循环体，不断调用waitpid()，直到失败为止。那是因为在系统繁忙时，信号可能会被合并，即两个子进程结束只会发送一次SIGCHLD信号，如果只wait()一次，就会产生僵尸进程。</p>

<p>最后，由于默认的sleep()函数会在接收到信号时立即返回，因此为了方便演示，这里定义了mysleep()函数。</p>

<h2>SIG_IGN</h2>

<p>除了在SIGCHLD信号处理函数中调用wait()来避免产生僵尸进程，我们还可以选择忽略SIGCHLD信号，告知操作系统父进程不关心子进程的退出状态，可以直接清理。</p>

<p><code>c
signal(SIGCHLD, SIG_IGN);
</code></p>

<p>但需要注意的是，在部分BSD系统中，这种做法仍会产生僵尸进程。因此更为通用的方法还是使用wait()函数。</p>

<h2>Perl中的fork()函数</h2>

<p>Perl语言提供了相应的内置函数来创建子进程：</p>

<p>```perl</p>

<h1>!/usr/bin/perl</h1>

<p>sub REAPER {</p>

<pre><code>my $pid;
while (($pid = waitpid(-1, WNOHANG)) &gt; 0) {
    print "SIGCHLD pid $pid\n";
}
</code></pre>

<p>}</p>

<p>$SIG{CHLD} = &amp;REAPER;</p>

<p>my $pid = fork();
if ($pid > 0) {</p>

<pre><code>print "[Parent] child pid $pid\n";
sleep(10);
</code></pre>

<p>} elsif ($pid == 0) {</p>

<pre><code>print "[Child] pid $$\n";
exit;
</code></pre>

<p>}
```</p>

<p>其思路和C语言基本是一致的。如果想要忽略SIGCHLD，可使用<code>$SIG{CHLD} = 'IGNORE';</code>，但还是要考虑BSD系统上的限制。</p>

<h2>参考资料</h2>

<ul>
<li><a href="http://linux.die.net/man/2/fork">fork(2) &ndash; Linux man page</a></li>
<li><a href="http://linux.die.net/man/2/waitpid">waitpid(2) &ndash; Linux man page</a></li>
<li><a href="http://book.jd.com/10137688.html">UNIX环境高级编程（英文版）（第2版）</a></li>
<li><a href="http://tech.idv2.com/2006/10/14/linux-multiprocess-info/">Linux多进程相关内容</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
