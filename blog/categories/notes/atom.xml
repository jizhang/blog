<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Notes | Ji ZHANG's Blog]]></title>
  <link href="http://shzhangji.com/blog/categories/notes/atom.xml" rel="self"/>
  <link href="http://shzhangji.com/"/>
  <updated>2014-12-30T11:52:15+08:00</updated>
  <id>http://shzhangji.com/</id>
  <author>
    <name><![CDATA[Ji ZHANG]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[使用git rebase让历史变得清晰]]></title>
    <link href="http://shzhangji.com/blog/2014/12/23/use-git-rebase-to-clarify-history/"/>
    <updated>2014-12-23T16:10:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/12/23/use-git-rebase-to-clarify-history</id>
    <content type="html"><![CDATA[<p>当多人协作开发一个分支时，历史记录通常如下方左图所示，比较凌乱。如果希望能像右图那样呈线性提交，就需要学习git rebase的用法。</p>

<p><img src="/images/git-rebase/rebase-result.png" alt="" /></p>

<h2>“Merge branch”提交的产生</h2>

<p>我们的工作流程是：修改代码→提交到本地仓库→拉取远程改动→推送。正是在git pull这一步产生的Merge branch提交。事实上，git pull等效于get fetch origin和get merge origin/master这两条命令，前者是拉取远程仓库到本地临时库，后者是将临时库中的改动合并到本地分支中。</p>

<p>要避免Merge branch提交也有一个“土法”：先pull、再commit、最后push。不过万一commit和push之间远程又发生了改动，还需要再pull一次，就又会产生Merge branch提交。</p>

<h2>使用git pull &mdash;rebase</h2>

<p>修改代码→commit→git pull &mdash;rebase→git push。也就是将get merge origin/master替换成了git rebase origin/master，它的过程是先将HEAD指向origin/master，然后逐一应用本地的修改，这样就不会产生Merge branch提交了。具体过程见下文扩展阅读。</p>

<!-- more -->


<p>使用git rebase是有条件的，你的本地仓库要“足够干净”。可以用git status命令查看当前改动：：</p>

<p><code>
$ git status
On branch master
Your branch is up-to-date with 'origin/master'.
nothing to commit, working directory clean
</code></p>

<p>本地没有任何未提交的改动，这是最“干净”的。稍差一些的是这样：</p>

<p>```
$ git status
On branch master
Your branch is up-to-date with &lsquo;origin/master&rsquo;.
Untracked files:
  (use &ldquo;git add <file>&hellip;&rdquo; to include in what will be committed)</p>

<pre><code>test.txt
</code></pre>

<p>nothing added to commit but untracked files present (use &ldquo;git add&rdquo; to track)
```</p>

<p>即本地只有新增文件未提交，没有改动文件。我们应该尽量保持本地仓库的“整洁”，这样才能顺利使用git rebase。特殊情况下也可以用git stash来解决问题，有兴趣的可自行搜索。</p>

<h2>修改git pull的默认行为</h2>

<p>每次都加&mdash;rebase似乎有些麻烦，我们可以指定某个分支在执行git pull时默认采用rebase方式：</p>

<p><code>
$ git config branch.master.rebase true
</code></p>

<p>如果你觉得所有的分支都应该用rebase，那就设置：</p>

<p><code>
$ git config --global branch.autosetuprebase always
</code></p>

<p>这样对于新建的分支都会设定上面的rebase=true了。已经创建好的分支还是需要手动配置的。</p>

<h2>扩展阅读[1]：git rebase工作原理</h2>

<p>先看看git merge的示意图：</p>

<p><img src="/images/git-rebase/merge.png" alt="" /></p>

<p><a href="https://www.atlassian.com/ja/git/tutorial/git-branches">图片来源</a></p>

<p>可以看到Some Feature分支的两个提交通过一个新的提交（蓝色）和master连接起来了。</p>

<p>再来看git rebase的示意图：</p>

<p><img src="/images/git-rebase/rebase-1.png" alt="" /></p>

<p><img src="/images/git-rebase/rebase-2.png" alt="" /></p>

<p>Feature分支中的两个提交被“嫁接”到了Master分支的头部，或者说Feature分支的“基”（base）变成了 Master，rebase也因此得名。</p>

<h2>扩展阅读[2]：git merge &mdash;no-ff</h2>

<p>在做项目开发时会用到分支，合并时采用以下步骤：</p>

<p><code>
$ git checkout feature-branch
$ git rebase master
$ git checkout master
$ git merge --no-ff feature-branch
$ git push origin master
</code></p>

<p>历史就成了这样：</p>

<p><img src="/images/git-rebase/no-ff.png" alt="" /></p>

<p>可以看到，Merge branch &lsquo;feature-branch'那段可以很好的展现出这些提交是属于某一特性的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[离线环境下构建sbt项目]]></title>
    <link href="http://shzhangji.com/blog/2014/11/07/sbt-offline/"/>
    <updated>2014-11-07T15:02:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/11/07/sbt-offline</id>
    <content type="html"><![CDATA[<p>在公司网络中使用<a href="http://www.scala-sbt.org/">sbt</a>、<a href="http://maven.apache.org/">Maven</a>等项目构建工具时，我们通常会搭建一个公用的<a href="http://www.sonatype.org/nexus/">Nexus</a>镜像服务，原因有以下几个：</p>

<ul>
<li>避免重复下载依赖，节省公司带宽；</li>
<li>国内网络环境不理想，下载速度慢；</li>
<li>IDC服务器没有外网访问权限；</li>
<li>用于发布内部模块。</li>
</ul>


<p>sbt的依赖管理基于<a href="http://ant.apache.org/ivy/">Ivy</a>，虽然它能直接使用<a href="http://search.maven.org/">Maven中央仓库</a>中的Jar包，在配置时还是有一些注意事项的。</p>

<!-- more -->


<h2>配置Nexus镜像</h2>

<p>根据这篇<a href="http://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html">官方文档</a>的描述，Ivy和Maven在依赖管理方面有些许差异，因此不能直接将两者的镜像仓库配置成一个，而需分别建立两个虚拟镜像组。</p>

<p><img src="http://www.scala-sbt.org/0.13/docs/files/proxy-ivy-mvn-setup.png" alt="" /></p>

<p>安装Nexus后默认会有一个Public Repositories组，可以将其作为Maven的镜像组，并添加一些常用的第三方镜像：</p>

<ul>
<li>cloudera: <a href="https://repository.cloudera.com/artifactory/cloudera-repos/">https://repository.cloudera.com/artifactory/cloudera-repos/</a></li>
<li>spring: <a href="http://repo.springsource.org/libs-release-remote/">http://repo.springsource.org/libs-release-remote/</a></li>
<li>scala-tools: <a href="https://oss.sonatype.org/content/groups/scala-tools/">https://oss.sonatype.org/content/groups/scala-tools/</a></li>
</ul>


<p>对于Ivy镜像，我们创建一个新的虚拟组：ivy-releases，并添加以下两个镜像：</p>

<ul>
<li>type-safe: <a href="http://repo.typesafe.com/typesafe/ivy-releases/">http://repo.typesafe.com/typesafe/ivy-releases/</a></li>
<li>sbt-plugin: <a href="http://dl.bintray.com/sbt/sbt-plugin-releases/">http://dl.bintray.com/sbt/sbt-plugin-releases/</a></li>
</ul>


<p>对于sbt-plugin，由于一些原因，Nexus会将其置为Automatically Blocked状态，因此要在配置中将这个选项关闭，否则将无法下载远程的依赖包。</p>

<h2>配置sbt</h2>

<p>为了让sbt使用Nexus镜像，需要创建一个~/.sbt/repositories文件，内容为：</p>

<p><code>
[repositories]
  local
  my-ivy-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]
  my-maven-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/public/
</code></p>

<p>这样配置对大部分项目来说是足够了。但是有些项目会在构建描述文件中添加其它仓库，我们需要覆盖这种行为，方法是：</p>

<p><code>bash
$ sbt -Dsbt.override.build.repos=true
</code></p>

<p>你也可以通过设置SBT_OPTS环境变量来进行全局配置。</p>

<p>经过以上步骤，sbt执行过程中就不需要访问外网了，因此速度会有很大提升。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL异常UTF-8字符的处理]]></title>
    <link href="http://shzhangji.com/blog/2014/10/14/mysql-incorrent-utf8-value/"/>
    <updated>2014-10-14T13:16:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/10/14/mysql-incorrent-utf8-value</id>
    <content type="html"><![CDATA[<p>ETL流程中，我们会将Hive中的数据导入MySQL——先用Hive命令行将数据保存为文本文件，然后用MySQL的LOAD DATA语句进行加载。最近有一张表在加载到MySQL时会报以下错误：</p>

<p><code>
Incorrect string value: '\xF0\x9D\x8C\x86' for column ...
</code></p>

<p>经查，这个字段中保存的是用户聊天记录，因此会有一些表情符号。这些符号在UTF-8编码下需要使用4个字节来记录，而MySQL中的utf8编码只支持3个字节，因此无法导入。</p>

<p>根据UTF-8的编码规范，3个字节支持的Unicode字符范围是U+0000–U+FFFF，因此可以在Hive中对数据做一下清洗：</p>

<p><code>sql
SELECT REGEXP_REPLACE(content, '[^\\u0000-\\uFFFF]', '') FROM ...
</code></p>

<p>这样就能排除那些需要使用3个以上字节来记录的字符了，从而成功导入MySQL。</p>

<p>以下是一些详细说明和参考资料。</p>

<!-- more -->


<h2>Unicode字符集和UTF编码</h2>

<p><a href="http://en.wikipedia.org/wiki/Unicode">Unicode字符集</a>是一种将全球所有文字都囊括在内的字符集，从而实现跨语言、跨平台的文字信息交换。它由<a href="http://en.wikipedia.org/wiki/Plane_(Unicode)#Basic_Multilingual_Plane">基本多语平面（BMP）</a>和多个扩展平面（non-BMP）组成。前者的编码范围是U+0000-U+FFFF，包括了绝大多数现代语言文字，因此最为常用。</p>

<p><a href="http://en.wikipedia.org/wiki/Unicode#Unicode_Transformation_Format_and_Universal_Character_Set">UTF</a>则是一种编码格式，负责将Unicode字符对应的编号转换为计算机可以识别的二进制数据，进行保存和读取。</p>

<p>比如，磁盘上记录了以下二进制数据：</p>

<p><code>
1101000 1100101 1101100 1101100 1101111
</code></p>

<p>读取它的程序知道这是以UTF-8编码保存的字符串，因此将其解析为以下编号：</p>

<p><code>
104 101 108 108 111
</code></p>

<p>又因为UTF-8编码对应的字符集是Unicode，所以上面这五个编号对应的字符便是“hello”。</p>

<p>很多人会将Unicode和UTF混淆，但两者并不具可比性，它们完成的功能是不同的。</p>

<h2>UTF-8编码</h2>

<p>UTF编码家族也有很多成员，其中<a href="http://en.wikipedia.org/wiki/UTF-8">UTF-8</a>最为常用。它是一种变长的编码格式，对于ASCII码中的字符使用1个字节进行编码，对于中文等则使用3个字节。这样做的优点是在存储西方语言文字时不会造成空间浪费，不像UTF-16和UTF-32，分别使用两个字节和四个字节对所有字符进行编码。</p>

<p>UTF-8编码的字节数上限并不是3个。对于U+0000-U+FFFF范围内的字符，使用3个字节可以表示完全；对于non-BMP中的字符，则会使用4-6个字节来表示。同样，UTF-16编码也会使用四个字节来表示non-BMP中的字符。</p>

<h2>MySQL的UTF-8编码</h2>

<p>根据MySQL的<a href="http://dev.mysql.com/doc/refman/5.5/en/charset-unicode.html">官方文档</a>，它的UTF-8编码支持是不完全的，最多使用3个字符，这也是导入数据时报错的原因。</p>

<p>MySQL5.5开始支持utf8mb4编码，至多使用4个字节，因此能包含到non-BMP字符。只是我们的MySQL版本仍是5.1，因此选择丢弃这些字符。</p>

<h2>参考资料</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8">http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8</a></li>
<li><a href="http://www.joelonsoftware.com/articles/Unicode.html">http://www.joelonsoftware.com/articles/Unicode.html</a></li>
<li><a href="http://apps.timwhitlock.info/emoji/tables/unicode">http://apps.timwhitlock.info/emoji/tables/unicode</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在CDH 4.5上安装Shark 0.9]]></title>
    <link href="http://shzhangji.com/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5/"/>
    <updated>2014-07-05T17:16:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5</id>
    <content type="html"><![CDATA[<p><a href="http://spark.apache.org">Spark</a>是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的<a href="http://hadoop.apache.org">Hadoop</a>生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于<a href="http://hive.apache.org">Hive</a>，Spark也有相应的替代项目——<a href="http://shark.cs.berkeley.edu/">Shark</a>，能做到 <strong>drop-in replacement</strong> ，直接构建在现有集群之上。本文就将简要阐述如何在CDH4.5上搭建Shark0.9集群。</p>

<h2>准备工作</h2>

<ul>
<li>安装方式：Spark使用CDH提供的Parcel，以Standalone模式启动</li>
<li>软件版本

<ul>
<li>Cloudera Manager 4.8.2</li>
<li>CDH 4.5</li>
<li>Spark 0.9.0 Parcel</li>
<li><a href="http://cloudera.rst.im/shark/">Shark 0.9.1 Binary</a></li>
</ul>
</li>
<li>服务器基础配置

<ul>
<li>可用的软件源（如<a href="http://mirrors.ustc.edu.cn/">中科大的源</a>）</li>
<li>配置主节点至子节点的root账户SSH无密码登录。</li>
<li>在<code>/etc/hosts</code>中写死IP和主机名，或者DNS做好正反解析。</li>
</ul>
</li>
</ul>


<!-- more -->


<h2>安装Spark</h2>

<ul>
<li>使用CM安装Parcel，不需要重启服务。</li>
<li>修改<code>/etc/spark/conf/spark-env.sh</code>：（其中one-843是主节点的域名）</li>
</ul>


<p><code>bash
STANDALONE_SPARK_MASTER_HOST=one-843
DEFAULT_HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*
export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native
</code></p>

<ul>
<li>修改<code>/etc/spark/conf/slaves</code>，添加各节点主机名。</li>
<li>将<code>/etc/spark/conf</code>目录同步至所有节点。</li>
<li>启动Spark服务（即Standalone模式）：</li>
</ul>


<p><code>bash
$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-master.sh
$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-slaves.sh
</code></p>

<ul>
<li>测试<code>spark-shell</code>是否可用：</li>
</ul>


<p><code>scala
sc.textFile("hdfs://one-843:8020/user/jizhang/zj_people.txt.lzo").count
</code></p>

<h2>安装Shark</h2>

<ul>
<li>安装Oracle JDK 1.7 Update 45至<code>/usr/lib/jvm/jdk1.7.0_45</code>。</li>
<li>下载别人编译好的二进制包：<a href="http://cloudera.rst.im/shark/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz">shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz</a></li>
<li>解压至<code>/opt</code>目录，修改<code>conf/shark-env.sh</code>：</li>
</ul>


<p>```bash
export JAVA_HOME=/usr/lib/jvm/jdk1.7.0_45
export SCALA_HOME=/opt/cloudera/parcels/SPARK/lib/spark
export SHARK_HOME=/root/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0</p>

<p>export HIVE_CONF_DIR=/etc/hive/conf</p>

<p>export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop
export SPARK_HOME=/opt/cloudera/parcels/SPARK/lib/spark
export MASTER=spark://one-843:7077</p>

<p>export SPARK_CLASSPATH=$SPARK_CLASSPATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*
export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native
```</p>

<ul>
<li>开启SharkServer2，使用Supervisord管理：</li>
</ul>


<p><code>
[program:sharkserver2]
command = /opt/shark/bin/shark --service sharkserver2
autostart = true
autorestart = true
stdout_logfile = /var/log/sharkserver2.log
redirect_stderr = true
</code></p>

<p><code>bash
$ supervisorctl start sharkserver2
</code></p>

<ul>
<li>测试</li>
</ul>


<p><code>bash
$ /opt/shark/bin/beeline -u jdbc:hive2://one-843:10000 -n root
</code></p>

<h2>版本问题</h2>

<h3>背景</h3>

<h4>CDH</h4>

<p>CDH是对Hadoop生态链各组件的打包，每个CDH版本都会对应一组Hadoop组件的版本，如<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH-Version-and-Packaging-Information/cdhvd_topic_3.html">CDH4.5</a>的部分对应关系如下：</p>

<ul>
<li>Apache Hadoop: hadoop-2.0.0+1518</li>
<li>Apache Hive: hive-0.10.0+214</li>
<li>Hue: hue-2.5.0+182</li>
</ul>


<p>可以看到，CDH4.5对应的Hive版本是0.10.0，因此它的<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH4-Installation-Guide/cdh4ig_hive_metastore_configure.html">Metastore Server</a>使用的也是0.10.0版本的API。</p>

<h4>Spark</h4>

<p>Spark目前最高版本是0.9.1，CDH前不久推出了0.9.0的Parcel，使得安装过程变的简单得多。CDH5中对Spark做了深度集成，即可以用CM来直接控制Spark的服务，且支持Spark on YARN架构。</p>

<h4>Shark</h4>

<p>Shark是基于Spark的一款应用，可以简单地认为是将Hive的MapReduce引擎替换为了Spark。</p>

<p>Shark的一个特点的是需要使用特定的Hive版本——<a href="https://github.com/amplab/hive">AMPLab patched Hive</a>：</p>

<ul>
<li>Shark 0.8.x: AMPLab Hive 0.9.0</li>
<li>Shark 0.9.x: AMPLab Hive 0.11.0</li>
</ul>


<p>在0.9.0以前，我们需要手动下载AMPLab Hive的二进制包，并在Shark的环境变量中设置$HIVE_HOME。在0.9.1以后，AMPLab将该版本的Hive包上传至了Maven，可以直接打进Shark的二进制包中。但是，这个Jar是用JDK7编译的，因此运行Shark需要使用Oracle JDK7。CDH建议使用Update 45这个小版本。</p>

<h4>Shark与Hive的并存</h4>

<p>Shark的一个卖点是和Hive的<a href="5">高度兼容</a>，也就是说它可以直接操作Hive的metastore db，或是和metastore server通信。当然，前提是两者的Hive版本需要一致，这也是目前遇到的最大问题。</p>

<h3>目前发现的不兼容SQL</h3>

<ul>
<li>DROP TABLE &hellip;</li>
</ul>


<p><code>
FAILED: Error in metadata: org.apache.thrift.TApplicationException: Invalid method name: 'drop_table_with_environment_context'
</code></p>

<ul>
<li>INSERT OVERWRITE TABLE &hellip; PARTITION (&hellip;) SELECT &hellip;</li>
<li>LOAD DATA INPATH &lsquo;&hellip;&rsquo; OVERWRITE INTO TABLE &hellip; PARTITION (&hellip;)</li>
</ul>


<p><code>
Failed with exception org.apache.thrift.TApplicationException: Invalid method name: 'partition_name_has_valid_characters'
</code></p>

<p>也就是说上述两个方法名是0.11.0接口中定义的，在0.10.0的定义中并不存在，所以出现上述问题。</p>

<h3>解决方案</h3>

<h4>对存在问题的SQL使用Hive命令去调</h4>

<p>因为Shark初期是想给分析师使用的，他们对分区表并不是很在意，而DROP TABLE可以在客户端做判断，转而使用Hive来执行。</p>

<p>这个方案的优点是可以在现有集群上立刻用起来，但缺点是需要做一些额外的开发，而且API不一致的问题可能还会有其他坑在里面。</p>

<h4>升级到CDH5</h4>

<p>CDH5中Hive的版本是0.12.0，所以不排除同样存在API不兼容的问题。不过网上也有人尝试跳过AMPLab Hive，让Shark直接调用CDH中的Hive，其可行性还需要我们自己测试。</p>

<p>对于这个问题，我只在<a href="https://groups.google.com/forum/#!starred/shark-users/x_Dh5-3isIc">Google Groups</a>上看到一篇相关的帖子，不过并没有给出解决方案。</p>

<p>目前我们实施的是 <strong>第一种方案</strong>，即在客户端和Shark之间添加一层，不支持的SQL语句直接降级用Hive执行，效果不错。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use WebJars in Scalatra Project]]></title>
    <link href="http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project/"/>
    <updated>2014-05-27T17:44:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project</id>
    <content type="html"><![CDATA[<p>As I&rsquo;m working with my first <a href="http://www.scalatra.org/">Scalatra</a> project, I automatically think of using <a href="http://www.webjars.org/">WebJars</a> to manage Javascript library dependencies, since it&rsquo;s more convenient and seems like a good practice. Though there&rsquo;s no <a href="http://www.webjars.org/documentation">official support</a> for Scalatra framework, the installation process is not very complex. But this doesn&rsquo;t mean I didn&rsquo;t spend much time on this. I&rsquo;m still a newbie to Scala, and there&rsquo;s only a few materials on this subject.</p>

<h2>Add WebJars Dependency in SBT Build File</h2>

<p>Scalatra uses <code>.scala</code> configuration file instead of <code>.sbt</code>, so let&rsquo;s add dependency into <code>project/build.scala</code>. Take <a href="http://dojotoolkit.org/">Dojo</a> for example.</p>

<p>```scala
object DwExplorerBuild extends Build {
  &hellip;
  lazy val project = Project (</p>

<pre><code>...
settings = Defaults.defaultSettings ++ ScalatraPlugin.scalatraWithJRebel ++ scalateSettings ++ Seq(
  ...
  libraryDependencies ++= Seq(
    ...
    "org.webjars" % "dojo" % "1.9.3"
  ),
  ...
)
</code></pre>

<p>  )
}
```</p>

<p>To view this dependency in Eclipse, you need to run <code>sbt eclipse</code> again. In the <em>Referenced Libraries</em> section, you can see a <code>dojo-1.9.3.jar</code>, and the library lies in <code>META-INF/resources/webjars/</code>.</p>

<!-- more -->


<h2>Add a Route for WebJars Resources</h2>

<p>Find the <code>ProjectNameStack.scala</code> file and add the following lines at the bottom of the trait:</p>

<p>```scala
trait ProjectNameStack extends ScalatraServlet with ScalateSupport {
  &hellip;
  get(&ldquo;/webjars/*&rdquo;) {</p>

<pre><code>val resourcePath = "/META-INF/resources/webjars/" + params("splat")
Option(getClass.getResourceAsStream(resourcePath)) match {
  case Some(inputStream) =&gt; {
    contentType = servletContext.getMimeType(resourcePath)
    IOUtil.loadBytes(inputStream)
  }
  case None =&gt; resourceNotFound()
}
</code></pre>

<p>  }
}
```</p>

<p><strong>That&rsquo;s it!</strong> Now you can refer to the WebJars resources in views, like this:</p>

<p>```ssp</p>

<h1>set (title)</h1>

<p>Hello, Dojo!</p>

<h1>end</h1>

<div id="greeting"></div>




<script type="text/javascript" src="${uri("/webjars/dojo/1.9.3/dojo/dojo.js")}" data-dojo-config="async: true"></script>


<script type="text/javascript">
require([
    'dojo/dom',
    'dojo/dom-construct'
], function (dom, domConstruct) {
    var greetingNode = dom.byId('greeting');
    domConstruct.place('<i>Dojo!</i>', greetingNode);
});
</script>


<p>```</p>

<h3>Some Explanations on This Route</h3>

<ul>
<li><code>/webjars/*</code> is a <a href="http://www.scalatra.org/2.2/guides/http/routes.html#toc_233">Wildcards</a> and <code>params("splat")</code> is to extract the asterisk part.</li>
<li><code>resourcePath</code> points to the WebJars resources in the jar file, as we saw in Eclipse. It is then fetched as an <code>InputStream</code> with <code>getResourceAsStream()</code>.</li>
<li><code>servletContext.getMimeType()</code> is a handy method to determine the content type of the requested resource, instead of parsing it by ourselves. I find this in SpringMVC&rsquo;s <a href="http://grepcode.com/file/repo1.maven.org/maven2/org.springframework/spring-webmvc/3.2.7.RELEASE/org/springframework/web/servlet/resource/ResourceHttpRequestHandler.java#ResourceHttpRequestHandler.handleRequest%28javax.servlet.http.HttpServletRequest%2Cjavax.servlet.http.HttpServletResponse%29">ResourceHttpRequestHandler</a>.</li>
<li><code>IOUtil</code> is a utiliy class that comes with <a href="http://scalate.fusesource.org/">Scalate</a>, so don&rsquo;t forget to import it first.</li>
</ul>


<p>At first I tried to figure out whether Scalatra provides a conveniet way to serve static files in classpath, I failed. So I decided to serve them by my own, and <a href="https://gist.github.com/laurilehmijoki/4483113">this gist</a> was very helpful.</p>

<p>Anyway, I&rsquo;ve spent more than half a day to solve this problem, and it turned out to be a very challenging yet interesting way to learn a new language, new framework, and new tools. Keep moving!</p>
]]></content>
  </entry>
  
</feed>
