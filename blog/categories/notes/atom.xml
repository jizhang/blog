<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: notes | Ji ZHANG's Blog]]></title>
  <link href="http://shzhangji.com/blog/categories/notes/atom.xml" rel="self"/>
  <link href="http://shzhangji.com/"/>
  <updated>2015-09-20T17:26:35+08:00</updated>
  <id>http://shzhangji.com/</id>
  <author>
    <name><![CDATA[Ji ZHANG]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[使用Spring AOP向领域模型注入依赖]]></title>
    <link href="http://shzhangji.com/blog/2015/09/12/model-dependency-injection-with-spring-aop/"/>
    <updated>2015-09-12T22:03:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/09/12/model-dependency-injection-with-spring-aop</id>
    <content type="html"><![CDATA[<p>在<a href="{%%20post_url%202015-09-05-anemic-domain-model%20%}">贫血领域模型</a>这篇译文中，Martin阐述了这种“反模式”的症状和问题，并引用了领域驱动设计中的话来说明领域模型和分层设计之间的关系。对于Spring项目的开发人员来说，贫血领域模型十分常见：模型（或实体）仅仅包含对数据表的映射，通常是一组私有属性和公有getter/setter，所有的业务逻辑都写在服务层中，领域模型仅仅用来传递数据。为了编写真正的领域模型，我们需要将业务逻辑移至模型对象中，这就引出另一个问题：业务逻辑通常需要调用其他服务或模型，而使用<code>new</code>关键字或由JPA创建的对象是不受Spring托管的，也就无法进行依赖注入。解决这个问题的方法有很多，比较之后我选择使用面向切面编程来实现。</p>

<h2>面向切面编程</h2>

<p>面向切面编程，或<a href="https://en.wikipedia.org/wiki/Aspect-oriented_programming">AOP</a>，是一种编程范式，和面向对象编程（<a href="https://en.wikipedia.org/wiki/Object-oriented_programming">OOP</a>）互为补充。简单来说，AOP可以在不修改既有代码的情况下改变代码的行为。开发者通过定义一组规则，在特定的类方法前后增加逻辑，如记录日志、性能监控、事务管理等。这些逻辑称为切面（Aspect），规则称为切点（Pointcut），在调用前还是调用后执行称为通知（Before advice, After advice）。最后，我们可以选择在编译期将这些逻辑写入类文件，或是在运行时动态加载这些逻辑，这是两种不同的织入方式（Compile-time weaving, Load-time weaving）。</p>

<p>对于领域模型的依赖注入，我们要做的就是使用AOP在对象创建后调用Spring框架来注入依赖。幸运的是，<a href="http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop">Spring AOP</a>已经提供了<code>@Configurable</code>注解来帮助我们实现这一需求。</p>

<!-- more -->


<h2>Configurable注解</h2>

<p>Spring应用程序会定义一个上下文容器，在该容器内创建的对象会由Spring负责注入依赖。对于容器外创建的对象，我们可以使用<code>@Configurable</code>来修饰类，告知Spring对这些类的实例也进行依赖注入。</p>

<p>假设有一个<code>Report</code>类（领域模型），其中一个方法需要解析JSON，我们可以使用<code>@Configurable</code>将容器内的<code>ObjectMapper</code>对象注入到类的实例中：</p>

<pre><code class="java">@Entity
@Configurable(autowire = Autowire.BY_TYPE)
public class Report {

    @Id @GeneratedValue
    private Integer id;

    @Autowired @Transient
    private ObjectMapper mapper;

    public String render() {
        mapper.readValue(...);
    }

}
</code></pre>

<ul>
<li><code>autowire</code>参数默认是<code>NO</code>，因此需要显式打开，否则只能使用XML定义依赖。<code>@Autowired</code>是目前比较推荐的注入方式。</li>
<li><code>@Transient</code>用于告知JPA该属性不需要进行持久化。你也可以使用<code>transient</code>关键字来声明，效果相同。</li>
<li>项目依赖中需要包含<code>spring-aspects</code>。如果已经使用了<code>spring-boot-starter-data-jpa</code>，则无需配置。</li>
<li>应用程序配置中需要加入<code>@EnableSpringConfigured</code>：</li>
</ul>


<pre><code class="java">@SpringBootApplication
@EnableTransactionManagement(mode = AdviceMode.ASPECTJ)
@EnableSpringConfigured
public class Application {

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }

}
</code></pre>

<ul>
<li>在<code>src/main/resources</code>目录下，新建<code>META-INF/aop.xml</code>文件，用来限定哪些包会用到AOP。否则，AOP的织入操作会作用于所有的类（包括第三方类库），产生不必要的的报错信息。</li>
</ul>


<pre><code class="xml">&lt;!DOCTYPE aspectj PUBLIC "-//AspectJ//DTD//EN" "http://www.eclipse.org/aspectj/dtd/aspectj.dtd"&gt;

&lt;aspectj&gt;
    &lt;weaver&gt;
        &lt;include within="com.foobar..*"/&gt;
    &lt;/weaver&gt;
&lt;/aspectj&gt;
</code></pre>

<h2>运行时织入（Load-Time Weaving, LTW）</h2>

<p>除了项目依赖和应用程序配置，我们还需要选择一种织入方式来使AOP生效。Spring AOP推荐的方式是运行时织入，并提供了一个专用的Jar包。运行时织入的原理是：当类加载器在读取类文件时，动态修改类的字节码。这一机制是从<a href="http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/instrument/package-summary.html">JDK1.5</a>开始提供的，需要使用<code>-javaagent</code>参数开启，如：</p>

<pre><code class="bash">$ java -javaagent:/path/to/spring-instrument.jar -jar app.jar
</code></pre>

<p>在测试时发现，Spring AOP提供的这一Jar包对普通的类是有效果的，但对于使用<code>@Entity</code>修饰的类就没有作用了。因此，我们改用AspectJ提供的Jar包（可到<a href="http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.aspectj%22%20AND%20a%3A%22aspectjweaver%22">Maven中央仓库</a>下载）：</p>

<pre><code class="bash">$ java -javaagent:/path/to/aspectjweaver.jar -jar app.jar
</code></pre>

<p>对于<a href="http://projects.spring.io/spring-boot/">Spring Boot</a>应用程序，可以在Maven命令中加入以下参数：</p>

<pre><code class="bash">$ mvn spring-boot:run -Drun.agent=/path/to/aspectjweaver.jar
</code></pre>

<p>此外，在使用AspectJ作为LTW的提供方后，会影响到Spring的事务管理，因此需要在应用程序配置中加入：</p>

<pre><code class="java">@EnableTransactionManagement(mode = AdviceMode.ASPECTJ)
</code></pre>

<h2>AnnotationBeanConfigurerAspect</h2>

<p>到这里我们已经通过简单配置完成了领域模型的依赖注入，这背后都是Spring中的<code>AnnotationBeanConfigurerAspect</code>在做工作。我们不妨浏览一下精简后的源码：</p>

<p><a href="https://github.com/spring-projects/spring-framework/blob/master/spring-aspects/src/main/java/org/springframework/beans/factory/aspectj/AnnotationBeanConfigurerAspect.aj">AnnotationBeanConfigurerAspect.aj</a></p>

<pre><code class="aj">public aspect AnnotationBeanConfigurerAspect implements BeanFactoryAware {

    private BeanConfigurerSupport beanConfigurerSupport = new BeanConfigurerSupport();

    public void setBeanFactory(BeanFactory beanFactory) {
        this.beanConfigurerSupport.setBeanFactory(beanFactory);
    }

    public void configureBean(Object bean) {
        this.beanConfigurerSupport.configureBean(bean);
    }

    public pointcut inConfigurableBean() : @this(Configurable);

    declare parents: @Configurable * implements ConfigurableObject;

    public pointcut beanConstruction(Object bean) :
            initialization(ConfigurableObject+.new(..)) &amp;&amp; this(bean);

    after(Object bean) returning :
        beanConstruction(bean) &amp;&amp; inConfigurableBean() {
        configureBean(bean);
    }
}
</code></pre>

<ul>
<li><code>.aj</code>文件是AspectJ定义的语言，增加了pointcut、after等关键字，用来定义切点、通知等；</li>
<li><code>inConfigurationBean</code>切点用于匹配使用<code>Configurable</code>修饰的类型；</li>
<li><code>declare parents</code>将这些类型声明为<code>ConfigurableObject</code>接口，从而匹配<code>beanConstruction</code>切点；</li>
<li><code>ConfigurableObject+.new(..)</code>表示匹配该类型所有的构造函数；</li>
<li><code>after</code>定义一个通知，表示对象创建完成后执行<code>configureBean</code>方法；</li>
<li>该方法会调用<code>BeanConfigurerSupport</code>来对新实例进行依赖注入。</li>
</ul>


<h2>其它方案</h2>

<ol>
<li>将依赖作为参数传入。比如上文中的<code>render</code>方法可以定义为<code>render(ObjectMapper mapper)</code>。</li>
<li>将<code>ApplicationContext</code>作为某个类的静态成员，领域模型通过这个引用来获取依赖。</li>
<li>编写一个工厂方法，所有新建对象都要通过这个方法生成，进行依赖注入。</li>
<li>如果领域模型大多从数据库获得，并且JPA的提供方是Hibernate，则可以使用它的拦截器功能进行依赖注入。</li>
</ol>


<h2>参考资料</h2>

<ul>
<li><a href="http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop-atconfigurable">http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop-atconfigurable</a></li>
<li><a href="http://blog.igorstoyanov.com/2005/12/dependency-injection-or-service.html">http://blog.igorstoyanov.com/2005/12/dependency-injection-or-service.html</a></li>
<li><a href="http://jblewitt.com/blog/?p=129">http://jblewitt.com/blog/?p=129</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[View Spark Source in Eclipse]]></title>
    <link href="http://shzhangji.com/blog/2015/09/01/view-spark-source-in-eclipse/"/>
    <updated>2015-09-01T18:38:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/09/01/view-spark-source-in-eclipse</id>
    <content type="html"><![CDATA[<p>Reading source code is a great way to learn opensource projects. I used to read Java projects' source code on <a href="http://grepcode.com/">GrepCode</a> for it is online and has very nice cross reference features. As for Scala projects such as <a href="http://spark.apache.org">Apache Spark</a>, though its source code can be found on <a href="https://github.com/apache/spark/">GitHub</a>, it&rsquo;s quite necessary to setup an IDE to view the code more efficiently. Here&rsquo;s a howto of viewing Spark source code in Eclipse.</p>

<h2>Install Eclipse and Scala IDE Plugin</h2>

<p>One can download Eclipse from <a href="http://www.eclipse.org/downloads/">here</a>. I recommend the &ldquo;Eclipse IDE for Java EE Developers&rdquo;, which contains a lot of daily-used features.</p>

<p><img src="/images/scala-ide.png" alt="" /></p>

<p>Then go to Scala IDE&rsquo;s <a href="http://scala-ide.org/download/current.html">official site</a> and install the plugin through update site or zip archive.</p>

<h2>Generate Project File with Maven</h2>

<p>Spark is mainly built with Maven, so make sure you have Maven installed on your box, and download the latest Spark source code from <a href="http://spark.apache.org/downloads.html">here</a>, unarchive it, and execute the following command:</p>

<pre><code class="bash">$ mvn -am -pl core dependency:resolve eclipse:eclipse
</code></pre>

<!-- more -->


<p>This command does a bunch of things. First, it indicates what modules should be built. Spark is a large project with multiple modules. Currently we&rsquo;re only interested in its core module, so <code>-pl</code> or <code>--projects</code> is used. <code>-am</code> or <code>--also-make</code> tells Maven to build core module&rsquo;s dependencies as well. We can see the module list in output:</p>

<pre><code class="text">[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO]
[INFO] Spark Project Parent POM
[INFO] Spark Launcher Project
[INFO] Spark Project Networking
[INFO] Spark Project Shuffle Streaming Service
[INFO] Spark Project Unsafe
[INFO] Spark Project Core
</code></pre>

<p><code>dependency:resolve</code> tells Maven to download all dependencies. <code>eclipse:eclipse</code> will generate the <code>.project</code> and <code>.classpath</code> files for Eclipse. But the result is not perfect, both files need some fixes.</p>

<p>Edit <code>core/.classpath</code>, change the following two lines:</p>

<pre><code class="xml">&lt;classpathentry kind="src" path="src/main/scala" including="**/*.java"/&gt;
&lt;classpathentry kind="src" path="src/test/scala" output="target/scala-2.10/test-classes" including="**/*.java"/&gt;
</code></pre>

<p>to</p>

<pre><code class="xml">&lt;classpathentry kind="src" path="src/main/scala" including="**/*.java|**/*.scala"/&gt;
&lt;classpathentry kind="src" path="src/test/scala" output="target/scala-2.10/test-classes" including="**/*.java|**/*.scala"/&gt;
</code></pre>

<p>Edit <code>core/.project</code>, make it looks like this:</p>

<pre><code class="xml">&lt;buildSpec&gt;
  &lt;buildCommand&gt;
    &lt;name&gt;org.scala-ide.sdt.core.scalabuilder&lt;/name&gt;
  &lt;/buildCommand&gt;
&lt;/buildSpec&gt;
&lt;natures&gt;
  &lt;nature&gt;org.scala-ide.sdt.core.scalanature&lt;/nature&gt;
  &lt;nature&gt;org.eclipse.jdt.core.javanature&lt;/nature&gt;
&lt;/natures&gt;
</code></pre>

<p>Now you can import &ldquo;Existing Projects into Workspace&rdquo;, including <code>core</code>, <code>launcher</code>, <code>network</code>, and <code>unsafe</code>.</p>

<h2>Miscellaneous</h2>

<h3>Access restriction: The type &lsquo;Unsafe&rsquo; is not API</h3>

<p>For module <code>spark-unsafe</code>, Eclipse will report an error &ldquo;Access restriction: The type &lsquo;Unsafe&rsquo; is not API (restriction on required library /path/to/jre/lib/rt.jar&rdquo;. To fix this, right click the &ldquo;JRE System Library&rdquo; entry in Package Explorer, change it to &ldquo;Workspace default JRE&rdquo;.</p>

<h3>Download Sources and Javadocs</h3>

<p>Add the following entry into pom&rsquo;s project / build / plugins:</p>

<pre><code class="xml">&lt;plugin&gt;
    &lt;artifactId&gt;maven-eclipse-plugin&lt;/artifactId&gt;
    &lt;configuration&gt;
        &lt;downloadSources&gt;true&lt;/downloadSources&gt;
        &lt;downloadJavadocs&gt;true&lt;/downloadJavadocs&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;
</code></pre>

<h3>build-helper-maven-plugin</h3>

<p>Since Spark is a mixture of Java and Scala code, and the maven-eclipse-plugin only knows about Java source files, so we need to use build-helper-maven-plugin to include the Scala sources, as is described <a href="http://docs.scala-lang.org/tutorials/scala-with-maven.html#integration-with-eclipse-scala-ide24">here</a>. Fortunately, Spark&rsquo;s pom.xml has already included this setting.</p>

<h2>References</h2>

<ul>
<li><a href="http://docs.scala-lang.org/tutorials/scala-with-maven.html">http://docs.scala-lang.org/tutorials/scala-with-maven.html</a></li>
<li><a href="https://wiki.scala-lang.org/display/SIW/ScalaEclipseMaven">https://wiki.scala-lang.org/display/SIW/ScalaEclipseMaven</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools">https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Streaming Logging Configuration]]></title>
    <link href="http://shzhangji.com/blog/2015/05/31/spark-streaming-logging-configuration/"/>
    <updated>2015-05-31T18:18:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/05/31/spark-streaming-logging-configuration</id>
    <content type="html"><![CDATA[<p>Spark Streaming applications tend to run forever, so their log files should be properly handled, to avoid exploding server hard drives. This article will give some practical advices of dealing with these log files, on both Spark on YARN and standalone mode.</p>

<h2>Log4j&rsquo;s RollingFileAppender</h2>

<p>Spark uses log4j as logging facility. The default configuraiton is to write all logs into standard error, which is fine for batch jobs. But for streaming jobs, we&rsquo;d better use rolling-file appender, to cut log files by size and keep only several recent files. Here&rsquo;s an example:</p>

<pre><code class="properties">log4j.rootLogger=INFO, rolling

log4j.appender.rolling=org.apache.log4j.RollingFileAppender
log4j.appender.rolling.layout=org.apache.log4j.PatternLayout
log4j.appender.rolling.layout.conversionPattern=[%d] %p %m (%c)%n
log4j.appender.rolling.maxFileSize=50MB
log4j.appender.rolling.maxBackupIndex=5
log4j.appender.rolling.file=/var/log/spark/${dm.logging.name}.log
log4j.appender.rolling.encoding=UTF-8

log4j.logger.org.apache.spark=WARN
log4j.logger.org.eclipse.jetty=WARN

log4j.logger.com.anjuke.dm=${dm.logging.level}
</code></pre>

<p>This means log4j will roll the log file by 50MB and keep only 5 recent files. These files are saved in <code>/var/log/spark</code> directory, with filename picked from system property <code>dm.logging.name</code>. We also set the logging level of our package <code>com.anjuke.dm</code> according to <code>dm.logging.level</code> property. Another thing to mention is that we set <code>org.apache.spark</code> to level <code>WARN</code>, so as to ignore verbose logs from spark.</p>

<!-- more -->


<h2>Standalone Mode</h2>

<p>In standalone mode, Spark Streaming driver is running on the machine where you submit the job, and each Spark worker node will run an executor for this job. So you need to setup log4j for both driver and executor.</p>

<p>For driver, since it&rsquo;s a long-running application, we tend to use some process management tools like <a href="http://supervisord.org/">supervisor</a> to monitor it. And supervisor itself provides the facility of rolling log files, so we can safely write all logs into standard output when setting up driver&rsquo;s log4j.</p>

<p>For executor, there&rsquo;re two approaches. One is using <code>spark.executor.logs.rolling.strategy</code> provided by Spark 1.1 and above. It has both time-based and size-based rolling methods. These log files are stored in Spark&rsquo;s work directory. You can find more details in the <a href="https://spark.apache.org/docs/1.1.0/configuration.html">documentation</a>.</p>

<p>The other approach is to setup log4j manually, when you&rsquo;re using a legacy version, or want to gain more control on the logging process. Here are the steps:</p>

<ol>
<li>Make sure the logging directory exists on all worker nodes. You can use some provisioning tools like <a href="https://github.com/ansible/ansible">ansbile</a> to create them.</li>
<li>Create driver&rsquo;s and executor&rsquo;s log4j configuration files, and distribute the executor&rsquo;s to all worker nodes.</li>
<li>Use the above two files in <code>spark-submit</code> command:</li>
</ol>


<pre><code>spark-submit
  --master spark://127.0.0.1:7077
  --driver-java-options "-Dlog4j.configuration=file:/path/to/log4j-driver.properties -Ddm.logging.level=DEBUG"
  --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/path/to/log4j-executor.properties -Ddm.logging.name=myapp -Ddm.logging.level=DEBUG"
  ...
</code></pre>

<h2>Spark on YARN</h2>

<p><a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/index.html">YARN</a> is a <strong>resource manager</strong> introduced by Hadoop2. Now we can run differenct computational frameworks on the same cluster, like MapReduce, Spark, Storm, etc. The basic unit of YARN is called container, which represents a certain amount of resource (currently memory and virtual CPU cores). Every container has its working directory, and all related files such as application command (jars) and log files are stored in this directory.</p>

<p>When running Spark on YARN, there is a system property <code>spark.yarn.app.container.log.dir</code> indicating the container&rsquo;s log directory. We only need to replace one line of the above log4j config:</p>

<pre><code class="properties">log4j.appender.rolling.file=${spark.yarn.app.container.log.dir}/spark.log
</code></pre>

<p>And these log files can be viewed on YARN&rsquo;s web UI:</p>

<p><img src="/images/spark/yarn-logs.png" alt="" /></p>

<p>The <code>spark-submit</code> command is as following:</p>

<pre><code>spark-submit
  --master yarn-cluster
  --files /path/to/log4j-spark.properties
  --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j-spark.properties"
  --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=log4j-spark.properties"
  ...
</code></pre>

<p>As you can see, both driver and executor use the same configuration file. That is because in <code>yarn-cluster</code> mode, driver is also run as a container in YARN. In fact, the <code>spark-submit</code> command will just quit after job submission.</p>

<p>If YARN&rsquo;s <a href="http://zh.hortonworks.com/blog/simplifying-user-logs-management-and-access-in-yarn/">log aggregation</a> is enabled, application logs will be saved in HDFS after the job is done. One can use <code>yarn logs</code> command to view the files or browse directly into HDFS directory indicated by <code>yarn.nodemanager.log-dirs</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ElasticSearch Performance Tips]]></title>
    <link href="http://shzhangji.com/blog/2015/04/28/elasticsearch-performance-tips/"/>
    <updated>2015-04-28T23:08:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/04/28/elasticsearch-performance-tips</id>
    <content type="html"><![CDATA[<p>Recently we&rsquo;re using ElasticSearch as a data backend of our recommendation API, to serve both offline and online computed data to users. Thanks to ElasticSearch&rsquo;s rich and out-of-the-box functionality, it doesn&rsquo;t take much trouble to setup the cluster. However, we still encounter some misuse and unwise configurations. So here&rsquo;s a list of ElasticSearch performance tips that we learned from practice.</p>

<h2>Tip 1 Set Num-of-shards to Num-of-nodes</h2>

<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard">Shard</a> is the foundation of ElasticSearch&rsquo;s distribution capability. Every index is splitted into several shards (default 5) and are distributed across cluster nodes. But this capability does not come free. Since data being queried reside in all shards (this behaviour can be changed by <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-routing">routing</a>), ElasticSearch has to run this query on every shard, fetch the result, and merge them, like a map-reduce process. So if there&rsquo;re too many shards, more than the number of cluter nodes, the query will be executed more than once on the same node, and it&rsquo;ll also impact the merge phase. On the other hand, too few shards will also reduce the performance, for not all nodes are being utilized.</p>

<p>Shards have two roles, primary shard and replica shard. Replica shard serves as a backup to the primary shard. When primary goes down, the replica takes its job. It also helps improving the search and get performance, for these requests can be executed on either primary or replica shard.</p>

<p>Shards can be visualized by <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard">elasticsearch-head</a> plugin:</p>

<p><img src="/images/elasticsearch/shards-head.png" alt="" /></p>

<p>The <code>cu_docs</code> index has two shards <code>0</code> and <code>1</code>, with <code>number_of_replicas</code> set to 1. Primary shard <code>0</code> (bold bordered) resides in server <code>Leon</code>, and its replica in <code>Pris</code>. They are green becuase all primary shards have enough repicas sitting in different servers, so the cluster is healthy.</p>

<p>Since <code>number_of_shards</code> of an index cannot be changed after creation (while <code>number_of_replicas</code> can), one should choose this config wisely. Here are some suggestions:</p>

<ol>
<li>How many nodes do you have, now and future? If you&rsquo;re sure you&rsquo;ll only have 3 nodes, set number of shards to 2 and replicas to 1, so there&rsquo;ll be 4 shards across 3 nodes. If you&rsquo;ll add some servers in the future, you can set number of shards to 3, so when the cluster grows to 5 nodes, there&rsquo;ll be 6 distributed shards.</li>
<li>How big is your index? If it&rsquo;s small, one shard with one replica will due.</li>
<li>How is the read and write frequency, respectively? If it&rsquo;s search heavy, setup more relicas.</li>
</ol>


<!-- more -->


<h2>Tip 2 Tuning Memory Usage</h2>

<p>ElasticSearch and its backend <a href="http://lucene.apache.org/">Lucene</a> are both Java application. There&rsquo;re various memory tuning settings related to heap and native memory.</p>

<h3>Set Max Heap Size to Half of Total Memory</h3>

<p>Generally speaking, more heap memory leads to better performance. But in ElasticSearch&rsquo;s case, Lucene also requires a lot of native memory (or off-heap memory), to store index segments and provide fast search performance. But it does not load the files by itself. Instead, it relies on the operating system to cache the segement files in memory.</p>

<p>Say we have 16G memory and set -Xmx to 8G, it doesn&rsquo;t mean the remaining 8G is wasted. Except for the memory OS preserves for itself, it will cache the frequently accessed disk files in memory automatically, which results in a huge performance gain.</p>

<p>Do not set heap size over 32G though, even you have more than 64G memory. The reason is described in <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops">this link</a>.</p>

<p>Also, you should probably set -Xms to 8G as well, to avoid the overhead of heap memory growth.</p>

<h3>Disable Swapping</h3>

<p>Swapping is a way to move unused program code and data to disk so as to provide more space for running applications and file caching. It also provides a buffer for the system to recover from memory exhaustion. But for critical application like ElasticSearch, being swapped is definitely a performance killer.</p>

<p>There&rsquo;re several ways to disable swapping, and our choice is setting <code>bootstrap.mlockall</code> to true. This tells ElasticSearch to lock its memory space in RAM so that OS will not swap it out. One can confirm this setting via <code>http://localhost:9200/_nodes/process?pretty</code>.</p>

<p>If ElasticSearch is not started as root (and it probably shouldn&rsquo;t), this setting may not take effect. For Ubuntu server, one needs to add <code>&lt;user&gt; hard memlock unlimited</code> to <code>/etc/security/limits.conf</code>, and run <code>ulimit -l unlimited</code> before starting ElasticSearch process.</p>

<h3>Increase <code>mmap</code> Counts</h3>

<p>ElasticSearch uses memory mapped files, and the default <code>mmap</code> counts is low. Add <code>vm.max_map_count=262144</code> to <code>/etc/sysctl.conf</code>, run <code>sysctl -p /etc/sysctl.conf</code> as root, and then restart ElasticSearch.</p>

<h2>Tip 3 Setup a Cluster with Unicast</h2>

<p>ElasticSearch has two options to form a cluster, multicast and unicast. The former is suitable when you have a large group of servers and a well configured network. But we found unicast more concise and less error-prone.</p>

<p>Here&rsquo;s an example of using unicast:</p>

<pre><code>node.name: "NODE-1"
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["node-1.example.com", "node-2.example.com", "node-3.example.com"]
discovery.zen.minimum_master_nodes: 2
</code></pre>

<p>The <code>discovery.zen.minimum_master_nodes</code> setting is a way to prevent split-brain symptom, i.e. more than one node thinks itself the master of the cluster. And for this setting to work, you should have an odd number of nodes, and set this config to <code>ceil(num_of_nodes / 2)</code>. In the above cluster, you can lose at most one node. It&rsquo;s much like a quorum in <a href="http://zookeeper.apache.org">Zookeeper</a>.</p>

<h2>Tip 4 Disable Unnecessary Features</h2>

<p>ElasticSearch is a full-featured search engine, but you should always tailor it to your own needs. Here&rsquo;s a brief list:</p>

<ul>
<li>Use corrent index type. There&rsquo;re <code>index</code>, <code>not_analyzed</code>, and <code>no</code>. If you don&rsquo;t need to search the field, set it to <code>no</code>; if you only search for full match, use <code>not_analyzed</code>.</li>
<li>For search-only fields, set <code>store</code> to false.</li>
<li>Disable <code>_all</code> field, if you always know which field to search.</li>
<li>Disable <code>_source</code> fields, if documents are big and you don&rsquo;t need the update capability.</li>
<li>If you have a document key, set this field in <code>_id</code> - <code>path</code>, instead of index the field twice.</li>
<li>Set <code>index.refresh_interval</code> to a larger number (default 1s), if you don&rsquo;t need near-realtime search. It&rsquo;s also an important option in bulk-load operation described below.</li>
</ul>


<h2>Tip 5 Use Bulk Operations</h2>

<p><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/bulk.html">Bulk is cheaper</a></p>

<ul>
<li>Bulk Read

<ul>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html">Multi Get</a> to retrieve multiple documents by a list of ids.</li>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html">Scroll</a> to search a large number of documents.</li>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/1.4/msearch.html">MultiSearch api</a> to run search requests in parallel.</li>
</ul>
</li>
<li>Bulk Write

<ul>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/1.4/bulk.html">Bulk API</a> to index, update, delete multiple documents.</li>
<li>Alter <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html">index aliases</a> simultaneously.</li>
</ul>
</li>
<li>Bulk Load: when initially building a large index, do the following,

<ul>
<li>Set <code>number_of_relicas</code> to 0, so no relicas will be created;</li>
<li>Set <code>index.refresh_interval</code> to -1, disabling nrt search;</li>
<li>Bulk build the documents;</li>
<li>Call <code>optimize</code> on the index, so newly built docs are available for search;</li>
<li>Reset replicas and refresh interval, let ES cluster recover to green.</li>
</ul>
</li>
</ul>


<h2>Miscellaneous</h2>

<ul>
<li>File descriptors: system default is too small for ES, set it to 64K will be OK. If <code>ulimit -n 64000</code> does not work, you need to add <code>&lt;user&gt; hard nofile 64000</code> to <code>/etc/security/limits.conf</code>, just like the <code>memlock</code> setting mentioned above.</li>
<li>When using ES client library, it will create a lot of worker threads according to the number of processors. Sometimes it&rsquo;s not necessary. This behaviour can be changed by setting <code>processors</code> to a lower value like 2:</li>
</ul>


<pre><code class="scala">val settings = ImmutableSettings.settingsBuilder()
    .put("cluster.name", "elasticsearch")
    .put("processors", 2)
    .build()
val uri = ElasticsearchClientUri("elasticsearch://127.0.0.1:9300")
ElasticClient.remote(settings, uri)
</code></pre>

<h2>References</h2>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html">https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html</a></li>
<li><a href="http://cpratt.co/how-many-shards-should-elasticsearch-indexes-have/">http://cpratt.co/how-many-shards-should-elasticsearch-indexes-have/</a></li>
<li><a href="https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing">https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing</a></li>
<li><a href="https://www.loggly.com/blog/nine-tips-configuring-elasticsearch-for-high-performance/">https://www.loggly.com/blog/nine-tips-configuring-elasticsearch-for-high-performance/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用git rebase让历史变得清晰]]></title>
    <link href="http://shzhangji.com/blog/2014/12/23/use-git-rebase-to-clarify-history/"/>
    <updated>2014-12-23T16:10:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/12/23/use-git-rebase-to-clarify-history</id>
    <content type="html"><![CDATA[<p>当多人协作开发一个分支时，历史记录通常如下方左图所示，比较凌乱。如果希望能像右图那样呈线性提交，就需要学习git rebase的用法。</p>

<p><img src="/images/git-rebase/rebase-result.png" alt="" /></p>

<h2>“Merge branch”提交的产生</h2>

<p>我们的工作流程是：修改代码→提交到本地仓库→拉取远程改动→推送。正是在git pull这一步产生的Merge branch提交。事实上，git pull等效于get fetch origin和get merge origin/master这两条命令，前者是拉取远程仓库到本地临时库，后者是将临时库中的改动合并到本地分支中。</p>

<p>要避免Merge branch提交也有一个“土法”：先pull、再commit、最后push。不过万一commit和push之间远程又发生了改动，还需要再pull一次，就又会产生Merge branch提交。</p>

<h2>使用git pull &ndash;rebase</h2>

<p>修改代码→commit→git pull &ndash;rebase→git push。也就是将get merge origin/master替换成了git rebase origin/master，它的过程是先将HEAD指向origin/master，然后逐一应用本地的修改，这样就不会产生Merge branch提交了。具体过程见下文扩展阅读。</p>

<!-- more -->


<p>使用git rebase是有条件的，你的本地仓库要“足够干净”。可以用git status命令查看当前改动：：</p>

<pre><code>$ git status
On branch master
Your branch is up-to-date with 'origin/master'.
nothing to commit, working directory clean
</code></pre>

<p>本地没有任何未提交的改动，这是最“干净”的。稍差一些的是这样：</p>

<pre><code>$ git status
On branch master
Your branch is up-to-date with 'origin/master'.
Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
    test.txt
nothing added to commit but untracked files present (use "git add" to track)
</code></pre>

<p>即本地只有新增文件未提交，没有改动文件。我们应该尽量保持本地仓库的“整洁”，这样才能顺利使用git rebase。特殊情况下也可以用git stash来解决问题，有兴趣的可自行搜索。</p>

<h2>修改git pull的默认行为</h2>

<p>每次都加&ndash;rebase似乎有些麻烦，我们可以指定某个分支在执行git pull时默认采用rebase方式：</p>

<pre><code>$ git config branch.master.rebase true
</code></pre>

<p>如果你觉得所有的分支都应该用rebase，那就设置：</p>

<pre><code>$ git config --global branch.autosetuprebase always
</code></pre>

<p>这样对于新建的分支都会设定上面的rebase=true了。已经创建好的分支还是需要手动配置的。</p>

<h2>扩展阅读[1]：git rebase工作原理</h2>

<p>先看看git merge的示意图：</p>

<p><img src="/images/git-rebase/merge.png" alt="" /></p>

<p><a href="https://www.atlassian.com/ja/git/tutorial/git-branches">图片来源</a></p>

<p>可以看到Some Feature分支的两个提交通过一个新的提交（蓝色）和master连接起来了。</p>

<p>再来看git rebase的示意图：</p>

<p><img src="/images/git-rebase/rebase-1.png" alt="" /></p>

<p><img src="/images/git-rebase/rebase-2.png" alt="" /></p>

<p>Feature分支中的两个提交被“嫁接”到了Master分支的头部，或者说Feature分支的“基”（base）变成了 Master，rebase也因此得名。</p>

<h2>扩展阅读[2]：git merge &ndash;no-ff</h2>

<p>在做项目开发时会用到分支，合并时采用以下步骤：</p>

<pre><code>$ git checkout feature-branch
$ git rebase master
$ git checkout master
$ git merge --no-ff feature-branch
$ git push origin master
</code></pre>

<p>历史就成了这样：</p>

<p><img src="/images/git-rebase/no-ff.png" alt="" /></p>

<p>可以看到，Merge branch &lsquo;feature-branch'那段可以很好的展现出这些提交是属于某一特性的。</p>
]]></content>
  </entry>
  
</feed>
