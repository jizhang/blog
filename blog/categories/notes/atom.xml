<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Notes | Ji ZHANG's Blog]]></title>
  <link href="http://shzhangji.com/blog/categories/notes/atom.xml" rel="self"/>
  <link href="http://shzhangji.com/"/>
  <updated>2014-11-27T12:07:29+08:00</updated>
  <id>http://shzhangji.com/</id>
  <author>
    <name><![CDATA[Ji ZHANG]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[离线环境下构建sbt项目]]></title>
    <link href="http://shzhangji.com/blog/2014/11/07/sbt-offline/"/>
    <updated>2014-11-07T15:02:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/11/07/sbt-offline</id>
    <content type="html"><![CDATA[<p>在公司网络中使用<a href="http://www.scala-sbt.org/">sbt</a>、<a href="http://maven.apache.org/">Maven</a>等项目构建工具时，我们通常会搭建一个公用的<a href="http://www.sonatype.org/nexus/">Nexus</a>镜像服务，原因有以下几个：</p>

<ul>
<li>避免重复下载依赖，节省公司带宽；</li>
<li>国内网络环境不理想，下载速度慢；</li>
<li>IDC服务器没有外网访问权限；</li>
<li>用于发布内部模块。</li>
</ul>


<p>sbt的依赖管理基于<a href="http://ant.apache.org/ivy/">Ivy</a>，虽然它能直接使用<a href="http://search.maven.org/">Maven中央仓库</a>中的Jar包，在配置时还是有一些注意事项的。</p>

<!-- more -->


<h2>配置Nexus镜像</h2>

<p>根据这篇<a href="http://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html">官方文档</a>的描述，Ivy和Maven在依赖管理方面有些许差异，因此不能直接将两者的镜像仓库配置成一个，而需分别建立两个虚拟镜像组。</p>

<p><img src="http://www.scala-sbt.org/0.13/docs/files/proxy-ivy-mvn-setup.png" alt="" /></p>

<p>安装Nexus后默认会有一个Public Repositories组，可以将其作为Maven的镜像组，并添加一些常用的第三方镜像：</p>

<ul>
<li>cloudera: <a href="https://repository.cloudera.com/artifactory/cloudera-repos/">https://repository.cloudera.com/artifactory/cloudera-repos/</a></li>
<li>spring: <a href="http://repo.springsource.org/libs-release-remote/">http://repo.springsource.org/libs-release-remote/</a></li>
<li>scala-tools: <a href="https://oss.sonatype.org/content/groups/scala-tools/">https://oss.sonatype.org/content/groups/scala-tools/</a></li>
</ul>


<p>对于Ivy镜像，我们创建一个新的虚拟组：ivy-releases，并添加以下两个镜像：</p>

<ul>
<li>type-safe: <a href="http://repo.typesafe.com/typesafe/ivy-releases/">http://repo.typesafe.com/typesafe/ivy-releases/</a></li>
<li>sbt-plugin: <a href="http://dl.bintray.com/sbt/sbt-plugin-releases/">http://dl.bintray.com/sbt/sbt-plugin-releases/</a></li>
</ul>


<p>对于sbt-plugin，由于一些原因，Nexus会将其置为Automatically Blocked状态，因此要在配置中将这个选项关闭，否则将无法下载远程的依赖包。</p>

<h2>配置sbt</h2>

<p>为了让sbt使用Nexus镜像，需要创建一个~/.sbt/repositories文件，内容为：</p>

<p><code>
[repositories]
  local
  my-ivy-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]
  my-maven-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/public/
</code></p>

<p>这样配置对大部分项目来说是足够了。但是有些项目会在构建描述文件中添加其它仓库，我们需要覆盖这种行为，方法是：</p>

<p><code>bash
$ sbt -Dsbt.override.build.repos=true
</code></p>

<p>你也可以通过设置SBT_OPTS环境变量来进行全局配置。</p>

<p>经过以上步骤，sbt执行过程中就不需要访问外网了，因此速度会有很大提升。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL异常UTF-8字符的处理]]></title>
    <link href="http://shzhangji.com/blog/2014/10/14/mysql-incorrent-utf8-value/"/>
    <updated>2014-10-14T13:16:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/10/14/mysql-incorrent-utf8-value</id>
    <content type="html"><![CDATA[<p>ETL流程中，我们会将Hive中的数据导入MySQL——先用Hive命令行将数据保存为文本文件，然后用MySQL的LOAD DATA语句进行加载。最近有一张表在加载到MySQL时会报以下错误：</p>

<p><code>
Incorrect string value: '\xF0\x9D\x8C\x86' for column ...
</code></p>

<p>经查，这个字段中保存的是用户聊天记录，因此会有一些表情符号。这些符号在UTF-8编码下需要使用4个字节来记录，而MySQL中的utf8编码只支持3个字节，因此无法导入。</p>

<p>根据UTF-8的编码规范，3个字节支持的Unicode字符范围是U+0000–U+FFFF，因此可以在Hive中对数据做一下清洗：</p>

<p><code>sql
SELECT REGEXP_REPLACE(content, '[^\\u0000-\\uFFFF]', '') FROM ...
</code></p>

<p>这样就能排除那些需要使用3个以上字节来记录的字符了，从而成功导入MySQL。</p>

<p>以下是一些详细说明和参考资料。</p>

<!-- more -->


<h2>Unicode字符集和UTF编码</h2>

<p><a href="http://en.wikipedia.org/wiki/Unicode">Unicode字符集</a>是一种将全球所有文字都囊括在内的字符集，从而实现跨语言、跨平台的文字信息交换。它由<a href="http://en.wikipedia.org/wiki/Plane_(Unicode)#Basic_Multilingual_Plane">基本多语平面（BMP）</a>和多个扩展平面（non-BMP）组成。前者的编码范围是U+0000-U+FFFF，包括了绝大多数现代语言文字，因此最为常用。</p>

<p><a href="http://en.wikipedia.org/wiki/Unicode#Unicode_Transformation_Format_and_Universal_Character_Set">UTF</a>则是一种编码格式，负责将Unicode字符对应的编号转换为计算机可以识别的二进制数据，进行保存和读取。</p>

<p>比如，磁盘上记录了以下二进制数据：</p>

<p><code>
1101000 1100101 1101100 1101100 1101111
</code></p>

<p>读取它的程序知道这是以UTF-8编码保存的字符串，因此将其解析为以下编号：</p>

<p><code>
104 101 108 108 111
</code></p>

<p>又因为UTF-8编码对应的字符集是Unicode，所以上面这五个编号对应的字符便是“hello”。</p>

<p>很多人会将Unicode和UTF混淆，但两者并不具可比性，它们完成的功能是不同的。</p>

<h2>UTF-8编码</h2>

<p>UTF编码家族也有很多成员，其中<a href="http://en.wikipedia.org/wiki/UTF-8">UTF-8</a>最为常用。它是一种变长的编码格式，对于ASCII码中的字符使用1个字节进行编码，对于中文等则使用3个字节。这样做的优点是在存储西方语言文字时不会造成空间浪费，不像UTF-16和UTF-32，分别使用两个字节和四个字节对所有字符进行编码。</p>

<p>UTF-8编码的字节数上限并不是3个。对于U+0000-U+FFFF范围内的字符，使用3个字节可以表示完全；对于non-BMP中的字符，则会使用4-6个字节来表示。同样，UTF-16编码也会使用四个字节来表示non-BMP中的字符。</p>

<h2>MySQL的UTF-8编码</h2>

<p>根据MySQL的<a href="http://dev.mysql.com/doc/refman/5.5/en/charset-unicode.html">官方文档</a>，它的UTF-8编码支持是不完全的，最多使用3个字符，这也是导入数据时报错的原因。</p>

<p>MySQL5.5开始支持utf8mb4编码，至多使用4个字节，因此能包含到non-BMP字符。只是我们的MySQL版本仍是5.1，因此选择丢弃这些字符。</p>

<h2>参考资料</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8">http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8</a></li>
<li><a href="http://www.joelonsoftware.com/articles/Unicode.html">http://www.joelonsoftware.com/articles/Unicode.html</a></li>
<li><a href="http://apps.timwhitlock.info/emoji/tables/unicode">http://apps.timwhitlock.info/emoji/tables/unicode</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在CDH 4.5上安装Shark 0.9]]></title>
    <link href="http://shzhangji.com/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5/"/>
    <updated>2014-07-05T17:16:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5</id>
    <content type="html"><![CDATA[<p><a href="http://spark.apache.org">Spark</a>是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的<a href="http://hadoop.apache.org">Hadoop</a>生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于<a href="http://hive.apache.org">Hive</a>，Spark也有相应的替代项目——<a href="http://shark.cs.berkeley.edu/">Shark</a>，能做到 <strong>drop-in replacement</strong> ，直接构建在现有集群之上。本文就将简要阐述如何在CDH4.5上搭建Shark0.9集群。</p>

<h2>准备工作</h2>

<ul>
<li>安装方式：Spark使用CDH提供的Parcel，以Standalone模式启动</li>
<li>软件版本

<ul>
<li>Cloudera Manager 4.8.2</li>
<li>CDH 4.5</li>
<li>Spark 0.9.0 Parcel</li>
<li><a href="http://cloudera.rst.im/shark/">Shark 0.9.1 Binary</a></li>
</ul>
</li>
<li>服务器基础配置

<ul>
<li>可用的软件源（如<a href="http://mirrors.ustc.edu.cn/">中科大的源</a>）</li>
<li>配置主节点至子节点的root账户SSH无密码登录。</li>
<li>在<code>/etc/hosts</code>中写死IP和主机名，或者DNS做好正反解析。</li>
</ul>
</li>
</ul>


<!-- more -->


<h2>安装Spark</h2>

<ul>
<li>使用CM安装Parcel，不需要重启服务。</li>
<li>修改<code>/etc/spark/conf/spark-env.sh</code>：（其中one-843是主节点的域名）</li>
</ul>


<p><code>bash
STANDALONE_SPARK_MASTER_HOST=one-843
DEFAULT_HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*
export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native
</code></p>

<ul>
<li>修改<code>/etc/spark/conf/slaves</code>，添加各节点主机名。</li>
<li>将<code>/etc/spark/conf</code>目录同步至所有节点。</li>
<li>启动Spark服务（即Standalone模式）：</li>
</ul>


<p><code>bash
$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-master.sh
$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-slaves.sh
</code></p>

<ul>
<li>测试<code>spark-shell</code>是否可用：</li>
</ul>


<p><code>scala
sc.textFile("hdfs://one-843:8020/user/jizhang/zj_people.txt.lzo").count
</code></p>

<h2>安装Shark</h2>

<ul>
<li>安装Oracle JDK 1.7 Update 45至<code>/usr/lib/jvm/jdk1.7.0_45</code>。</li>
<li>下载别人编译好的二进制包：<a href="http://cloudera.rst.im/shark/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz">shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz</a></li>
<li>解压至<code>/opt</code>目录，修改<code>conf/shark-env.sh</code>：</li>
</ul>


<p>```bash
export JAVA_HOME=/usr/lib/jvm/jdk1.7.0_45
export SCALA_HOME=/opt/cloudera/parcels/SPARK/lib/spark
export SHARK_HOME=/root/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0</p>

<p>export HIVE_CONF_DIR=/etc/hive/conf</p>

<p>export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop
export SPARK_HOME=/opt/cloudera/parcels/SPARK/lib/spark
export MASTER=spark://one-843:7077</p>

<p>export SPARK_CLASSPATH=$SPARK_CLASSPATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*
export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native
```</p>

<ul>
<li>开启SharkServer2，使用Supervisord管理：</li>
</ul>


<p><code>
[program:sharkserver2]
command = /opt/shark/bin/shark --service sharkserver2
autostart = true
autorestart = true
stdout_logfile = /var/log/sharkserver2.log
redirect_stderr = true
</code></p>

<p><code>bash
$ supervisorctl start sharkserver2
</code></p>

<ul>
<li>测试</li>
</ul>


<p><code>bash
$ /opt/shark/bin/beeline -u jdbc:hive2://one-843:10000 -n root
</code></p>

<h2>版本问题</h2>

<h3>背景</h3>

<h4>CDH</h4>

<p>CDH是对Hadoop生态链各组件的打包，每个CDH版本都会对应一组Hadoop组件的版本，如<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH-Version-and-Packaging-Information/cdhvd_topic_3.html">CDH4.5</a>的部分对应关系如下：</p>

<ul>
<li>Apache Hadoop: hadoop-2.0.0+1518</li>
<li>Apache Hive: hive-0.10.0+214</li>
<li>Hue: hue-2.5.0+182</li>
</ul>


<p>可以看到，CDH4.5对应的Hive版本是0.10.0，因此它的<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH4-Installation-Guide/cdh4ig_hive_metastore_configure.html">Metastore Server</a>使用的也是0.10.0版本的API。</p>

<h4>Spark</h4>

<p>Spark目前最高版本是0.9.1，CDH前不久推出了0.9.0的Parcel，使得安装过程变的简单得多。CDH5中对Spark做了深度集成，即可以用CM来直接控制Spark的服务，且支持Spark on YARN架构。</p>

<h4>Shark</h4>

<p>Shark是基于Spark的一款应用，可以简单地认为是将Hive的MapReduce引擎替换为了Spark。</p>

<p>Shark的一个特点的是需要使用特定的Hive版本——<a href="https://github.com/amplab/hive">AMPLab patched Hive</a>：</p>

<ul>
<li>Shark 0.8.x: AMPLab Hive 0.9.0</li>
<li>Shark 0.9.x: AMPLab Hive 0.11.0</li>
</ul>


<p>在0.9.0以前，我们需要手动下载AMPLab Hive的二进制包，并在Shark的环境变量中设置$HIVE_HOME。在0.9.1以后，AMPLab将该版本的Hive包上传至了Maven，可以直接打进Shark的二进制包中。但是，这个Jar是用JDK7编译的，因此运行Shark需要使用Oracle JDK7。CDH建议使用Update 45这个小版本。</p>

<h4>Shark与Hive的并存</h4>

<p>Shark的一个卖点是和Hive的<a href="5">高度兼容</a>，也就是说它可以直接操作Hive的metastore db，或是和metastore server通信。当然，前提是两者的Hive版本需要一致，这也是目前遇到的最大问题。</p>

<h3>目前发现的不兼容SQL</h3>

<ul>
<li>DROP TABLE &hellip;</li>
</ul>


<p><code>
FAILED: Error in metadata: org.apache.thrift.TApplicationException: Invalid method name: 'drop_table_with_environment_context'
</code></p>

<ul>
<li>INSERT OVERWRITE TABLE &hellip; PARTITION (&hellip;) SELECT &hellip;</li>
<li>LOAD DATA INPATH &lsquo;&hellip;&rsquo; OVERWRITE INTO TABLE &hellip; PARTITION (&hellip;)</li>
</ul>


<p><code>
Failed with exception org.apache.thrift.TApplicationException: Invalid method name: 'partition_name_has_valid_characters'
</code></p>

<p>也就是说上述两个方法名是0.11.0接口中定义的，在0.10.0的定义中并不存在，所以出现上述问题。</p>

<h3>解决方案</h3>

<h4>对存在问题的SQL使用Hive命令去调</h4>

<p>因为Shark初期是想给分析师使用的，他们对分区表并不是很在意，而DROP TABLE可以在客户端做判断，转而使用Hive来执行。</p>

<p>这个方案的优点是可以在现有集群上立刻用起来，但缺点是需要做一些额外的开发，而且API不一致的问题可能还会有其他坑在里面。</p>

<h4>升级到CDH5</h4>

<p>CDH5中Hive的版本是0.12.0，所以不排除同样存在API不兼容的问题。不过网上也有人尝试跳过AMPLab Hive，让Shark直接调用CDH中的Hive，其可行性还需要我们自己测试。</p>

<p>对于这个问题，我只在<a href="https://groups.google.com/forum/#!starred/shark-users/x_Dh5-3isIc">Google Groups</a>上看到一篇相关的帖子，不过并没有给出解决方案。</p>

<p>目前我们实施的是 <strong>第一种方案</strong>，即在客户端和Shark之间添加一层，不支持的SQL语句直接降级用Hive执行，效果不错。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use WebJars in Scalatra Project]]></title>
    <link href="http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project/"/>
    <updated>2014-05-27T17:44:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project</id>
    <content type="html"><![CDATA[<p>As I&rsquo;m working with my first <a href="http://www.scalatra.org/">Scalatra</a> project, I automatically think of using <a href="http://www.webjars.org/">WebJars</a> to manage Javascript library dependencies, since it&rsquo;s more convenient and seems like a good practice. Though there&rsquo;s no <a href="http://www.webjars.org/documentation">official support</a> for Scalatra framework, the installation process is not very complex. But this doesn&rsquo;t mean I didn&rsquo;t spend much time on this. I&rsquo;m still a newbie to Scala, and there&rsquo;s only a few materials on this subject.</p>

<h2>Add WebJars Dependency in SBT Build File</h2>

<p>Scalatra uses <code>.scala</code> configuration file instead of <code>.sbt</code>, so let&rsquo;s add dependency into <code>project/build.scala</code>. Take <a href="http://dojotoolkit.org/">Dojo</a> for example.</p>

<p>```scala
object DwExplorerBuild extends Build {
  &hellip;
  lazy val project = Project (</p>

<pre><code>...
settings = Defaults.defaultSettings ++ ScalatraPlugin.scalatraWithJRebel ++ scalateSettings ++ Seq(
  ...
  libraryDependencies ++= Seq(
    ...
    "org.webjars" % "dojo" % "1.9.3"
  ),
  ...
)
</code></pre>

<p>  )
}
```</p>

<p>To view this dependency in Eclipse, you need to run <code>sbt eclipse</code> again. In the <em>Referenced Libraries</em> section, you can see a <code>dojo-1.9.3.jar</code>, and the library lies in <code>META-INF/resources/webjars/</code>.</p>

<!-- more -->


<h2>Add a Route for WebJars Resources</h2>

<p>Find the <code>ProjectNameStack.scala</code> file and add the following lines at the bottom of the trait:</p>

<p>```scala
trait ProjectNameStack extends ScalatraServlet with ScalateSupport {
  &hellip;
  get(&ldquo;/webjars/*&rdquo;) {</p>

<pre><code>val resourcePath = "/META-INF/resources/webjars/" + params("splat")
Option(getClass.getResourceAsStream(resourcePath)) match {
  case Some(inputStream) =&gt; {
    contentType = servletContext.getMimeType(resourcePath)
    IOUtil.loadBytes(inputStream)
  }
  case None =&gt; resourceNotFound()
}
</code></pre>

<p>  }
}
```</p>

<p><strong>That&rsquo;s it!</strong> Now you can refer to the WebJars resources in views, like this:</p>

<p>```ssp</p>

<h1>set (title)</h1>

<p>Hello, Dojo!</p>

<h1>end</h1>

<div id="greeting"></div>




<script type="text/javascript" src="${uri("/webjars/dojo/1.9.3/dojo/dojo.js")}" data-dojo-config="async: true"></script>


<script type="text/javascript">
require([
    'dojo/dom',
    'dojo/dom-construct'
], function (dom, domConstruct) {
    var greetingNode = dom.byId('greeting');
    domConstruct.place('<i>Dojo!</i>', greetingNode);
});
</script>


<p>```</p>

<h3>Some Explanations on This Route</h3>

<ul>
<li><code>/webjars/*</code> is a <a href="http://www.scalatra.org/2.2/guides/http/routes.html#toc_233">Wildcards</a> and <code>params("splat")</code> is to extract the asterisk part.</li>
<li><code>resourcePath</code> points to the WebJars resources in the jar file, as we saw in Eclipse. It is then fetched as an <code>InputStream</code> with <code>getResourceAsStream()</code>.</li>
<li><code>servletContext.getMimeType()</code> is a handy method to determine the content type of the requested resource, instead of parsing it by ourselves. I find this in SpringMVC&rsquo;s <a href="http://grepcode.com/file/repo1.maven.org/maven2/org.springframework/spring-webmvc/3.2.7.RELEASE/org/springframework/web/servlet/resource/ResourceHttpRequestHandler.java#ResourceHttpRequestHandler.handleRequest%28javax.servlet.http.HttpServletRequest%2Cjavax.servlet.http.HttpServletResponse%29">ResourceHttpRequestHandler</a>.</li>
<li><code>IOUtil</code> is a utiliy class that comes with <a href="http://scalate.fusesource.org/">Scalate</a>, so don&rsquo;t forget to import it first.</li>
</ul>


<p>At first I tried to figure out whether Scalatra provides a conveniet way to serve static files in classpath, I failed. So I decided to serve them by my own, and <a href="https://gist.github.com/laurilehmijoki/4483113">this gist</a> was very helpful.</p>

<p>Anyway, I&rsquo;ve spent more than half a day to solve this problem, and it turned out to be a very challenging yet interesting way to learn a new language, new framework, and new tools. Keep moving!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hive小文件问题的处理]]></title>
    <link href="http://shzhangji.com/blog/2014/04/07/hive-small-files/"/>
    <updated>2014-04-07T17:09:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/04/07/hive-small-files</id>
    <content type="html"><![CDATA[<p>Hive的后端存储是HDFS，它对大文件的处理是非常高效的，如果合理配置文件系统的块大小，NameNode可以支持很大的数据量。但是在数据仓库中，越是上层的表其汇总程度就越高，数据量也就越小。而且这些表通常会按日期进行分区，随着时间的推移，HDFS的文件数目就会逐渐增加。</p>

<h2>小文件带来的问题</h2>

<p>关于这个问题的阐述可以读一读Cloudera的<a href="http://blog.cloudera.com/blog/2009/02/the-small-files-problem/">这篇文章</a>。简单来说，HDFS的文件元信息，包括位置、大小、分块信息等，都是保存在NameNode的内存中的。每个对象大约占用150个字节，因此一千万个文件及分块就会占用约3G的内存空间，一旦接近这个量级，NameNode的性能就会开始下降了。</p>

<p>此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，每个脚本只处理很少的数据，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决。</p>

<!-- more -->


<h2>Hive小文件产生的原因</h2>

<p>前面已经提到，汇总后的数据量通常比源数据要少得多。而为了提升运算速度，我们会增加Reducer的数量，Hive本身也会做类似优化——Reducer数量等于源数据的量除以hive.exec.reducers.bytes.per.reducer所配置的量（默认1G）。Reducer数量的增加也即意味着结果文件的增加，从而产生小文件的问题。</p>

<h2>配置Hive结果合并</h2>

<p>我们可以通过一些配置项来使Hive在执行结束后对结果文件进行合并：</p>

<ul>
<li><code>hive.merge.mapfiles</code> 在map-only job后合并文件，默认<code>true</code></li>
<li><code>hive.merge.mapredfiles</code> 在map-reduce job后合并文件，默认<code>false</code></li>
<li><code>hive.merge.size.per.task</code> 合并后每个文件的大小，默认<code>256000000</code></li>
<li><code>hive.merge.smallfiles.avgsize</code> 平均文件大小，是决定是否执行合并操作的阈值，默认<code>16000000</code></li>
</ul>


<p>Hive在对结果文件进行合并时会执行一个额外的map-only脚本，mapper的数量是文件总大小除以size.per.task参数所得的值，触发合并的条件是：</p>

<ol>
<li>根据查询类型不同，相应的mapfiles/mapredfiles参数需要打开；</li>
<li>结果文件的平均大小需要大于avgsize参数的值。</li>
</ol>


<p>示例：</p>

<p>```sql
&mdash; map-red job，5个reducer，产生5个60K的文件。
create table dw_stage.zj_small as
select paid, count(*)
from dw_db.dw_soj_imp_dtl
where log_dt = &lsquo;2014-04-14&rsquo;
group by paid;</p>

<p>&mdash; 执行额外的map-only job，一个mapper，产生一个300K的文件。
set hive.merge.mapredfiles=true;
create table dw_stage.zj_small as
select paid, count(*)
from dw_db.dw_soj_imp_dtl
where log_dt = &lsquo;2014-04-14&rsquo;
group by paid;</p>

<p>&mdash; map-only job，45个mapper，产生45个25M左右的文件。
create table dw_stage.zj_small as
select *
from dw_db.dw_soj_imp_dtl
where log_dt = &lsquo;2014-04-14&rsquo;
and paid like &lsquo;%baidu%&rsquo;;</p>

<p>&mdash; 执行额外的map-only job，4个mapper，产生4个250M左右的文件。
set hive.merge.smallfiles.avgsize=100000000;
create table dw_stage.zj_small as
select *
from dw_db.dw_soj_imp_dtl
where log_dt = &lsquo;2014-04-14&rsquo;
and paid like &lsquo;%baidu%&rsquo;;
```</p>

<h3>压缩文件的处理</h3>

<p>如果结果表使用了压缩格式，则必须配合SequenceFile来存储，否则无法进行合并，以下是示例：</p>

<p>```sql
set mapred.output.compression.type=BLOCK;
set hive.exec.compress.output=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.LzoCodec;
set hive.merge.smallfiles.avgsize=100000000;</p>

<p>drop table if exists dw_stage.zj_small;
create table dw_stage.zj_small
STORED AS SEQUENCEFILE
as select *
from dw_db.dw_soj_imp_dtl
where log_dt = &lsquo;2014-04-14&rsquo;
and paid like &lsquo;%baidu%&rsquo;;
```</p>

<h2>使用HAR归档文件</h2>

<p>Hadoop的<a href="http://hadoop.apache.org/docs/stable1/hadoop_archives.html">归档文件</a>格式也是解决小文件问题的方式之一。而且Hive提供了<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Archiving">原生支持</a>：</p>

<p>```
set hive.archive.enabled=true;
set hive.archive.har.parentdir.settable=true;
set har.partfile.size=1099511627776;</p>

<p>ALTER TABLE srcpart ARCHIVE PARTITION(ds=&lsquo;2008-04-08&rsquo;, hr=&lsquo;12&rsquo;);</p>

<p>ALTER TABLE srcpart UNARCHIVE PARTITION(ds=&lsquo;2008-04-08&rsquo;, hr=&lsquo;12&rsquo;);
```</p>

<p>如果使用的不是分区表，则可创建成外部表，并使用<code>har://</code>协议来指定路径。</p>

<h2>HDFS Federation</h2>

<p>Hadoop V2引入了HDFS Federation的概念：</p>

<p><img src="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/images/federation.gif" alt="" /></p>

<p>实则是将NameNode做了拆分，从而增强了它的扩展性，小文件的问题也能够得到缓解。</p>

<h2>其他工具</h2>

<p>对于通常的应用，使用Hive结果合并就能达到很好的效果。如果不想因此增加运行时间，可以自行编写一些脚本，在系统空闲时对分区内的文件进行合并，也能达到目的。</p>
]]></content>
  </entry>
  
</feed>
