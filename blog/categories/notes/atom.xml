<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Notes | Ji ZHANG's Blog]]></title>
  <link href="http://shzhangji.com/blog/categories/notes/atom.xml" rel="self"/>
  <link href="http://shzhangji.com/"/>
  <updated>2014-11-07T16:38:24+08:00</updated>
  <id>http://shzhangji.com/</id>
  <author>
    <name><![CDATA[Ji ZHANG]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[离线环境下构建sbt项目]]></title>
    <link href="http://shzhangji.com/blog/2014/11/07/sbt-offline/"/>
    <updated>2014-11-07T15:02:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/11/07/sbt-offline</id>
    <content type="html"><![CDATA[<p>在公司网络中使用<a href="http://www.scala-sbt.org/">sbt</a>、<a href="http://maven.apache.org/">Maven</a>等项目构建工具时，我们通常会搭建一个公用的<a href="http://www.sonatype.org/nexus/">Nexus</a>镜像服务，原因有以下几个：</p>

<ul>
<li>避免重复下载依赖，节省公司带宽；</li>
<li>国内网络环境不理想，下载速度慢；</li>
<li>IDC服务器没有外网访问权限；</li>
<li>用于发布内部模块。</li>
</ul>


<p>sbt的依赖管理是基于<a href="http://ant.apache.org/ivy/">ivy</a>的，虽然它能直接使用<a href="http://search.maven.org/">Maven中央仓库</a>中的Jar包，在配置时还是有一些注意事项的。</p>

<!-- more -->


<h2>配置Nexus镜像</h2>

<p>根据这篇<a href="http://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html">官方文档</a>的描述，Ivy和Maven在依赖管理方面有些许差异，因此不能直接将两者的镜像仓库配置成一个，而需分别建立两个虚拟镜像组。</p>

<p><img src="http://www.scala-sbt.org/0.13/docs/files/proxy-ivy-mvn-setup.png" alt="" /></p>

<p>安装Nexus后默认会有一个Public Repositories组，可以将其作为Maven的镜像组，并添加一些常用的第三方镜像：</p>

<ul>
<li>cloudera: <a href="https://repository.cloudera.com/artifactory/cloudera-repos/">https://repository.cloudera.com/artifactory/cloudera-repos/</a></li>
<li>spring: <a href="http://repo.springsource.org/libs-release-remote/">http://repo.springsource.org/libs-release-remote/</a></li>
<li>scala-tools: <a href="https://oss.sonatype.org/content/groups/scala-tools/">https://oss.sonatype.org/content/groups/scala-tools/</a></li>
</ul>


<p>对于Ivy镜像，我们创建一个新的虚拟组：ivy-releases，并添加以下两个镜像：</p>

<ul>
<li>type-safe: <a href="http://repo.typesafe.com/typesafe/ivy-releases/">http://repo.typesafe.com/typesafe/ivy-releases/</a></li>
<li>sbt-plugin: <a href="http://dl.bintray.com/sbt/sbt-plugin-releases/">http://dl.bintray.com/sbt/sbt-plugin-releases/</a></li>
</ul>


<p>对于sbt-plugin，由于一些原因，Nexus会将其置为Automatically Blocked状态，因此要在配置中将这个选项关闭，否则将无法下载远程的依赖包。</p>

<h2>配置sbt</h2>

<p>为了让sbt使用Nexus镜像，需要创建一个~/.sbt/repositories文件，内容为：</p>

<p><code>
[repositories]
  local
  my-ivy-proxy-releases: http://10.x.x.x:8081/nexus/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]
  my-maven-proxy-releases: http://10.x.x.x:8081/nexus/public/
</code></p>

<p>这样配置对大部分项目来说是足够了。但是有些项目会在构建描述文件中添加其它仓库，我们需要覆盖这种行为，方法是：</p>

<p><code>bash
$ sbt -Dsbt.override.build.repos=true
</code></p>

<p>你也可以通过设置SBT_OPTS环境变量来进行全局配置。</p>

<p>经过以上步骤，sbt执行过程中就不需要访问外网了，因此速度会有很大提升。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL异常UTF-8字符的处理]]></title>
    <link href="http://shzhangji.com/blog/2014/10/14/mysql-incorrent-utf8-value/"/>
    <updated>2014-10-14T13:16:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/10/14/mysql-incorrent-utf8-value</id>
    <content type="html"><![CDATA[<p>ETL流程中，我们会将Hive中的数据导入MySQL——先用Hive命令行将数据保存为文本文件，然后用MySQL的LOAD DATA语句进行加载。最近有一张表在加载到MySQL时会报以下错误：</p>

<p><code>
Incorrect string value: '\xF0\x9D\x8C\x86' for column ...
</code></p>

<p>经查，这个字段中保存的是用户聊天记录，因此会有一些表情符号。这些符号在UTF-8编码下需要使用4个字节来记录，而MySQL中的utf8编码只支持3个字节，因此无法导入。</p>

<p>根据UTF-8的编码规范，3个字节支持的Unicode字符范围是U+0000–U+FFFF，因此可以在Hive中对数据做一下清洗：</p>

<p><code>sql
SELECT REGEXP_REPLACE(content, '[^\\u0000-\\uFFFF]', '') FROM ...
</code></p>

<p>这样就能排除那些需要使用3个以上字节来记录的字符了，从而成功导入MySQL。</p>

<p>以下是一些详细说明和参考资料。</p>

<!-- more -->


<h2>Unicode字符集和UTF编码</h2>

<p><a href="http://en.wikipedia.org/wiki/Unicode">Unicode字符集</a>是一种将全球所有文字都囊括在内的字符集，从而实现跨语言、跨平台的文字信息交换。它由<a href="http://en.wikipedia.org/wiki/Plane_(Unicode)#Basic_Multilingual_Plane">基本多语平面（BMP）</a>和多个扩展平面（non-BMP）组成。前者的编码范围是U+0000-U+FFFF，包括了绝大多数现代语言文字，因此最为常用。</p>

<p><a href="http://en.wikipedia.org/wiki/Unicode#Unicode_Transformation_Format_and_Universal_Character_Set">UTF</a>则是一种编码格式，负责将Unicode字符对应的编号转换为计算机可以识别的二进制数据，进行保存和读取。</p>

<p>比如，磁盘上记录了以下二进制数据：</p>

<p><code>
1101000 1100101 1101100 1101100 1101111
</code></p>

<p>读取它的程序知道这是以UTF-8编码保存的字符串，因此将其解析为以下编号：</p>

<p><code>
104 101 108 108 111
</code></p>

<p>又因为UTF-8编码对应的字符集是Unicode，所以上面这五个编号对应的字符便是“hello”。</p>

<p>很多人会将Unicode和UTF混淆，但两者并不具可比性，它们完成的功能是不同的。</p>

<h2>UTF-8编码</h2>

<p>UTF编码家族也有很多成员，其中<a href="http://en.wikipedia.org/wiki/UTF-8">UTF-8</a>最为常用。它是一种变长的编码格式，对于ASCII码中的字符使用1个字节进行编码，对于中文等则使用3个字节。这样做的优点是在存储西方语言文字时不会造成空间浪费，不像UTF-16和UTF-32，分别使用两个字节和四个字节对所有字符进行编码。</p>

<p>UTF-8编码的字节数上限并不是3个。对于U+0000-U+FFFF范围内的字符，使用3个字节可以表示完全；对于non-BMP中的字符，则会使用4-6个字节来表示。同样，UTF-16编码也会使用四个字节来表示non-BMP中的字符。</p>

<h2>MySQL的UTF-8编码</h2>

<p>根据MySQL的<a href="http://dev.mysql.com/doc/refman/5.5/en/charset-unicode.html">官方文档</a>，它的UTF-8编码支持是不完全的，最多使用3个字符，这也是导入数据时报错的原因。</p>

<p>MySQL5.5开始支持utf8mb4编码，至多使用4个字节，因此能包含到non-BMP字符。只是我们的MySQL版本仍是5.1，因此选择丢弃这些字符。</p>

<h2>参考资料</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8">http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8</a></li>
<li><a href="http://www.joelonsoftware.com/articles/Unicode.html">http://www.joelonsoftware.com/articles/Unicode.html</a></li>
<li><a href="http://apps.timwhitlock.info/emoji/tables/unicode">http://apps.timwhitlock.info/emoji/tables/unicode</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在CDH 4.5上安装Shark 0.9]]></title>
    <link href="http://shzhangji.com/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5/"/>
    <updated>2014-07-05T17:16:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5</id>
    <content type="html"><![CDATA[<p><a href="http://spark.apache.org">Spark</a>是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的<a href="http://hadoop.apache.org">Hadoop</a>生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于<a href="http://hive.apache.org">Hive</a>，Spark也有相应的替代项目——<a href="http://shark.cs.berkeley.edu/">Shark</a>，能做到 <strong>drop-in replacement</strong> ，直接构建在现有集群之上。本文就将简要阐述如何在CDH4.5上搭建Shark0.9集群。</p>

<h2>准备工作</h2>

<ul>
<li>安装方式：Spark使用CDH提供的Parcel，以Standalone模式启动</li>
<li>软件版本

<ul>
<li>Cloudera Manager 4.8.2</li>
<li>CDH 4.5</li>
<li>Spark 0.9.0 Parcel</li>
<li><a href="http://cloudera.rst.im/shark/">Shark 0.9.1 Binary</a></li>
</ul>
</li>
<li>服务器基础配置

<ul>
<li>可用的软件源（如<a href="http://mirrors.ustc.edu.cn/">中科大的源</a>）</li>
<li>配置主节点至子节点的root账户SSH无密码登录。</li>
<li>在<code>/etc/hosts</code>中写死IP和主机名，或者DNS做好正反解析。</li>
</ul>
</li>
</ul>


<!-- more -->


<h2>安装Spark</h2>

<ul>
<li>使用CM安装Parcel，不需要重启服务。</li>
<li>修改<code>/etc/spark/conf/spark-env.sh</code>：（其中one-843是主节点的域名）</li>
</ul>


<p><code>bash
STANDALONE_SPARK_MASTER_HOST=one-843
DEFAULT_HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*
export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native
</code></p>

<ul>
<li>修改<code>/etc/spark/conf/slaves</code>，添加各节点主机名。</li>
<li>将<code>/etc/spark/conf</code>目录同步至所有节点。</li>
<li>启动Spark服务（即Standalone模式）：</li>
</ul>


<p><code>bash
$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-master.sh
$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-slaves.sh
</code></p>

<ul>
<li>测试<code>spark-shell</code>是否可用：</li>
</ul>


<p><code>scala
sc.textFile("hdfs://one-843:8020/user/jizhang/zj_people.txt.lzo").count
</code></p>

<h2>安装Shark</h2>

<ul>
<li>安装Oracle JDK 1.7 Update 45至<code>/usr/lib/jvm/jdk1.7.0_45</code>。</li>
<li>下载别人编译好的二进制包：<a href="http://cloudera.rst.im/shark/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz">shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz</a></li>
<li>解压至<code>/opt</code>目录，修改<code>conf/shark-env.sh</code>：</li>
</ul>


<p>```bash
export JAVA_HOME=/usr/lib/jvm/jdk1.7.0_45
export SCALA_HOME=/opt/cloudera/parcels/SPARK/lib/spark
export SHARK_HOME=/root/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0</p>

<p>export HIVE_CONF_DIR=/etc/hive/conf</p>

<p>export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop
export SPARK_HOME=/opt/cloudera/parcels/SPARK/lib/spark
export MASTER=spark://one-843:7077</p>

<p>export SPARK_CLASSPATH=$SPARK_CLASSPATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*
export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native
```</p>

<ul>
<li>开启SharkServer2，使用Supervisord管理：</li>
</ul>


<p><code>
[program:sharkserver2]
command = /opt/shark/bin/shark --service sharkserver2
autostart = true
autorestart = true
stdout_logfile = /var/log/sharkserver2.log
redirect_stderr = true
</code></p>

<p><code>bash
$ supervisorctl start sharkserver2
</code></p>

<ul>
<li>测试</li>
</ul>


<p><code>bash
$ /opt/shark/bin/beeline -u jdbc:hive2://one-843:10000 -n root
</code></p>

<h2>版本问题</h2>

<h3>背景</h3>

<h4>CDH</h4>

<p>CDH是对Hadoop生态链各组件的打包，每个CDH版本都会对应一组Hadoop组件的版本，如<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH-Version-and-Packaging-Information/cdhvd_topic_3.html">CDH4.5</a>的部分对应关系如下：</p>

<ul>
<li>Apache Hadoop: hadoop-2.0.0+1518</li>
<li>Apache Hive: hive-0.10.0+214</li>
<li>Hue: hue-2.5.0+182</li>
</ul>


<p>可以看到，CDH4.5对应的Hive版本是0.10.0，因此它的<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH4-Installation-Guide/cdh4ig_hive_metastore_configure.html">Metastore Server</a>使用的也是0.10.0版本的API。</p>

<h4>Spark</h4>

<p>Spark目前最高版本是0.9.1，CDH前不久推出了0.9.0的Parcel，使得安装过程变的简单得多。CDH5中对Spark做了深度集成，即可以用CM来直接控制Spark的服务，且支持Spark on YARN架构。</p>

<h4>Shark</h4>

<p>Shark是基于Spark的一款应用，可以简单地认为是将Hive的MapReduce引擎替换为了Spark。</p>

<p>Shark的一个特点的是需要使用特定的Hive版本——<a href="https://github.com/amplab/hive">AMPLab patched Hive</a>：</p>

<ul>
<li>Shark 0.8.x: AMPLab Hive 0.9.0</li>
<li>Shark 0.9.x: AMPLab Hive 0.11.0</li>
</ul>


<p>在0.9.0以前，我们需要手动下载AMPLab Hive的二进制包，并在Shark的环境变量中设置$HIVE_HOME。在0.9.1以后，AMPLab将该版本的Hive包上传至了Maven，可以直接打进Shark的二进制包中。但是，这个Jar是用JDK7编译的，因此运行Shark需要使用Oracle JDK7。CDH建议使用Update 45这个小版本。</p>

<h4>Shark与Hive的并存</h4>

<p>Shark的一个卖点是和Hive的<a href="5">高度兼容</a>，也就是说它可以直接操作Hive的metastore db，或是和metastore server通信。当然，前提是两者的Hive版本需要一致，这也是目前遇到的最大问题。</p>

<h3>目前发现的不兼容SQL</h3>

<ul>
<li>DROP TABLE &hellip;</li>
</ul>


<p><code>
FAILED: Error in metadata: org.apache.thrift.TApplicationException: Invalid method name: 'drop_table_with_environment_context'
</code></p>

<ul>
<li>INSERT OVERWRITE TABLE &hellip; PARTITION (&hellip;) SELECT &hellip;</li>
<li>LOAD DATA INPATH &lsquo;&hellip;&rsquo; OVERWRITE INTO TABLE &hellip; PARTITION (&hellip;)</li>
</ul>


<p><code>
Failed with exception org.apache.thrift.TApplicationException: Invalid method name: 'partition_name_has_valid_characters'
</code></p>

<p>也就是说上述两个方法名是0.11.0接口中定义的，在0.10.0的定义中并不存在，所以出现上述问题。</p>

<h3>解决方案</h3>

<h4>对存在问题的SQL使用Hive命令去调</h4>

<p>因为Shark初期是想给分析师使用的，他们对分区表并不是很在意，而DROP TABLE可以在客户端做判断，转而使用Hive来执行。</p>

<p>这个方案的优点是可以在现有集群上立刻用起来，但缺点是需要做一些额外的开发，而且API不一致的问题可能还会有其他坑在里面。</p>

<h4>升级到CDH5</h4>

<p>CDH5中Hive的版本是0.12.0，所以不排除同样存在API不兼容的问题。不过网上也有人尝试跳过AMPLab Hive，让Shark直接调用CDH中的Hive，其可行性还需要我们自己测试。</p>

<p>对于这个问题，我只在<a href="https://groups.google.com/forum/#!starred/shark-users/x_Dh5-3isIc">Google Groups</a>上看到一篇相关的帖子，不过并没有给出解决方案。</p>

<p>目前我们实施的是 <strong>第一种方案</strong>，即在客户端和Shark之间添加一层，不支持的SQL语句直接降级用Hive执行，效果不错。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use WebJars in Scalatra Project]]></title>
    <link href="http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project/"/>
    <updated>2014-05-27T17:44:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project</id>
    <content type="html"><![CDATA[<p>As I&rsquo;m working with my first <a href="http://www.scalatra.org/">Scalatra</a> project, I automatically think of using <a href="http://www.webjars.org/">WebJars</a> to manage Javascript library dependencies, since it&rsquo;s more convenient and seems like a good practice. Though there&rsquo;s no <a href="http://www.webjars.org/documentation">official support</a> for Scalatra framework, the installation process is not very complex. But this doesn&rsquo;t mean I didn&rsquo;t spend much time on this. I&rsquo;m still a newbie to Scala, and there&rsquo;s only a few materials on this subject.</p>

<h2>Add WebJars Dependency in SBT Build File</h2>

<p>Scalatra uses <code>.scala</code> configuration file instead of <code>.sbt</code>, so let&rsquo;s add dependency into <code>project/build.scala</code>. Take <a href="http://dojotoolkit.org/">Dojo</a> for example.</p>

<p>```scala
object DwExplorerBuild extends Build {
  &hellip;
  lazy val project = Project (</p>

<pre><code>...
settings = Defaults.defaultSettings ++ ScalatraPlugin.scalatraWithJRebel ++ scalateSettings ++ Seq(
  ...
  libraryDependencies ++= Seq(
    ...
    "org.webjars" % "dojo" % "1.9.3"
  ),
  ...
)
</code></pre>

<p>  )
}
```</p>

<p>To view this dependency in Eclipse, you need to run <code>sbt eclipse</code> again. In the <em>Referenced Libraries</em> section, you can see a <code>dojo-1.9.3.jar</code>, and the library lies in <code>META-INF/resources/webjars/</code>.</p>

<!-- more -->


<h2>Add a Route for WebJars Resources</h2>

<p>Find the <code>ProjectNameStack.scala</code> file and add the following lines at the bottom of the trait:</p>

<p>```scala
trait ProjectNameStack extends ScalatraServlet with ScalateSupport {
  &hellip;
  get(&ldquo;/webjars/*&rdquo;) {</p>

<pre><code>val resourcePath = "/META-INF/resources/webjars/" + params("splat")
Option(getClass.getResourceAsStream(resourcePath)) match {
  case Some(inputStream) =&gt; {
    contentType = servletContext.getMimeType(resourcePath)
    IOUtil.loadBytes(inputStream)
  }
  case None =&gt; resourceNotFound()
}
</code></pre>

<p>  }
}
```</p>

<p><strong>That&rsquo;s it!</strong> Now you can refer to the WebJars resources in views, like this:</p>

<p>```ssp</p>

<h1>set (title)</h1>

<p>Hello, Dojo!</p>

<h1>end</h1>

<div id="greeting"></div>




<script type="text/javascript" src="${uri("/webjars/dojo/1.9.3/dojo/dojo.js")}" data-dojo-config="async: true"></script>


<script type="text/javascript">
require([
    'dojo/dom',
    'dojo/dom-construct'
], function (dom, domConstruct) {
    var greetingNode = dom.byId('greeting');
    domConstruct.place('<i>Dojo!</i>', greetingNode);
});
</script>


<p>```</p>

<h3>Some Explanations on This Route</h3>

<ul>
<li><code>/webjars/*</code> is a <a href="http://www.scalatra.org/2.2/guides/http/routes.html#toc_233">Wildcards</a> and <code>params("splat")</code> is to extract the asterisk part.</li>
<li><code>resourcePath</code> points to the WebJars resources in the jar file, as we saw in Eclipse. It is then fetched as an <code>InputStream</code> with <code>getResourceAsStream()</code>.</li>
<li><code>servletContext.getMimeType()</code> is a handy method to determine the content type of the requested resource, instead of parsing it by ourselves. I find this in SpringMVC&rsquo;s <a href="http://grepcode.com/file/repo1.maven.org/maven2/org.springframework/spring-webmvc/3.2.7.RELEASE/org/springframework/web/servlet/resource/ResourceHttpRequestHandler.java#ResourceHttpRequestHandler.handleRequest%28javax.servlet.http.HttpServletRequest%2Cjavax.servlet.http.HttpServletResponse%29">ResourceHttpRequestHandler</a>.</li>
<li><code>IOUtil</code> is a utiliy class that comes with <a href="http://scalate.fusesource.org/">Scalate</a>, so don&rsquo;t forget to import it first.</li>
</ul>


<p>At first I tried to figure out whether Scalatra provides a conveniet way to serve static files in classpath, I failed. So I decided to serve them by my own, and <a href="https://gist.github.com/laurilehmijoki/4483113">this gist</a> was very helpful.</p>

<p>Anyway, I&rsquo;ve spent more than half a day to solve this problem, and it turned out to be a very challenging yet interesting way to learn a new language, new framework, and new tools. Keep moving!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Generate Auto-increment Id in Map-reduce Job]]></title>
    <link href="http://shzhangji.com/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job/"/>
    <updated>2013-10-31T09:35:00+08:00</updated>
    <id>http://shzhangji.com/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job</id>
    <content type="html"><![CDATA[<p>In DBMS world, it&rsquo;s easy to generate a unique, auto-increment id, using MySQL&rsquo;s <a href="http://dev.mysql.com/doc/refman/5.1/en/example-auto-increment.html">AUTO_INCREMENT attribute</a> on a primary key or MongoDB&rsquo;s <a href="http://docs.mongodb.org/manual/tutorial/create-an-auto-incrementing-field/">Counters Collection</a> pattern. But when it comes to a distributed, parallel processing framework, like Hadoop Map-reduce, it is not that straight forward. The best solution to identify every record in such framework is to use UUID. But when an integer id is required, it&rsquo;ll take some steps.</p>

<h2>Solution A: Single Reducer</h2>

<p>This is the most obvious and simple one, just use the following code to specify reducer numbers to 1:</p>

<p><code>java
job.setNumReduceTasks(1);
</code></p>

<p>And also obvious, there are several demerits:</p>

<ol>
<li>All mappers output will be copied to one task tracker.</li>
<li>Only one process is working on shuffel &amp; sort.</li>
<li>When producing output, there&rsquo;s also only one process.</li>
</ol>


<p>The above is not a problem for small data sets, or at least small mapper outputs. And it is also the approach that Pig and Hive use when they need to perform a total sort. But when hitting a certain threshold, the sort and copy phase will become very slow and unacceptable.</p>

<!-- more -->


<h2>Solution B: Increment by Number of Tasks</h2>

<p>Inspired by a <a href="http://mail-archives.apache.org/mod_mbox/hadoop-common-user/200904.mbox/%3C49E13557.7090504@domaintools.com%3E">mailing list</a> that is quite hard to find, which is inspired by MySQL master-master setup (with auto_increment_increment and auto_increment_offset), there&rsquo;s a brilliant way to generate a globally unique integer id across mappers or reducers. Let&rsquo;s take mapper for example:</p>

<p>```java
public static class JobMapper extends Mapper&lt;LongWritable, Text, LongWritable, Text> {</p>

<pre><code>private long id;
private int increment;

@Override
protected void setup(Context context) throws IOException,
        InterruptedException {

    super.setup(context);

    id = context.getTaskAttemptID().getTaskID().getId();
    increment = context.getConfiguration().getInt("mapred.map.tasks", 0);
    if (increment == 0) {
        throw new IllegalArgumentException("mapred.map.tasks is zero");
    }
}

@Override
protected void map(LongWritable key, Text value, Context context)
        throws IOException, InterruptedException {

    id += increment;
    context.write(new LongWritable(id),
            new Text(String.format("%d, %s", key.get(), value.toString())));
}
</code></pre>

<p>}
```</p>

<p>The basic idea is simple:</p>

<ol>
<li>Set the initial id to current tasks&rsquo;s id.</li>
<li>When mapping each row, increment the id by the number of tasks.</li>
</ol>


<p>It&rsquo;s also applicable to reducers.</p>

<h2>Solution C: Sorted Auto-increment Id</h2>

<p>Here&rsquo;s a real senario: we have several log files pulled from different machines, and we want to identify each row by an auto-increment id, and they should be in time sequence order.</p>

<p>We know Hadoop has a sort phase, so we can use timestamp as the mapper output key, and the framework will do the trick. But the sorting thing happends in one reducer (partition, in fact), so when using multiple reducer tasks, the result is not in total order. To achieve this, we can use the <a href="http://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.html">TotalOrderPartitioner</a>.</p>

<p>How about the incremental id? Even though the outputs are in total order, Solution B is not applicable here. So we take another approach: seperate the job in two phases, use the reducer to do sorting <em>and</em> counting, then use the second mapper to generate the id.</p>

<p>Here&rsquo;s what we gonna do:</p>

<ol>
<li>Use TotalOrderPartitioner, and generate the partition file.</li>
<li>Parse logs in mapper A, use time as the output key.</li>
<li>Let the framework do partitioning and sorting.</li>
<li>Count records in reducer, write it with <a href="http://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.html">MultipleOutput</a>.</li>
<li>In mapper B, use count as offset, and increment by 1.</li>
</ol>


<p>To simplify the situation, we assume to have the following inputs and outputs:</p>

<p>```text
 Input       Output</p>

<p>11:00 a     1 11:00 a
12:00 b     2 11:01 aa
13:00 c     3 11:02 aaa</p>

<p>11:01 aa    4 12:00 b
12:01 bb    5 12:01 bb
13:01 cc    6 12:02 bbb</p>

<p>11:02 aaa   7 13:00 c
12:02 bbb   8 13:01 cc
13:02 ccc   9 13:02 ccc
```</p>

<h3>Generate Partition File</h3>

<p>To use TotalOrderpartitioner, we need a partition file (i.e. boundaries) to tell the partitioner how to partition the mapper outputs. Usually we&rsquo;ll use <a href="https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapreduce/lib/partition/InputSampler.RandomSampler.html">InputSampler.RandomSampler</a> class, but this time let&rsquo;s use a manual partition file.</p>

<p>```java
SequenceFile.Writer writer = new SequenceFile.Writer(fs, getConf(), partition,</p>

<pre><code>    Text.class, NullWritable.class);
</code></pre>

<p>Text key = new Text();
NullWritable value = NullWritable.get();
key.set(&ldquo;12:00&rdquo;);
writer.append(key, value);
key.set(&ldquo;13:00&rdquo;);
writer.append(key, value);
writer.close();
```</p>

<p>So basically, the partitioner will partition the mapper outputs into three parts, the first part will be less than &ldquo;12:00&rdquo;, seceond part [&ldquo;12:00&rdquo;, &ldquo;13:00&rdquo;), thrid [&ldquo;13:00&rdquo;, ).</p>

<p>And then, indicate the job to use this partition file:</p>

<p>```java
job.setPartitionerClass(TotalOrderPartitioner.class);
otalOrderPartitioner.setPartitionFile(job.getConfiguration(), partition);</p>

<p>// The number of reducers should equal the number of partitions.
job.setNumReduceTasks(3);
```</p>

<h3>Use MutipleOutputs</h3>

<p>In the reducer, we need to note down the row count of this partition, to do that, we&rsquo;ll need the MultipleOutputs class, which let use output multiple result files apart from the default &ldquo;part-r-xxxxx&rdquo;. The reducer&rsquo;s code is as following:</p>

<p>```java
public static class JobReducer extends Reducer&lt;Text, Text, NullWritable, Text> {</p>

<pre><code>private MultipleOutputs&lt;NullWritable, Text&gt; mos;
private long count;

@Override
protected void setup(Context context)
        throws IOException, InterruptedException {

    super.setup(context);
    mos = new MultipleOutputs&lt;NullWritable, Text&gt;(context);
    count = 0;
}

@Override
protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
        throws IOException, InterruptedException {

    for (Text value : values) {
        context.write(NullWritable.get(), value);
        ++count;
    }
}

@Override
protected void cleanup(Context context)
        throws IOException, InterruptedException {

    super.cleanup(context);
    mos.write("count", NullWritable.get(), new LongWritable(count));
    mos.close();
}
</code></pre>

<p>}
```</p>

<p>There&rsquo;re several things to pay attention to:</p>

<ol>
<li>MultipleOutputs is declared as class member, defined in Reducer#setup method, and must be closed at Reducer#cleanup (otherwise the file will be empty).</li>
<li>When instantiating MultipleOutputs class, the generic type needs to be the same as reducer&rsquo;s output key/value class.</li>
<li>In order to use a different output key/value class, additional setup needs to be done at job definition:</li>
</ol>


<p>```java
Job job = new Job(getConf());
MultipleOutputs.addNamedOutput(job, &ldquo;count&rdquo;, SequenceFileOutputFormat.class,</p>

<pre><code>NullWritable.class, LongWritable.class);
</code></pre>

<p>```</p>

<p>For example, if the output folder is &ldquo;/tmp/total-sort/&rdquo;, there&rsquo;ll be the following files when job is done:</p>

<p><code>text
/tmp/total-sort/count-r-00001
/tmp/total-sort/count-r-00002
/tmp/total-sort/count-r-00003
/tmp/total-sort/part-r-00001
/tmp/total-sort/part-r-00002
/tmp/total-sort/part-r-00003
</code></p>

<h3>Pass Start Ids to Mapper</h3>

<p>When the second mapper processes the inputs, we want them to know the initial id of its partition, which can be calculated from the &ldquo;count-*&rdquo; files we produce before. To pass this information, we can use the job&rsquo;s Configuration object.</p>

<p>```java
// Read and calculate the start id from those row-count files.
Map&lt;String, Long> startIds = new HashMap&lt;String, Long>();
long startId = 1;
FileSystem fs = FileSystem.get(getConf());
for (FileStatus file : fs.listStatus(countPath)) {</p>

<pre><code>Path path = file.getPath();
String name = path.getName();
if (!name.startsWith("count-")) {
    continue;
}

startIds.put(name.substring(name.length() - 5), startId);

SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, getConf());
NullWritable key = NullWritable.get();
LongWritable value = new LongWritable();
if (!reader.next(key, value)) {
    continue;
}
startId += value.get();
reader.close();
</code></pre>

<p>}</p>

<p>// Serialize the map and pass it to Configuration.
job.getConfiguration().set(&ldquo;startIds&rdquo;, Base64.encodeBase64String(</p>

<pre><code>    SerializationUtils.serialize((Serializable) startIds)));
</code></pre>

<p>// Recieve it in Mapper#setup
public static class JobMapperB extends Mapper&lt;NullWritable, Text, LongWritable, Text> {</p>

<pre><code>private Map&lt;String, Long&gt; startIds;
private long startId;

@SuppressWarnings("unchecked")
@Override
protected void setup(Context context)
        throws IOException, InterruptedException {

    super.setup(context);
    startIds = (Map&lt;String, Long&gt;) SerializationUtils.deserialize(
            Base64.decodeBase64(context.getConfiguration().get("startIds")));
    String name = ((FileSplit) context.getInputSplit()).getPath().getName();
    startId = startIds.get(name.substring(name.length() - 5));
}

@Override
protected void map(NullWritable key, Text value, Context context)
        throws IOException, InterruptedException {

    context.write(new LongWritable(startId++), value);
}
</code></pre>

<p>}
```</p>

<h3>Set the Input Non-splitable</h3>

<p>When the file is bigger than a block or so (depending on some configuration entries), Hadoop will split it, which is not good for us. So let&rsquo;s define a new InputFormat class to disable the splitting behaviour:</p>

<p>```java
public static class NonSplitableSequence extends SequenceFileInputFormat&lt;NullWritable, Text> {</p>

<pre><code>@Override
protected boolean isSplitable(JobContext context, Path filename) {
    return false;
}
</code></pre>

<p>}</p>

<p>// use it
job.setInputFormatClass(NonSplitableSequence.class);
```</p>

<p>And that&rsquo;s it, we are able to generate a unique, auto-increment id for a sorted collection, with Hadoop&rsquo;s parallel computing capability. The process is rather complicated, which requires several techniques about Hadoop. It&rsquo;s worthwhile to dig.</p>

<p>A workable example can be found in my <a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/AutoIncrementId2Job.java">Github repository</a>. If you have some more straight-forward approach, please do let me know.</p>
]]></content>
  </entry>
  
</feed>
