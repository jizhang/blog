<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Real-time Exactly-once ETL with Apache Flink | Ji ZHANG&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Apache Flink is another popular big data processing framework, which differs from Apache Spark in that Flink uses stream processing to mimic batch processing and provides sub-second latency along with">
<meta property="og:type" content="article">
<meta property="og:title" content="Real-time Exactly-once ETL with Apache Flink">
<meta property="og:url" content="https://shzhangji.com/blog/2018/12/23/real-time-exactly-once-etl-with-apache-flink/index.html">
<meta property="og:site_name" content="Ji ZHANG&#39;s Blog">
<meta property="og:description" content="Apache Flink is another popular big data processing framework, which differs from Apache Spark in that Flink uses stream processing to mimic batch processing and provides sub-second latency along with">
<meta property="og:locale">
<meta property="og:image" content="https://shzhangji.com/images/flink/arch.png">
<meta property="og:image" content="https://shzhangji.com/images/flink/dashboard.png">
<meta property="og:image" content="https://shzhangji.com/images/flink/stream-barrier.png">
<meta property="article:published_time" content="2018-12-23T13:42:44.000Z">
<meta property="article:modified_time" content="2022-06-19T06:57:28.604Z">
<meta property="article:author" content="Ji ZHANG">
<meta property="article:tag" content="flink">
<meta property="article:tag" content="etl">
<meta property="article:tag" content="kafka">
<meta property="article:tag" content="java">
<meta property="article:tag" content="hdfs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://shzhangji.com/images/flink/arch.png">
<meta name="twitter:creator" content="@zjerryj">
<link rel="publisher" href="zhangji87@gmail.com">
  
    <link rel="alternate" href="/atom.xml" title="Ji ZHANG&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="/css/source-code-pro.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-XGPVRTV36D"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-XGPVRTV36D');
  </script>
<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Ji ZHANG&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">If I rest, I rust.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/categories/Big-Data">Big Data</a>
        
          <a class="main-nav-link" href="/categories/Programming">Programming</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
        <a class="main-nav-link" href="http://shzhangji.com/cnblogs"><img src="/images/cnblogs.png"></a>
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://shzhangji.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-real-time-exactly-once-etl-with-apache-flink" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2018/12/23/real-time-exactly-once-etl-with-apache-flink/" class="article-date">
  <time datetime="2018-12-23T13:42:44.000Z" itemprop="datePublished">2018-12-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Big-Data/">Big Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Real-time Exactly-once ETL with Apache Flink
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Apache Flink is another popular big data processing framework, which differs from Apache Spark in that Flink uses stream processing to mimic batch processing and provides sub-second latency along with exactly-once semantics. One of its use cases is to build a real-time data pipeline, move and transform data between different stores. This article will show you how to build such an application, and explain how Flink guarantees its correctness.</p>
<p><img src="/images/flink/arch.png" alt="Apache Flink"></p>
<h2 id="Demo-ETL-Application"><a href="#Demo-ETL-Application" class="headerlink" title="Demo ETL Application"></a>Demo ETL Application</h2><p>Let us build a project that extracts data from Kafka and loads them into HDFS. The result files should be stored in bucketed directories according to event time. Source messages are encoded in JSON, and the event time is stored as timestamp. Samples are:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;timestamp&quot;:1545184226.432,&quot;event&quot;:&quot;page_view&quot;,&quot;uuid&quot;:&quot;ac0e50bf-944c-4e2f-bbf5-a34b22718e0c&quot;&#125;</span><br><span class="line">&#123;&quot;timestamp&quot;:1545184602.640,&quot;event&quot;:&quot;adv_click&quot;,&quot;uuid&quot;:&quot;9b220808-2193-44d1-a0e9-09b9743dec55&quot;&#125;</span><br><span class="line">&#123;&quot;timestamp&quot;:1545184608.969,&quot;event&quot;:&quot;thumbs_up&quot;,&quot;uuid&quot;:&quot;b44c3137-4c91-4f36-96fb-80f56561c914&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>The result directory structure should be:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/user/flink/event_log/dt=20181219/part-0-1</span><br><span class="line">/user/flink/event_log/dt=20181220/part-1-9</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h3 id="Create-Project"><a href="#Create-Project" class="headerlink" title="Create Project"></a>Create Project</h3><p>Flink application requires Java 8, and we can create a project from Maven template.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mvn archetype:generate \</span><br><span class="line">  -DarchetypeGroupId=org.apache.flink \</span><br><span class="line">  -DarchetypeArtifactId=flink-quickstart-java \</span><br><span class="line">  -DarchetypeVersion=1.7.0</span><br></pre></td></tr></table></figure>

<p>Import it into your favorite IDE, and we can see a class named <code>StreamingJob</code>. We will start from there.</p>
<h3 id="Kafka-Consumer-Source"><a href="#Kafka-Consumer-Source" class="headerlink" title="Kafka Consumer Source"></a>Kafka Consumer Source</h3><p>Flink provides <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/connectors/kafka.html">native support</a> for consuming messages from Kafka. Choose the right version and add to dependencies:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.10_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>We need some Kafka server to bootstrap this source. For testing, one can follow the Kafka <a target="_blank" rel="noopener" href="https://kafka.apache.org/quickstart">official document</a> to setup a local broker. Create the source and pass the host and topic name.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>);</span><br><span class="line">FlinkKafkaConsumer010&lt;String&gt; consumer = <span class="keyword">new</span> <span class="title class_">FlinkKafkaConsumer010</span>&lt;&gt;(</span><br><span class="line">    <span class="string">&quot;flink_test&quot;</span>, <span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>(), props);</span><br><span class="line">DataStream&lt;String&gt; stream = env.addSource(consumer);</span><br></pre></td></tr></table></figure>

<p>Flink will read data from a local Kafka broker, with topic <code>flink_test</code>, and transform it into simple strings, indicated by <code>SimpleStringSchema</code>. There are other built-in deserialization schema like JSON and Avro, or you can create a custom one.</p>
<h3 id="Streaming-File-Sink"><a href="#Streaming-File-Sink" class="headerlink" title="Streaming File Sink"></a>Streaming File Sink</h3><p><code>StreamingFileSink</code> is replacing the previous <code>BucketingSink</code> to store data into HDFS in different directories. The key concept here is bucket assigner, which defaults to <code>DateTimeBucketAssigner</code>, that divides messages into timed buckets according to processing time, i.e. the time when messages arrive the operator. But what we want is to divide messages by event time, so we have to write one on our own.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">EventTimeBucketAssigner</span> <span class="keyword">implements</span> <span class="title class_">BucketAssigner</span>&lt;String, String&gt; &#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> String <span class="title function_">getBucketId</span><span class="params">(String element, Context context)</span> &#123;</span><br><span class="line">    <span class="type">JsonNode</span> <span class="variable">node</span> <span class="operator">=</span> mapper.readTree(element);</span><br><span class="line">    <span class="type">long</span> <span class="variable">date</span> <span class="operator">=</span> (<span class="type">long</span>) (node.path(<span class="string">&quot;timestamp&quot;</span>).floatValue() * <span class="number">1000</span>);</span><br><span class="line">    <span class="type">String</span> <span class="variable">partitionValue</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SimpleDateFormat</span>(<span class="string">&quot;yyyyMMdd&quot;</span>).format(<span class="keyword">new</span> <span class="title class_">Date</span>(date));</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;dt=&quot;</span> + partitionValue;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>We create a bucket assigner that ingests a string, decodes with Jackson, extracts the timestamp, and returns the bucket name of this message. Then the sink will know where to put them. Full code can be found on GitHub (<a target="_blank" rel="noopener" href="https://github.com/jizhang/flink-sandbox/blob/blog-etl/src/main/java/com/shzhangji/flinksandbox/kafka/EventTimeBucketAssigner.java">link</a>).</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">StreamingFileSink&lt;String&gt; sink = StreamingFileSink</span><br><span class="line">    .forRowFormat(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/tmp/kafka-loader&quot;</span>), <span class="keyword">new</span> <span class="title class_">SimpleStringEncoder</span>&lt;String&gt;())</span><br><span class="line">    .withBucketAssigner(<span class="keyword">new</span> <span class="title class_">EventTimeBucketAssigner</span>())</span><br><span class="line">    .build();</span><br><span class="line">stream.addSink(sink);</span><br></pre></td></tr></table></figure>

<p>There is also a <code>forBulkFormat</code>, if you prefer storing data in a more compact way like Parquet.</p>
<p>A note on <code>StreamingFileSink</code> though, it only works with Hadoop 2.7 and above, because it requires the file system supporting <code>truncate</code>, which helps recovering the writing process from the last checkpoint.</p>
<h3 id="Enable-Checkpointing"><a href="#Enable-Checkpointing" class="headerlink" title="Enable Checkpointing"></a>Enable Checkpointing</h3><p>So far, the application can be put into work by invoking <code>env.execute()</code>, but it only guarantees at-least-once semantics. To achieve exactly-once, we simply turn on Flink’s checkpointing:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">env.enableCheckpointing(<span class="number">60_000</span>);</span><br><span class="line">env.setStateBackend((StateBackend) <span class="keyword">new</span> <span class="title class_">FsStateBackend</span>(<span class="string">&quot;/tmp/flink/checkpoints&quot;</span>));</span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(</span><br><span class="line">    ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION);</span><br></pre></td></tr></table></figure>

<p>Checkpoint is Flink’s solution to fault tolerance, which we will cover later. Here we switch the state backend from default <code>MemoryStateBackend</code> to <code>FsStateBackend</code>, that stores state into filesystem like HDFS, instead of in memory, to help surviving job manager failure. Flink also recommends using <code>RocksDBStateBackend</code>, when job state is very large and requires incremental checkpointing.</p>
<h3 id="Submit-and-Manage-Jobs"><a href="#Submit-and-Manage-Jobs" class="headerlink" title="Submit and Manage Jobs"></a>Submit and Manage Jobs</h3><p>Flink application can be directly run in IDE, or you can setup a local <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/cluster_setup.html">standalone cluster</a> and submit jobs with Flink CLI:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.shzhangji.flinksandbox.kafka.KafkaLoader target/flink-sandbox-0.1.0.jar</span><br></pre></td></tr></table></figure>

<p>We can check out the job information in Flink dashboard:</p>
<p><img src="/images/flink/dashboard.png" alt="Flink Dashboard"></p>
<h4 id="Cancel-and-Resume-Job-with-Savepoint"><a href="#Cancel-and-Resume-Job-with-Savepoint" class="headerlink" title="Cancel and Resume Job with Savepoint"></a>Cancel and Resume Job with Savepoint</h4><p>To cancel or restart the job, say we want to upgrade the code logic, we need to create a savepoint. A savepoint is like a checkpoint, storing state of the running tasks. But savepoint is usually manually created, for planned backup or upgrade, while checkpoint is managed by Flink to provide fault tolerance. The <code>cancel</code> sub-command accepts <code>-s</code> option to write savepoint into some directory.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flink cancel -s /tmp/flink/savepoints 1253cc85e5c702dbe963dd7d8d279038</span><br><span class="line">Cancelled job 1253cc85e5c702dbe963dd7d8d279038. Savepoint stored in file:/tmp/flink/savepoints/savepoint-1253cc-0df030f4f2ee.</span><br></pre></td></tr></table></figure>

<p>For our ETL application, savepoint will include current Kafka offsets, in-progress output file names, etc. To resume from a savepoint, pass <code>-s</code> to <code>run</code> sub-command. The application will start from the savepoint, i.e. consume messages right after the saved offsets, without losing or duplicating data.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink run -s /tmp/flink/savepoints/savepoint-1253cc-0df030f4f2ee -c com.shzhangji.flinksandbox.kafka.KafkaLoader target/flink-sandbox-0.1.0.jar</span><br></pre></td></tr></table></figure>

<h4 id="YARN-Support"><a href="#YARN-Support" class="headerlink" title="YARN Support"></a>YARN Support</h4><p>Running Flink jobs on YARN also uses <code>flink run</code>. Replace the file paths with HDFS prefix, re-package and run the following command:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ export HADOOP_CONF_DIR=/path/to/hadoop/conf</span><br><span class="line">$ bin/flink run -m yarn-cluster -c com.shzhangji.flinksandbox.kafka.KafkaLoader target/flink-sandbox-0.1.0.jar</span><br><span class="line">Submitted application application_1545534487726_0001</span><br></pre></td></tr></table></figure>

<p>Flink dashboard will run in YARN application master. The returned application ID can be used to manage the jobs through Flink CLI:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink cancel -s hdfs://localhost:9000/tmp/flink/savepoints -yid application_1545534487726_0001 84de00a5e193f26c937f72a9dc97f386</span><br></pre></td></tr></table></figure>

<h2 id="How-Flink-Guarantees-Exactly-once-Semantics"><a href="#How-Flink-Guarantees-Exactly-once-Semantics" class="headerlink" title="How Flink Guarantees Exactly-once Semantics"></a>How Flink Guarantees Exactly-once Semantics</h2><p>Flink streaming application can be divided into three parts, source, process, and sink. Different sources and sinks, or <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/connectors/guarantees.html">connectors</a>, give different guarantees, and the Flink stream processing gives either at-least-once or exactly-once semantics, based on whether checkpointing is enabled.</p>
<h3 id="Stream-Processing-with-Checkpointing"><a href="#Stream-Processing-with-Checkpointing" class="headerlink" title="Stream Processing with Checkpointing"></a>Stream Processing with Checkpointing</h3><p>Flink’s checkpointing mechanism is based on Chandy-Lamport algorithm. It periodically inserts light-weight barriers into data stream, dividing the stream into sets of records. After an operator has processed all records in the current set, a checkpoint is made and sent to the coordinator, i.e. job manager. Then the operator will send this barrier to its down-streams. When all sinks finish checkpointing, this checkpoint is marked as completed, which means all data before the checkpoint has been properly processed, all operator states are saved, and the application can recover from this checkpoint when encountering failures.</p>
<p><img src="/images/flink/stream-barrier.png" alt="Stream Barrier"></p>
<p>For operators with multiple up-streams, a technique called stream aligning is applied. If one of the up-streams is delayed, the operator will stop processing data from other up-streams, until the slow one catches up. This guarantees exactly-once semantics of the operator state, but will certainly introduce some latency. Apart from this <code>EXACTLY_ONCE</code> mode of checkpointing, Flink also provides <code>AT_LEAST_ONCE</code> mode, to minimize the delay. One can refer to <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/internals/stream_checkpointing.html">document</a> for further details.</p>
<h3 id="Rewindable-Data-Source"><a href="#Rewindable-Data-Source" class="headerlink" title="Rewindable Data Source"></a>Rewindable Data Source</h3><p>When recovering from the last checkpoint, Flink needs to re-fetch some messages, and data source like Kafka supports consuming messages from given offsets. In detail, <code>FlinkKafkaConsumer</code> implements the <code>CheckpointedFunction</code> and stores topic name, partition ID, and offsets in operator state.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">FlinkKafkaConsumerBase</span> <span class="keyword">implements</span> <span class="title class_">CheckpointedFunction</span> &#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initializeState</span><span class="params">(FunctionInitializationContext context)</span> &#123;</span><br><span class="line">    <span class="type">OperatorStateStore</span> <span class="variable">stateStore</span> <span class="operator">=</span> context.getOperatorStateStore();</span><br><span class="line">    <span class="built_in">this</span>.unionOffsetStates = stateStore.getUnionListState(<span class="keyword">new</span> <span class="title class_">ListStateDescriptor</span>&lt;&gt;(</span><br><span class="line">        OFFSETS_STATE_NAME,</span><br><span class="line">        TypeInformation.of(<span class="keyword">new</span> <span class="title class_">TypeHint</span>&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt;() &#123;&#125;)));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">      <span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : unionOffsetStates.get()) &#123;</span><br><span class="line">        restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> &#123;</span><br><span class="line">    unionOffsetStates.clear();</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">	  unionOffsetStates.add(Tuple2.of(kafkaTopicPartitionLongEntry.getKey(),</span><br><span class="line">          kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">	&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>When resuming a job from a savepoint, you can find the following lines in task manager logs, indicating that the source will consume from the offsets that are restored from checkpoint.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2018-12-23 10:56:47,380 INFO FlinkKafkaConsumerBase</span><br><span class="line">  Consumer subtask 0 will start reading 2 partitions with offsets in restored state:</span><br><span class="line">    &#123;KafkaTopicPartition&#123;topic=&#x27;flink_test&#x27;, partition=1&#125;=725,</span><br><span class="line">     KafkaTopicPartition&#123;topic=&#x27;flink_test&#x27;, partition=0&#125;=721&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Recover-In-progress-Output-Files"><a href="#Recover-In-progress-Output-Files" class="headerlink" title="Recover In-progress Output Files"></a>Recover In-progress Output Files</h3><p>As the application runs, <code>StreamingFileSink</code> will first write to a temporary file, prefixed with dot and suffixed with <code>in-progress</code>. The in-progress files are renamed to normal files according to some <code>RollingPolicy</code>, which defaults to both time-based (60 seconds) and size-based (128 MB). When task failure happens, or job is canceled, the in-progress files are simply closed. During recovery, the sink can retrieve in-progress file names from checkpointed state, truncate the files to a specific length, so that they do not contain any data after the checkpoint, and then the stream processing can resume.</p>
<p>Take Hadoop file system for instance, the recovering process happens in <code>HadoopRecoverableFsDataOutputStream</code> class constructor. It is invoked with a <code>HadoopFsRecoverable</code> object that contains the temporary file name, target name, and offset. This object is a member of <code>BucketState</code>, which is stored in operator state.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HadoopRecoverableFsDataOutputStream(FileSystem fs, HadoopFsRecoverable recoverable) &#123;</span><br><span class="line">  <span class="built_in">this</span>.tempFile = checkNotNull(recoverable.tempFile());</span><br><span class="line">  truncate(fs, tempFile, recoverable.offset());</span><br><span class="line">  out = fs.append(tempFile);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>Apache Flink builds upon stream processing, state management is considered from day one, and it integrates well with Hadoop ecosystem. All of these make it a very competitive product in big data field. It is under active development, and gradually gains more features like table API, stream SQL, machine learning, etc. Big companies like Alibaba are also using and contributing to this project. It supports a wide range of use-cases, and is definitely worth a try.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shzhangji.com/blog/2018/12/23/real-time-exactly-once-etl-with-apache-flink/" data-id="cl4kyi60d002acgow8gkz8lkb" class="article-share-link">Share</a>
      
        <a href="https://shzhangji.com/blog/2018/12/23/real-time-exactly-once-etl-with-apache-flink/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/etl/" rel="tag">etl</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/flink/" rel="tag">flink</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hdfs/" rel="tag">hdfs</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2019/06/10/understanding-hive-acid-transactional-table/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Understanding Hive ACID Transactional Table
        
      </div>
    </a>
  
  
    <a href="/blog/2018/12/08/spark-datasource-api-v2/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Spark DataSource API V2</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/algorithm/" style="font-size: 10px;">algorithm</a> <a href="/tags/analytics/" style="font-size: 15px;">analytics</a> <a href="/tags/apache-beam/" style="font-size: 10px;">apache beam</a> <a href="/tags/bootstrap/" style="font-size: 10px;">bootstrap</a> <a href="/tags/canal/" style="font-size: 10px;">canal</a> <a href="/tags/clojure/" style="font-size: 10px;">clojure</a> <a href="/tags/crossfilter/" style="font-size: 10px;">crossfilter</a> <a href="/tags/dc-js/" style="font-size: 10px;">dc.js</a> <a href="/tags/eclipse/" style="font-size: 10px;">eclipse</a> <a href="/tags/elasticsearch/" style="font-size: 10px;">elasticsearch</a> <a href="/tags/es6/" style="font-size: 10px;">es6</a> <a href="/tags/eslint/" style="font-size: 10px;">eslint</a> <a href="/tags/etl/" style="font-size: 13.33px;">etl</a> <a href="/tags/flask/" style="font-size: 10px;">flask</a> <a href="/tags/flink/" style="font-size: 11.67px;">flink</a> <a href="/tags/flume/" style="font-size: 13.33px;">flume</a> <a href="/tags/frontend/" style="font-size: 15px;">frontend</a> <a href="/tags/functional-programming/" style="font-size: 10px;">functional programming</a> <a href="/tags/github/" style="font-size: 10px;">github</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 10px;">hbase</a> <a href="/tags/hdfs/" style="font-size: 11.67px;">hdfs</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/hive/" style="font-size: 11.67px;">hive</a> <a href="/tags/java/" style="font-size: 18.33px;">java</a> <a href="/tags/javascript/" style="font-size: 16.67px;">javascript</a> <a href="/tags/kafka/" style="font-size: 11.67px;">kafka</a> <a href="/tags/kubernetes/" style="font-size: 10px;">kubernetes</a> <a href="/tags/lodash/" style="font-size: 11.67px;">lodash</a> <a href="/tags/machine-learning/" style="font-size: 10px;">machine learning</a> <a href="/tags/mapreduce/" style="font-size: 10px;">mapreduce</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/openapi/" style="font-size: 10px;">openapi</a> <a href="/tags/ops/" style="font-size: 10px;">ops</a> <a href="/tags/pandas/" style="font-size: 11.67px;">pandas</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/react/" style="font-size: 10px;">react</a> <a href="/tags/restful/" style="font-size: 10px;">restful</a> <a href="/tags/scala/" style="font-size: 11.67px;">scala</a> <a href="/tags/scalatra/" style="font-size: 10px;">scalatra</a> <a href="/tags/source-code/" style="font-size: 10px;">source code</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/spark-streaming/" style="font-size: 10px;">spark streaming</a> <a href="/tags/spring/" style="font-size: 10px;">spring</a> <a href="/tags/sql/" style="font-size: 11.67px;">sql</a> <a href="/tags/stream-processing/" style="font-size: 13.33px;">stream processing</a> <a href="/tags/tensorflow/" style="font-size: 10px;">tensorflow</a> <a href="/tags/thrift/" style="font-size: 10px;">thrift</a> <a href="/tags/typescript/" style="font-size: 10px;">typescript</a> <a href="/tags/vite/" style="font-size: 10px;">vite</a> <a href="/tags/vue/" style="font-size: 13.33px;">vue</a> <a href="/tags/vuex/" style="font-size: 10px;">vuex</a> <a href="/tags/webjars/" style="font-size: 10px;">webjars</a> <a href="/tags/websocket/" style="font-size: 10px;">websocket</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/05/">May 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/10/">October 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/04/">April 2013</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2022/06/19/openapi-workflow-with-flask-and-typescript/">OpenAPI Workflow with Flask and TypeScript</a>
          </li>
        
          <li>
            <a href="/blog/2022/06/11/use-bootstrap-v5-in-vue3-project/">Use Bootstrap V5 in Vue 3 Project</a>
          </li>
        
          <li>
            <a href="/blog/2022/06/03/migrate-from-hexo-deployer-git-to-github-actions/">Migrate from hexo-deployer-git to GitHub Actions</a>
          </li>
        
          <li>
            <a href="/blog/2019/08/24/deploy-flink-job-cluster-on-kubernetes/">Deploy Flink Job Cluster on Kubernetes</a>
          </li>
        
          <li>
            <a href="/blog/2019/06/10/understanding-hive-acid-transactional-table/">Understanding Hive ACID Transactional Table</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://mirrors.creativecommons.org/presskit/buttons/80x15/svg/by-nc-sa.svg"></a>
      <br>
      &copy; 2022 Ji ZHANG<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/categories/Big-Data" class="mobile-nav-link">Big Data</a>
  
    <a href="/categories/Programming" class="mobile-nav-link">Programming</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
  <a href="http://shzhangji.com/cnblogs" class="mobile-nav-link">中文</a>
</nav>

    
<script>
  var disqus_shortname = 'jizhang';
  
  var disqus_url = 'https://shzhangji.com/blog/2018/12/23/real-time-exactly-once-etl-with-apache-flink/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>



<script src="/js/jquery.min.js"></script>



  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div>
</body>
</html>