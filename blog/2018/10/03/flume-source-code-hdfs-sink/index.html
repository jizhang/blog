<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Flume Source Code: HDFS Sink | Ji ZHANG&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Sink is the last component of Apache Flume data flow, and it is used to output data into storages like local files, HDFS, ElasticSearch, etc. In this article, I will illustrate how Flume’s HDFS sink w">
<meta property="og:type" content="article">
<meta property="og:title" content="Flume Source Code: HDFS Sink">
<meta property="og:url" content="https://shzhangji.com/blog/2018/10/03/flume-source-code-hdfs-sink/index.html">
<meta property="og:site_name" content="Ji ZHANG&#39;s Blog">
<meta property="og:description" content="Sink is the last component of Apache Flume data flow, and it is used to output data into storages like local files, HDFS, ElasticSearch, etc. In this article, I will illustrate how Flume’s HDFS sink w">
<meta property="og:locale">
<meta property="og:image" content="https://shzhangji.com/images/flume/sink-component-lifecycle.png">
<meta property="og:image" content="https://shzhangji.com/images/flume/hdfs-sink-classes.png">
<meta property="og:image" content="https://shzhangji.com/images/flume/process-method-flow-chart.png">
<meta property="article:published_time" content="2018-10-03T11:34:11.000Z">
<meta property="article:modified_time" content="2018-10-03T11:34:11.000Z">
<meta property="article:author" content="Ji ZHANG">
<meta property="article:tag" content="flume">
<meta property="article:tag" content="java">
<meta property="article:tag" content="hdfs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://shzhangji.com/images/flume/sink-component-lifecycle.png">
<meta name="twitter:creator" content="@zjerryj">
<link rel="publisher" href="zhangji87@gmail.com">
  
    <link rel="alternate" href="/atom.xml" title="Ji ZHANG&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="/css/source-code-pro.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-XGPVRTV36D"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-XGPVRTV36D');
  </script>
<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Ji ZHANG&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">If I rest, I rust.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/categories/Big-Data">Big Data</a>
        
          <a class="main-nav-link" href="/categories/Programming">Programming</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
        <a class="main-nav-link" href="https://shzhangji.com/cnblogs"><img src="/images/cnblogs.png"></a>
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://shzhangji.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-flume-source-code-hdfs-sink" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2018/10/03/flume-source-code-hdfs-sink/" class="article-date">
  <time datetime="2018-10-03T11:34:11.000Z" itemprop="datePublished">2018-10-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Big-Data/">Big Data</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Flume Source Code: HDFS Sink
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Sink is the last component of Apache Flume data flow, and it is used to output data into storages like local files, HDFS, ElasticSearch, etc. In this article, I will illustrate how Flume’s HDFS sink works, by analyzing its source code with diagrams.</p>
<h2 id="Sink-Component-Lifecycle"><a href="#Sink-Component-Lifecycle" class="headerlink" title="Sink Component Lifecycle"></a>Sink Component Lifecycle</h2><p>In the <a href="http://shzhangji.com/blog/2017/10/23/flume-source-code-component-lifecycle/">previous article</a>, we learnt that every Flume component implements <code>LifecycleAware</code> interface, and is started and monitored by <code>LifecycleSupervisor</code>. Sink component is not directly invoked by this supervisor, but wrapped in <code>SinkRunner</code> and <code>SinkProcessor</code> classes. Flume supports three different <a target="_blank" rel="noopener" href="https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors">sink processors</a>, to connect channel and sinks in different semantics. But here we only consider the <code>DefaultSinkProcessor</code>, that accepts only one sink, and we will skip the concept of sink group as well.</p>
<p><img src="/images/flume/sink-component-lifecycle.png" alt="Sink Component LifeCycle"></p>
<span id="more"></span>

<h2 id="HDFS-Sink-Classes"><a href="#HDFS-Sink-Classes" class="headerlink" title="HDFS Sink Classes"></a>HDFS Sink Classes</h2><p>HDFS sink’s source code locates in <code>flume-hdfs-sink</code> sub-module, and is composed of the following classes:</p>
<p><img src="/images/flume/hdfs-sink-classes.png" alt="HDFS Sink Classes"></p>
<p><code>HDFSEventSink</code> class implements the lifecycle methods, including <code>configure</code>, <code>start</code>, <code>process</code>, and <code>stop</code>. It maintains a list of <code>BucketWriter</code>, according to the output file paths, and delegates received events to them. With different implementations of <code>HDFSWriter</code>, <code>BucketWriter</code> can append data to either text file, compressed file, or sequence file.</p>
<h2 id="Configure-and-Start"><a href="#Configure-and-Start" class="headerlink" title="Configure and Start"></a>Configure and Start</h2><p>When Flume configuration file is loaded, <code>configure</code> method is called on every sink component. In <code>HDFSEventSink#configure</code>, it reads properties that are prefixed with <code>hdfs.</code> from the context, provides default values, and does some sanity checks. For instance, <code>batchSize</code> must be greater than 0, <code>codeC</code> must be provided when <code>fileType</code> is <code>CompressedStream</code>, etc. It also initializes a <code>SinkCounter</code> to provide various metrics for monitoring.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line">  filePath = Preconditions.checkNotNull(</span><br><span class="line">      context.getString(<span class="string">&quot;hdfs.path&quot;</span>), <span class="string">&quot;hdfs.path is required&quot;</span>);</span><br><span class="line">  rollInterval = context.getLong(<span class="string">&quot;hdfs.rollInterval&quot;</span>, defaultRollInterval);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (sinkCounter == <span class="literal">null</span>) &#123;</span><br><span class="line">    sinkCounter = <span class="keyword">new</span> <span class="title class_">SinkCounter</span>(getName());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>SinkProcessor</code> will invoke the <code>HDFSEventSink#start</code> method, in which two thread pools are created. <code>callTimeoutPool</code> is used by <code>BucketWriter#callWithTimeout</code> to limit the time that HDFS calls may take, such as <a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/fs/FileSystem.html"><code>FileSystem#create</code></a>, or <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/fs/FSDataOutputStream.html"><code>FSDataOutputStream#hflush</code></a>. <code>timedRollerPool</code> is used to schedule a periodic task to do time-based file rolling, if <code>rollInterval</code> property is provided. More details will be covered in the next section.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">start</span><span class="params">()</span> &#123;</span><br><span class="line">  callTimeoutPool = Executors.newFixedThreadPool(threadsPoolSize,</span><br><span class="line">      <span class="keyword">new</span> <span class="title class_">ThreadFactoryBuilder</span>().setNameFormat(timeoutName).build());</span><br><span class="line">  timedRollerPool = Executors.newScheduledThreadPool(rollTimerPoolSize,</span><br><span class="line">      <span class="keyword">new</span> <span class="title class_">ThreadFactoryBuilder</span>().setNameFormat(rollerName).build());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Process-Events"><a href="#Process-Events" class="headerlink" title="Process Events"></a>Process Events</h2><p>The <code>process</code> method contains the main logic, i.e. pull events from upstream channel and send them to HDFS. Here is the flow chart of this method.</p>
<p><img src="/images/flume/process-method-flow-chart.png" alt="Process Method Flow Chart"></p>
<h3 id="Channel-Transaction"><a href="#Channel-Transaction" class="headerlink" title="Channel Transaction"></a>Channel Transaction</h3><p>Codes are wrapped in a channel transaction, with some exception handlings. Take Kafka channel for instance, when transaction begins, it takes events without committing the offset. Only after it successfully writes these events into HDFS, the consumed offset will be sent to Kafka. And in the next transaction, it can consume messages from the new offset.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Channel</span> <span class="variable">channel</span> <span class="operator">=</span> getChannel();</span><br><span class="line"><span class="type">Transaction</span> <span class="variable">transaction</span> <span class="operator">=</span> channel.getTransaction();</span><br><span class="line">transaction.begin()</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  event = channel.take();</span><br><span class="line">  bucketWriter.append(event);</span><br><span class="line">  transaction.commit()</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable th) &#123;</span><br><span class="line">  transaction.rollback();</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">EventDeliveryException</span>(th);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  transaction.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Find-or-Create-BucketWriter"><a href="#Find-or-Create-BucketWriter" class="headerlink" title="Find or Create BucketWriter"></a>Find or Create BucketWriter</h3><p><code>BucketWriter</code> corresponds to an HDFS file, and the file path is generated from configuration. For example:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a1.sinks.access_log.hdfs.path = /user/flume/access_log/dt=%Y%m%d</span><br><span class="line">a1.sinks.access_log.hdfs.filePrefix = events.%[localhost]</span><br><span class="line">a1.sinks.access_log.hdfs.inUsePrefix = .</span><br><span class="line">a1.sinks.access_log.hdfs.inUseSuffix = .tmp</span><br><span class="line">a1.sinks.access_log.hdfs.rollInterval = 300</span><br><span class="line">a1.sinks.access_log.hdfs.fileType = CompressedStream</span><br><span class="line">a1.sinks.access_log.hdfs.codeC = lzop</span><br></pre></td></tr></table></figure>

<p>The generated file paths, temporary and final, will be:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/user/flume/access_log/dt=20180925/.events.hostname1.1537848761307.lzo.tmp</span><br><span class="line">/user/flume/access_log/dt=20180925/events.hostname1.1537848761307.lzo</span><br></pre></td></tr></table></figure>

<p>Placeholders are replaced in <a target="_blank" rel="noopener" href="https://flume.apache.org/releases/content/1.4.0/apidocs/org/apache/flume/formatter/output/BucketPath.html"><code>BucketPath#escapeString</code></a>. It supports three kinds of placeholders:</p>
<ul>
<li><code>%&#123;...&#125;</code>: replace with arbitrary header values;</li>
<li><code>%[...]</code>: currently only supports <code>%[localhost]</code>, <code>%[ip]</code>, and <code>%[fqdn]</code>;</li>
<li><code>%x</code>: date time patterns, which requires a <code>timestamp</code> entry in headers, or <code>useLocalTimeStamp</code> is enabled.</li>
</ul>
<p>And the prefix and suffix is added in <code>BucketWriter#open</code>. <code>counter</code> is the timestamp when this bucket is opened or re-opened, and <code>lzo</code> is the default extension of the configured compression codec.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">fullFileName</span> <span class="operator">=</span> fileName + <span class="string">&quot;.&quot;</span> + counter;</span><br><span class="line">fullFileName += fileSuffix;</span><br><span class="line">fullFileName += codeC.getDefaultExtension();</span><br><span class="line">bucketPath = filePath + <span class="string">&quot;/&quot;</span> + inUsePrefix + fullFileName + inUseSuffix;</span><br><span class="line">targetPath = filePath + <span class="string">&quot;/&quot;</span> + fullFileName;</span><br></pre></td></tr></table></figure>

<p>If no <code>BucketWriter</code> is associated with the file path, a new one will be created. First, it creates an <code>HDFSWriter</code> corresponding to the <code>fileType</code> config. Flume supports three kinds of writers: <code>HDFSSequenceFile</code>, <code>HDFSDataStream</code>, and <code>HDFSCompressedDataStream</code>. They handle the actual writing to HDFS files, and will be assigned to the new <code>BucketWriter</code>.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bucketWriter = sfWriters.get(lookupPath);</span><br><span class="line"><span class="keyword">if</span> (bucketWriter == <span class="literal">null</span>) &#123;</span><br><span class="line">  hdfsWriter = writerFactory.getWriter(fileType);</span><br><span class="line">  bucketWriter = <span class="keyword">new</span> <span class="title class_">BucketWriter</span>(hdfsWriter);</span><br><span class="line">  sfWriters.put(lookupPath, bucketWriter);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Append-Data-and-Flush"><a href="#Append-Data-and-Flush" class="headerlink" title="Append Data and Flush"></a>Append Data and Flush</h3><p>Before appending data, <code>BucketWriter</code> will first self-check whether it is opened. If not, it will call its underlying <code>HDFSWriter</code> to open a new file on HDFS filesystem. Take <code>HDFSCompressedDataStream</code> for instance:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">(String filePath, CompressionCodec codec)</span> &#123;</span><br><span class="line">  <span class="type">FileSystem</span> <span class="variable">hdfs</span> <span class="operator">=</span> dstPath.getFileSystem(conf);</span><br><span class="line">  fsOut = hdfs.append(dstPath)</span><br><span class="line">  compressor = CodedPool.getCompressor(codec, conf);</span><br><span class="line">  cmpOut = codec.createOutputStream(fsOut, compressor);</span><br><span class="line">  serializer = EventSerializerFactory.getInstance(serializerType, cmpOut);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">append</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IO Exception &#123;</span><br><span class="line">  serializer.write(event);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Flume’s default <code>serializerType</code> is <code>TEXT</code>, i.e. <a target="_blank" rel="noopener" href="https://flume.apache.org/releases/content/1.4.0/apidocs/org/apache/flume/serialization/BodyTextEventSerializer.html">BodyTextEventSerializer</a> that simply writes the event content to the output stream.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  out.write(e.getBody());</span><br><span class="line">  <span class="keyword">if</span> (appendNewline) &#123;</span><br><span class="line">    out.write(<span class="string">&#x27;\n&#x27;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>When <code>BucketWriter</code> is about to close or re-open, it calls <code>sync</code> on <code>HDFSWrtier</code>, which in turn calls <code>flush</code> on serializer and underlying output stream.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sync</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  serializer.flush();</span><br><span class="line">  compOut.finish();</span><br><span class="line">  fsOut.flush();</span><br><span class="line">  hflushOrSync(fsOut);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>From Hadoop 0.21.0, the <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/fs/Syncable.html"><code>Syncable#sync</code></a> method is divided into <code>hflush</code> and <code>hsync</code> methods. Former just flushes data out of client’s buffer, while latter guarantees data is synced to disk device. In order to handle both old and new API, Flume will use Java reflection to determine whether <code>hflush</code> exists, or fall back to <code>sync</code>. The <code>flushOrSync</code> method will invoke the right method.</p>
<h3 id="File-Rotation"><a href="#File-Rotation" class="headerlink" title="File Rotation"></a>File Rotation</h3><p>In HDFS sink, files can be rotated by file size, event count, or time interval. <code>BucketWriter#shouldRotate</code> is called in every <code>append</code>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">shouldRotate</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">boolean</span> <span class="variable">doRotate</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">if</span> ((rollCount &gt; <span class="number">0</span>) &amp;&amp; (rollCount &lt;= eventCounter)) &#123;</span><br><span class="line">    doRotate = <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> ((rollSize &gt; <span class="number">0</span>) &amp;&amp; (rollSize &lt;= processSize)) &#123;</span><br><span class="line">    doRotate = <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> doRotate;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Time-based rolling, on the other hand, is scheduled in the previously mentioned <code>timedRollerPool</code>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">  <span class="keyword">if</span> (rollInterval &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    Callable&lt;Void&gt; action = <span class="keyword">new</span> <span class="title class_">Callable</span>&lt;Void&gt;() &#123;</span><br><span class="line">      <span class="keyword">public</span> Void <span class="title function_">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        close(<span class="literal">true</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    timedRollFuture = timedRollerPool.schedule(action, rollInterval);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Close-and-Stop"><a href="#Close-and-Stop" class="headerlink" title="Close and Stop"></a>Close and Stop</h2><p>In <code>HDFSEventSink#close</code>, it iterates every <code>BucketWriter</code> and calls its <code>close</code> method, which in turn calls its underlying <code>HDFSWriter</code>‘s <code>close</code> method. What it does is mostly like <code>flush</code> method, but also closes the output stream and invokes some callback functions, like removing current <code>BucketWriter</code> from the <code>sfWriters</code> hash map.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">(<span class="type">boolean</span> callCloseCallback)</span> &#123;</span><br><span class="line">  writer.close();</span><br><span class="line">  timedRollFuture.cancel(<span class="literal">false</span>);</span><br><span class="line">  onCloseCallback.run(onCloseCallbackPath);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The <code>onCloseCallback</code> is passed from <code>HDFSEventSink</code> when initializing the <code>BucketWriter</code>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">WriterCallback</span> <span class="variable">closeCallback</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">WriterCallback</span>() &#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(String bucketPath)</span> &#123;</span><br><span class="line">      <span class="keyword">synchronized</span> (sfWritersLock) &#123;</span><br><span class="line">        sfWriters.remove(bucketPath);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">bucketWriter = <span class="keyword">new</span> <span class="title class_">BucketWriter</span>(lookPath, closeCallback);</span><br></pre></td></tr></table></figure>

<p>After all <code>BucketWriter</code>s are closed, <code>HDFSEventSink</code> then shutdown the <code>callTimeoutPool</code> and <code>timedRollerPool</code> executer services.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ExecutorService[] toShutdown = &#123; callTimeoutPool, timedRollerPool &#125;;</span><br><span class="line"><span class="keyword">for</span> (ExecutorService execService : toShutdown) &#123;</span><br><span class="line">  execService.shutdown();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://flume.apache.org/FlumeUserGuide.html#hdfs-sink">https://flume.apache.org/FlumeUserGuide.html#hdfs-sink</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/apache/flume">https://github.com/apache/flume</a></li>
<li><a target="_blank" rel="noopener" href="https://data-flair.training/blogs/flume-sink-processors/">https://data-flair.training/blogs/flume-sink-processors/</a></li>
<li><a target="_blank" rel="noopener" href="http://hadoop-hbase.blogspot.com/2012/05/hbase-hdfs-and-durable-sync.html">http://hadoop-hbase.blogspot.com/2012/05/hbase-hdfs-and-durable-sync.html</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://shzhangji.com/blog/2018/10/03/flume-source-code-hdfs-sink/" data-id="cl53l7zqt0020auos7p930mnh" class="article-share-link">Share</a>
      
        <a href="https://shzhangji.com/blog/2018/10/03/flume-source-code-hdfs-sink/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/flume/" rel="tag">flume</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hdfs/" rel="tag">hdfs</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/" rel="tag">java</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2018/12/08/spark-datasource-api-v2/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Spark DataSource API V2
        
      </div>
    </a>
  
  
    <a href="/blog/2018/09/20/how-to-avoid-null-pointer-exception/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">How to Avoid NullPointerException</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/algorithm/" style="font-size: 10px;">algorithm</a> <a href="/tags/analytics/" style="font-size: 15px;">analytics</a> <a href="/tags/apache-beam/" style="font-size: 10px;">apache beam</a> <a href="/tags/bootstrap/" style="font-size: 10px;">bootstrap</a> <a href="/tags/canal/" style="font-size: 10px;">canal</a> <a href="/tags/clojure/" style="font-size: 10px;">clojure</a> <a href="/tags/crossfilter/" style="font-size: 10px;">crossfilter</a> <a href="/tags/dc-js/" style="font-size: 10px;">dc.js</a> <a href="/tags/devops/" style="font-size: 10px;">devops</a> <a href="/tags/eclipse/" style="font-size: 10px;">eclipse</a> <a href="/tags/elasticsearch/" style="font-size: 10px;">elasticsearch</a> <a href="/tags/es6/" style="font-size: 10px;">es6</a> <a href="/tags/eslint/" style="font-size: 10px;">eslint</a> <a href="/tags/etl/" style="font-size: 13.33px;">etl</a> <a href="/tags/flask/" style="font-size: 11.67px;">flask</a> <a href="/tags/flink/" style="font-size: 11.67px;">flink</a> <a href="/tags/flume/" style="font-size: 13.33px;">flume</a> <a href="/tags/frontend/" style="font-size: 15px;">frontend</a> <a href="/tags/functional-programming/" style="font-size: 10px;">functional programming</a> <a href="/tags/github/" style="font-size: 10px;">github</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 10px;">hbase</a> <a href="/tags/hdfs/" style="font-size: 11.67px;">hdfs</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/hive/" style="font-size: 11.67px;">hive</a> <a href="/tags/java/" style="font-size: 18.33px;">java</a> <a href="/tags/javascript/" style="font-size: 16.67px;">javascript</a> <a href="/tags/kafka/" style="font-size: 11.67px;">kafka</a> <a href="/tags/kubernetes/" style="font-size: 11.67px;">kubernetes</a> <a href="/tags/lodash/" style="font-size: 11.67px;">lodash</a> <a href="/tags/machine-learning/" style="font-size: 10px;">machine learning</a> <a href="/tags/mapreduce/" style="font-size: 10px;">mapreduce</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/openapi/" style="font-size: 10px;">openapi</a> <a href="/tags/ops/" style="font-size: 10px;">ops</a> <a href="/tags/pandas/" style="font-size: 11.67px;">pandas</a> <a href="/tags/prometheus/" style="font-size: 10px;">prometheus</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/react/" style="font-size: 10px;">react</a> <a href="/tags/restful/" style="font-size: 10px;">restful</a> <a href="/tags/scala/" style="font-size: 11.67px;">scala</a> <a href="/tags/scalatra/" style="font-size: 10px;">scalatra</a> <a href="/tags/source-code/" style="font-size: 10px;">source code</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/spark-streaming/" style="font-size: 10px;">spark streaming</a> <a href="/tags/spring/" style="font-size: 10px;">spring</a> <a href="/tags/sql/" style="font-size: 11.67px;">sql</a> <a href="/tags/sqlalchemy/" style="font-size: 10px;">sqlalchemy</a> <a href="/tags/stream-processing/" style="font-size: 13.33px;">stream processing</a> <a href="/tags/tensorflow/" style="font-size: 10px;">tensorflow</a> <a href="/tags/thrift/" style="font-size: 10px;">thrift</a> <a href="/tags/typescript/" style="font-size: 10px;">typescript</a> <a href="/tags/vite/" style="font-size: 10px;">vite</a> <a href="/tags/vue/" style="font-size: 13.33px;">vue</a> <a href="/tags/vuex/" style="font-size: 10px;">vuex</a> <a href="/tags/webjars/" style="font-size: 10px;">webjars</a> <a href="/tags/websocket/" style="font-size: 10px;">websocket</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">July 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/05/">May 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/10/">October 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/04/">April 2013</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2022/07/01/monitor-kubernetes-volume-storage/">Monitor Kubernetes Volume Storage</a>
          </li>
        
          <li>
            <a href="/blog/2022/06/26/write-your-own-flask-sqlalchemy-extension/">Write Your Own Flask SQLAlchemy Extension</a>
          </li>
        
          <li>
            <a href="/blog/2022/06/19/openapi-workflow-with-flask-and-typescript/">OpenAPI Workflow with Flask and TypeScript</a>
          </li>
        
          <li>
            <a href="/blog/2022/06/11/use-bootstrap-v5-in-vue3-project/">Use Bootstrap V5 in Vue 3 Project</a>
          </li>
        
          <li>
            <a href="/blog/2022/06/03/migrate-from-hexo-deployer-git-to-github-actions/">Migrate from hexo-deployer-git to GitHub Actions</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://mirrors.creativecommons.org/presskit/buttons/80x15/svg/by-nc-sa.svg"></a>
      <br>
      &copy; 2022 Ji ZHANG<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/categories/Big-Data" class="mobile-nav-link">Big Data</a>
  
    <a href="/categories/Programming" class="mobile-nav-link">Programming</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
  <a href="https://shzhangji.com/cnblogs" class="mobile-nav-link">中文</a>
</nav>

    
<script>
  var disqus_shortname = 'jizhang';
  
  var disqus_url = 'https://shzhangji.com/blog/2018/10/03/flume-source-code-hdfs-sink/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>



<script src="/js/jquery.min.js"></script>



  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div>
</body>
</html>