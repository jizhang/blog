<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Ji ZHANG's Blog]]></title>
  <link href="http://shzhangji.com/atom.xml" rel="self"/>
  <link href="http://shzhangji.com/"/>
  <updated>2015-09-10T13:41:39+08:00</updated>
  <id>http://shzhangji.com/</id>
  <author>
    <name><![CDATA[Ji ZHANG]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[贫血领域模型]]></title>
    <link href="http://shzhangji.com/blog/2015/09/05/anemic-domain-model/"/>
    <updated>2015-09-05T19:02:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/09/05/anemic-domain-model</id>
    <content type="html"><![CDATA[<p>原文：<a href="http://www.martinfowler.com/bliki/AnemicDomainModel.html">http://www.martinfowler.com/bliki/AnemicDomainModel.html</a></p>

<p>贫血领域模型是一个存在已久的反模式，目前仍有许多拥趸者。一次我和Eric Evans聊天谈到它时，都觉得这个模型似乎越来越流行了。作为<a href="http://martinfowler.com/eaaCatalog/domainModel.html">领域模型</a>的推广者，我们觉得这不是一件好事。</p>

<p>贫血领域模型的最初症状是：它第一眼看起来还真像这么回事儿。项目中有许多对象，它们的命名都是根据领域来的。对象之间有着丰富的连接方式，和真正的领域模型非常相似。但当你检视这些对象的行为时，会发现它们基本上没有任何行为，仅仅是一堆getter和setter的集合。其实这些对象在设计之初就被定义为只能包含数据，不能加入领域逻辑。这些逻辑要全部写入一组叫Service的对象中。这些Service构建在领域模型之上，使用这些模型来传递数据。</p>

<p>这种反模式的恐怖之处在于，它完全是和面向对象设计背道而驰。面向对象设计主张将数据和行为绑定在一起，而贫血领域模型则更像是一种面向过程设计，我和Eric在Smalltalk时就极力反对这种做法。更糟糕的时，很多人认为这些贫血领域对象是真正的对象，从而彻底误解了面向对象设计的涵义。</p>

<!-- more -->


<p>如今，面向对象的概念已经传播得很广泛了，而要反对这种贫血领域模型的做法，我还需要更多论据。贫血领域模型的根本问题在于，它引入了领域模型设计的所有成本，却没有带来任何好处。最主要的成本是将对象映射到数据库中，从而产生了一个对象关系映射层。只有当你充分使用了面向对象设计来组织复杂的业务逻辑后，这一成本才能够被抵消。如果将所有行为都写入到Service对象，那最终你会得到一组<a href="http://martinfowler.com/eaaCatalog/transactionScript.html">事务处理脚本</a>，从而错过了领域模型带来的好处。正如我在<a href="http://martinfowler.com/books/eaa.html">企业应用架构模式</a>一书中说到的，领域模型并不一定是最好的工具。</p>

<p>还需要强调的是，将行为放入领域模型，这点和分层设计（领域层、持久化层、展现层等）并不冲突。因为领域模型中放入的是和领域相关的逻辑——验证、计算、商业规则等。如果你要讨论能否将数据访问和展现逻辑放入到领域模型中，这就不在本文论述范围之内了。</p>

<p>一些面向对象专家的观点有时会让人产生疑惑，他们认为的确应该有一个面向过程的<a href="http://martinfowler.com/eaaCatalog/serviceLayer.html">服务层</a>。但是，这并不意味着领域模型就不应该包含行为。事实上，服务层需要和一组富含行为的领域模型结合起来使用。</p>

<p>Eric Evans的<a href="http://domaindrivendesign.org/books/">领域驱动设计</a>一书中有关于分层的论述：</p>

<blockquote><p>应用层（也就是上文中的服务层）：用来描述应用程序所要做的工作，并调度丰富的领域模型来完成它。这个层次的任务是描述业务逻辑，或和其它项目的应用层做交互。这个层次很薄，它不包含任何业务规则或知识，仅用于调度和派发任务给下一层的领域模型。这个层次没有业务状态，但可以为用户或程序提供任务状态。</p>

<p>领域层（或者叫模型层）：用于表示业务逻辑、业务场景和规则。这个层次会控制和使用业务状态，即使这些状态最终会交由持久化层来存储。总之，这个层次是软件的核心。</p></blockquote>

<p>关键点在于服务层是很薄的——所有重要的业务逻辑都写在领域层。他在服务模式中复述了这一观点：</p>

<blockquote><p>如今人们常犯的错误是不愿花时间将业务逻辑放置到合适的领域模型中，从而逐渐形成面向过程的程序设计。</p></blockquote>

<p>我不清楚为什么这种反模式会那么常见。我怀疑是因为大多数人并没有使用过一个设计良好的领域模型，特别是那些以数据为中心的开发人员。此外，有些技术也会推动这种反模式，比如J2EE的Entity Bean，这会让我更倾向于使用<a href="http://www.martinfowler.com/bliki/POJO.html">POJO</a>领域模型。</p>

<p>总之，如果你将大部分行为都放置在服务层，那么你就会失去领域模型带来的好处。如果你将所有行为都放在服务层，那就无可救药了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[View Spark Source in Eclipse]]></title>
    <link href="http://shzhangji.com/blog/2015/09/01/view-spark-source-in-eclipse/"/>
    <updated>2015-09-01T18:38:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/09/01/view-spark-source-in-eclipse</id>
    <content type="html"><![CDATA[<p>Reading source code is a great way to learn opensource projects. I used to read Java projects&#8217; source code on <a href="http://grepcode.com/">GrepCode</a> for it is online and has very nice cross reference features. As for Scala projects such as <a href="http://spark.apache.org">Apache Spark</a>, though its source code can be found on <a href="https://github.com/apache/spark/">GitHub</a>, it&rsquo;s quite necessary to setup an IDE to view the code more efficiently. Here&rsquo;s a howto of viewing Spark source code in Eclipse.</p>

<h2>Install Eclipse and Scala IDE Plugin</h2>

<p>One can download Eclipse from <a href="http://www.eclipse.org/downloads/">here</a>. I recommend the &ldquo;Eclipse IDE for Java EE Developers&rdquo;, which contains a lot of daily-used features.</p>

<p><img src="http://shzhangji.com/images/scala-ide.png" alt="" /></p>

<p>Then go to Scala IDE&rsquo;s <a href="http://scala-ide.org/download/current.html">official site</a> and install the plugin through update site or zip archive.</p>

<h2>Generate Project File with Maven</h2>

<p>Spark is mainly built with Maven, so make sure you have Maven installed on your box, and download the latest Spark source code from <a href="http://spark.apache.org/downloads.html">here</a>, unarchive it, and execute the following command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>mvn -am -pl core -DskipTests package eclipse:eclipse
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>This command does a bunch of things. First, it indicates what modules should be built. Spark is a large project with multiple modules. Currently we&rsquo;re only interested in its core module, so <code>-pl</code> or <code>--projects</code> is used. <code>-am</code> or <code>--also-make</code> tells Maven to build core module&rsquo;s dependencies as well. We can see the module list in output:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>[INFO] Scanning for projects...
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Reactor Build Order:
</span><span class='line'>[INFO]
</span><span class='line'>[INFO] Spark Project Parent POM
</span><span class='line'>[INFO] Spark Launcher Project
</span><span class='line'>[INFO] Spark Project Networking
</span><span class='line'>[INFO] Spark Project Shuffle Streaming Service
</span><span class='line'>[INFO] Spark Project Unsafe
</span><span class='line'>[INFO] Spark Project Core
</span></code></pre></td></tr></table></div></figure>


<p><code>package</code> tells Maven to download all dependencies and compile the source code. If you encounter some <code>OutOfMemoryException</code>, try enlarging the heap size by setting <code>MAVEN_OPTS</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ MAVEN_OPTS</span><span class="o">=</span>-Xmx1G mvn ...
</span></code></pre></td></tr></table></div></figure>


<p><code>eclipse:eclipse</code> will generate the <code>.project</code> and <code>.classpath</code> files for Eclipse. But the result is not perfect, both files need some fixes.</p>

<p>Edit <code>core/.classpath</code>, change the following two lines:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;classpathentry</span> <span class="na">kind=</span><span class="s">&quot;src&quot;</span> <span class="na">path=</span><span class="s">&quot;src/main/scala&quot;</span> <span class="na">including=</span><span class="s">&quot;**/*.java&quot;</span><span class="nt">/&gt;</span>
</span><span class='line'><span class="nt">&lt;classpathentry</span> <span class="na">kind=</span><span class="s">&quot;src&quot;</span> <span class="na">path=</span><span class="s">&quot;src/test/scala&quot;</span> <span class="na">output=</span><span class="s">&quot;target/scala-2.10/test-classes&quot;</span> <span class="na">including=</span><span class="s">&quot;**/*.java&quot;</span><span class="nt">/&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>to</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;classpathentry</span> <span class="na">kind=</span><span class="s">&quot;src&quot;</span> <span class="na">path=</span><span class="s">&quot;src/main/scala&quot;</span> <span class="na">including=</span><span class="s">&quot;**/*.java|**/*.scala&quot;</span><span class="nt">/&gt;</span>
</span><span class='line'><span class="nt">&lt;classpathentry</span> <span class="na">kind=</span><span class="s">&quot;src&quot;</span> <span class="na">path=</span><span class="s">&quot;src/test/scala&quot;</span> <span class="na">output=</span><span class="s">&quot;target/scala-2.10/test-classes&quot;</span> <span class="na">including=</span><span class="s">&quot;**/*.java|**/*.scala&quot;</span><span class="nt">/&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Edit <code>core/.project</code>, make it looks like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;buildSpec&gt;</span>
</span><span class='line'>  <span class="nt">&lt;buildCommand&gt;</span>
</span><span class='line'>    <span class="nt">&lt;name&gt;</span>org.scala-ide.sdt.core.scalabuilder<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/buildCommand&gt;</span>
</span><span class='line'><span class="nt">&lt;/buildSpec&gt;</span>
</span><span class='line'><span class="nt">&lt;natures&gt;</span>
</span><span class='line'>  <span class="nt">&lt;nature&gt;</span>org.scala-ide.sdt.core.scalanature<span class="nt">&lt;/nature&gt;</span>
</span><span class='line'>  <span class="nt">&lt;nature&gt;</span>org.eclipse.jdt.core.javanature<span class="nt">&lt;/nature&gt;</span>
</span><span class='line'><span class="nt">&lt;/natures&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now you can import &ldquo;Existing Projects into Workspace&rdquo;, including <code>core</code>, <code>launcher</code>, <code>network</code>, and <code>unsafe</code>.</p>

<h2>Miscellaneous</h2>

<h3>Access restriction: The type &lsquo;Unsafe&rsquo; is not API</h3>

<p>For module <code>spark-unsafe</code>, Eclipse will report an error &ldquo;Access restriction: The type &lsquo;Unsafe&rsquo; is not API (restriction on required library /path/to/jre/lib/rt.jar&rdquo;. To fix this, right click the &ldquo;JRE System Library&rdquo; entry in Package Explorer, change it to &ldquo;Workspace default JRE&rdquo;.</p>

<h3>Download Sources and Javadocs</h3>

<p>Add the following entry into pom&rsquo;s project / build / plugins:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;plugin&gt;</span>
</span><span class='line'>    <span class="nt">&lt;artifactId&gt;</span>maven-eclipse-plugin<span class="nt">&lt;/artifactId&gt;</span>
</span><span class='line'>    <span class="nt">&lt;configuration&gt;</span>
</span><span class='line'>        <span class="nt">&lt;downloadSources&gt;</span>true<span class="nt">&lt;/downloadSources&gt;</span>
</span><span class='line'>        <span class="nt">&lt;downloadJavadocs&gt;</span>true<span class="nt">&lt;/downloadJavadocs&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/configuration&gt;</span>
</span><span class='line'><span class="nt">&lt;/plugin&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>build-helper-maven-plugin</h3>

<p>Since Spark is a mixture of Java and Scala code, and the maven-eclipse-plugin only knows about Java source files, so we need to use build-helper-maven-plugin to include the Scala sources, as is described <a href="http://docs.scala-lang.org/tutorials/scala-with-maven.html#integration-with-eclipse-scala-ide24">here</a>. Fortunately, Spark&rsquo;s pom.xml has already included this setting.</p>

<h2>References</h2>

<ul>
<li><a href="http://docs.scala-lang.org/tutorials/scala-with-maven.html">http://docs.scala-lang.org/tutorials/scala-with-maven.html</a></li>
<li><a href="https://wiki.scala-lang.org/display/SIW/ScalaEclipseMaven">https://wiki.scala-lang.org/display/SIW/ScalaEclipseMaven</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools">https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HotSpot JVM中的对象指针压缩]]></title>
    <link href="http://shzhangji.com/blog/2015/06/25/compressed-oops-in-the-hotspot-jvm/"/>
    <updated>2015-06-25T17:41:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/06/25/compressed-oops-in-the-hotspot-jvm</id>
    <content type="html"><![CDATA[<p>原文：<a href="https://wiki.openjdk.java.net/display/HotSpot/CompressedOops">https://wiki.openjdk.java.net/display/HotSpot/CompressedOops</a></p>

<h2>什么是一般对象指针？</h2>

<p>一般对象指针（oop, ordinary object pointer）是HotSpot虚拟机的一个术语，表示受托管的对象指针。它的大小通常和本地指针是一样的。Java应用程序和GC子系统会非常小心地跟踪这些受托管的指针，以便在销毁对象时回收内存空间，或是在对空间进行整理时移动（复制）对象。</p>

<p>在一些从Smalltalk和Self演变而来的虚拟机实现中都有一般对象指针这个术语，包括：</p>

<ul>
<li><a href="https://github.com/russellallen/self/blob/master/vm/src/any/objects/oop.hh">Self</a>：一门基于原型的语言，是Smalltalk的近亲</li>
<li><a href="http://code.google.com/p/strongtalk/wiki/VMTypesForSmalltalkObjects">Strongtalk</a>：Smalltalk的一种实现</li>
<li><a href="http://hg.openjdk.java.net/hsx/hotspot-main/hotspot/file/0/src/share/vm/oops/oop.hpp">Hotspot</a></li>
<li><a href="http://code.google.com/p/v8/source/browse/trunk/src/objects.h">V8</a></li>
</ul>


<p>部分系统中会使用小整型（smi, small integers）这个名称，表示一个指向30位整型的虚拟指针。这个术语在Smalltalk的V8实现中也可以看到。</p>

<h2>为什么需要压缩？</h2>

<p>在<a href="http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html">LP64</a>系统中，指针需要使用64位来表示；<a href="http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html">ILP32</a>系统中则只需要32位。在ILP32系统中，堆内存的大小只能支持到4Gb，这对很多应用程序来说是不够的。在LP64系统中，所有应用程序运行时占用的空间都会比ILP32大1.5倍左右，这是因为指针占用的空间增加了。虽然内存是比较廉价的，但网络带宽和缓存容量是紧张的。所以，为了解决4Gb的限制而增加堆内存的占用空间，就有些得不偿失了。</p>

<p>在x86芯片中，ILP32模式可用的寄存器数量是LP64模式的一半。SPARC没有此限制；RISC芯片本来就提供了很多寄存器，LP64模式下会提供更多。</p>

<p>压缩后的一般对象指针在使用时需要将32位整型按因数8进行扩展，并加到一个64位的基础地址上，从而找到所指向的对象。这种方法可以表示四十亿个对象，相当于32Gb的堆内存。同时，使用此法压缩数据结构也能达到和ILP32系统相近的效果。</p>

<p>我们使用<em>解码</em>来表示从32位对象指针转换成64位地址的过程，其反过程则称为<em>编码</em>。</p>

<!-- more -->


<h2>什么情况下会进行压缩？</h2>

<p>运行在ILP32模式下的Java虚拟机，或在运行时将<code>UseCompressedOops</code>标志位关闭，则所有的对象指针都不会被压缩。</p>

<p>如果<code>UseCompressedOops</code>是打开的，则以下对象的指针会被压缩：</p>

<ul>
<li>所有对象的<a href="http://stackoverflow.com/questions/16721021/what-is-klass-klassklass">klass</a>属性</li>
<li>所有<a href="http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/sun/jvm/hotspot/oops/Oop.java#Oop">对象指针实例</a>的属性</li>
<li>所有对象指针数组的元素（objArray）</li>
</ul>


<p>HotSpot VM中，用于表示Java类的数据结构是不会压缩的，这部分数据都存放在永久代（PermGen）中。</p>

<p>在解释器中，一般对象指针也是不压缩的，包括JVM本地变量和栈内元素、调用参数、返回值等。解释器会在读取堆内对象时解码对象指针，并在存入时进行编码。</p>

<p>同样，方法调用序列（method calling sequence），无论是解释执行还是编译执行，都不会使用对象指针压缩。</p>

<p>在编译后的代码中，对象指针是否压缩取决于不同的优化结果。优化后的代码可能会将压缩后的对象指针直接从一处搬往另一处，而不进行编解码操作。如果芯片（如x86）支持解码，那在使用对象指针时就不需要自行解码了。</p>

<p>所以，以下数据结构在编译后的代码中既可以是压缩后的对象指针，也可能是本地地址：</p>

<ul>
<li>寄存器或溢出槽（spill slot）中的数据</li>
<li>对象指针映射表（GC映射表）</li>
<li>调试信息</li>
<li>嵌套在机器码中的对象指针（在非RISC芯片中支持，如x86）</li>
<li><a href="http://openjdk.java.net/groups/hotspot/docs/HotSpotGlossary.html#nmethod">nmethod</a>常量区（包括那些影响到机器码的重定位操作）</li>
</ul>


<p>在HotSpot JVM的C++代码部分，对象指针压缩与否反映在C++的静态类型系统中。通常情况下，对象指针是不压缩的。具体来说，C++的成员函数在操作本地代码传递过来的指针时（如<em>this</em>），其执行过程不会有什么不同。JVM中的部分方法则提供了重载，能够处理压缩和不压缩的对象指针。</p>

<p>重要的C++数据不会被压缩：</p>

<ul>
<li>C++对象指针（<em>this</em>）</li>
<li>受托管指针的句柄（Handle类型等）</li>
<li>JNI句柄（jobject类型）</li>
</ul>


<p>C++在使用对象指针压缩时（加载和存储等），会以<code>narrowOop</code>作为标记。</p>

<h2>使用压缩寻址</h2>

<p>以下是使用对象指针压缩的x86指令示例：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>! int R8; oop[] R9;  // R9是64位
</span><span class='line'>! oop R10 = R9[R8];  // R10是32位
</span><span class='line'>! 从原始基址指针加载压缩对象指针：
</span><span class='line'>movl R10, [R9 + R8&lt;&lt;3 + 16]
</span><span class='line'>! klassOop R11 = R10._klass;  // R11是32位
</span><span class='line'>! void* const R12 = GetHeapBase();
</span><span class='line'>! 从压缩基址指针加载klass指针：
</span><span class='line'>movl R11, [R12 + R10&lt;&lt;3 + 8]
</span></code></pre></td></tr></table></div></figure>


<p>以下sparc指令用于解压对象指针（可为空）：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>! java.lang.Thread::getThreadGroup@1 (line 1072)
</span><span class='line'>! L1 = L7.group
</span><span class='line'>ld  [ %l7 + 0x44 ], %l1
</span><span class='line'>! L3 = decode(L1)
</span><span class='line'>cmp  %l1, 0
</span><span class='line'>sllx  %l1, 3, %l3
</span><span class='line'>brnz,a   %l3, .+8
</span><span class='line'>add  %l3, %g6, %l3  ! %g6是常量堆基址
</span></code></pre></td></tr></table></div></figure>


<p><em>输出中的注解来自<a href="https://wiki.openjdk.java.net/display/HotSpot/PrintAssembly">PrintAssembly插件</a>。</em></p>

<h2>空值处理</h2>

<p>32位零值会被解压为64位空值，这就需要在解码逻辑中加入一段特殊的逻辑。或者说可以默认某些压缩对象指针肯定不会空（如klass的属性），这样就能使用简单一些的编解码逻辑了。</p>

<p>隐式空值检测对JVM的性能至关重要，包括解释执行和编译执行的字节码。对于一个偏移量较小的对象指针，如果基址指针为空，那很有可能造成系统崩溃，因为虚拟地址空间的前几页通常是没有映射的。</p>

<p>对于压缩对象指针，我们可以用一种类似的技巧来欺骗它：将堆内存前几页的映射去除，如果解压出的指针为空（相对于基址指针），仍可以用它来做加载和存储的操作，隐式空值检测也能照常运行。</p>

<h2>对象头信息</h2>

<p>对象头信息通常包含几个部分：固定长度的标志位；klass信息；如果对象是数组，则包含一个32位的信息，并可能追加一个32位的空隙进行对齐；零个或多个实例属性，数组元素，元信息等。（有趣的是，Klass的对象头信息包含了一个C++的<a href="https://en.wikipedia.org/wiki/Virtual_method_table">虚拟方法表</a>）</p>

<p>上述追加的32位空隙通常也可用于存储属性信息。</p>

<p>如果<code>UseCompressedOops</code>关闭，标志位和klass都是正常长度。对于数组，32位空隙在LP64系统中总是存在；而ILP32系统中，只有当数组元素是64位数据时才存在这个空隙。</p>

<p>如果<code>UseCompressedOops</code>打开，则klass是32位的。非数组对象在klass后会追加一个空隙，而数组对象则直接开始存储元素信息。</p>

<h2>零基压缩技术</h2>

<p>压缩对象指针（narrow-oop）是基于某个地址的偏移量，这个基础地址（narrow-oop-base）是由Java堆内存基址减去一个内存页的大小得来的，从而支持隐式空值检测。所以一个属性字段的地址可以这样得到：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>&lt;narrow-oop-base&gt; + (&lt;narrow-oop&gt; &lt;&lt; 3) + &lt;field-offset&gt;.
</span></code></pre></td></tr></table></div></figure>


<p>如果基础地址可以是0（Java堆内存不一定要从0偏移量开始），那么公式就可以简化为：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>(&lt;narrow-oop &lt;&lt; 3) + &lt;field-offset&gt;
</span></code></pre></td></tr></table></div></figure>


<p>理论上说，这一步可以省去一次寄存器上的加和操作。而且使用零基压缩技术后，空值检测也就不需要了。</p>

<p>之前的解压代码是：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>if (&lt;narrow-oop&gt; == NULL)
</span><span class='line'>    &lt;wide_oop&gt; = NULL
</span><span class='line'>else
</span><span class='line'>    &lt;wide_oop&gt; = &lt;narrow-oop-base&gt; + (&lt;narrow-oop&gt; &lt;&lt; 3)
</span></code></pre></td></tr></table></div></figure>


<p>使用零基压缩后，只需使用移位操作：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>&lt;wide_oop&gt; = &lt;narrow-oop&gt; &lt;&lt; 3
</span></code></pre></td></tr></table></div></figure>


<p>零基压缩技术会根据堆内存的大小以及平台特性来选择不同的策略：</p>

<ol>
<li>堆内存小于4Gb，直接使用压缩对象指针进行寻址，无需压缩和解压；</li>
<li>堆内存大于4Gb，则尝试分配小于32Gb的堆内存，并使用零基压缩技术；</li>
<li>如果仍然失败，则使用普通的对象指针压缩技术，即<code>narrow-oop-base</code>。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Streaming Logging Configuration]]></title>
    <link href="http://shzhangji.com/blog/2015/05/31/spark-streaming-logging-configuration/"/>
    <updated>2015-05-31T18:18:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/05/31/spark-streaming-logging-configuration</id>
    <content type="html"><![CDATA[<p>Spark Streaming applications tend to run forever, so their log files should be properly handled, to avoid exploding server hard drives. This article will give some practical advices of dealing with these log files, on both Spark on YARN and standalone mode.</p>

<h2>Log4j&rsquo;s RollingFileAppender</h2>

<p>Spark uses log4j as logging facility. The default configuraiton is to write all logs into standard error, which is fine for batch jobs. But for streaming jobs, we&rsquo;d better use rolling-file appender, to cut log files by size and keep only several recent files. Here&rsquo;s an example:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='properties'><span class='line'><span class="na">log4j.rootLogger</span><span class="o">=</span><span class="s">INFO, rolling</span>
</span><span class='line'>
</span><span class='line'><span class="na">log4j.appender.rolling</span><span class="o">=</span><span class="s">org.apache.log4j.RollingFileAppender</span>
</span><span class='line'><span class="na">log4j.appender.rolling.layout</span><span class="o">=</span><span class="s">org.apache.log4j.PatternLayout</span>
</span><span class='line'><span class="na">log4j.appender.rolling.layout.conversionPattern</span><span class="o">=</span><span class="s">[%d] %p %m (%c)%n</span>
</span><span class='line'><span class="na">log4j.appender.rolling.maxFileSize</span><span class="o">=</span><span class="s">50MB</span>
</span><span class='line'><span class="na">log4j.appender.rolling.maxBackupIndex</span><span class="o">=</span><span class="s">5</span>
</span><span class='line'><span class="na">log4j.appender.rolling.file</span><span class="o">=</span><span class="s">/var/log/spark/${dm.logging.name}.log</span>
</span><span class='line'><span class="na">log4j.appender.rolling.encoding</span><span class="o">=</span><span class="s">UTF-8</span>
</span><span class='line'>
</span><span class='line'><span class="na">log4j.logger.org.apache.spark</span><span class="o">=</span><span class="s">WARN</span>
</span><span class='line'><span class="na">log4j.logger.org.eclipse.jetty</span><span class="o">=</span><span class="s">WARN</span>
</span><span class='line'>
</span><span class='line'><span class="na">log4j.logger.com.anjuke.dm</span><span class="o">=</span><span class="s">${dm.logging.level}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This means log4j will roll the log file by 50MB and keep only 5 recent files. These files are saved in <code>/var/log/spark</code> directory, with filename picked from system property <code>dm.logging.name</code>. We also set the logging level of our package <code>com.anjuke.dm</code> according to <code>dm.logging.level</code> property. Another thing to mention is that we set <code>org.apache.spark</code> to level <code>WARN</code>, so as to ignore verbose logs from spark.</p>

<!-- more -->


<h2>Standalone Mode</h2>

<p>In standalone mode, Spark Streaming driver is running on the machine where you submit the job, and each Spark worker node will run an executor for this job. So you need to setup log4j for both driver and executor.</p>

<p>For driver, since it&rsquo;s a long-running application, we tend to use some process management tools like <a href="http://supervisord.org/">supervisor</a> to monitor it. And supervisor itself provides the facility of rolling log files, so we can safely write all logs into standard output when setting up driver&rsquo;s log4j.</p>

<p>For executor, there&rsquo;re two approaches. One is using <code>spark.executor.logs.rolling.strategy</code> provided by Spark 1.1 and above. It has both time-based and size-based rolling methods. These log files are stored in Spark&rsquo;s work directory. You can find more details in the <a href="https://spark.apache.org/docs/1.1.0/configuration.html">documentation</a>.</p>

<p>The other approach is to setup log4j manually, when you&rsquo;re using a legacy version, or want to gain more control on the logging process. Here are the steps:</p>

<ol>
<li>Make sure the logging directory exists on all worker nodes. You can use some provisioning tools like <a href="https://github.com/ansible/ansible">ansbile</a> to create them.</li>
<li>Create driver&rsquo;s and executor&rsquo;s log4j configuration files, and distribute the executor&rsquo;s to all worker nodes.</li>
<li>Use the above two files in <code>spark-submit</code> command:</li>
</ol>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='properties'><span class='line'><span class="err">spark-submit</span>
</span><span class='line'>  <span class="na">--master spark</span><span class="o">:</span><span class="s">//127.0.0.1:7077</span>
</span><span class='line'>  <span class="na">--driver-java-options &quot;-Dlog4j.configuration</span><span class="o">=</span><span class="s">file:/path/to/log4j-driver.properties -Ddm.logging.level=DEBUG&quot;</span>
</span><span class='line'>  <span class="na">--conf &quot;spark.executor.extraJavaOptions</span><span class="o">=</span><span class="s">-Dlog4j.configuration=file:/path/to/log4j-executor.properties -Ddm.logging.name=myapp -Ddm.logging.level=DEBUG&quot;</span>
</span><span class='line'>  <span class="err">...</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Spark on YARN</h2>

<p><a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/index.html">YARN</a> is a <strong>resource manager</strong> introduced by Hadoop2. Now we can run differenct computational frameworks on the same cluster, like MapReduce, Spark, Storm, etc. The basic unit of YARN is called container, which represents a certain amount of resource (currently memory and virtual CPU cores). Every container has its working directory, and all related files such as application command (jars) and log files are stored in this directory.</p>

<p>When running Spark on YARN, there is a system property <code>spark.yarn.app.container.log.dir</code> indicating the container&rsquo;s log directory. We only need to replace one line of the above log4j config:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='properties'><span class='line'><span class="na">log4j.appender.rolling.file</span><span class="o">=</span><span class="s">${spark.yarn.app.container.log.dir}/spark.log</span>
</span></code></pre></td></tr></table></div></figure>


<p>And these log files can be viewed on YARN&rsquo;s web UI:</p>

<p><img src="http://shzhangji.com/images/spark/yarn-logs.png" alt="" /></p>

<p>The <code>spark-submit</code> command is as following:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='properties'><span class='line'><span class="err">spark-submit</span>
</span><span class='line'>  <span class="err">--master</span> <span class="err">yarn-cluster</span>
</span><span class='line'>  <span class="err">--files</span> <span class="err">/path/to/log4j-spark.properties</span>
</span><span class='line'>  <span class="na">--conf &quot;spark.driver.extraJavaOptions</span><span class="o">=</span><span class="s">-Dlog4j.configuration=log4j-spark.properties&quot;</span>
</span><span class='line'>  <span class="na">--conf &quot;spark.executor.extraJavaOptions</span><span class="o">=</span><span class="s">-Dlog4j.configuration=log4j-spark.properties&quot;</span>
</span><span class='line'>  <span class="err">...</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see, both driver and executor use the same configuration file. That is because in <code>yarn-cluster</code> mode, driver is also run as a container in YARN. In fact, the <code>spark-submit</code> command will just quit after job submission.</p>

<p>If YARN&rsquo;s <a href="http://zh.hortonworks.com/blog/simplifying-user-logs-management-and-access-in-yarn/">log aggregation</a> is enabled, application logs will be saved in HDFS after the job is done. One can use <code>yarn logs</code> command to view the files or browse directly into HDFS directory indicated by <code>yarn.nodemanager.log-dirs</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ElasticSearch Performance Tips]]></title>
    <link href="http://shzhangji.com/blog/2015/04/28/elasticsearch-performance-tips/"/>
    <updated>2015-04-28T23:08:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/04/28/elasticsearch-performance-tips</id>
    <content type="html"><![CDATA[<p>Recently we&rsquo;re using ElasticSearch as a data backend of our recommendation API, to serve both offline and online computed data to users. Thanks to ElasticSearch&rsquo;s rich and out-of-the-box functionality, it doesn&rsquo;t take much trouble to setup the cluster. However, we still encounter some misuse and unwise configurations. So here&rsquo;s a list of ElasticSearch performance tips that we learned from practice.</p>

<h2>Tip 1 Set Num-of-shards to Num-of-nodes</h2>

<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard">Shard</a> is the foundation of ElasticSearch&rsquo;s distribution capability. Every index is splitted into several shards (default 5) and are distributed across cluster nodes. But this capability does not come free. Since data being queried reside in all shards (this behaviour can be changed by <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-routing">routing</a>), ElasticSearch has to run this query on every shard, fetch the result, and merge them, like a map-reduce process. So if there&rsquo;re too many shards, more than the number of cluter nodes, the query will be executed more than once on the same node, and it&rsquo;ll also impact the merge phase. On the other hand, too few shards will also reduce the performance, for not all nodes are being utilized.</p>

<p>Shards have two roles, primary shard and replica shard. Replica shard serves as a backup to the primary shard. When primary goes down, the replica takes its job. It also helps improving the search and get performance, for these requests can be executed on either primary or replica shard.</p>

<p>Shards can be visualized by <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard">elasticsearch-head</a> plugin:</p>

<p><img src="http://shzhangji.com/images/elasticsearch/shards-head.png" alt="" /></p>

<p>The <code>cu_docs</code> index has two shards <code>0</code> and <code>1</code>, with <code>number_of_replicas</code> set to 1. Primary shard <code>0</code> (bold bordered) resides in server <code>Leon</code>, and its replica in <code>Pris</code>. They are green becuase all primary shards have enough repicas sitting in different servers, so the cluster is healthy.</p>

<p>Since <code>number_of_shards</code> of an index cannot be changed after creation (while <code>number_of_replicas</code> can), one should choose this config wisely. Here are some suggestions:</p>

<ol>
<li>How many nodes do you have, now and future? If you&rsquo;re sure you&rsquo;ll only have 3 nodes, set number of shards to 2 and replicas to 1, so there&rsquo;ll be 4 shards across 3 nodes. If you&rsquo;ll add some servers in the future, you can set number of shards to 3, so when the cluster grows to 5 nodes, there&rsquo;ll be 6 distributed shards.</li>
<li>How big is your index? If it&rsquo;s small, one shard with one replica will due.</li>
<li>How is the read and write frequency, respectively? If it&rsquo;s search heavy, setup more relicas.</li>
</ol>


<!-- more -->


<h2>Tip 2 Tuning Memory Usage</h2>

<p>ElasticSearch and its backend <a href="http://lucene.apache.org/">Lucene</a> are both Java application. There&rsquo;re various memory tuning settings related to heap and native memory.</p>

<h3>Set Max Heap Size to Half of Total Memory</h3>

<p>Generally speaking, more heap memory leads to better performance. But in ElasticSearch&rsquo;s case, Lucene also requires a lot of native memory (or off-heap memory), to store index segments and provide fast search performance. But it does not load the files by itself. Instead, it relies on the operating system to cache the segement files in memory.</p>

<p>Say we have 16G memory and set -Xmx to 8G, it doesn&rsquo;t mean the remaining 8G is wasted. Except for the memory OS preserves for itself, it will cache the frequently accessed disk files in memory automatically, which results in a huge performance gain.</p>

<p>Do not set heap size over 32G though, even you have more than 64G memory. The reason is described in <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops">this link</a>.</p>

<p>Also, you should probably set -Xms to 8G as well, to avoid the overhead of heap memory growth.</p>

<h3>Disable Swapping</h3>

<p>Swapping is a way to move unused program code and data to disk so as to provide more space for running applications and file caching. It also provides a buffer for the system to recover from memory exhaustion. But for critical application like ElasticSearch, being swapped is definitely a performance killer.</p>

<p>There&rsquo;re several ways to disable swapping, and our choice is setting <code>bootstrap.mlockall</code> to true. This tells ElasticSearch to lock its memory space in RAM so that OS will not swap it out. One can confirm this setting via <code>http://localhost:9200/_nodes/process?pretty</code>.</p>

<p>If ElasticSearch is not started as root (and it probably shouldn&rsquo;t), this setting may not take effect. For Ubuntu server, one needs to add <code>&lt;user&gt; hard memlock unlimited</code> to <code>/etc/security/limits.conf</code>, and run <code>ulimit -l unlimited</code> before starting ElasticSearch process.</p>

<h3>Increase <code>mmap</code> Counts</h3>

<p>ElasticSearch uses memory mapped files, and the default <code>mmap</code> counts is low. Add <code>vm.max_map_count=262144</code> to <code>/etc/sysctl.conf</code>, run <code>sysctl -p /etc/sysctl.conf</code> as root, and then restart ElasticSearch.</p>

<h2>Tip 3 Setup a Cluster with Unicast</h2>

<p>ElasticSearch has two options to form a cluster, multicast and unicast. The former is suitable when you have a large group of servers and a well configured network. But we found unicast more concise and less error-prone.</p>

<p>Here&rsquo;s an example of using unicast:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>node.name: "NODE-1"
</span><span class='line'>discovery.zen.ping.multicast.enabled: false
</span><span class='line'>discovery.zen.ping.unicast.hosts: ["node-1.example.com", "node-2.example.com", "node-3.example.com"]
</span><span class='line'>discovery.zen.minimum_master_nodes: 2</span></code></pre></td></tr></table></div></figure>


<p>The <code>discovery.zen.minimum_master_nodes</code> setting is a way to prevent split-brain symptom, i.e. more than one node thinks itself the master of the cluster. And for this setting to work, you should have an odd number of nodes, and set this config to <code>ceil(num_of_nodes / 2)</code>. In the above cluster, you can lose at most one node. It&rsquo;s much like a quorum in <a href="http://zookeeper.apache.org">Zookeeper</a>.</p>

<h2>Tip 4 Disable Unnecessary Features</h2>

<p>ElasticSearch is a full-featured search engine, but you should always tailor it to your own needs. Here&rsquo;s a brief list:</p>

<ul>
<li>Use corrent index type. There&rsquo;re <code>index</code>, <code>not_analyzed</code>, and <code>no</code>. If you don&rsquo;t need to search the field, set it to <code>no</code>; if you only search for full match, use <code>not_analyzed</code>.</li>
<li>For search-only fields, set <code>store</code> to false.</li>
<li>Disable <code>_all</code> field, if you always know which field to search.</li>
<li>Disable <code>_source</code> fields, if documents are big and you don&rsquo;t need the update capability.</li>
<li>If you have a document key, set this field in <code>_id</code> &ndash; <code>path</code>, instead of index the field twice.</li>
<li>Set <code>index.refresh_interval</code> to a larger number (default 1s), if you don&rsquo;t need near-realtime search. It&rsquo;s also an important option in bulk-load operation described below.</li>
</ul>


<h2>Tip 5 Use Bulk Operations</h2>

<p><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/bulk.html">Bulk is cheaper</a></p>

<ul>
<li>Bulk Read

<ul>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html">Multi Get</a> to retrieve multiple documents by a list of ids.</li>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html">Scroll</a> to search a large number of documents.</li>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/1.4/msearch.html">MultiSearch api</a> to run search requests in parallel.</li>
</ul>
</li>
<li>Bulk Write

<ul>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/1.4/bulk.html">Bulk API</a> to index, update, delete multiple documents.</li>
<li>Alter <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html">index aliases</a> simultaneously.</li>
</ul>
</li>
<li>Bulk Load: when initially building a large index, do the following,

<ul>
<li>Set <code>number_of_relicas</code> to 0, so no relicas will be created;</li>
<li>Set <code>index.refresh_interval</code> to -1, disabling nrt search;</li>
<li>Bulk build the documents;</li>
<li>Call <code>optimize</code> on the index, so newly built docs are available for search;</li>
<li>Reset replicas and refresh interval, let ES cluster recover to green.</li>
</ul>
</li>
</ul>


<h2>Miscellaneous</h2>

<ul>
<li>File descriptors: system default is too small for ES, set it to 64K will be OK. If <code>ulimit -n 64000</code> does not work, you need to add <code>&lt;user&gt; hard nofile 64000</code> to <code>/etc/security/limits.conf</code>, just like the <code>memlock</code> setting mentioned above.</li>
<li>When using ES client library, it will create a lot of worker threads according to the number of processors. Sometimes it&rsquo;s not necessary. This behaviour can be changed by setting <code>processors</code> to a lower value like 2:</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">settings</span> <span class="k">=</span> <span class="nc">ImmutableSettings</span><span class="o">.</span><span class="n">settingsBuilder</span><span class="o">()</span>
</span><span class='line'>    <span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;cluster.name&quot;</span><span class="o">,</span> <span class="s">&quot;elasticsearch&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">&quot;processors&quot;</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="n">build</span><span class="o">()</span>
</span><span class='line'><span class="k">val</span> <span class="n">uri</span> <span class="k">=</span> <span class="nc">ElasticsearchClientUri</span><span class="o">(</span><span class="s">&quot;elasticsearch://127.0.0.1:9300&quot;</span><span class="o">)</span>
</span><span class='line'><span class="nc">ElasticClient</span><span class="o">.</span><span class="n">remote</span><span class="o">(</span><span class="n">settings</span><span class="o">,</span> <span class="n">uri</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<h2>References</h2>

<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html">https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html</a></li>
<li><a href="http://cpratt.co/how-many-shards-should-elasticsearch-indexes-have/">http://cpratt.co/how-many-shards-should-elasticsearch-indexes-have/</a></li>
<li><a href="https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing">https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing</a></li>
<li><a href="https://www.loggly.com/blog/nine-tips-configuring-elasticsearch-for-high-performance/">https://www.loggly.com/blog/nine-tips-configuring-elasticsearch-for-high-performance/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache HBase的适用场景]]></title>
    <link href="http://shzhangji.com/blog/2015/03/08/hbase-dos-and-donts/"/>
    <updated>2015-03-08T08:03:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/03/08/hbase-dos-and-donts</id>
    <content type="html"><![CDATA[<p>原文：<a href="http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/">http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/</a></p>

<p>最近我在<a href="http://www.meetup.com/LA-HUG/">洛杉矶Hadoop用户组</a>做了一次关于<a href="http://www.meetup.com/LA-HUG/pages/Video_from_April_13th_HBASE_DO%27S_and_DON%27TS/">HBase适用场景</a>的分享。在场的听众水平都很高，给到了我很多值得深思的反馈。主办方是来自Shopzilla的Jody，我非常感谢他能给我一个在60多位Hadoop使用者面前演讲的机会。可能一些朋友没有机会来洛杉矶参加这次会议，我将分享中的主要内容做了一个整理。如果你没有时间阅读全文，以下是一些摘要：</p>

<ul>
<li>HBase很棒，但不是关系型数据库或HDFS的替代者；</li>
<li>配置得当才能运行良好；</li>
<li>监控，监控，监控，重要的事情要说三遍。</li>
</ul>


<p>Cloudera是HBase的铁杆粉丝。我们热爱这项技术，热爱这个社区，发现它能适用于非常多的应用场景。HBase如今已经有很多<a href="#use-cases">成功案例</a>，所以很多公司也在考虑如何将其应用到自己的架构中。我做这次分享以及写这篇文章的动因就是希望能列举出HBase的适用场景，并提醒各位哪些场景是不适用的，以及如何做好HBase的部署。</p>

<!-- more -->


<h2>何时使用HBase</h2>

<p>虽然HBase是一种绝佳的工具，但我们一定要记住，它并非银弹。HBase并不擅长传统的事务处理程序或关联分析，它也不能完全替代MapReduce过程中使用到的HDFS。从文末的<a href="#use-cases">成功案例</a>中你可以大致了解HBase适用于怎样的应用场景。如果你还有疑问，可以到<a href="http://www.cloudera.com/community/">社区</a>中提问，我说过这是一个非常棒的社区。</p>

<p>除去上述限制之外，你为何要选择HBase呢？如果你的应用程序中，数据表每一行的结构是有差别的，那就可以考虑使用HBase，比如在标准化建模的过程中使用它；如果你需要经常追加字段，且大部分字段是NULL值的，那可以考虑HBase；如果你的数据（包括元数据、消息、二进制数据等）都有着同一个主键，那就可以使用HBase；如果你需要通过键来访问和修改数据，使用HBase吧。</p>

<h2>后台服务</h2>

<p>如果你已决定尝试一下HBase，那以下是一些部署过程中的提示。HBase会用到一些后台服务，这些服务非常关键。如果你之前没有了解过ZooKeeper，那现在是个好时候。HBase使用ZooKeeper作为它的分布式协调服务，用于选举Master等。随着HBase的发展，ZooKeeper发挥的作用越来越重要。另外，你需要搭建合适的网络基础设施，如NTP和DNS。HBase要求集群内的所有服务器时间一致，并且能正确地访问其它服务器。正确配置NTP和DNS可以杜绝一些奇怪的问题，如服务器A认为当前是明天，B认为当前是昨天；再如Master要求服务器C开启新的Region，而C不知道自己的机器名，从而无法响应。NTP和DNS服务器可以让你减少很多麻烦。</p>

<p>我前面提到过，在考虑是否使用HBase时，需要针对你自己的应用场景来进行判别。而在真正使用HBase时，监控则成了第一要务。和大多数分布式服务一样，HBase服务器宕机会有多米诺骨牌效应。如果一台服务器因内存不足开始swap数据，它会失去和Master的联系，这时Master会命令其他服务器接过这部分请求，可能会导致第二台服务器也发生宕机。所以，你需要密切监控服务器的CPU、I/O以及网络延迟，确保每台HBase服务器都在良好地工作。监控对于维护HBase集群的健康至关重要。</p>

<h2>HBase架构最佳实践</h2>

<p>当你找到了适用场景，并搭建起一个健康的HBase集群后，我们来看一些使用过程中的最佳实践。键的前缀要有良好的分布性。如果你使用时间戳或其他类似的递增量作为前缀，那就会让单个Region承载所有请求，而不是分布到各个Region上。此外，你需要根据Memstore和内存的大小来控制Region的数量。RegionServer的JVM堆内存应该控制在12G以内，从而避免过长的GC停顿。举个例子，在一台内存为36G的服务器上部署RegionServer，同时还运行着DataNode，那大约可以提供100个48M大小的Region。这样的配置对HDFS、HBase、以及Linux本身的文件缓存都是有利的。</p>

<p>其他一些设置包括禁用自动合并机制（默认的合并操作会在HBase启动后每隔24小时进行），改为手动的方式在低峰期间执行。你还应该配置数据文件压缩（如LZO），并将正确的配置文件加入HBase的CLASSPATH中。</p>

<h2>非适用场景</h2>

<p>上文讲述了HBase的适用场景和最佳实践，以下则是一些需要规避的问题。比如，不要期许HBase可以完全替代关系型数据库——虽然它在许多方面都表现优秀。它不支持SQL，也没有优化器，更不能支持跨越多条记录的事务或关联查询。如果你用不到这些特性，那HBase将是你的不二选择。</p>

<p>在复用HBase的服务器时有一些注意事项。如果你需要保证HBase的服务器质量，同时又想在HBase上运行批处理脚本（如使用Pig从HBase中获取数据进行处理），建议还是另搭一套集群。HBase在处理大量顺序I/O操作时（如MapReduce），其CPU和内存资源将会十分紧张。将这两类应用放置在同一集群上会造成不可预估的服务延迟。此外，共享集群时还需要调低任务槽（task slot）的数量，至少要留一半的CPU核数给HBase。密切关注内存，因为一旦发生swap，HBase很可能会停止心跳，从而被集群判为无效，最终产生一系列宕机。</p>

<h2>总结</h2>

<p>最后要提的一点是，在加载数据到HBase时，应该使用MapReduce+HFileOutputFormat来实现。如果仅使用客户端API，不仅速度慢，也没有充分利用HBase的分布式特性。</p>

<p>用一句话概述，HBase可以让你用键来存储和搜索数据，且无需定义表结构。</p>

<h2><a id="use-cases"></a>使用案例</h2>

<ul>
<li>Apache HBase: <a href="http://wiki.apache.org/hadoop/Hbase/PoweredBy">Powered By HBase Wiki</a></li>
<li>Mozilla: <a href="http://blog.mozilla.com/webdev/2010/07/26/moving-socorro-to-hbase/">Moving Socorro to HBase</a></li>
<li>Facebook: <a href="http://highscalability.com/blog/2010/11/16/facebooks-new-real-time-messaging-system-hbase-to-store-135.html">Facebook’s New Real-Time Messaging System: HBase</a></li>
<li>StumbleUpon: <a href="http://www.stumbleupon.com/devblog/hbase_at_stumbleupon/">HBase at StumbleUpon</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深入理解Reduce-side Join]]></title>
    <link href="http://shzhangji.com/blog/2015/01/13/understand-reduce-side-join/"/>
    <updated>2015-01-13T14:20:00+08:00</updated>
    <id>http://shzhangji.com/blog/2015/01/13/understand-reduce-side-join</id>
    <content type="html"><![CDATA[<p>在《<a href="http://www.amazon.com/MapReduce-Design-Patterns-Effective-Algorithms/dp/1449327176">MapReduce Design Patterns</a>》一书中，作者给出了Reduce-side Join的实现方法，大致步骤如下：</p>

<p><img src="http://shzhangji.com/images/reduce-side-join/reduce-side-join.png" alt="" /></p>

<ol>
<li>使用<a href="https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/MultipleInputs.html">MultipleInputs</a>指定不同的来源表和相应的Mapper类；</li>
<li>Mapper输出的Key为Join的字段内容，Value为打了来源表标签的记录；</li>
<li>Reducer在接收到同一个Key的记录后，执行以下两步：

<ol>
<li>遍历Values，根据标签将来源表的记录分别放到两个List中；</li>
<li>遍历两个List，输出Join结果。</li>
</ol>
</li>
</ol>


<p>具体实现可以参考<a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/InnerJoinJob.java">这段代码</a>。但是这种实现方法有一个问题：如果同一个Key的记录数过多，存放在List中就会占用很多内存，严重的会造成内存溢出（Out of Memory, OOM）。这种方法在一对一的情况下没有问题，而一对多、多对多的情况就会有隐患。那么，Hive在做Reduce-side Join时是如何避免OOM的呢？两个关键点：</p>

<ol>
<li>Reducer在遍历Values时，会将前面的表缓存在内存中，对于最后一张表则边扫描边输出；</li>
<li>如果前面几张表内存中放不下，就写入磁盘。</li>
</ol>


<!-- more -->


<p>按照我们的实现，Mapper输出的Key是<code>product_id</code>，Values是打了标签的产品表（Product）和订单表（Order）的记录。从数据量来看，应该缓存产品表，扫描订单表。这就要求两表记录到达Reducer时是有序的，产品表在前，边扫描边放入内存；订单表在后，边扫描边结合产品表的记录进行输出。要让Hadoop在Shuffle&amp;Sort阶段先按<code>product_id</code>排序、再按表的标签排序，就需要用到二次排序。</p>

<p>二次排序的概念很简单，将Mapper输出的Key由单一的<code>product_id</code>修改为<code>product_id+tag</code>的复合Key就可以了，但需通过以下几步实现：</p>

<h3>自定义Key类型</h3>

<p>原来<code>product_id</code>是Text类型，我们的复合Key则要包含<code>product_id</code>和<code>tag</code>两个数据，并实现<code>WritableComparable</code>接口：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">TaggedKey</span> <span class="kd">implements</span> <span class="n">WritableComparable</span><span class="o">&lt;</span><span class="n">TaggedKey</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">private</span> <span class="n">Text</span> <span class="n">joinKey</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Text</span><span class="o">();</span>
</span><span class='line'>    <span class="kd">private</span> <span class="n">IntWritable</span> <span class="n">tag</span> <span class="o">=</span> <span class="k">new</span> <span class="n">IntWritable</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">int</span> <span class="nf">compareTo</span><span class="o">(</span><span class="n">TaggedKey</span> <span class="n">taggedKey</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="kt">int</span> <span class="n">compareValue</span> <span class="o">=</span> <span class="n">joinKey</span><span class="o">.</span><span class="na">compareTo</span><span class="o">(</span><span class="n">taggedKey</span><span class="o">.</span><span class="na">getJoinKey</span><span class="o">());</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">(</span><span class="n">compareValue</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">compareValue</span> <span class="o">=</span> <span class="n">tag</span><span class="o">.</span><span class="na">compareTo</span><span class="o">(</span><span class="n">taggedKey</span><span class="o">.</span><span class="na">getTag</span><span class="o">());</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>        <span class="k">return</span> <span class="n">compareValue</span><span class="o">;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// 此处省略部分代码</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>可以看到，在比较两个TaggedKey时，会先比较joinKey（即<code>product_id</code>），再比较<code>tag</code>。</p>

<h3>自定义分区方法</h3>

<p>默认情况下，Hadoop会对Key进行哈希，以保证相同的Key会分配到同一个Reducer中。由于我们改变了Key的结构，因此需要重新编 写分区函数：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">TaggedJoiningPartitioner</span> <span class="kd">extends</span> <span class="n">Partitioner</span><span class="o">&lt;</span><span class="n">TaggedKey</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">int</span> <span class="nf">getPartition</span><span class="o">(</span><span class="n">TaggedKey</span> <span class="n">taggedKey</span><span class="o">,</span> <span class="n">Text</span> <span class="n">text</span><span class="o">,</span> <span class="kt">int</span> <span class="n">numPartitions</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">return</span> <span class="n">taggedKey</span><span class="o">.</span><span class="na">getJoinKey</span><span class="o">().</span><span class="na">hashCode</span><span class="o">()</span> <span class="o">%</span> <span class="n">numPartitions</span><span class="o">;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>自定义分组方法</h3>

<p>同理，调用reduce函数需要传入同一个Key的所有记录，这就需要重新定义分组函数：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">TaggedJoiningGroupingComparator</span> <span class="kd">extends</span> <span class="n">WritableComparator</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">public</span> <span class="nf">TaggedJoiningGroupingComparator</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>        <span class="kd">super</span><span class="o">(</span><span class="n">TaggedKey</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@SuppressWarnings</span><span class="o">(</span><span class="s">&quot;rawtypes&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">int</span> <span class="nf">compare</span><span class="o">(</span><span class="n">WritableComparable</span> <span class="n">a</span><span class="o">,</span> <span class="n">WritableComparable</span> <span class="n">b</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">TaggedKey</span> <span class="n">taggedKey1</span> <span class="o">=</span> <span class="o">(</span><span class="n">TaggedKey</span><span class="o">)</span> <span class="n">a</span><span class="o">;</span>
</span><span class='line'>        <span class="n">TaggedKey</span> <span class="n">taggedKey2</span> <span class="o">=</span> <span class="o">(</span><span class="n">TaggedKey</span><span class="o">)</span> <span class="n">b</span><span class="o">;</span>
</span><span class='line'>        <span class="k">return</span> <span class="n">taggedKey1</span><span class="o">.</span><span class="na">getJoinKey</span><span class="o">().</span><span class="na">compareTo</span><span class="o">(</span><span class="n">taggedKey2</span><span class="o">.</span><span class="na">getJoinKey</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>配置Job</h3>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">job</span><span class="o">.</span><span class="na">setMapOutputKeyClass</span><span class="o">(</span><span class="n">TaggedKey</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class='line'><span class="n">job</span><span class="o">.</span><span class="na">setMapOutputValueClass</span><span class="o">(</span><span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'><span class="n">job</span><span class="o">.</span><span class="na">setPartitionerClass</span><span class="o">(</span><span class="n">TaggedJoiningPartitioner</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class='line'><span class="n">job</span><span class="o">.</span><span class="na">setGroupingComparatorClass</span><span class="o">(</span><span class="n">TaggedJoiningGroupingComparator</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<h3>MapReduce过程</h3>

<p>最后，我们在Mapper阶段使用TaggedKey，在Reducer阶段按照tag进行不同的操作就可以了：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'><span class="kd">protected</span> <span class="kt">void</span> <span class="nf">reduce</span><span class="o">(</span><span class="n">TaggedKey</span> <span class="n">key</span><span class="o">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">&gt;</span> <span class="n">values</span><span class="o">,</span> <span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class='line'>        <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">products</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;();</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span><span class="n">Text</span> <span class="n">value</span> <span class="o">:</span> <span class="n">values</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">switch</span> <span class="o">(</span><span class="n">key</span><span class="o">.</span><span class="na">getTag</span><span class="o">().</span><span class="na">get</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">case</span> <span class="mi">1</span><span class="o">:</span> <span class="c1">// Product</span>
</span><span class='line'>            <span class="n">products</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
</span><span class='line'>            <span class="k">break</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">case</span> <span class="mi">2</span><span class="o">:</span> <span class="c1">// Order</span>
</span><span class='line'>            <span class="n">String</span><span class="o">[]</span> <span class="n">order</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="na">toString</span><span class="o">().</span><span class="na">split</span><span class="o">(</span><span class="s">&quot;,&quot;</span><span class="o">);</span>
</span><span class='line'>            <span class="k">for</span> <span class="o">(</span><span class="n">String</span> <span class="n">productString</span> <span class="o">:</span> <span class="n">products</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>                <span class="n">String</span><span class="o">[]</span> <span class="n">product</span> <span class="o">=</span> <span class="n">productString</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot;,&quot;</span><span class="o">);</span>
</span><span class='line'>                <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;();</span>
</span><span class='line'>                <span class="n">output</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">order</span><span class="o">[</span><span class="mi">0</span><span class="o">]);</span>
</span><span class='line'>                <span class="c1">// ...</span>
</span><span class='line'>                <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">NullWritable</span><span class="o">.</span><span class="na">get</span><span class="o">(),</span> <span class="k">new</span> <span class="n">Text</span><span class="o">(</span><span class="n">StringUtils</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">output</span><span class="o">,</span> <span class="s">&quot;,&quot;</span><span class="o">)));</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>            <span class="k">break</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">default</span><span class="o">:</span>
</span><span class='line'>            <span class="k">assert</span> <span class="kc">false</span><span class="o">;</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>遍历values时，开始都是tag=1的记录，之后都是tag=2的记录。以上代码可以<a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/ReduceSideJoinJob.java">在这里</a>查看。</p>

<p>对于第二个问题，超过缓存大小的记录（默认25000条）就会存入临时文件，由Hive的RowContainer类实现，具体可以看<a href="http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hive/hive-exec/0.10.0-cdh4.5.0/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java#RowContainer.add%28java.util.List%29">这个链接</a>。</p>

<p>需要注意的是，Hive默认是按SQL中表的书写顺序来决定排序的，因此应该将大表放在最后。如果要人工改变顺序，可以使用STREAMTABLE配置：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="cm">/*+ STREAMTABLE(a) */</span> <span class="n">a</span><span class="p">.</span><span class="n">val</span><span class="p">,</span> <span class="n">b</span><span class="p">.</span><span class="n">val</span><span class="p">,</span> <span class="k">c</span><span class="p">.</span><span class="n">val</span> <span class="k">FROM</span> <span class="n">a</span> <span class="k">JOIN</span> <span class="n">b</span> <span class="k">ON</span> <span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="k">key</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="n">key1</span><span class="p">)</span> <span class="k">JOIN</span> <span class="k">c</span> <span class="k">ON</span> <span class="p">(</span><span class="k">c</span><span class="p">.</span><span class="k">key</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="n">key1</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>但不要将这点和Map-side Join混淆，在配置了<code>hive.auto.convert.join=true</code>后，是不需要注意表的顺序的，Hive会自动将小表缓存在Mapper的内存中。</p>

<h2>参考资料</h2>

<ol>
<li><a href="http://codingjunkie.net/mapreduce-reduce-joins/">http://codingjunkie.net/mapreduce-reduce-joins/</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用git rebase让历史变得清晰]]></title>
    <link href="http://shzhangji.com/blog/2014/12/23/use-git-rebase-to-clarify-history/"/>
    <updated>2014-12-23T16:10:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/12/23/use-git-rebase-to-clarify-history</id>
    <content type="html"><![CDATA[<p>当多人协作开发一个分支时，历史记录通常如下方左图所示，比较凌乱。如果希望能像右图那样呈线性提交，就需要学习git rebase的用法。</p>

<p><img src="http://shzhangji.com/images/git-rebase/rebase-result.png" alt="" /></p>

<h2>“Merge branch”提交的产生</h2>

<p>我们的工作流程是：修改代码→提交到本地仓库→拉取远程改动→推送。正是在git pull这一步产生的Merge branch提交。事实上，git pull等效于get fetch origin和get merge origin/master这两条命令，前者是拉取远程仓库到本地临时库，后者是将临时库中的改动合并到本地分支中。</p>

<p>要避免Merge branch提交也有一个“土法”：先pull、再commit、最后push。不过万一commit和push之间远程又发生了改动，还需要再pull一次，就又会产生Merge branch提交。</p>

<h2>使用git pull &mdash;rebase</h2>

<p>修改代码→commit→git pull &mdash;rebase→git push。也就是将get merge origin/master替换成了git rebase origin/master，它的过程是先将HEAD指向origin/master，然后逐一应用本地的修改，这样就不会产生Merge branch提交了。具体过程见下文扩展阅读。</p>

<!-- more -->


<p>使用git rebase是有条件的，你的本地仓库要“足够干净”。可以用git status命令查看当前改动：：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git status
</span><span class='line'>On branch master
</span><span class='line'>Your branch is up-to-date with 'origin/master'.
</span><span class='line'>nothing to commit, working directory clean</span></code></pre></td></tr></table></div></figure>


<p>本地没有任何未提交的改动，这是最“干净”的。稍差一些的是这样：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git status
</span><span class='line'>On branch master
</span><span class='line'>Your branch is up-to-date with 'origin/master'.
</span><span class='line'>Untracked files:
</span><span class='line'>  (use "git add &lt;file&gt;..." to include in what will be committed)
</span><span class='line'>    test.txt
</span><span class='line'>nothing added to commit but untracked files present (use "git add" to track)</span></code></pre></td></tr></table></div></figure>


<p>即本地只有新增文件未提交，没有改动文件。我们应该尽量保持本地仓库的“整洁”，这样才能顺利使用git rebase。特殊情况下也可以用git stash来解决问题，有兴趣的可自行搜索。</p>

<h2>修改git pull的默认行为</h2>

<p>每次都加&mdash;rebase似乎有些麻烦，我们可以指定某个分支在执行git pull时默认采用rebase方式：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git config branch.master.rebase true</span></code></pre></td></tr></table></div></figure>


<p>如果你觉得所有的分支都应该用rebase，那就设置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git config --global branch.autosetuprebase always</span></code></pre></td></tr></table></div></figure>


<p>这样对于新建的分支都会设定上面的rebase=true了。已经创建好的分支还是需要手动配置的。</p>

<h2>扩展阅读[1]：git rebase工作原理</h2>

<p>先看看git merge的示意图：</p>

<p><img src="http://shzhangji.com/images/git-rebase/merge.png" alt="" /></p>

<p><a href="https://www.atlassian.com/ja/git/tutorial/git-branches">图片来源</a></p>

<p>可以看到Some Feature分支的两个提交通过一个新的提交（蓝色）和master连接起来了。</p>

<p>再来看git rebase的示意图：</p>

<p><img src="http://shzhangji.com/images/git-rebase/rebase-1.png" alt="" /></p>

<p><img src="http://shzhangji.com/images/git-rebase/rebase-2.png" alt="" /></p>

<p>Feature分支中的两个提交被“嫁接”到了Master分支的头部，或者说Feature分支的“基”（base）变成了 Master，rebase也因此得名。</p>

<h2>扩展阅读[2]：git merge &mdash;no-ff</h2>

<p>在做项目开发时会用到分支，合并时采用以下步骤：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git checkout feature-branch
</span><span class='line'>$ git rebase master
</span><span class='line'>$ git checkout master
</span><span class='line'>$ git merge --no-ff feature-branch
</span><span class='line'>$ git push origin master</span></code></pre></td></tr></table></div></figure>


<p>历史就成了这样：</p>

<p><img src="http://shzhangji.com/images/git-rebase/no-ff.png" alt="" /></p>

<p>可以看到，Merge branch &lsquo;feature-branch&#8217;那段可以很好的展现出这些提交是属于某一特性的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark快速入门]]></title>
    <link href="http://shzhangji.com/blog/2014/12/16/spark-quick-start/"/>
    <updated>2014-12-16T15:59:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/12/16/spark-quick-start</id>
    <content type="html"><![CDATA[<p><img src="http://spark.apache.org/images/spark-logo.png" alt="" /></p>

<p><a href="http://spark.apache.org">Apache Spark</a>是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：</p>

<ul>
<li><strong>通用计算引擎</strong> 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；</li>
<li><strong>基于内存</strong> 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；</li>
<li><strong>与Hadoop集成</strong> 能够直接读写HDFS中的数据，并能运行在YARN之上。</li>
</ul>


<p>Spark是用<a href="http://www.scala-lang.org/">Scala语言</a>编写的，所提供的API也很好地利用了这门语言的特性。它也可以使用Java和Python编写应用。本文将用Scala进行讲解。</p>

<h2>安装Spark和SBT</h2>

<ul>
<li>从<a href="http://spark.apache.org/downloads.html">官网</a>上下载编译好的压缩包，解压到一个文件夹中。下载时需注意对应的Hadoop版本，如要读写CDH4 HDFS中的数据，则应下载Pre-built for CDH4这个版本。</li>
<li>为了方便起见，可以将spark/bin添加到$PATH环境变量中：</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span>/path/to/spark
</span><span class='line'><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$SPARK_HOME</span>/bin
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>在练习例子时，我们还会用到<a href="http://www.scala-sbt.org/">SBT</a>这个工具，它是用来编译打包Scala项目的。Linux下的安装过程比较简单：

<ul>
<li>下载<a href="https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.7/sbt-launch.jar">sbt-launch.jar</a>到$HOME/bin目录；</li>
<li>新建$HOME/bin/sbt文件，权限设置为755，内容如下：</li>
</ul>
</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">SBT_OPTS</span><span class="o">=</span><span class="s2">&quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M&quot;</span>
</span><span class='line'>java <span class="nv">$SBT_OPTS</span> -jar <span class="sb">`</span>dirname <span class="nv">$0</span><span class="sb">`</span>/sbt-launch.jar <span class="s2">&quot;$@&quot;</span>
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>日志分析示例</h2>

<p>假设我们有如下格式的日志文件，保存在/tmp/logs.txt文件中：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>2014-12-11 18:33:52  INFO    Java    some message
</span><span class='line'>2014-12-11 18:34:33   INFO    MySQL   some message
</span><span class='line'>2014-12-11 18:34:54   WARN    Java    some message
</span><span class='line'>2014-12-11 18:35:25   WARN    Nginx   some message
</span><span class='line'>2014-12-11 18:36:09   INFO    Java    some message
</span></code></pre></td></tr></table></div></figure>


<p>每条记录有四个字段，即时间、级别、应用、信息，使用制表符分隔。</p>

<p>Spark提供了一个交互式的命令行工具，可以直接执行Spark查询：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>$ spark-shell
</span><span class='line'>Welcome to
</span><span class='line'>      ____              __
</span><span class='line'>     / __/__  ___ _____/ /__
</span><span class='line'>    _\ \/ _ \/ _ `/ __/  &#39;_/
</span><span class='line'>   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0
</span><span class='line'>      /_/
</span><span class='line'>Spark context available as sc.
</span><span class='line'>scala&gt;
</span></code></pre></td></tr></table></div></figure>


<h3>加载并预览数据</h3>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;/tmp/logs.txt&quot;</span><span class="o">)</span>
</span><span class='line'><span class="n">lines</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">logs</span><span class="o">.</span><span class="n">txt</span> <span class="nc">MappedRDD</span><span class="o">[</span><span class="err">1</span><span class="o">]</span> <span class="n">at</span> <span class="n">textFile</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">12</span>
</span><span class='line'>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">lines</span><span class="o">.</span><span class="n">first</span><span class="o">()</span>
</span><span class='line'><span class="n">res0</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="mi">2014</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">11</span> <span class="mi">18</span><span class="k">:</span><span class="err">33</span><span class="kt">:</span><span class="err">52</span>    <span class="kt">INFO</span>    <span class="kt">Java</span>    <span class="kt">some</span> <span class="kt">message</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>sc是一个SparkContext类型的变量，可以认为是Spark的入口，这个对象在spark-shell中已经自动创建了。</li>
<li>sc.textFile()用于生成一个RDD，并声明该RDD指向的是/tmp/logs.txt文件。RDD可以暂时认为是一个列表，列表中的元素是一行行日志（因此是String类型）。这里的路径也可以是HDFS上的文件，如hdfs://127.0.0.1:8020/user/hadoop/logs.txt。</li>
<li>lines.first()表示调用RDD提供的一个方法：first()，返回第一行数据。</li>
</ul>


<h3>解析日志</h3>

<p>为了能对日志进行筛选，如只处理级别为ERROR的日志，我们需要将每行日志按制表符进行分割：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">logs</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;\t&quot;</span><span class="o">))</span>
</span><span class='line'><span class="n">logs</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">]]</span> <span class="k">=</span> <span class="nc">MappedRDD</span><span class="o">[</span><span class="err">2</span><span class="o">]</span> <span class="n">at</span> <span class="n">map</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">14</span>
</span><span class='line'>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">logs</span><span class="o">.</span><span class="n">first</span><span class="o">()</span>
</span><span class='line'><span class="n">res1</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">2014</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">11</span> <span class="mi">18</span><span class="k">:</span><span class="err">33</span><span class="kt">:</span><span class="err">52</span><span class="o">,</span> <span class="nc">INFO</span><span class="o">,</span> <span class="nc">Java</span><span class="o">,</span> <span class="n">some</span> <span class="n">message</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>lines.map(f)表示对RDD中的每一个元素使用f函数来处理，并返回一个新的RDD。</li>
<li>line => line.split(&ldquo;\t&rdquo;)是一个匿名函数，又称为Lambda表达式、闭包等。它的作用和普通的函数是一样的，如这个匿名函数的参数是line（String类型），返回值是Array数组类型，因为String.split()函数返回的是数组。</li>
<li>同样使用first()方法来看这个RDD的首条记录，可以发现日志已经被拆分成四个元素了。</li>
</ul>


<h3>过滤并计数</h3>

<p>我们想要统计错误日志的数量：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">errors</span> <span class="k">=</span> <span class="n">logs</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">log</span> <span class="k">=&gt;</span> <span class="n">log</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="o">==</span> <span class="s">&quot;ERROR&quot;</span><span class="o">)</span>
</span><span class='line'><span class="n">errors</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">]]</span> <span class="k">=</span> <span class="nc">FilteredRDD</span><span class="o">[</span><span class="err">3</span><span class="o">]</span> <span class="n">at</span> <span class="n">filter</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">16</span>
</span><span class='line'>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">errors</span><span class="o">.</span><span class="n">first</span><span class="o">()</span>
</span><span class='line'><span class="n">res2</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">2014</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">11</span> <span class="mi">18</span><span class="k">:</span><span class="err">39</span><span class="kt">:</span><span class="err">42</span><span class="o">,</span> <span class="nc">ERROR</span><span class="o">,</span> <span class="nc">Java</span><span class="o">,</span> <span class="n">some</span> <span class="n">message</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">errors</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>
</span><span class='line'><span class="n">res3</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">158</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>logs.filter(f)表示筛选出满足函数f的记录，其中函数f需要返回一个布尔值。</li>
<li>log(1) == &ldquo;ERROR&#8221;表示获取每行日志的第二个元素（即日志级别），并判断是否等于ERROR。</li>
<li>errors.count()用于返回该RDD中的记录。</li>
</ul>


<h3>缓存</h3>

<p>由于我们还会对错误日志做一些处理，为了加快速度，可以将错误日志缓存到内存中，从而省去解析和过滤的过程：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">errors</span><span class="o">.</span><span class="n">cache</span><span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>


<p>errors.cache()函数会告知Spark计算完成后将结果保存在内存中。所以说Spark是否缓存结果是需要用户手动触发的。在实际应用中，我们需要迭代处理的往往只是一部分数据，因此很适合放到内存里。</p>

<p>需要注意的是，cache函数并不会立刻执行缓存操作，事实上map、filter等函数都不会立刻执行，而是在用户执行了一些特定操作后才会触发，比如first、count、reduce等。这两类操作分别称为Transformations和Actions。</p>

<h3>显示前10条记录</h3>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">firstTenErrors</span> <span class="k">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">10</span><span class="o">)</span>
</span><span class='line'><span class="n">firstTenErrors</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">]]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">2014</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">11</span> <span class="mi">18</span><span class="k">:</span><span class="err">39</span><span class="kt">:</span><span class="err">42</span><span class="o">,</span> <span class="nc">ERROR</span><span class="o">,</span> <span class="nc">Java</span><span class="o">,</span> <span class="n">some</span> <span class="n">message</span><span class="o">),</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">2014</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">11</span> <span class="mi">18</span><span class="k">:</span><span class="err">40</span><span class="kt">:</span><span class="err">23</span><span class="o">,</span> <span class="nc">ERROR</span><span class="o">,</span> <span class="nc">Nginx</span><span class="o">,</span> <span class="n">some</span> <span class="n">message</span><span class="o">),</span> <span class="o">...)</span>
</span><span class='line'>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">firstTenErrors</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">log</span> <span class="k">=&gt;</span> <span class="n">log</span><span class="o">.</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;\t&quot;</span><span class="o">)).</span><span class="n">foreach</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="n">println</span><span class="o">(</span><span class="n">line</span><span class="o">))</span>
</span><span class='line'><span class="mi">2014</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">11</span> <span class="mi">18</span><span class="k">:</span><span class="err">39</span><span class="kt">:</span><span class="err">42</span>    <span class="kt">ERROR</span>   <span class="kt">Java</span>    <span class="kt">some</span> <span class="kt">message</span>
</span><span class='line'><span class="mi">2014</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">11</span> <span class="mi">18</span><span class="k">:</span><span class="err">40</span><span class="kt">:</span><span class="err">23</span>    <span class="kt">ERROR</span>   <span class="kt">Nginx</span>   <span class="kt">some</span> <span class="kt">message</span>
</span><span class='line'><span class="o">...</span>
</span></code></pre></td></tr></table></div></figure>


<p>errors.take(n)方法可用于返回RDD前N条记录，它的返回值是一个数组。之后对firstTenErrors的处理使用的是Scala集合类库中的方法，如map、foreach，和RDD提供的接口基本一致。所以说用Scala编写Spark程序是最自然的。</p>

<h3>按应用进行统计</h3>

<p>我们想要知道错误日志中有几条Java、几条Nginx，这和常见的Wordcount思路是一样的。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">apps</span> <span class="k">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">log</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">log</span><span class="o">(</span><span class="mi">2</span><span class="o">),</span> <span class="mi">1</span><span class="o">))</span>
</span><span class='line'><span class="n">apps</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="nc">MappedRDD</span><span class="o">[</span><span class="err">15</span><span class="o">]</span> <span class="n">at</span> <span class="n">map</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">18</span>
</span><span class='line'>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">apps</span><span class="o">.</span><span class="n">first</span><span class="o">()</span>
</span><span class='line'><span class="n">res20</span><span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">(</span><span class="nc">Java</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">apps</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span>
</span><span class='line'><span class="n">counts</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="nc">ShuffledRDD</span><span class="o">[</span><span class="err">17</span><span class="o">]</span> <span class="n">at</span> <span class="n">reduceByKey</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">20</span>
</span><span class='line'>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">counts</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">t</span> <span class="k">=&gt;</span> <span class="n">println</span><span class="o">(</span><span class="n">t</span><span class="o">))</span>
</span><span class='line'><span class="o">(</span><span class="nc">Java</span><span class="o">,</span><span class="mi">58</span><span class="o">)</span>
</span><span class='line'><span class="o">(</span><span class="nc">Nginx</span><span class="o">,</span><span class="mi">53</span><span class="o">)</span>
</span><span class='line'><span class="o">(</span><span class="nc">MySQL</span><span class="o">,</span><span class="mi">47</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>errors.map(log => (log(2), 1))用于将每条日志转换为键值对，键是应用（Java、Nginx等），值是1，如<code>("Java", 1)</code>，这种数据结构在Scala中称为元组（Tuple），这里它有两个元素，因此称为二元组。</p>

<p>对于数据类型是二元组的RDD，Spark提供了额外的方法，reduceByKey(f)就是其中之一。它的作用是按键进行分组，然后对同一个键下的所有值使用f函数进行归约（reduce）。归约的过程是：使用列表中第一、第二个元素进行计算，然后用结果和第三元素进行计算，直至列表耗尽。如：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">).</span><span class="n">reduce</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span>
</span><span class='line'><span class="n">res23</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">10</span>
</span></code></pre></td></tr></table></div></figure>


<p>上述代码的计算过程即<code>((1 + 2) + 3) + 4</code>。</p>

<p>counts.foreach(f)表示遍历RDD中的每条记录，并应用f函数。这里的f函数是一条打印语句（println）。</p>

<h2>打包应用程序</h2>

<p>为了让我们的日志分析程序能够在集群上运行，我们需要创建一个Scala项目。项目的大致结构是：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">spark</span><span class="o">-</span><span class="n">sandbox</span>
</span><span class='line'><span class="o">├──</span> <span class="n">build</span><span class="o">.</span><span class="n">sbt</span>
</span><span class='line'><span class="o">├──</span> <span class="n">project</span>
</span><span class='line'><span class="o">│</span><span class="err">  </span> <span class="o">├──</span> <span class="n">build</span><span class="o">.</span><span class="n">properties</span>
</span><span class='line'><span class="o">│</span><span class="err">  </span> <span class="o">└──</span> <span class="n">plugins</span><span class="o">.</span><span class="n">sbt</span>
</span><span class='line'><span class="o">└──</span> <span class="n">src</span>
</span><span class='line'>    <span class="o">└──</span> <span class="n">main</span>
</span><span class='line'>        <span class="o">└──</span> <span class="n">scala</span>
</span><span class='line'>            <span class="o">└──</span> <span class="nc">LogMining</span><span class="o">.</span><span class="n">scala</span>
</span></code></pre></td></tr></table></div></figure>


<p>你可以直接使用<a href="https://github.com/jizhang/spark-sandbox">这个项目</a>作为模板。下面说明一些关键部分：</p>

<h3>配置依赖</h3>

<p><code>build.sbt</code></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">libraryDependencies</span> <span class="o">+=</span> <span class="s">&quot;org.apache.spark&quot;</span> <span class="o">%%</span> <span class="s">&quot;spark-core&quot;</span> <span class="o">%</span> <span class="s">&quot;1.1.1&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>程序内容</h3>

<p><code>src/main/scala/LogMining.scala</code></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">org.apache.spark.SparkContext</span>
</span><span class='line'><span class="k">import</span> <span class="nn">org.apache.spark.SparkContext._</span>
</span><span class='line'><span class="k">import</span> <span class="nn">org.apache.spark.SparkConf</span>
</span><span class='line'>
</span><span class='line'><span class="k">object</span> <span class="nc">LogMining</span> <span class="k">extends</span> <span class="nc">App</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">setAppName</span><span class="o">(</span><span class="s">&quot;LogMining&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">sc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">inputFile</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="n">inputFile</span><span class="o">)</span>
</span><span class='line'>  <span class="c1">// 解析日志</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">logs</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;\t&quot;</span><span class="o">))</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">errors</span> <span class="k">=</span> <span class="n">logs</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="o">==</span> <span class="s">&quot;ERROR&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="c1">// 缓存错误日志</span>
</span><span class='line'>  <span class="n">errors</span><span class="o">.</span><span class="n">cache</span><span class="o">()</span>
</span><span class='line'>  <span class="c1">// 统计错误日志记录数</span>
</span><span class='line'>  <span class="n">println</span><span class="o">(</span><span class="n">errors</span><span class="o">.</span><span class="n">count</span><span class="o">())</span>
</span><span class='line'>  <span class="c1">// 获取前10条MySQL的错误日志</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">mysqlErrors</span> <span class="k">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span> <span class="o">==</span> <span class="s">&quot;MySQL&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="n">mysqlErrors</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span> <span class="n">mkString</span> <span class="s">&quot;\t&quot;</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</span><span class='line'>  <span class="c1">// 统计每个应用的错误日志数</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">errorApps</span> <span class="k">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="mi">1</span><span class="o">)</span>
</span><span class='line'>  <span class="n">errorApps</span><span class="o">.</span><span class="n">countByKey</span><span class="o">().</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>打包运行</h3>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span><span class="nb">cd </span>spark-sandbox
</span><span class='line'><span class="nv">$ </span>sbt package
</span><span class='line'><span class="nv">$ </span>spark-submit --class LogMining --master <span class="nb">local </span>target/scala-2.10/spark-sandbox_2.10-0.1.0.jar data/logs.txt
</span></code></pre></td></tr></table></div></figure>


<h2>参考资料</h2>

<ul>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html">Spark Programming Guide</a></li>
<li><a href="http://www.slideshare.net/cloudera/spark-devwebinarslides-final">Introduction to Spark Developer Training</a></li>
<li><a href="http://www.slideshare.net/liancheng/dtcc-14-spark-runtime-internals">Spark Runtime Internals</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[离线环境下构建sbt项目]]></title>
    <link href="http://shzhangji.com/blog/2014/11/07/sbt-offline/"/>
    <updated>2014-11-07T15:02:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/11/07/sbt-offline</id>
    <content type="html"><![CDATA[<p>在公司网络中使用<a href="http://www.scala-sbt.org/">sbt</a>、<a href="http://maven.apache.org/">Maven</a>等项目构建工具时，我们通常会搭建一个公用的<a href="http://www.sonatype.org/nexus/">Nexus</a>镜像服务，原因有以下几个：</p>

<ul>
<li>避免重复下载依赖，节省公司带宽；</li>
<li>国内网络环境不理想，下载速度慢；</li>
<li>IDC服务器没有外网访问权限；</li>
<li>用于发布内部模块。</li>
</ul>


<p>sbt的依赖管理基于<a href="http://ant.apache.org/ivy/">Ivy</a>，虽然它能直接使用<a href="http://search.maven.org/">Maven中央仓库</a>中的Jar包，在配置时还是有一些注意事项的。</p>

<!-- more -->


<h2>配置Nexus镜像</h2>

<p>根据这篇<a href="http://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html">官方文档</a>的描述，Ivy和Maven在依赖管理方面有些许差异，因此不能直接将两者的镜像仓库配置成一个，而需分别建立两个虚拟镜像组。</p>

<p><img src="http://www.scala-sbt.org/0.13/docs/files/proxy-ivy-mvn-setup.png" alt="" /></p>

<p>安装Nexus后默认会有一个Public Repositories组，可以将其作为Maven的镜像组，并添加一些常用的第三方镜像：</p>

<ul>
<li>cloudera: <a href="https://repository.cloudera.com/artifactory/cloudera-repos/">https://repository.cloudera.com/artifactory/cloudera-repos/</a></li>
<li>spring: <a href="http://repo.springsource.org/libs-release-remote/">http://repo.springsource.org/libs-release-remote/</a></li>
<li>scala-tools: <a href="https://oss.sonatype.org/content/groups/scala-tools/">https://oss.sonatype.org/content/groups/scala-tools/</a></li>
</ul>


<p>对于Ivy镜像，我们创建一个新的虚拟组：ivy-releases，并添加以下两个镜像：</p>

<ul>
<li>type-safe: <a href="http://repo.typesafe.com/typesafe/ivy-releases/">http://repo.typesafe.com/typesafe/ivy-releases/</a></li>
<li>sbt-plugin: <a href="http://dl.bintray.com/sbt/sbt-plugin-releases/">http://dl.bintray.com/sbt/sbt-plugin-releases/</a></li>
</ul>


<p>对于sbt-plugin，由于一些原因，Nexus会将其置为Automatically Blocked状态，因此要在配置中将这个选项关闭，否则将无法下载远程的依赖包。</p>

<h2>配置sbt</h2>

<p>为了让sbt使用Nexus镜像，需要创建一个~/.sbt/repositories文件，内容为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[repositories]
</span><span class='line'>  local
</span><span class='line'>  my-ivy-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]
</span><span class='line'>  my-maven-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/public/</span></code></pre></td></tr></table></div></figure>


<p>这样配置对大部分项目来说是足够了。但是有些项目会在构建描述文件中添加其它仓库，我们需要覆盖这种行为，方法是：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>sbt -Dsbt.override.build.repos<span class="o">=</span><span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>


<p>你也可以通过设置SBT_OPTS环境变量来进行全局配置。</p>

<p>经过以上步骤，sbt执行过程中就不需要访问外网了，因此速度会有很大提升。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL异常UTF-8字符的处理]]></title>
    <link href="http://shzhangji.com/blog/2014/10/14/mysql-incorrent-utf8-value/"/>
    <updated>2014-10-14T13:16:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/10/14/mysql-incorrent-utf8-value</id>
    <content type="html"><![CDATA[<p>ETL流程中，我们会将Hive中的数据导入MySQL——先用Hive命令行将数据保存为文本文件，然后用MySQL的LOAD DATA语句进行加载。最近有一张表在加载到MySQL时会报以下错误：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Incorrect string value: '\xF0\x9D\x8C\x86' for column ...</span></code></pre></td></tr></table></div></figure>


<p>经查，这个字段中保存的是用户聊天记录，因此会有一些表情符号。这些符号在UTF-8编码下需要使用4个字节来记录，而MySQL中的utf8编码只支持3个字节，因此无法导入。</p>

<p>根据UTF-8的编码规范，3个字节支持的Unicode字符范围是U+0000–U+FFFF，因此可以在Hive中对数据做一下清洗：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="n">REGEXP_REPLACE</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="s1">&#39;[^\\u0000-\\uFFFF]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="k">FROM</span> <span class="p">...</span>
</span></code></pre></td></tr></table></div></figure>


<p>这样就能排除那些需要使用3个以上字节来记录的字符了，从而成功导入MySQL。</p>

<p>以下是一些详细说明和参考资料。</p>

<!-- more -->


<h2>Unicode字符集和UTF编码</h2>

<p><a href="http://en.wikipedia.org/wiki/Unicode">Unicode字符集</a>是一种将全球所有文字都囊括在内的字符集，从而实现跨语言、跨平台的文字信息交换。它由<a href="http://en.wikipedia.org/wiki/Plane_(Unicode)#Basic_Multilingual_Plane">基本多语平面（BMP）</a>和多个扩展平面（non-BMP）组成。前者的编码范围是U+0000-U+FFFF，包括了绝大多数现代语言文字，因此最为常用。</p>

<p><a href="http://en.wikipedia.org/wiki/Unicode#Unicode_Transformation_Format_and_Universal_Character_Set">UTF</a>则是一种编码格式，负责将Unicode字符对应的编号转换为计算机可以识别的二进制数据，进行保存和读取。</p>

<p>比如，磁盘上记录了以下二进制数据：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="mi">1101000</span> <span class="mi">1100101</span> <span class="mi">1101100</span> <span class="mi">1101100</span> <span class="mi">1101111</span>
</span></code></pre></td></tr></table></div></figure>


<p>读取它的程序知道这是以UTF-8编码保存的字符串，因此将其解析为以下编号：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="mi">104</span> <span class="mi">101</span> <span class="mi">108</span> <span class="mi">108</span> <span class="mi">111</span>
</span></code></pre></td></tr></table></div></figure>


<p>又因为UTF-8编码对应的字符集是Unicode，所以上面这五个编号对应的字符便是“hello”。</p>

<p>很多人会将Unicode和UTF混淆，但两者并不具可比性，它们完成的功能是不同的。</p>

<h2>UTF-8编码</h2>

<p>UTF编码家族也有很多成员，其中<a href="http://en.wikipedia.org/wiki/UTF-8">UTF-8</a>最为常用。它是一种变长的编码格式，对于ASCII码中的字符使用1个字节进行编码，对于中文等则使用3个字节。这样做的优点是在存储西方语言文字时不会造成空间浪费，不像UTF-16和UTF-32，分别使用两个字节和四个字节对所有字符进行编码。</p>

<p>UTF-8编码的字节数上限并不是3个。对于U+0000-U+FFFF范围内的字符，使用3个字节可以表示完全；对于non-BMP中的字符，则会使用4-6个字节来表示。同样，UTF-16编码也会使用四个字节来表示non-BMP中的字符。</p>

<h2>MySQL的UTF-8编码</h2>

<p>根据MySQL的<a href="http://dev.mysql.com/doc/refman/5.5/en/charset-unicode.html">官方文档</a>，它的UTF-8编码支持是不完全的，最多使用3个字符，这也是导入数据时报错的原因。</p>

<p>MySQL5.5开始支持utf8mb4编码，至多使用4个字节，因此能包含到non-BMP字符。只是我们的MySQL版本仍是5.1，因此选择丢弃这些字符。</p>

<h2>参考资料</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8">http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8</a></li>
<li><a href="http://www.joelonsoftware.com/articles/Unicode.html">http://www.joelonsoftware.com/articles/Unicode.html</a></li>
<li><a href="http://apps.timwhitlock.info/emoji/tables/unicode">http://apps.timwhitlock.info/emoji/tables/unicode</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在CDH 4.5上安装Shark 0.9]]></title>
    <link href="http://shzhangji.com/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5/"/>
    <updated>2014-07-05T17:16:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5</id>
    <content type="html"><![CDATA[<p><a href="http://spark.apache.org">Spark</a>是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的<a href="http://hadoop.apache.org">Hadoop</a>生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于<a href="http://hive.apache.org">Hive</a>，Spark也有相应的替代项目——<a href="http://shark.cs.berkeley.edu/">Shark</a>，能做到 <strong>drop-in replacement</strong> ，直接构建在现有集群之上。本文就将简要阐述如何在CDH4.5上搭建Shark0.9集群。</p>

<h2>准备工作</h2>

<ul>
<li>安装方式：Spark使用CDH提供的Parcel，以Standalone模式启动</li>
<li>软件版本

<ul>
<li>Cloudera Manager 4.8.2</li>
<li>CDH 4.5</li>
<li>Spark 0.9.0 Parcel</li>
<li><a href="http://cloudera.rst.im/shark/">Shark 0.9.1 Binary</a></li>
</ul>
</li>
<li>服务器基础配置

<ul>
<li>可用的软件源（如<a href="http://mirrors.ustc.edu.cn/">中科大的源</a>）</li>
<li>配置主节点至子节点的root账户SSH无密码登录。</li>
<li>在<code>/etc/hosts</code>中写死IP和主机名，或者DNS做好正反解析。</li>
</ul>
</li>
</ul>


<!-- more -->


<h2>安装Spark</h2>

<ul>
<li>使用CM安装Parcel，不需要重启服务。</li>
<li>修改<code>/etc/spark/conf/spark-env.sh</code>：（其中one-843是主节点的域名）</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">STANDALONE_SPARK_MASTER_HOST</span><span class="o">=</span>one-843
</span><span class='line'><span class="nv">DEFAULT_HADOOP_HOME</span><span class="o">=</span>/opt/cloudera/parcels/CDH/lib/hadoop
</span><span class='line'><span class="nb">export </span><span class="nv">SPARK_CLASSPATH</span><span class="o">=</span><span class="nv">$SPARK_CLASSPATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*
</span><span class='line'><span class="nb">export </span><span class="nv">SPARK_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$SPARK_LIBRARY_PATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>修改<code>/etc/spark/conf/slaves</code>，添加各节点主机名。</li>
<li>将<code>/etc/spark/conf</code>目录同步至所有节点。</li>
<li>启动Spark服务（即Standalone模式）：</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>/opt/cloudera/parcels/SPARK/lib/spark/sbin/start-master.sh
</span><span class='line'><span class="nv">$ </span>/opt/cloudera/parcels/SPARK/lib/spark/sbin/start-slaves.sh
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>测试<code>spark-shell</code>是否可用：</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;hdfs://one-843:8020/user/jizhang/zj_people.txt.lzo&quot;</span><span class="o">).</span><span class="n">count</span>
</span></code></pre></td></tr></table></div></figure>


<h2>安装Shark</h2>

<ul>
<li>安装Oracle JDK 1.7 Update 45至<code>/usr/lib/jvm/jdk1.7.0_45</code>。</li>
<li>下载别人编译好的二进制包：<a href="http://cloudera.rst.im/shark/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz">shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz</a></li>
<li>解压至<code>/opt</code>目录，修改<code>conf/shark-env.sh</code>：</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/usr/lib/jvm/jdk1.7.0_45
</span><span class='line'><span class="nb">export </span><span class="nv">SCALA_HOME</span><span class="o">=</span>/opt/cloudera/parcels/SPARK/lib/spark
</span><span class='line'><span class="nb">export </span><span class="nv">SHARK_HOME</span><span class="o">=</span>/root/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0
</span><span class='line'>
</span><span class='line'><span class="nb">export </span><span class="nv">HIVE_CONF_DIR</span><span class="o">=</span>/etc/hive/conf
</span><span class='line'>
</span><span class='line'><span class="nb">export </span><span class="nv">HADOOP_HOME</span><span class="o">=</span>/opt/cloudera/parcels/CDH/lib/hadoop
</span><span class='line'><span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span>/opt/cloudera/parcels/SPARK/lib/spark
</span><span class='line'><span class="nb">export </span><span class="nv">MASTER</span><span class="o">=</span>spark://one-843:7077
</span><span class='line'>
</span><span class='line'><span class="nb">export </span><span class="nv">SPARK_CLASSPATH</span><span class="o">=</span><span class="nv">$SPARK_CLASSPATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*
</span><span class='line'><span class="nb">export </span><span class="nv">SPARK_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$SPARK_LIBRARY_PATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>开启SharkServer2，使用Supervisord管理：</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="o">[</span>program:sharkserver2<span class="o">]</span>
</span><span class='line'><span class="nb">command</span> <span class="o">=</span> /opt/shark/bin/shark --service sharkserver2
</span><span class='line'><span class="nv">autostart</span> <span class="o">=</span> <span class="nb">true</span>
</span><span class='line'><span class="nv">autorestart</span> <span class="o">=</span> <span class="nb">true</span>
</span><span class='line'><span class="nv">stdout_logfile</span> <span class="o">=</span> /var/log/sharkserver2.log
</span><span class='line'><span class="nv">redirect_stderr</span> <span class="o">=</span> <span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>supervisorctl start sharkserver2
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>测试</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>/opt/shark/bin/beeline -u jdbc:hive2://one-843:10000 -n root
</span></code></pre></td></tr></table></div></figure>


<h2>版本问题</h2>

<h3>背景</h3>

<h4>CDH</h4>

<p>CDH是对Hadoop生态链各组件的打包，每个CDH版本都会对应一组Hadoop组件的版本，如<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH-Version-and-Packaging-Information/cdhvd_topic_3.html">CDH4.5</a>的部分对应关系如下：</p>

<ul>
<li>Apache Hadoop: hadoop-2.0.0+1518</li>
<li>Apache Hive: hive-0.10.0+214</li>
<li>Hue: hue-2.5.0+182</li>
</ul>


<p>可以看到，CDH4.5对应的Hive版本是0.10.0，因此它的<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH4-Installation-Guide/cdh4ig_hive_metastore_configure.html">Metastore Server</a>使用的也是0.10.0版本的API。</p>

<h4>Spark</h4>

<p>Spark目前最高版本是0.9.1，CDH前不久推出了0.9.0的Parcel，使得安装过程变的简单得多。CDH5中对Spark做了深度集成，即可以用CM来直接控制Spark的服务，且支持Spark on YARN架构。</p>

<h4>Shark</h4>

<p>Shark是基于Spark的一款应用，可以简单地认为是将Hive的MapReduce引擎替换为了Spark。</p>

<p>Shark的一个特点的是需要使用特定的Hive版本——<a href="https://github.com/amplab/hive">AMPLab patched Hive</a>：</p>

<ul>
<li>Shark 0.8.x: AMPLab Hive 0.9.0</li>
<li>Shark 0.9.x: AMPLab Hive 0.11.0</li>
</ul>


<p>在0.9.0以前，我们需要手动下载AMPLab Hive的二进制包，并在Shark的环境变量中设置$HIVE_HOME。在0.9.1以后，AMPLab将该版本的Hive包上传至了Maven，可以直接打进Shark的二进制包中。但是，这个Jar是用JDK7编译的，因此运行Shark需要使用Oracle JDK7。CDH建议使用Update 45这个小版本。</p>

<h4>Shark与Hive的并存</h4>

<p>Shark的一个卖点是和Hive的<a href="5">高度兼容</a>，也就是说它可以直接操作Hive的metastore db，或是和metastore server通信。当然，前提是两者的Hive版本需要一致，这也是目前遇到的最大问题。</p>

<h3>目前发现的不兼容SQL</h3>

<ul>
<li>DROP TABLE &hellip;</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>FAILED: Error in metadata: org.apache.thrift.TApplicationException: Invalid method name: <span class="s1">&#39;drop_table_with_environment_context&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>INSERT OVERWRITE TABLE &hellip; PARTITION (&hellip;) SELECT &hellip;</li>
<li>LOAD DATA INPATH &lsquo;&hellip;&rsquo; OVERWRITE INTO TABLE &hellip; PARTITION (&hellip;)</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>Failed with exception org.apache.thrift.TApplicationException: Invalid method name: <span class="s1">&#39;partition_name_has_valid_characters&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<p>也就是说上述两个方法名是0.11.0接口中定义的，在0.10.0的定义中并不存在，所以出现上述问题。</p>

<h3>解决方案</h3>

<h4>对存在问题的SQL使用Hive命令去调</h4>

<p>因为Shark初期是想给分析师使用的，他们对分区表并不是很在意，而DROP TABLE可以在客户端做判断，转而使用Hive来执行。</p>

<p>这个方案的优点是可以在现有集群上立刻用起来，但缺点是需要做一些额外的开发，而且API不一致的问题可能还会有其他坑在里面。</p>

<h4>升级到CDH5</h4>

<p>CDH5中Hive的版本是0.12.0，所以不排除同样存在API不兼容的问题。不过网上也有人尝试跳过AMPLab Hive，让Shark直接调用CDH中的Hive，其可行性还需要我们自己测试。</p>

<p>对于这个问题，我只在<a href="https://groups.google.com/forum/#!starred/shark-users/x_Dh5-3isIc">Google Groups</a>上看到一篇相关的帖子，不过并没有给出解决方案。</p>

<p>目前我们实施的是 <strong>第一种方案</strong>，即在客户端和Shark之间添加一层，不支持的SQL语句直接降级用Hive执行，效果不错。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use WebJars in Scalatra Project]]></title>
    <link href="http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project/"/>
    <updated>2014-05-27T17:44:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project</id>
    <content type="html"><![CDATA[<p>As I&rsquo;m working with my first <a href="http://www.scalatra.org/">Scalatra</a> project, I automatically think of using <a href="http://www.webjars.org/">WebJars</a> to manage Javascript library dependencies, since it&rsquo;s more convenient and seems like a good practice. Though there&rsquo;s no <a href="http://www.webjars.org/documentation">official support</a> for Scalatra framework, the installation process is not very complex. But this doesn&rsquo;t mean I didn&rsquo;t spend much time on this. I&rsquo;m still a newbie to Scala, and there&rsquo;s only a few materials on this subject.</p>

<h2>Add WebJars Dependency in SBT Build File</h2>

<p>Scalatra uses <code>.scala</code> configuration file instead of <code>.sbt</code>, so let&rsquo;s add dependency into <code>project/build.scala</code>. Take <a href="http://dojotoolkit.org/">Dojo</a> for example.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">object</span> <span class="nc">DwExplorerBuild</span> <span class="k">extends</span> <span class="nc">Build</span> <span class="o">{</span>
</span><span class='line'>  <span class="o">...</span>
</span><span class='line'>  <span class="k">lazy</span> <span class="k">val</span> <span class="n">project</span> <span class="k">=</span> <span class="nc">Project</span> <span class="o">(</span>
</span><span class='line'>    <span class="o">...</span>
</span><span class='line'>    <span class="n">settings</span> <span class="k">=</span> <span class="nc">Defaults</span><span class="o">.</span><span class="n">defaultSettings</span> <span class="o">++</span> <span class="nc">ScalatraPlugin</span><span class="o">.</span><span class="n">scalatraWithJRebel</span> <span class="o">++</span> <span class="n">scalateSettings</span> <span class="o">++</span> <span class="nc">Seq</span><span class="o">(</span>
</span><span class='line'>      <span class="o">...</span>
</span><span class='line'>      <span class="n">libraryDependencies</span> <span class="o">++=</span> <span class="nc">Seq</span><span class="o">(</span>
</span><span class='line'>        <span class="o">...</span>
</span><span class='line'>        <span class="s">&quot;org.webjars&quot;</span> <span class="o">%</span> <span class="s">&quot;dojo&quot;</span> <span class="o">%</span> <span class="s">&quot;1.9.3&quot;</span>
</span><span class='line'>      <span class="o">),</span>
</span><span class='line'>      <span class="o">...</span>
</span><span class='line'>    <span class="o">)</span>
</span><span class='line'>  <span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To view this dependency in Eclipse, you need to run <code>sbt eclipse</code> again. In the <em>Referenced Libraries</em> section, you can see a <code>dojo-1.9.3.jar</code>, and the library lies in <code>META-INF/resources/webjars/</code>.</p>

<!-- more -->


<h2>Add a Route for WebJars Resources</h2>

<p>Find the <code>ProjectNameStack.scala</code> file and add the following lines at the bottom of the trait:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">trait</span> <span class="nc">ProjectNameStack</span> <span class="k">extends</span> <span class="nc">ScalatraServlet</span> <span class="k">with</span> <span class="nc">ScalateSupport</span> <span class="o">{</span>
</span><span class='line'>  <span class="o">...</span>
</span><span class='line'>  <span class="n">get</span><span class="o">(</span><span class="s">&quot;/webjars/*&quot;</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">resourcePath</span> <span class="k">=</span> <span class="s">&quot;/META-INF/resources/webjars/&quot;</span> <span class="o">+</span> <span class="n">params</span><span class="o">(</span><span class="s">&quot;splat&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="nc">Option</span><span class="o">(</span><span class="n">getClass</span><span class="o">.</span><span class="n">getResourceAsStream</span><span class="o">(</span><span class="n">resourcePath</span><span class="o">))</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">case</span> <span class="nc">Some</span><span class="o">(</span><span class="n">inputStream</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">contentType</span> <span class="k">=</span> <span class="n">servletContext</span><span class="o">.</span><span class="n">getMimeType</span><span class="o">(</span><span class="n">resourcePath</span><span class="o">)</span>
</span><span class='line'>        <span class="nc">IOUtil</span><span class="o">.</span><span class="n">loadBytes</span><span class="o">(</span><span class="n">inputStream</span><span class="o">)</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="k">case</span> <span class="nc">None</span> <span class="k">=&gt;</span> <span class="n">resourceNotFound</span><span class="o">()</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p><strong>That&rsquo;s it!</strong> Now you can refer to the WebJars resources in views, like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='ssp'><span class='line'>#set (title)
</span><span class='line'>Hello, Dojo!
</span><span class='line'>#end
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;greeting&quot;</span><span class="nt">&gt;&lt;/div&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;script</span> <span class="na">type=</span><span class="s">&quot;text/javascript&quot;</span> <span class="na">src=</span><span class="s">&quot;${uri(&quot;</span><span class="err">/webjars/dojo/1.9.3/dojo/dojo.js&quot;)}&quot;</span> <span class="na">data-dojo-config=</span><span class="s">&quot;async: true&quot;</span><span class="nt">&gt;&lt;/script&gt;</span>
</span><span class='line'><span class="nt">&lt;script</span> <span class="na">type=</span><span class="s">&quot;text/javascript&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>require([
</span><span class='line'>    &#39;dojo/dom&#39;,
</span><span class='line'>    &#39;dojo/dom-construct&#39;
</span><span class='line'>], function (dom, domConstruct) {
</span><span class='line'>    var greetingNode = dom.byId(&#39;greeting&#39;);
</span><span class='line'>    domConstruct.place(&#39;<span class="nt">&lt;i&gt;</span>Dojo!<span class="nt">&lt;/i&gt;</span>&#39;, greetingNode);
</span><span class='line'>});
</span><span class='line'><span class="nt">&lt;/script&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Some Explanations on This Route</h3>

<ul>
<li><code>/webjars/*</code> is a <a href="http://www.scalatra.org/2.2/guides/http/routes.html#toc_233">Wildcards</a> and <code>params("splat")</code> is to extract the asterisk part.</li>
<li><code>resourcePath</code> points to the WebJars resources in the jar file, as we saw in Eclipse. It is then fetched as an <code>InputStream</code> with <code>getResourceAsStream()</code>.</li>
<li><code>servletContext.getMimeType()</code> is a handy method to determine the content type of the requested resource, instead of parsing it by ourselves. I find this in SpringMVC&rsquo;s <a href="http://grepcode.com/file/repo1.maven.org/maven2/org.springframework/spring-webmvc/3.2.7.RELEASE/org/springframework/web/servlet/resource/ResourceHttpRequestHandler.java#ResourceHttpRequestHandler.handleRequest%28javax.servlet.http.HttpServletRequest%2Cjavax.servlet.http.HttpServletResponse%29">ResourceHttpRequestHandler</a>.</li>
<li><code>IOUtil</code> is a utiliy class that comes with <a href="http://scalate.fusesource.org/">Scalate</a>, so don&rsquo;t forget to import it first.</li>
</ul>


<p>At first I tried to figure out whether Scalatra provides a conveniet way to serve static files in classpath, I failed. So I decided to serve them by my own, and <a href="https://gist.github.com/laurilehmijoki/4483113">this gist</a> was very helpful.</p>

<p>Anyway, I&rsquo;ve spent more than half a day to solve this problem, and it turned out to be a very challenging yet interesting way to learn a new language, new framework, and new tools. Keep moving!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hive小文件问题的处理]]></title>
    <link href="http://shzhangji.com/blog/2014/04/07/hive-small-files/"/>
    <updated>2014-04-07T17:09:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/04/07/hive-small-files</id>
    <content type="html"><![CDATA[<p>Hive的后端存储是HDFS，它对大文件的处理是非常高效的，如果合理配置文件系统的块大小，NameNode可以支持很大的数据量。但是在数据仓库中，越是上层的表其汇总程度就越高，数据量也就越小。而且这些表通常会按日期进行分区，随着时间的推移，HDFS的文件数目就会逐渐增加。</p>

<h2>小文件带来的问题</h2>

<p>关于这个问题的阐述可以读一读Cloudera的<a href="http://blog.cloudera.com/blog/2009/02/the-small-files-problem/">这篇文章</a>。简单来说，HDFS的文件元信息，包括位置、大小、分块信息等，都是保存在NameNode的内存中的。每个对象大约占用150个字节，因此一千万个文件及分块就会占用约3G的内存空间，一旦接近这个量级，NameNode的性能就会开始下降了。</p>

<p>此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，每个脚本只处理很少的数据，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决。</p>

<!-- more -->


<h2>Hive小文件产生的原因</h2>

<p>前面已经提到，汇总后的数据量通常比源数据要少得多。而为了提升运算速度，我们会增加Reducer的数量，Hive本身也会做类似优化——Reducer数量等于源数据的量除以hive.exec.reducers.bytes.per.reducer所配置的量（默认1G）。Reducer数量的增加也即意味着结果文件的增加，从而产生小文件的问题。</p>

<h2>配置Hive结果合并</h2>

<p>我们可以通过一些配置项来使Hive在执行结束后对结果文件进行合并：</p>

<ul>
<li><code>hive.merge.mapfiles</code> 在map-only job后合并文件，默认<code>true</code></li>
<li><code>hive.merge.mapredfiles</code> 在map-reduce job后合并文件，默认<code>false</code></li>
<li><code>hive.merge.size.per.task</code> 合并后每个文件的大小，默认<code>256000000</code></li>
<li><code>hive.merge.smallfiles.avgsize</code> 平均文件大小，是决定是否执行合并操作的阈值，默认<code>16000000</code></li>
</ul>


<p>Hive在对结果文件进行合并时会执行一个额外的map-only脚本，mapper的数量是文件总大小除以size.per.task参数所得的值，触发合并的条件是：</p>

<ol>
<li>根据查询类型不同，相应的mapfiles/mapredfiles参数需要打开；</li>
<li>结果文件的平均大小需要大于avgsize参数的值。</li>
</ol>


<p>示例：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="c1">-- map-red job，5个reducer，产生5个60K的文件。</span>
</span><span class='line'><span class="k">create</span> <span class="k">table</span> <span class="n">dw_stage</span><span class="p">.</span><span class="n">zj_small</span> <span class="k">as</span>
</span><span class='line'><span class="k">select</span> <span class="n">paid</span><span class="p">,</span> <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
</span><span class='line'><span class="k">from</span> <span class="n">dw_db</span><span class="p">.</span><span class="n">dw_soj_imp_dtl</span>
</span><span class='line'><span class="k">where</span> <span class="n">log_dt</span> <span class="o">=</span> <span class="s1">&#39;2014-04-14&#39;</span>
</span><span class='line'><span class="k">group</span> <span class="k">by</span> <span class="n">paid</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="c1">-- 执行额外的map-only job，一个mapper，产生一个300K的文件。</span>
</span><span class='line'><span class="k">set</span> <span class="n">hive</span><span class="p">.</span><span class="n">merge</span><span class="p">.</span><span class="n">mapredfiles</span><span class="o">=</span><span class="k">true</span><span class="p">;</span>
</span><span class='line'><span class="k">create</span> <span class="k">table</span> <span class="n">dw_stage</span><span class="p">.</span><span class="n">zj_small</span> <span class="k">as</span>
</span><span class='line'><span class="k">select</span> <span class="n">paid</span><span class="p">,</span> <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
</span><span class='line'><span class="k">from</span> <span class="n">dw_db</span><span class="p">.</span><span class="n">dw_soj_imp_dtl</span>
</span><span class='line'><span class="k">where</span> <span class="n">log_dt</span> <span class="o">=</span> <span class="s1">&#39;2014-04-14&#39;</span>
</span><span class='line'><span class="k">group</span> <span class="k">by</span> <span class="n">paid</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="c1">-- map-only job，45个mapper，产生45个25M左右的文件。</span>
</span><span class='line'><span class="k">create</span> <span class="k">table</span> <span class="n">dw_stage</span><span class="p">.</span><span class="n">zj_small</span> <span class="k">as</span>
</span><span class='line'><span class="k">select</span> <span class="o">*</span>
</span><span class='line'><span class="k">from</span> <span class="n">dw_db</span><span class="p">.</span><span class="n">dw_soj_imp_dtl</span>
</span><span class='line'><span class="k">where</span> <span class="n">log_dt</span> <span class="o">=</span> <span class="s1">&#39;2014-04-14&#39;</span>
</span><span class='line'><span class="k">and</span> <span class="n">paid</span> <span class="k">like</span> <span class="s1">&#39;%baidu%&#39;</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="c1">-- 执行额外的map-only job，4个mapper，产生4个250M左右的文件。</span>
</span><span class='line'><span class="k">set</span> <span class="n">hive</span><span class="p">.</span><span class="n">merge</span><span class="p">.</span><span class="n">smallfiles</span><span class="p">.</span><span class="n">avgsize</span><span class="o">=</span><span class="mi">100000000</span><span class="p">;</span>
</span><span class='line'><span class="k">create</span> <span class="k">table</span> <span class="n">dw_stage</span><span class="p">.</span><span class="n">zj_small</span> <span class="k">as</span>
</span><span class='line'><span class="k">select</span> <span class="o">*</span>
</span><span class='line'><span class="k">from</span> <span class="n">dw_db</span><span class="p">.</span><span class="n">dw_soj_imp_dtl</span>
</span><span class='line'><span class="k">where</span> <span class="n">log_dt</span> <span class="o">=</span> <span class="s1">&#39;2014-04-14&#39;</span>
</span><span class='line'><span class="k">and</span> <span class="n">paid</span> <span class="k">like</span> <span class="s1">&#39;%baidu%&#39;</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>压缩文件的处理</h3>

<p>如果结果表使用了压缩格式，则必须配合SequenceFile来存储，否则无法进行合并，以下是示例：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">set</span> <span class="n">mapred</span><span class="p">.</span><span class="k">output</span><span class="p">.</span><span class="n">compression</span><span class="p">.</span><span class="k">type</span><span class="o">=</span><span class="n">BLOCK</span><span class="p">;</span>
</span><span class='line'><span class="k">set</span> <span class="n">hive</span><span class="p">.</span><span class="k">exec</span><span class="p">.</span><span class="n">compress</span><span class="p">.</span><span class="k">output</span><span class="o">=</span><span class="k">true</span><span class="p">;</span>
</span><span class='line'><span class="k">set</span> <span class="n">mapred</span><span class="p">.</span><span class="k">output</span><span class="p">.</span><span class="n">compression</span><span class="p">.</span><span class="n">codec</span><span class="o">=</span><span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">hadoop</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="n">compress</span><span class="p">.</span><span class="n">LzoCodec</span><span class="p">;</span>
</span><span class='line'><span class="k">set</span> <span class="n">hive</span><span class="p">.</span><span class="n">merge</span><span class="p">.</span><span class="n">smallfiles</span><span class="p">.</span><span class="n">avgsize</span><span class="o">=</span><span class="mi">100000000</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="k">drop</span> <span class="k">table</span> <span class="n">if</span> <span class="k">exists</span> <span class="n">dw_stage</span><span class="p">.</span><span class="n">zj_small</span><span class="p">;</span>
</span><span class='line'><span class="k">create</span> <span class="k">table</span> <span class="n">dw_stage</span><span class="p">.</span><span class="n">zj_small</span>
</span><span class='line'><span class="n">STORED</span> <span class="k">AS</span> <span class="n">SEQUENCEFILE</span>
</span><span class='line'><span class="k">as</span> <span class="k">select</span> <span class="o">*</span>
</span><span class='line'><span class="k">from</span> <span class="n">dw_db</span><span class="p">.</span><span class="n">dw_soj_imp_dtl</span>
</span><span class='line'><span class="k">where</span> <span class="n">log_dt</span> <span class="o">=</span> <span class="s1">&#39;2014-04-14&#39;</span>
</span><span class='line'><span class="k">and</span> <span class="n">paid</span> <span class="k">like</span> <span class="s1">&#39;%baidu%&#39;</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<h2>使用HAR归档文件</h2>

<p>Hadoop的<a href="http://hadoop.apache.org/docs/stable1/hadoop_archives.html">归档文件</a>格式也是解决小文件问题的方式之一。而且Hive提供了<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Archiving">原生支持</a>：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">set</span> <span class="n">hive</span><span class="p">.</span><span class="n">archive</span><span class="p">.</span><span class="n">enabled</span><span class="o">=</span><span class="k">true</span><span class="p">;</span>
</span><span class='line'><span class="k">set</span> <span class="n">hive</span><span class="p">.</span><span class="n">archive</span><span class="p">.</span><span class="n">har</span><span class="p">.</span><span class="n">parentdir</span><span class="p">.</span><span class="n">settable</span><span class="o">=</span><span class="k">true</span><span class="p">;</span>
</span><span class='line'><span class="k">set</span> <span class="n">har</span><span class="p">.</span><span class="n">partfile</span><span class="p">.</span><span class="k">size</span><span class="o">=</span><span class="mi">1099511627776</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">srcpart</span> <span class="n">ARCHIVE</span> <span class="n">PARTITION</span><span class="p">(</span><span class="n">ds</span><span class="o">=</span><span class="s1">&#39;2008-04-08&#39;</span><span class="p">,</span> <span class="n">hr</span><span class="o">=</span><span class="s1">&#39;12&#39;</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'><span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">srcpart</span> <span class="n">UNARCHIVE</span> <span class="n">PARTITION</span><span class="p">(</span><span class="n">ds</span><span class="o">=</span><span class="s1">&#39;2008-04-08&#39;</span><span class="p">,</span> <span class="n">hr</span><span class="o">=</span><span class="s1">&#39;12&#39;</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>如果使用的不是分区表，则可创建成外部表，并使用<code>har://</code>协议来指定路径。</p>

<h2>HDFS Federation</h2>

<p>Hadoop V2引入了HDFS Federation的概念：</p>

<p><img src="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/images/federation.gif" alt="" /></p>

<p>实则是将NameNode做了拆分，从而增强了它的扩展性，小文件的问题也能够得到缓解。</p>

<h2>其他工具</h2>

<p>对于通常的应用，使用Hive结果合并就能达到很好的效果。如果不想因此增加运行时间，可以自行编写一些脚本，在系统空闲时对分区内的文件进行合并，也能达到目的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java反射机制]]></title>
    <link href="http://shzhangji.com/blog/2014/01/25/java-reflection-tutorial/"/>
    <updated>2014-01-25T09:42:00+08:00</updated>
    <id>http://shzhangji.com/blog/2014/01/25/java-reflection-tutorial</id>
    <content type="html"><![CDATA[<p>原文：<a href="http://www.programcreek.com/2013/09/java-reflection-tutorial/">http://www.programcreek.com/2013/09/java-reflection-tutorial/</a></p>

<p>什么是反射？它有何用处？</p>

<h2>1. 什么是反射？</h2>

<p>“反射（Reflection）能够让运行于JVM中的程序检测和修改运行时的行为。”这个概念常常会和内省（Introspection）混淆，以下是这两个术语在Wikipedia中的解释：</p>

<ol>
<li>内省用于在运行时检测某个对象的类型和其包含的属性；</li>
<li>反射用于在运行时检测和修改某个对象的结构及其行为。</li>
</ol>


<p>从他们的定义可以看出，内省是反射的一个子集。有些语言支持内省，但并不支持反射，如C++。</p>

<p><img src="http://www.programcreek.com/wp-content/uploads/2013/09/reflection-introspection-650x222.png" alt="反射和内省" /></p>

<!-- more -->


<p>内省示例：<code>instanceof</code>运算符用于检测某个对象是否属于特定的类。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="k">if</span> <span class="o">(</span><span class="n">obj</span> <span class="k">instanceof</span> <span class="n">Dog</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">Dog</span> <span class="n">d</span> <span class="o">=</span> <span class="o">(</span><span class="n">Dog</span><span class="o">)</span> <span class="n">obj</span><span class="o">;</span>
</span><span class='line'>    <span class="n">d</span><span class="o">.</span><span class="na">bark</span><span class="o">();</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>反射示例：<code>Class.forName()</code>方法可以通过类或接口的名称（一个字符串或完全限定名）来获取对应的<code>Class</code>对象。<code>forName</code>方法会触发类的初始化。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="c1">// 使用反射</span>
</span><span class='line'><span class="n">Class</span><span class="o">&lt;?&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">Class</span><span class="o">.</span><span class="na">forName</span><span class="o">(</span><span class="s">&quot;classpath.and.classname&quot;</span><span class="o">);</span>
</span><span class='line'><span class="n">Object</span> <span class="n">dog</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="na">newInstance</span><span class="o">();</span>
</span><span class='line'><span class="n">Method</span> <span class="n">m</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="na">getDeclaredMethod</span><span class="o">(</span><span class="s">&quot;bark&quot;</span><span class="o">,</span> <span class="k">new</span> <span class="n">Class</span><span class="o">&lt;?&gt;[</span><span class="mi">0</span><span class="o">]);</span>
</span><span class='line'><span class="n">m</span><span class="o">.</span><span class="na">invoke</span><span class="o">(</span><span class="n">dog</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>在Java中，反射更接近于内省，因为你无法改变一个对象的结构。虽然一些API可以用来修改方法和属性的可见性，但并不能修改结构。</p>

<h2>2. 我们为何需要反射？</h2>

<p>反射能够让我们：</p>

<ul>
<li>在运行时检测对象的类型；</li>
<li>动态构造某个类的对象；</li>
<li>检测类的属性和方法；</li>
<li>任意调用对象的方法；</li>
<li>修改构造函数、方法、属性的可见性；</li>
<li>以及其他</li>
</ul>


<p>反射是框架中常用的方法。</p>

<p>例如，<a href="http://www.programcreek.com/2012/02/junit-tutorial-2-annotations/">JUnit</a>通过反射来遍历包含 <em>@Test</em> 注解的方法，并在运行单元测试时调用它们。（<a href="http://www.programcreek.com/2012/02/junit-tutorial-2-annotations/">这个连接</a>中包含了一些JUnit的使用案例）</p>

<p>对于Web框架，开发人员在配置文件中定义他们对各种接口和类的实现。通过反射机制，框架能够快速地动态初始化所需要的类。</p>

<p>例如，Spring框架使用如下的配置文件：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;bean</span> <span class="na">id=</span><span class="s">&quot;someID&quot;</span> <span class="na">class=</span><span class="s">&quot;com.programcreek.Foo&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;property</span> <span class="na">name=</span><span class="s">&quot;someField&quot;</span> <span class="na">value=</span><span class="s">&quot;someValue&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'><span class="nt">&lt;/bean&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>当Spring容器处理&lt;bean&gt;元素时，会使用<code>Class.forName("com.programcreek.Foo")</code>来初始化这个类，并再次使用反射获取&lt;property&gt;元素对应的<code>setter</code>方法，为对象的属性赋值。</p>

<p>Servlet也会使用相同的机制：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;servlet&gt;</span>
</span><span class='line'>    <span class="nt">&lt;servlet-name&gt;</span>someServlet<span class="nt">&lt;/servlet-name&gt;</span>
</span><span class='line'>    <span class="nt">&lt;servlet-class&gt;</span>com.programcreek.WhyReflectionServlet<span class="nt">&lt;/servlet-class&gt;</span>
</span><span class='line'><span class="nt">&lt;servlet&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<h2>3. 如何使用反射？</h2>

<p>让我们通过几个典型的案例来学习如何使用反射。</p>

<p>示例1：获取对象的类型名称。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kn">package</span> <span class="n">myreflection</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">java.lang.reflect.Method</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">ReflectionHelloWorld</span> <span class="o">{</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">){</span>
</span><span class='line'>        <span class="n">Foo</span> <span class="n">f</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Foo</span><span class="o">();</span>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="n">f</span><span class="o">.</span><span class="na">getClass</span><span class="o">().</span><span class="na">getName</span><span class="o">());</span>           
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="kd">class</span> <span class="nc">Foo</span> <span class="o">{</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">print</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&quot;abc&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>输出：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>myreflection.Foo
</span></code></pre></td></tr></table></div></figure>


<p>示例2：调用未知对象的方法。</p>

<p>在下列代码中，设想对象的类型是未知的。通过反射，我们可以判断它是否包含<code>print</code>方法，并调用它。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kn">package</span> <span class="n">myreflection</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">java.lang.reflect.Method</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">ReflectionHelloWorld</span> <span class="o">{</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">){</span>
</span><span class='line'>        <span class="n">Foo</span> <span class="n">f</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Foo</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">Method</span> <span class="n">method</span><span class="o">;</span>
</span><span class='line'>        <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">method</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="na">getClass</span><span class="o">().</span><span class="na">getMethod</span><span class="o">(</span><span class="s">&quot;print&quot;</span><span class="o">,</span> <span class="k">new</span> <span class="n">Class</span><span class="o">&lt;?&gt;[</span><span class="mi">0</span><span class="o">]);</span>
</span><span class='line'>            <span class="n">method</span><span class="o">.</span><span class="na">invoke</span><span class="o">(</span><span class="n">f</span><span class="o">);</span>
</span><span class='line'>        <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="kd">class</span> <span class="nc">Foo</span> <span class="o">{</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">print</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&quot;abc&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>abc
</span></code></pre></td></tr></table></div></figure>


<p>示例3：创建对象</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kn">package</span> <span class="n">myreflection</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">ReflectionHelloWorld</span> <span class="o">{</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">){</span>
</span><span class='line'>        <span class="c1">// 创建Class实例</span>
</span><span class='line'>        <span class="n">Class</span><span class="o">&lt;?&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class='line'>        <span class="k">try</span><span class="o">{</span>
</span><span class='line'>            <span class="n">c</span><span class="o">=</span><span class="n">Class</span><span class="o">.</span><span class="na">forName</span><span class="o">(</span><span class="s">&quot;myreflection.Foo&quot;</span><span class="o">);</span>
</span><span class='line'>        <span class="o">}</span><span class="k">catch</span><span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">){</span>
</span><span class='line'>            <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// 创建Foo实例</span>
</span><span class='line'>        <span class="n">Foo</span> <span class="n">f</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">f</span> <span class="o">=</span> <span class="o">(</span><span class="n">Foo</span><span class="o">)</span> <span class="n">c</span><span class="o">.</span><span class="na">newInstance</span><span class="o">();</span>
</span><span class='line'>        <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">f</span><span class="o">.</span><span class="na">print</span><span class="o">();</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="kd">class</span> <span class="nc">Foo</span> <span class="o">{</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">print</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&quot;abc&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>示例4：获取构造函数，并创建对象。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kn">package</span> <span class="n">myreflection</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kn">import</span> <span class="nn">java.lang.reflect.Constructor</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">ReflectionHelloWorld</span> <span class="o">{</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">){</span>
</span><span class='line'>        <span class="c1">// 创建Class实例</span>
</span><span class='line'>        <span class="n">Class</span><span class="o">&lt;?&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class='line'>        <span class="k">try</span><span class="o">{</span>
</span><span class='line'>            <span class="n">c</span><span class="o">=</span><span class="n">Class</span><span class="o">.</span><span class="na">forName</span><span class="o">(</span><span class="s">&quot;myreflection.Foo&quot;</span><span class="o">);</span>
</span><span class='line'>        <span class="o">}</span><span class="k">catch</span><span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">){</span>
</span><span class='line'>            <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// 创建Foo实例</span>
</span><span class='line'>        <span class="n">Foo</span> <span class="n">f1</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class='line'>        <span class="n">Foo</span> <span class="n">f2</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// 获取所有的构造函数</span>
</span><span class='line'>        <span class="n">Constructor</span><span class="o">&lt;?&gt;</span> <span class="n">cons</span><span class="o">[]</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="na">getConstructors</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">f1</span> <span class="o">=</span> <span class="o">(</span><span class="n">Foo</span><span class="o">)</span> <span class="n">cons</span><span class="o">[</span><span class="mi">0</span><span class="o">].</span><span class="na">newInstance</span><span class="o">();</span>
</span><span class='line'>            <span class="n">f2</span> <span class="o">=</span> <span class="o">(</span><span class="n">Foo</span><span class="o">)</span> <span class="n">cons</span><span class="o">[</span><span class="mi">1</span><span class="o">].</span><span class="na">newInstance</span><span class="o">(</span><span class="s">&quot;abc&quot;</span><span class="o">);</span>
</span><span class='line'>        <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">f1</span><span class="o">.</span><span class="na">print</span><span class="o">();</span>
</span><span class='line'>        <span class="n">f2</span><span class="o">.</span><span class="na">print</span><span class="o">();</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="kd">class</span> <span class="nc">Foo</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">String</span> <span class="n">s</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">public</span> <span class="nf">Foo</span><span class="o">(){}</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">public</span> <span class="nf">Foo</span><span class="o">(</span><span class="n">String</span> <span class="n">s</span><span class="o">){</span>
</span><span class='line'>        <span class="k">this</span><span class="o">.</span><span class="na">s</span><span class="o">=</span><span class="n">s</span><span class="o">;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">print</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="n">s</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>null
</span><span class='line'>abc
</span></code></pre></td></tr></table></div></figure>


<p>此外，你可以通过<code>Class</code>实例来获取该类实现的接口、父类、声明的属性等。</p>

<p>示例5：通过反射来修改数组的大小。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kn">package</span> <span class="n">myreflection</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kn">import</span> <span class="nn">java.lang.reflect.Array</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">ReflectionHelloWorld</span> <span class="o">{</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="kt">int</span><span class="o">[]</span> <span class="n">intArray</span> <span class="o">=</span> <span class="o">{</span> <span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">5</span> <span class="o">};</span>
</span><span class='line'>        <span class="kt">int</span><span class="o">[]</span> <span class="n">newIntArray</span> <span class="o">=</span> <span class="o">(</span><span class="kt">int</span><span class="o">[])</span> <span class="n">changeArraySize</span><span class="o">(</span><span class="n">intArray</span><span class="o">,</span> <span class="mi">10</span><span class="o">);</span>
</span><span class='line'>        <span class="n">print</span><span class="o">(</span><span class="n">newIntArray</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">String</span><span class="o">[]</span> <span class="n">atr</span> <span class="o">=</span> <span class="o">{</span> <span class="s">&quot;a&quot;</span><span class="o">,</span> <span class="s">&quot;b&quot;</span><span class="o">,</span> <span class="s">&quot;c&quot;</span><span class="o">,</span> <span class="s">&quot;d&quot;</span><span class="o">,</span> <span class="s">&quot;e&quot;</span> <span class="o">};</span>
</span><span class='line'>        <span class="n">String</span><span class="o">[]</span> <span class="n">str1</span> <span class="o">=</span> <span class="o">(</span><span class="n">String</span><span class="o">[])</span> <span class="n">changeArraySize</span><span class="o">(</span><span class="n">atr</span><span class="o">,</span> <span class="mi">10</span><span class="o">);</span>
</span><span class='line'>        <span class="n">print</span><span class="o">(</span><span class="n">str1</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// 修改数组的大小</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kd">static</span> <span class="n">Object</span> <span class="nf">changeArraySize</span><span class="o">(</span><span class="n">Object</span> <span class="n">obj</span><span class="o">,</span> <span class="kt">int</span> <span class="n">len</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">Class</span><span class="o">&lt;?&gt;</span> <span class="n">arr</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="na">getClass</span><span class="o">().</span><span class="na">getComponentType</span><span class="o">();</span>
</span><span class='line'>        <span class="n">Object</span> <span class="n">newArray</span> <span class="o">=</span> <span class="n">Array</span><span class="o">.</span><span class="na">newInstance</span><span class="o">(</span><span class="n">arr</span><span class="o">,</span> <span class="n">len</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// 复制数组</span>
</span><span class='line'>        <span class="kt">int</span> <span class="n">co</span> <span class="o">=</span> <span class="n">Array</span><span class="o">.</span><span class="na">getLength</span><span class="o">(</span><span class="n">obj</span><span class="o">);</span>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">arraycopy</span><span class="o">(</span><span class="n">obj</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="n">newArray</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="n">co</span><span class="o">);</span>
</span><span class='line'>        <span class="k">return</span> <span class="n">newArray</span><span class="o">;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// 打印</span>
</span><span class='line'>    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">print</span><span class="o">(</span><span class="n">Object</span> <span class="n">obj</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">Class</span><span class="o">&lt;?&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="na">getClass</span><span class="o">();</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">(!</span><span class="n">c</span><span class="o">.</span><span class="na">isArray</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>            <span class="k">return</span><span class="o">;</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&quot;\nArray length: &quot;</span> <span class="o">+</span> <span class="n">Array</span><span class="o">.</span><span class="na">getLength</span><span class="o">(</span><span class="n">obj</span><span class="o">));</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">Array</span><span class="o">.</span><span class="na">getLength</span><span class="o">(</span><span class="n">obj</span><span class="o">);</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">Array</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">obj</span><span class="o">,</span> <span class="n">i</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; &quot;</span><span class="o">);</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>输出：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>Array length: 10
</span><span class='line'>1 2 3 4 5 0 0 0 0 0
</span><span class='line'>Array length: 10
</span><span class='line'>a b c d e null null null null null
</span></code></pre></td></tr></table></div></figure>


<h2>总结</h2>

<p>上述示例代码仅仅展现了Java反射机制很小一部分的功能。如果你觉得意犹未尽，可以前去阅读<a href="http://docs.oracle.com/javase/tutorial/reflect/">官方文档</a>。</p>

<p>参考资料：</p>

<ol>
<li><a href="http://en.wikipedia.org/wiki/Reflection_">http://en.wikipedia.org/wiki/Reflection_</a>(computer_programming)</li>
<li><a href="http://docs.oracle.com/javase/tutorial/reflect/">http://docs.oracle.com/javase/tutorial/reflect/</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[抽象泄漏定律]]></title>
    <link href="http://shzhangji.com/blog/2013/12/17/the-law-of-leaky-abstractions/"/>
    <updated>2013-12-17T13:05:00+08:00</updated>
    <id>http://shzhangji.com/blog/2013/12/17/the-law-of-leaky-abstractions</id>
    <content type="html"><![CDATA[<p>原文：<a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html">http://www.joelonsoftware.com/articles/LeakyAbstractions.html</a></p>

<p>TCP协议是互联网的基石，我们每天都需要依靠它来构建各类互联网应用。也正是在这一协议中，时刻发生着一件近乎神奇的事情。</p>

<p>TCP是一种 <em>可靠的</em> 数据传输协议，也就是说，当你通过TCP协议在网络上传输一条消息时，它一定会到达目的地，而且不会失真或毁坏。</p>

<p>我们可以使用TCP来做很多事情，从浏览网页信息到收发邮件。TCP的可靠性使得东非贪污受贿的新闻能够一字一句地传递到世界各地。真是太棒了！</p>

<p>和TCP协议相比，IP协议也是一种传输协议，但它是 <em>不可靠的</em> 。没有人可以保证你的数据一定会到达目的地，或者在它到达前就已经被破坏了。如果你发送了一组消息，不要惊讶为何只有一半的消息到达，有些消息的顺序会不正确，甚至消息的内容被替换成了黑猩猩宝宝的图片，或是一堆无法阅读的垃圾数据，像极了台湾人的邮件标题。</p>

<p>这就是TCP协议神奇的地方：它是构建在IP协议之上的。换句话说，TCP协议能够 <em>使用一个不可靠的工具来可靠地传输数据</em> 。</p>

<!--more-->


<p>为了更好地说明这有多么神奇，让我们设想下面的场景。虽然有些荒诞，但本质上是相同的。</p>

<p>假设我们用一辆辆汽车将百老汇的演员们运送到好莱坞，这是一条横跨美国的漫长线路。其中一些汽车出了交通事故，车上的演员在事故中死亡。有些演员则在车上酗酒嗑药，兴奋之余将自己的头发剃了，或是纹上了丑陋的纹身，这样一来就失去了他们原先的样貌，无法在好莱坞演出。更普遍的情况是，演员们没有按照出发的顺序到达目的地，因为他们走的都是不同的线路。现在再让我们设想有一个名为“好莱坞快线”的运输服务，在运送这些演员时能够保证三点：他们都能够到达；到达顺序和出发顺序一致；并且都完好无损。神奇的是，好莱坞快线除了用汽车来运输这些演员之外，没有任何其他的方法。所以它能做的就是检查每一个到达目的地的演员，看他们是否和原先的相貌一致。如果有所差别，它就立刻通知百老汇的办公室，派出该演员的双胞胎兄妹，重新发送过来。如果演员到达的顺序不同，好莱坞快线会负责重新排序。如果有一架UFO在飞往51区的途中不慎坠毁在内华达州，造成高速公路阻塞，这时所有打算从这条路经过的演员会绕道亚利桑那州。好莱坞快线不会告诉加利福尼亚州的导演路上发生了什么，只是这些演员到的比较迟而已。</p>

<p>这大致上就是TCP协议的神奇之处，计算机科学家们通常会将其称作为“抽象”：将复杂的问题用简单的方式表现出来。事实上，很多计算机编程工作都是在进行抽象。字符串库做了什么？它能让我们觉得计算机可以像处理数字那样处理文字。文件系统是什么？它让硬盘不再是一组高速旋转的磁性盘块，而是一个有着目录层级结构、能够按字节存储字符信息的设备。</p>

<p>我们继续说TCP。刚才我打了一个比方，有些人可能觉得那很疯狂。但是，当我说TCP协议可以保证消息一定能够到达，事实上并非如此。如果你的宠物蛇把网线给咬坏了，那即便是TCP协议也无法传输数据；如果你和网络管理员闹了矛盾，他将你的网口接到了一台负载很高的交换机上，那即便你的数据包可以传输，速度也会奇慢无比。</p>

<p>这就是我所说的“抽象泄漏”。TCP协议试图提供一个完整的抽象，将底层不可靠的数据传输包装起来，但是，底层的传输有时也会发生问题，即便是TCP协议也无法解决，这时你会发现，它也不是万能的。TCP协议就是“抽象泄漏定律”的示例之一，其实，几乎所有的抽象都是泄漏的。这种泄漏有时很小，有时会很严重。下面再举一些例子：</p>

<ul>
<li><p>对于一个简单的操作，如循环遍历一个二维数组，当遍历的方式不同（横向或纵向），也会对性能造成很大影响，这主要取决于数组中数据的分布——按某个方向遍历时可能会产生更多的页缺失（page fault），而页缺失往往是非常消耗性能的。即使是汇编程序员，他们在编写代码时也会假设程序的内存空间是连续的，这是系统底层的虚拟内存机制提供的抽象，而这一机制在遇到页缺失时就会消耗更多时间。</p></li>
<li><p>SQL语言意图将过程式的数据库访问操作封装起来，你只需要告诉操作系统你想要的数据，系统会自动生成各个步骤并加以执行。但在有些情况下，某些SQL查询会比其逻辑等同的查询语句要慢得多。一个著名的示例是，对大多数SQL服务器，指定“WHERE a = b AND b = c AND a = c”要比单纯指定“WHERE a = b AND b = c”快的多，即便它们的结果集是一致的。在使用SQL时，我们不需要思考过程，只需关注定义。但有时，这种抽象会造成性能上的大幅下降，你需要去了解SQL语法分析器的工作原理，找出问题的原因，并想出应对措施，让自己的查询运行得更快。</p></li>
<li><p>即便有NFS、SMB这样的协议可以让你像在处理本地文件一样处理远程文件，如果网络传输很慢，或是完全中断了，程序员就需要手动处理这种情况。所以，这种“远程文件即本地文件”的抽象机制是存在<a href="http://www.joelonsoftware.com/articles/fog0000000041.html">泄漏</a>的。这里举一个现实的例子：如果你将用户的home目录加载到NFS上（一次抽象），你的用户创建了.forward文件，用来转发他所有的电子邮件（二次抽象），当NFS服务器宕机，.forward文件会找不到，这样就无法转发邮件了，造成丢失。</p></li>
<li><p>C++的字符串处理类库相当于增加了一种基础数据类型：字符串，将<a href="http://www.joelonsoftware.com/articles/fog0000000319.html">各种操作细节</a>封装起来，让程序员可以方便地使用它。几乎所有的C++字符串类都会重载+操作符，这样你就能用 <em>s + &ldquo;bar&rdquo;</em> 来拼接字符串了。但是，无论哪种类库都无法实现 <em>&ldquo;foo&rdquo; + &ldquo;bar&rdquo;</em> 这种语句，因为在C++中，字符串字面量（string literal）都是char *类型的。这就是一种泄漏。（有趣的是，C++语言的发展历程很大一部分是在争论字符串是否应该在语言层面支持。我个人并不太能理解这为何需要争论。）</p></li>
<li><p>当你在雨天开车，虽然你坐在车里，前窗有雨刷，车内有空调，这些措施将“天气”给抽象走了。但是，你还是要小心雨天的轮胎打滑，有时这雨下得太大，可见度很糟，所以你还是得慢行。也就是说，“天气”因素并没有被完全抽象走，它也是存在泄漏的。</p></li>
</ul>


<p>抽象泄漏引发的麻烦之一是，它并没有完全简化我们的工作。当我指导别人学习C++时，我当然希望可以跳过char *和指针运算，直接讲解STL字符串类库的使用。但是，当某一天他写出了 <em>&ldquo;foo&rdquo; + &ldquo;bar&rdquo;</em> 这样的代码，并询问我为什么编译错误时，我还是需要告诉它char *的存在。或者说，当他需要调用一个Windows API，需要指定OUT LPTSTR参数，这时他就必须学习char *、指针、Unicode、wchar_t、TCHAR头文件等一系列知识，这些都是抽象泄漏。</p>

<p>在指导COM编程时，我希望可以直接让大家如何使用Visual Studio的代码生成向导。但将来如果出现问题，学员面对这些生成的代码会不知所从，这时还是要回过头来学习IUnknown、CLSID、ProgIDS等等。天呐！</p>

<p>在指导ASP.NET编程时，我希望可以直接告诉大家双击页面上的控件，在弹出的代码框中输入点击响应事件。的确，ASP.NET将处理点击的HTML代码抽象掉了，但问题在于，ASP.NET的设计者需要动用JS来模拟表单的提交，因为HTML中的&lt;a/&gt;标签是没有这一功能的。这样一来，如果终端用户将JS禁止了，这个程序将无法运行。初学者会不知所措，直至他了解ASP.NET的运作方式，了解它究竟将什么样的工作封装起来了，才能进一步排查。</p>

<p>由于抽象定律的存在，每当有人说自己发现了一款新的代码生成工具，能够大大提高我们的编程效率时，你会听很多人说“先学习手工编写，再去用工具生成”。代码生成工具是一种抽象，同样也会泄漏，唯一的解决方法是学习它的实现原理，即它抽象了什么。所以说抽象只是用于提高我们的工作效率的，而不会节省我们的学习时间。</p>

<p>这就形成了一个悖论：当我们拥有越来越高级的开发工具，越来越好的“抽象”，要成为一个高水平的程序员反而越来越困难了。</p>

<p>我在微软实习的第一年，是为Macintosh编写字符串处理类库。很普通的一个任务：编写 <em>strcat</em> 函数，返回一个指针，指向新字符串的尾部。几行C语言代码就能实现了，这些都是从K&amp;R这本C语言编程书上学习到的。</p>

<p>如今，我在CityDesk供职，需要使用Visual Basic、COM、ATL、C++、InnoSetup、Internet Explorer原理、正则表达式、DOM、HTML、CSS、XML等等，这些相对于古老的K&amp;R来说都是非常高级的工具，但是我仍然需要用到K&amp;R的相关知识，否则会困难重重。</p>

<p>十年前，我们会想象未来能够出现各种新式的编程范型，简化我们的工作。的确，这些年我们创造的各类抽象使得开发复杂的大型软件变得比十五年前要简单得多，就像GUI和网络编程。现代的面向对象编程语言让我们的工作变得高效快速。但突然有一天，这种抽象泄漏出一个问题，解决它需要耗费两星期。如果你需要招录一个VB程序员，那不是一个好主意，因为当他碰到VB语言泄漏的问题时，他会变得寸步难行。</p>

<p>抽象泄漏定律正在阻碍我们前进。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Generate Auto-increment Id in Map-reduce Job]]></title>
    <link href="http://shzhangji.com/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job/"/>
    <updated>2013-10-31T09:35:00+08:00</updated>
    <id>http://shzhangji.com/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job</id>
    <content type="html"><![CDATA[<p>In DBMS world, it&rsquo;s easy to generate a unique, auto-increment id, using MySQL&rsquo;s <a href="http://dev.mysql.com/doc/refman/5.1/en/example-auto-increment.html">AUTO_INCREMENT attribute</a> on a primary key or MongoDB&rsquo;s <a href="http://docs.mongodb.org/manual/tutorial/create-an-auto-incrementing-field/">Counters Collection</a> pattern. But when it comes to a distributed, parallel processing framework, like Hadoop Map-reduce, it is not that straight forward. The best solution to identify every record in such framework is to use UUID. But when an integer id is required, it&rsquo;ll take some steps.</p>

<h2>Solution A: Single Reducer</h2>

<p>This is the most obvious and simple one, just use the following code to specify reducer numbers to 1:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">job</span><span class="o">.</span><span class="na">setNumReduceTasks</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>And also obvious, there are several demerits:</p>

<ol>
<li>All mappers output will be copied to one task tracker.</li>
<li>Only one process is working on shuffel &amp; sort.</li>
<li>When producing output, there&rsquo;s also only one process.</li>
</ol>


<p>The above is not a problem for small data sets, or at least small mapper outputs. And it is also the approach that Pig and Hive use when they need to perform a total sort. But when hitting a certain threshold, the sort and copy phase will become very slow and unacceptable.</p>

<!-- more -->


<h2>Solution B: Increment by Number of Tasks</h2>

<p>Inspired by a <a href="http://mail-archives.apache.org/mod_mbox/hadoop-common-user/200904.mbox/%3C49E13557.7090504@domaintools.com%3E">mailing list</a> that is quite hard to find, which is inspired by MySQL master-master setup (with auto_increment_increment and auto_increment_offset), there&rsquo;s a brilliant way to generate a globally unique integer id across mappers or reducers. Let&rsquo;s take mapper for example:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">JobMapper</span> <span class="kd">extends</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">private</span> <span class="kt">long</span> <span class="n">id</span><span class="o">;</span>
</span><span class='line'>    <span class="kd">private</span> <span class="kt">int</span> <span class="n">increment</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">setup</span><span class="o">(</span><span class="n">Context</span> <span class="n">context</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span>
</span><span class='line'>            <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>        <span class="kd">super</span><span class="o">.</span><span class="na">setup</span><span class="o">(</span><span class="n">context</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">id</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getTaskAttemptID</span><span class="o">().</span><span class="na">getTaskID</span><span class="o">().</span><span class="na">getId</span><span class="o">();</span>
</span><span class='line'>        <span class="n">increment</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">().</span><span class="na">getInt</span><span class="o">(</span><span class="s">&quot;mapred.map.tasks&quot;</span><span class="o">,</span> <span class="mi">0</span><span class="o">);</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">(</span><span class="n">increment</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>            <span class="k">throw</span> <span class="k">new</span> <span class="nf">IllegalArgumentException</span><span class="o">(</span><span class="s">&quot;mapred.map.tasks is zero&quot;</span><span class="o">);</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">map</span><span class="o">(</span><span class="n">LongWritable</span> <span class="n">key</span><span class="o">,</span> <span class="n">Text</span> <span class="n">value</span><span class="o">,</span> <span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class='line'>            <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">id</span> <span class="o">+=</span> <span class="n">increment</span><span class="o">;</span>
</span><span class='line'>        <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="k">new</span> <span class="n">LongWritable</span><span class="o">(</span><span class="n">id</span><span class="o">),</span>
</span><span class='line'>                <span class="k">new</span> <span class="nf">Text</span><span class="o">(</span><span class="n">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;%d, %s&quot;</span><span class="o">,</span> <span class="n">key</span><span class="o">.</span><span class="na">get</span><span class="o">(),</span> <span class="n">value</span><span class="o">.</span><span class="na">toString</span><span class="o">())));</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The basic idea is simple:</p>

<ol>
<li>Set the initial id to current tasks&rsquo;s id.</li>
<li>When mapping each row, increment the id by the number of tasks.</li>
</ol>


<p>It&rsquo;s also applicable to reducers.</p>

<h2>Solution C: Sorted Auto-increment Id</h2>

<p>Here&rsquo;s a real senario: we have several log files pulled from different machines, and we want to identify each row by an auto-increment id, and they should be in time sequence order.</p>

<p>We know Hadoop has a sort phase, so we can use timestamp as the mapper output key, and the framework will do the trick. But the sorting thing happends in one reducer (partition, in fact), so when using multiple reducer tasks, the result is not in total order. To achieve this, we can use the <a href="http://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.html">TotalOrderPartitioner</a>.</p>

<p>How about the incremental id? Even though the outputs are in total order, Solution B is not applicable here. So we take another approach: seperate the job in two phases, use the reducer to do sorting <em>and</em> counting, then use the second mapper to generate the id.</p>

<p>Here&rsquo;s what we gonna do:</p>

<ol>
<li>Use TotalOrderPartitioner, and generate the partition file.</li>
<li>Parse logs in mapper A, use time as the output key.</li>
<li>Let the framework do partitioning and sorting.</li>
<li>Count records in reducer, write it with <a href="http://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.html">MultipleOutput</a>.</li>
<li>In mapper B, use count as offset, and increment by 1.</li>
</ol>


<p>To simplify the situation, we assume to have the following inputs and outputs:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'> Input       Output
</span><span class='line'>
</span><span class='line'>11:00 a     1 11:00 a
</span><span class='line'>12:00 b     2 11:01 aa
</span><span class='line'>13:00 c     3 11:02 aaa
</span><span class='line'>
</span><span class='line'>11:01 aa    4 12:00 b
</span><span class='line'>12:01 bb    5 12:01 bb
</span><span class='line'>13:01 cc    6 12:02 bbb
</span><span class='line'>
</span><span class='line'>11:02 aaa   7 13:00 c
</span><span class='line'>12:02 bbb   8 13:01 cc
</span><span class='line'>13:02 ccc   9 13:02 ccc
</span></code></pre></td></tr></table></div></figure>


<h3>Generate Partition File</h3>

<p>To use TotalOrderpartitioner, we need a partition file (i.e. boundaries) to tell the partitioner how to partition the mapper outputs. Usually we&rsquo;ll use <a href="https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapreduce/lib/partition/InputSampler.RandomSampler.html">InputSampler.RandomSampler</a> class, but this time let&rsquo;s use a manual partition file.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">SequenceFile</span><span class="o">.</span><span class="na">Writer</span> <span class="n">writer</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SequenceFile</span><span class="o">.</span><span class="na">Writer</span><span class="o">(</span><span class="n">fs</span><span class="o">,</span> <span class="n">getConf</span><span class="o">(),</span> <span class="n">partition</span><span class="o">,</span>
</span><span class='line'>        <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">NullWritable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class='line'><span class="n">Text</span> <span class="n">key</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Text</span><span class="o">();</span>
</span><span class='line'><span class="n">NullWritable</span> <span class="n">value</span> <span class="o">=</span> <span class="n">NullWritable</span><span class="o">.</span><span class="na">get</span><span class="o">();</span>
</span><span class='line'><span class="n">key</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">&quot;12:00&quot;</span><span class="o">);</span>
</span><span class='line'><span class="n">writer</span><span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">value</span><span class="o">);</span>
</span><span class='line'><span class="n">key</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">&quot;13:00&quot;</span><span class="o">);</span>
</span><span class='line'><span class="n">writer</span><span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">value</span><span class="o">);</span>
</span><span class='line'><span class="n">writer</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>So basically, the partitioner will partition the mapper outputs into three parts, the first part will be less than &ldquo;12:00&rdquo;, seceond part [&ldquo;12:00&rdquo;, &ldquo;13:00&rdquo;), thrid [&ldquo;13:00&rdquo;, ).</p>

<p>And then, indicate the job to use this partition file:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">job</span><span class="o">.</span><span class="na">setPartitionerClass</span><span class="o">(</span><span class="n">TotalOrderPartitioner</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class='line'><span class="n">otalOrderPartitioner</span><span class="o">.</span><span class="na">setPartitionFile</span><span class="o">(</span><span class="n">job</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">(),</span> <span class="n">partition</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// The number of reducers should equal the number of partitions.</span>
</span><span class='line'><span class="n">job</span><span class="o">.</span><span class="na">setNumReduceTasks</span><span class="o">(</span><span class="mi">3</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Use MutipleOutputs</h3>

<p>In the reducer, we need to note down the row count of this partition, to do that, we&rsquo;ll need the MultipleOutputs class, which let use output multiple result files apart from the default &ldquo;part-r-xxxxx&rdquo;. The reducer&rsquo;s code is as following:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">JobReducer</span> <span class="kd">extends</span> <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">NullWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">private</span> <span class="n">MultipleOutputs</span><span class="o">&lt;</span><span class="n">NullWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="n">mos</span><span class="o">;</span>
</span><span class='line'>    <span class="kd">private</span> <span class="kt">long</span> <span class="n">count</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">setup</span><span class="o">(</span><span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class='line'>            <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>        <span class="kd">super</span><span class="o">.</span><span class="na">setup</span><span class="o">(</span><span class="n">context</span><span class="o">);</span>
</span><span class='line'>        <span class="n">mos</span> <span class="o">=</span> <span class="k">new</span> <span class="n">MultipleOutputs</span><span class="o">&lt;</span><span class="n">NullWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;(</span><span class="n">context</span><span class="o">);</span>
</span><span class='line'>        <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">reduce</span><span class="o">(</span><span class="n">Text</span> <span class="n">key</span><span class="o">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">&gt;</span> <span class="n">values</span><span class="o">,</span> <span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class='line'>            <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">for</span> <span class="o">(</span><span class="n">Text</span> <span class="n">value</span> <span class="o">:</span> <span class="n">values</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">NullWritable</span><span class="o">.</span><span class="na">get</span><span class="o">(),</span> <span class="n">value</span><span class="o">);</span>
</span><span class='line'>            <span class="o">++</span><span class="n">count</span><span class="o">;</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">cleanup</span><span class="o">(</span><span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class='line'>            <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>        <span class="kd">super</span><span class="o">.</span><span class="na">cleanup</span><span class="o">(</span><span class="n">context</span><span class="o">);</span>
</span><span class='line'>        <span class="n">mos</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="s">&quot;count&quot;</span><span class="o">,</span> <span class="n">NullWritable</span><span class="o">.</span><span class="na">get</span><span class="o">(),</span> <span class="k">new</span> <span class="n">LongWritable</span><span class="o">(</span><span class="n">count</span><span class="o">));</span>
</span><span class='line'>        <span class="n">mos</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>There&rsquo;re several things to pay attention to:</p>

<ol>
<li>MultipleOutputs is declared as class member, defined in Reducer#setup method, and must be closed at Reducer#cleanup (otherwise the file will be empty).</li>
<li>When instantiating MultipleOutputs class, the generic type needs to be the same as reducer&rsquo;s output key/value class.</li>
<li>In order to use a different output key/value class, additional setup needs to be done at job definition:</li>
</ol>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Job</span><span class="o">(</span><span class="n">getConf</span><span class="o">());</span>
</span><span class='line'><span class="n">MultipleOutputs</span><span class="o">.</span><span class="na">addNamedOutput</span><span class="o">(</span><span class="n">job</span><span class="o">,</span> <span class="s">&quot;count&quot;</span><span class="o">,</span> <span class="n">SequenceFileOutputFormat</span><span class="o">.</span><span class="na">class</span><span class="o">,</span>
</span><span class='line'>    <span class="n">NullWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>For example, if the output folder is &ldquo;/tmp/total-sort/&rdquo;, there&rsquo;ll be the following files when job is done:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>/tmp/total-sort/count-r-00001
</span><span class='line'>/tmp/total-sort/count-r-00002
</span><span class='line'>/tmp/total-sort/count-r-00003
</span><span class='line'>/tmp/total-sort/part-r-00001
</span><span class='line'>/tmp/total-sort/part-r-00002
</span><span class='line'>/tmp/total-sort/part-r-00003
</span></code></pre></td></tr></table></div></figure>


<h3>Pass Start Ids to Mapper</h3>

<p>When the second mapper processes the inputs, we want them to know the initial id of its partition, which can be calculated from the &ldquo;count-*&rdquo; files we produce before. To pass this information, we can use the job&rsquo;s Configuration object.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="c1">// Read and calculate the start id from those row-count files.</span>
</span><span class='line'><span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;</span> <span class="n">startIds</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashMap</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;();</span>
</span><span class='line'><span class="kt">long</span> <span class="n">startId</span> <span class="o">=</span> <span class="mi">1</span><span class="o">;</span>
</span><span class='line'><span class="n">FileSystem</span> <span class="n">fs</span> <span class="o">=</span> <span class="n">FileSystem</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">getConf</span><span class="o">());</span>
</span><span class='line'><span class="k">for</span> <span class="o">(</span><span class="n">FileStatus</span> <span class="n">file</span> <span class="o">:</span> <span class="n">fs</span><span class="o">.</span><span class="na">listStatus</span><span class="o">(</span><span class="n">countPath</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Path</span> <span class="n">path</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="na">getPath</span><span class="o">();</span>
</span><span class='line'>    <span class="n">String</span> <span class="n">name</span> <span class="o">=</span> <span class="n">path</span><span class="o">.</span><span class="na">getName</span><span class="o">();</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(!</span><span class="n">name</span><span class="o">.</span><span class="na">startsWith</span><span class="o">(</span><span class="s">&quot;count-&quot;</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">continue</span><span class="o">;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">startIds</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="n">name</span><span class="o">.</span><span class="na">substring</span><span class="o">(</span><span class="n">name</span><span class="o">.</span><span class="na">length</span><span class="o">()</span> <span class="o">-</span> <span class="mi">5</span><span class="o">),</span> <span class="n">startId</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">SequenceFile</span><span class="o">.</span><span class="na">Reader</span> <span class="n">reader</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SequenceFile</span><span class="o">.</span><span class="na">Reader</span><span class="o">(</span><span class="n">fs</span><span class="o">,</span> <span class="n">path</span><span class="o">,</span> <span class="n">getConf</span><span class="o">());</span>
</span><span class='line'>    <span class="n">NullWritable</span> <span class="n">key</span> <span class="o">=</span> <span class="n">NullWritable</span><span class="o">.</span><span class="na">get</span><span class="o">();</span>
</span><span class='line'>    <span class="n">LongWritable</span> <span class="n">value</span> <span class="o">=</span> <span class="k">new</span> <span class="n">LongWritable</span><span class="o">();</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(!</span><span class="n">reader</span><span class="o">.</span><span class="na">next</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">value</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">continue</span><span class="o">;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="n">startId</span> <span class="o">+=</span> <span class="n">value</span><span class="o">.</span><span class="na">get</span><span class="o">();</span>
</span><span class='line'>    <span class="n">reader</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Serialize the map and pass it to Configuration.</span>
</span><span class='line'><span class="n">job</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">().</span><span class="na">set</span><span class="o">(</span><span class="s">&quot;startIds&quot;</span><span class="o">,</span> <span class="n">Base64</span><span class="o">.</span><span class="na">encodeBase64String</span><span class="o">(</span>
</span><span class='line'>        <span class="n">SerializationUtils</span><span class="o">.</span><span class="na">serialize</span><span class="o">((</span><span class="n">Serializable</span><span class="o">)</span> <span class="n">startIds</span><span class="o">)));</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Recieve it in Mapper#setup</span>
</span><span class='line'><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">JobMapperB</span> <span class="kd">extends</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">NullWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">private</span> <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;</span> <span class="n">startIds</span><span class="o">;</span>
</span><span class='line'>    <span class="kd">private</span> <span class="kt">long</span> <span class="n">startId</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@SuppressWarnings</span><span class="o">(</span><span class="s">&quot;unchecked&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">setup</span><span class="o">(</span><span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class='line'>            <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>        <span class="kd">super</span><span class="o">.</span><span class="na">setup</span><span class="o">(</span><span class="n">context</span><span class="o">);</span>
</span><span class='line'>        <span class="n">startIds</span> <span class="o">=</span> <span class="o">(</span><span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;)</span> <span class="n">SerializationUtils</span><span class="o">.</span><span class="na">deserialize</span><span class="o">(</span>
</span><span class='line'>                <span class="n">Base64</span><span class="o">.</span><span class="na">decodeBase64</span><span class="o">(</span><span class="n">context</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="s">&quot;startIds&quot;</span><span class="o">)));</span>
</span><span class='line'>        <span class="n">String</span> <span class="n">name</span> <span class="o">=</span> <span class="o">((</span><span class="n">FileSplit</span><span class="o">)</span> <span class="n">context</span><span class="o">.</span><span class="na">getInputSplit</span><span class="o">()).</span><span class="na">getPath</span><span class="o">().</span><span class="na">getName</span><span class="o">();</span>
</span><span class='line'>        <span class="n">startId</span> <span class="o">=</span> <span class="n">startIds</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">name</span><span class="o">.</span><span class="na">substring</span><span class="o">(</span><span class="n">name</span><span class="o">.</span><span class="na">length</span><span class="o">()</span> <span class="o">-</span> <span class="mi">5</span><span class="o">));</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">map</span><span class="o">(</span><span class="n">NullWritable</span> <span class="n">key</span><span class="o">,</span> <span class="n">Text</span> <span class="n">value</span><span class="o">,</span> <span class="n">Context</span> <span class="n">context</span><span class="o">)</span>
</span><span class='line'>            <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="k">new</span> <span class="n">LongWritable</span><span class="o">(</span><span class="n">startId</span><span class="o">++),</span> <span class="n">value</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Set the Input Non-splitable</h3>

<p>When the file is bigger than a block or so (depending on some configuration entries), Hadoop will split it, which is not good for us. So let&rsquo;s define a new InputFormat class to disable the splitting behaviour:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">NonSplitableSequence</span> <span class="kd">extends</span> <span class="n">SequenceFileInputFormat</span><span class="o">&lt;</span><span class="n">NullWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Override</span>
</span><span class='line'>    <span class="kd">protected</span> <span class="kt">boolean</span> <span class="nf">isSplitable</span><span class="o">(</span><span class="n">JobContext</span> <span class="n">context</span><span class="o">,</span> <span class="n">Path</span> <span class="n">filename</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">return</span> <span class="kc">false</span><span class="o">;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// use it</span>
</span><span class='line'><span class="n">job</span><span class="o">.</span><span class="na">setInputFormatClass</span><span class="o">(</span><span class="n">NonSplitableSequence</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>And that&rsquo;s it, we are able to generate a unique, auto-increment id for a sorted collection, with Hadoop&rsquo;s parallel computing capability. The process is rather complicated, which requires several techniques about Hadoop. It&rsquo;s worthwhile to dig.</p>

<p>A workable example can be found in my <a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/AutoIncrementId2Job.java">Github repository</a>. If you have some more straight-forward approach, please do let me know.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hive并发情况下报DELETEME表不存在的异常]]></title>
    <link href="http://shzhangji.com/blog/2013/09/06/hive-deleteme-error/"/>
    <updated>2013-09-06T11:20:00+08:00</updated>
    <id>http://shzhangji.com/blog/2013/09/06/hive-deleteme-error</id>
    <content type="html"><![CDATA[<p>在每天运行的Hive脚本中，偶尔会抛出以下错误：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2013-09-03 01:39:00,973 ERROR parse.SemanticAnalyzer (SemanticAnalyzer.java:getMetaData(1128)) - org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table dw_xxx_xxx
</span><span class='line'>        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:896)
</span><span class='line'>        ...
</span><span class='line'>Caused by: javax.jdo.JDODataStoreException: Exception thrown obtaining schema column information from datastore
</span><span class='line'>NestedThrowables:
</span><span class='line'>com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'hive.DELETEME1378143540925' doesn't exist
</span><span class='line'>        at org.datanucleus.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:313)
</span><span class='line'>        ...</span></code></pre></td></tr></table></div></figure>


<p>查阅了网上的资料，是DataNucleus的问题。</p>

<p>背景1：我们知道MySQL中的库表信息是存放在information_schema库中的，Hive也有类似的机制，它会将库表信息存放在一个第三方的RDBMS中，目前我们线上配置的是本机MySQL，即：</p>

<p>$ mysql -uhive -ppassword hive</p>

<p><img src="http://shzhangji.com/images/hive-deleteme-error/1.png" alt="1.png" /></p>

<!--more-->


<p>背景2：Hive使用的是DataNuclues ORM库来操作数据库的，而基本上所有的ORM框架（对象关系映射）都会提供自动建表的功能，即开发者只需编写Java对象，ORM会自动生成DDL。DataNuclues也有这一功能，而且它在初始化时会通过生成临时表的方式来获取数据库的Catalog和Schema，也就是 DELETEME表：</p>

<p><img src="http://shzhangji.com/images/hive-deleteme-error/2.png" alt="2.png" /></p>

<p>这样就有一个问题：在并发量大的情况下，DELETEME表名中的毫秒数可能相同，那在pt.drop(conn)的时候就会产生找不到表的报错。</p>

<p>解决办法已经可以在代码中看到了：将datanucleus.fixedDataStore选项置为true，即告知DataNuclues该数据库的表结构是既定的，不允许执行DDL操作。</p>

<p>这样配置会有什么问题？让我们回忆一下Hive的安装步骤：</p>

<ol>
<li>解压hive-xyz.tar.gz；</li>
<li>在conf/hive-site.xml中配置Hadoop以及用于存放库表信息的第三方数据库；</li>
<li>执行bin/hive -e &ldquo;&hellip;&#8221;即可使用。DataNucleus会按需创建上述的DBS等表。</li>
</ol>


<p>这对新手来说很有用，因为不需要手动去执行建表语句，但对生产环境来说，普通帐号是没有DDL权限的，我们公司建表也都是提DB-RT给DBA操作。同理，线上Hive数据库也应该采用手工创建的方式，导入scripts/metastore/upgrade/mysql/hive-schema-0.9.0.mysql.sql文件即可。这样一来，就可以放心地配置datanucleus.fixedDataStore以及 datanecleus.autoCreateSchema两个选项了。</p>

<p>这里我们也明确了一个问题：设置datanucleus.fixedDataStore=true不会影响Hive建库建表，因为Hive中的库表只是DBS、TBLS表中的一条记录而已。</p>

<p>建议的操作：</p>

<ol>
<li>在线上导入hive-schema-0.9.0.mysql.sql，将尚未创建的表创建好（比如我们没有用过Hive的权限管理，所以DataNucleus没有自动创建DB_PRIVS表）；</li>
<li>在hive-site.xml中配置 datanucleus.fixedDataStore=true；datanecleus.autoCreateSchema=false。</li>
</ol>


<p>这样就可以彻底解决这个异常了。</p>

<p>为什么HWI没有遇到类似问题？因为它是常驻内存的，DELETEME表只会在启动的时候创建，后续的查询不会创建。而我们这里每次调用hive命令行都会去创建，所以才有这样的问题。</p>

<p>参考链接：</p>

<ul>
<li><a href="http://www.cnblogs.com/ggjucheng/archive/2012/07/25/2608633.html">http://www.cnblogs.com/ggjucheng/archive/2012/07/25/2608633.html</a></li>
<li><a href="https://github.com/dianping/cosmos-hive/issues/10">https://github.com/dianping/cosmos-hive/issues/10</a></li>
<li><a href="https://issues.apache.org/jira/browse/HIVE-1841">https://issues.apache.org/jira/browse/HIVE-1841</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ansible FAQ]]></title>
    <link href="http://shzhangji.com/blog/2013/06/11/ansible-faq/"/>
    <updated>2013-06-11T21:18:00+08:00</updated>
    <id>http://shzhangji.com/blog/2013/06/11/ansible-faq</id>
    <content type="html"><![CDATA[<p>本文是从原Ansible官网的FAQ页面翻译而来，网站改版后该页面已无法访问，但可以从<a href="https://github.com/ansible/ansible.github.com/blob/4a2bf7f60a020f0d0a7b042056fc3dd8716588f2/faq.html">Github历史提交</a>中获得。翻译这篇原始FAQ文档是因为它陈述了Ansible这款工具诞生的原因，设计思路和特性，以及与Puppet、Fabric等同类软件的比较，可以让我们对Ansible有一个整体的了解，所以值得使用者一读。</p>

<h2>目录</h2>

<ul>
<li>为什么命名为“Ansible”？</li>
<li>Ansible受到了谁的启发？</li>
<li>与同类软件比较

<ul>
<li>Func？</li>
<li>Puppet？</li>
<li>Chef？</li>
<li>Capistrano/Fabric？</li>
</ul>
</li>
<li>其它问题

<ul>
<li>Ansible的安全性如何？</li>
<li>Ansible如何扩展？</li>
<li>是否支持SSH以外的协议？</li>
<li>Ansible的适用场景有哪些？</li>
</ul>
</li>
</ul>


<h2>为什么命名为“Ansible”？</h2>

<p>我最喜爱的书籍之一是奥森·斯科特·卡特的《安德的游戏》。在这本书中，“Ansible”是一种能够跨越时空的即时通讯工具。强烈推荐这本书！</p>

<!-- more -->


<h2>Ansible受到了谁的启发？</h2>

<p>我在Red Hat任职期间主要开发Cobbler，很快我和几个同事就发现在部署工具（Cobbler）和配置管理工具（cfengine、Puppet等）之间有一个空缺，即如何更高效地执行临时性的任务。虽然当时有一些并行调用SSH脚本的方案，但并没有形成统一的API。所以我们（Adrian Likins、Seth Vidal、我）就开发了一个SSH分布式脚本框架——Func。</p>

<p>我一直想在Func的基础上开发一个配置管理工具，但因为忙于Cobbler和其他项目的开发，一直没有动手。在此期间，John Eckersberg开发了名为Taboot的自动化部署工具，它基于Func，采用YAML描述，和目前Ansible中的Playbooks很像。</p>

<p>近期我在一家新公司尝试引入Func，但遇到一些SSL和DNS方面的问题，所以想要开发一个更为简单的工具，吸收Func中优秀的理念，并与我在Puppet Labs的工作经验相结合。我希望这一工具能够易于学习，且不需要进行任何安装步骤。使用它不需要引入一整套新的理论，像Puppet和Chef那样，从而降低被某些运维团队排挤的可能。</p>

<p>我也曾参与过一些大型网站的应用部署，发觉现有的配置管理工具都太过复杂了，超过了这些公司的需求。程序发布的过程很繁复，需要一个简单的工具来帮助开发和运维人员。我不想教授他们Puppet或Chef，而且他们也不愿学习这些工具。</p>

<p>于是我便思考，应用程序的部署就应该那么复杂吗？答案是否定的。</p>

<p>我是否能开发一款工具，让运维人员能够在15分钟内学会使用，并用自己熟悉的语言来扩展它？这就是Ansible的由来。运维人员对自己的服务器设施最清楚，Ansible深知这一点，并将同类工具中最核心的功能提取出来，供我们使用。</p>

<p>Ansible不仅易于学习和扩展，它更是集配置管理、应用部署、临时任务等功能于一身。它非常强大，甚至前所未有。</p>

<p>我很想知道你对Ansible的看法，到邮件列表里发表一下意见吧。</p>

<h2>与同类软件比较</h2>

<h3>Func？</h3>

<p>Ansible默认使用SSH，而非SSL和守护进程，无需在远程服务器上安装任何软件。你可以使用任何语言编写插件，只要它能够返回JSON格式即可。Ansible的API深受Func的影响，但它和Func相较提供了配置管理和多节点统一化部署（Playbooks）等功能。</p>

<h3>Puppet？</h3>

<p>首先我要强调的是，如果没有Puppet，就不会有Ansible。Puppet从cfengine中吸收了配置管理的概念，并更合理地加以实现。但是，我依旧认为它可以再简单一些。</p>

<p>Ansible的playbook是一套完整的配置管理系统。和Puppet不同，playbook在编写时就隐含了执行顺序（和Chef类似），但同时也提供了事件机制（和Puppet类似），可以说是结合了两者的优点。</p>

<p>Ansible没有中心节点的概念，从而避免了惊群效应。它一开始就是为多节点部署设计的，这点Puppet很难做到，因为它是一种“拉取”的架构。Ansible以“推送”为基础，从而能够定义执行顺序，同时只操作一部分服务器，无需关注它们的依赖关系。又因为Ansible可以用任何语言进行扩展，因此并不是只有专业的程序员才能为其开发插件。</p>

<p>Ansible中资源的概念深受Puppet的启发，甚至“state”这一关键字直接来自Puppet的“ensure”一词。和Puppet不同的是，Ansbile可以用任何语言进行扩展，甚至是Bash，只需返回JSON格式的输出即可。你不需要懂得Ruby。</p>

<p>和Puppet不同，Ansible若在配置某台服务器时发生错误，它会立即终止这台服务器的配置过程。它提倡的是“提前崩溃”，修正错误，而非最大化应用。这一点在我们需要配置包含依赖关系的服务器架构时尤为重要。</p>

<p>Ansible的学习曲线非常平滑，你不需要掌握编程技能，更不需要学习新的语言。Ansible内置的功能应该能够满足超过80%的用户需求，而且它不会遇到扩展性方面的瓶颈（因为没有中心节点）。</p>

<p>如果系统中安装了factor，Ansible同样支持从中获取系统信息。Ansible使用jinja2作为模板语言，类似于Puppet使用erb文件作为模板。Ansible可以使用自己的信息收集工具，因此factor并不是必需的。</p>

<h3>Chef？</h3>

<p>Ansible与Chef的区别和Puppet类似。Chef的配置非常困难，而且需要你掌握Ruby语言。也因为如此，Chef在Rails使用者中很流行。</p>

<p>Ansible是按照编写顺序来执行任务的，而不是显示地定义依赖关系，这点和Chef相似。但Ansible更进一步，它支持事件触发，比如修改了Apache的配置文件，Apache就会被重启。</p>

<p>和Chef不同的是，Ansible的playbook不是一门编程语言，而是一种可以存储的数据结构。这就意味着你的运维工作不是一项开发型的任务，测试起来也相对简单。</p>

<p>无论你有怎样的语言背景，都可以使用Ansible。Chef和Puppet有超过六万行的代码，而Ansible则是一段小巧简单的程序。我相信这一点会使得Ansible更加健壮和可靠，并汇聚一批活跃的社区贡献者——因为任何人都可以提交补丁或是模块。</p>

<p>Ansible同样支持从ohai中获取系统信息，当然这同样不是必需的。</p>

<h3>Capistrano/Fabric？</h3>

<p>这些工具并不适合用作服务器配置工具，它们主要用于应用程序的部署。</p>

<p>而Ansible则提供了完整的配置管理，以及在扩展性方面提供了一些高级特性。</p>

<p>Ansible playbook的语法简介只占一个HTML页面，有着非常平缓的学习曲线。由于Ansible使用了“推送”的设计，因此对系统管理员（不仅仅是开发者）同样适用，并能用它处理各种临时性的任务。</p>

<h2>其它问题</h2>

<h3>Ansible的安全性如何？</h3>

<p>Ansible没有守护进程，主要使用OpenSSH进行通信，这是一款已被反复检验并广泛使用的软件。其它工具都会在远程服务器上以root用户运行守护进程，因此相较于这些工具，Ansible会更为安全，且无需担心网络方面的问题。</p>

<p>如果你的中心节点遭到入侵（或是被恶意员工登录），只要你是使用SSH-agent、或是经过加密的密码，那你的密钥仍然是被锁定的，别人无法操控你的节点。而对于Chef、Puppet等工具来说，一旦配置文件遭到篡改，那危及的将是整个网络。</p>

<p>此外，由于Ansible没有守护进程，可以节省下一部分内存和计算资源，这对需要最大化性能的用户来说也是一个优点。</p>

<h3>Ansible如何扩展？</h3>

<p>无论是在单次执行模式还是playbook模式下，Ansible都可以并行执行任务，这要感谢Python提供的多进程处理模块。</p>

<p>你可以自行决定要一次性配置5台还是50台服务器，这取决于服务器的计算能力，以及你想要多快完成任务。</p>

<p>由于没有守护进程，所以平时不会占用任何资源，而且你不用担心一次性有太多节点一起从控制节点上获取信息。</p>

<p>对于SSH，Ansible默认使用paramiko库，当然也能使用原始的openssh。Ansible可以利用SSH的ControlMaster特性来重用网络连接。</p>

<p>当要维护上万个节点时，单个Ansible playbook可能不太合理，这时你就能使用Ansible的“拉取”模式。这种模式下需要配合git和cron，可以扩展到任意多台服务器。“拉取”模式可以使用本地连接，或是SSH。关于这个模式的详细说明可以在帮助文档的“Advanced Playbooks”一节查阅。即使在“拉取”模式下，你同样能够享受到Ansible的种种便利。</p>

<p>如果你想进一步探讨扩展性，可以加入到邮件列表中。</p>

<h3>是否支持SSH以外的协议？</h3>

<p>目前Ansible支持SSH和本地连接，但它的接口实际上是非常易于扩展的，因此你可以编写补丁来使Ansible运行于消息系统或XMPP协议之上。</p>

<p>如果你有任何建议，可以加入到邮件列表中一起探讨。Ansible中对于连接的管理都已单独抽象出来，有很强的可扩性。</p>

<h3>Ansible的适用场景有哪些？</h3>

<p>最适场景？使用playbook进行多节点云主机部署；从一个初始的操作系统开始部署应用，或是配置一个现有的系统。</p>

<p>Ansible同样适用于执行临时性的任务，能够用于各类 Unix-like 系统，因为它使用的就是系统本身自带的工具，无需安装额外软件。</p>

<p>你还可以用Ansible来编写各类脚本，用于收集信息、执行各种任务，对QA、运维等团队均适用。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Hadoop YARN - 项目背景与简介]]></title>
    <link href="http://shzhangji.com/blog/2013/05/25/apache-hadoop-yarn-background-and-an-overview/"/>
    <updated>2013-05-25T10:57:00+08:00</updated>
    <id>http://shzhangji.com/blog/2013/05/25/apache-hadoop-yarn-background-and-an-overview</id>
    <content type="html"><![CDATA[<p>原文：<a href="http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/">http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/</a></p>

<p>日前，Apache Hadoop YARN已被提升为Apache软件基金会的子项目，这是一个值得庆祝的里程碑。这里我们也第一时间为各位献上Apache Hadoop YARN项目的系列介绍文章。YARN是一个普适的、分布式的应用管理框架，运行于Hadoop集群之上，用以替代传统的Apache Hadoop MapReduce框架。</p>

<h2>MapReduce 模式</h2>

<p>本质上来说，MapReduce模型包含两个部分：一是Map过程，将数据拆分成若干份，分别处理，彼此之间没有依赖关系；二是Reduce过程，将中间结果汇总计算成最终结果。这是一种简单而又条件苛刻的模型，但也促使它成为高效和极易扩展的并行计算方式。</p>

<p>Apache Hadoop MapReduce是当下最流行的开源MapReduce模型。</p>

<p>特别地，当MapReduce配合分布式文件系统，类似Apache Hadoop HDFS，就能在大集群上提供高吞吐量的计算，这一经济效应是Hadoop得以流行的重要原因。</p>

<p>这一模式成功的原因之一是，它使用的是“移动计算能力至数据节点”而非通过网络“移动数据至计算节点”的方式。具体来说，一个MapReduce任务会被调度到输入数据所在的HDFS节点执行，这会极大地减少I/O支出，因为大部分I/O会发生在本地磁盘或是同一机架中——这是核心优势。</p>

<!-- more -->


<h3>回顾2011年的Apache Hadoop MapReduce</h3>

<p>Apache Hadoop MapReduce是<a href="http://www.apache.org/">Apache基金会</a>下的开源项目，实现了如上所述的MapReduce编程模式。作为一个在该项目中全职开发了六年的工作者，我通常会将它细分为以下几个部分：</p>

<ul>
<li>提供给最终用户使用的 <strong>MapReduce API</strong> ，用来编写MapReduce应用程序。</li>
<li><strong>MapReduce框架</strong> ，用来实现运行时的各个阶段，即map、sort/shuffle/merge、reduce。</li>
<li><strong>MapReduce系统</strong> ，一个完整的后端系统，用来运行用户的MapReduce应用程序，管理集群资源，调度上千个并发脚本。</li>
</ul>


<p>这样的划分可以带来非常明显的优势，即最终用户只需关心MapReduce API，而让框架和后端系统去处理资源管理、容错、调度等细节。</p>

<p>目前，Apache Hadoop MapReduce系统由一个JobTracker和多个TaskTracker组成，也分别称他们为master和slave节点。</p>

<p><img src="http://hortonworks.com/wp-content/uploads/2012/08/MRArch.png" alt="MRArch.png" /></p>

<p>JobTracker负责的工作包括资源管理（即管理工作节点TaskTracker），跟踪资源消耗和可用情况，以及每个脚本的生命周期（脚本调度，进度跟踪，容错等）。</p>

<p>TaskTracker的职责比较简单：根据JobTracker的指令来启动和关闭工作进程，并定时向JobTracker汇报处理进度。</p>

<p>其实很早我们就意识到Hadoop的MapReduce框架需要被拆解和调整，特别是JobTracker，我们需要提升它的可扩展性，提高对集群的利用率，让用户能够方便地进行升级（即用户需要的敏捷性），并能支持MapReduce以外的脚本类型。</p>

<p>长久以来，我们都在做修复和更新，如近期加入的JobTracker高可用和HDFS故障恢复（这两个特性都已包含在<a href="http://hortonworks.com/download/">Hortonworks Data Platform v1</a>中）。但我们渐渐发现，这些特性会增加维护成本，而且并不能解决一些核心问题，如支持非MapReduce脚本，以及敏捷性。</p>

<h3>为什么要支持非MapReduce类型的脚本？</h3>

<p>MapReduce对大部分应用程序来说已经足够，但仍有一些场景并不适用，如图形计算（<a href="http://googleresearch.blogspot.com/2009/06/large-scale-graph-computing-at-google.html">Google Pregel</a> / <a href="http://giraph.apache.org/">Apache Giraph</a>）、交互式建模（<a href="http://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>）。当所有的企业数据都已存放在Hadoop HDFS中时，支持多种处理模型就变得额外重要。</p>

<p>此外，MapReduce本质上是以批量处理为核心的，对于日益增长的实时和近实时处理的客户需求，如流式计算以及CEPFresil等，就无能为力了。</p>

<p>如果Hadoop能够支持这一特性，企业会从对Hadoop的投资中得到更多回报，因为他们可以减少数据迁移所需要的管理和维护成本。</p>

<h3>为何要提升可扩展性？</h3>

<p>根据摩尔定律，同样的价格所能购买到的计算能力一直在大幅上升。让我们看看以下两组数字：</p>

<ul>
<li>2009年：8核CPU，16GB内存，4x1TB硬盘；</li>
<li>2012年：16核以上的CPU，48至96GB内存，12x2TB或12x3TB的硬盘。</li>
</ul>


<p>同样价格的服务器，其各方面的计算能力要比两到三年以前提升了两倍。Hadoop的MapReduce在2009年便能支持约5000台节点，所以随着机器性能的提升，对其高可扩的要求也与日俱增。</p>

<h3>集群资源利用率不高的典型症候是？</h3>

<p>在现有的系统中，集群由节点组成，节点上有map槽位和reduce槽位，两者不能互相替代。这样一来，很有可能map槽位已经耗尽，而reduce还是空闲的，反之亦然。修复这一问题对于提升集群资源利用率来说是必不可少的。</p>

<h3>敏捷性为何重要？</h3>

<p>在现实应用中，Hadoop通常会部署在共享的、多租户的系统上。所以，对Hadoop进行升级时会影响很大一部分甚至是所有的应用。基于这一点，用户会对升级持保守态度，因为不想因此引发一系列的问题。所以，一个支持多版本Hadoop的架构就变得非常重要。</p>

<h2>Apache Hadoop YARN 诞生</h2>

<p>YARN的核心思想是将JobTracker的两个职能，即资源管理和脚本调度/监控，分解为两个独立的组件：全局ResourceManager以及按应用拆分的ApplicationMaster（AM）。</p>

<p>主节点的ResourceManager以及其它节电的NodeManager（NM），形成了一个新的更为通用的分布式应用管理模式。</p>

<p>ResourceManager负责应用程序的资源分配。ApplicationMaster会和ResourceManager进行协商，并与节点上的NodeManager协作，运行和监控每个工作进程。</p>

<p>ResourceManager的调度器是可定制的，能够根据计算能力、队列大小进行资源调配。调度器不包含任何对工作进程的监控和跟踪，不会去重启失败的脚本。调度器会根据应用程序申请的资源进行分配，它是建立在一个资源容器抽象层（Resource Container）之上的，其中包括了内存、CPU、硬盘、网络等要素信息。</p>

<p>NodeManager运行在每个节点之上，负责运行应用程序的工作进程，监控它们的资源占用情况，并向ResourceManager汇报。</p>

<p>每个应用都会有一个专属的ApplicationMaster，它会负责和调度器协商资源分配，跟踪工作进程的状态和进度。ApplicationMaster本身也是以一个工作进程来运行的。</p>

<p>以下是YARN的架构图：</p>

<p><img src="http://hortonworks.com/wp-content/uploads/2012/08/YARNArch.png" alt="YARNArch.png" /></p>

<p>值得一提的是，我们在为YARN开发MapReduce API时没有做任何较大的改动，所以现有的程序可以很方便地进行迁移。关于这点我们会在以后的文章中详述。</p>

<p>下一节我们会深入了解YARN的架构，阐述它所带来的各种优点，如高可扩、支持多类型脚本（MapReduce、MPI等），以及它是如何提升集群资源利用率的。</p>
]]></content>
  </entry>
  
</feed>
