<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ji ZHANG&#39;s Blog</title>
  
  <subtitle>If I rest, I rust.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://shzhangji.com/"/>
  <updated>2022-06-12T05:01:34.283Z</updated>
  <id>http://shzhangji.com/</id>
  
  <author>
    <name>Ji ZHANG</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Use Bootstrap V5 in Vue 3 Project</title>
    <link href="http://shzhangji.com/blog/2022/06/11/use-bootstrap-v5-in-vue3-project/"/>
    <id>http://shzhangji.com/blog/2022/06/11/use-bootstrap-v5-in-vue3-project/</id>
    <published>2022-06-11T12:06:26.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p>Bootstrap V5 and Vue 3.x have been released for a while, but the widely used BootstrapVue library is still based on Bootstrap V4 and Vue 2.x. A <a href="https://github.com/bootstrap-vue/bootstrap-vue/issues/5196" target="_blank" rel="noopener">new version</a> of BootstrapVue is under development, and there is an alternative project <a href="https://cdmoro.github.io/bootstrap-vue-3/" target="_blank" rel="noopener">BootstrapVue 3</a> in alpha version. However, since Bootstrap is mainly a CSS framework, and it has dropped jQuery dependency in V5, it is not that difficult to integrate into a Vue 3.x project on your own. In this article, we will go through the steps of creating such a project.</p><h2 id="Create-Vite-project"><a href="#Create-Vite-project" class="headerlink" title="Create Vite project"></a>Create Vite project</h2><p>The recommended way of using Vue 3.x is with Vite. Install <code>yarn</code> and create from the <code>vue-ts</code> template:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yarn create vite bootstrap-vue3 --template vue-ts</span><br><span class="line">cd bootstrap-vue3</span><br><span class="line">yarn install</span><br><span class="line">yarn dev</span><br></pre></td></tr></table></figure><h2 id="Add-Bootstrap-dependencies"><a href="#Add-Bootstrap-dependencies" class="headerlink" title="Add Bootstrap dependencies"></a>Add Bootstrap dependencies</h2><p>Bootstrap is published on npm, and it has an extra dependency Popper, so let’s install them both:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn add bootstrap @popperjs/core</span><br></pre></td></tr></table></figure><p>You may also need the type definitions:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn add -D @types/bootstrap</span><br></pre></td></tr></table></figure><h2 id="Use-Bootstrap-CSS"><a href="#Use-Bootstrap-CSS" class="headerlink" title="Use Bootstrap CSS"></a>Use Bootstrap CSS</h2><p>Just add a line to your <code>App.vue</code> file and you are free to use Bootstrap CSS:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;script setup lang=&quot;ts&quot;&gt;</span><br><span class="line">import &apos;bootstrap/dist/css/bootstrap.min.css&apos;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;template&gt;</span><br><span class="line">  &lt;button type=&quot;button&quot; class=&quot;btn btn-primary&quot;&gt;Primary&lt;/button&gt;</span><br><span class="line">&lt;/template&gt;</span><br></pre></td></tr></table></figure><p>You can also use Sass for further <a href="https://getbootstrap.com/docs/5.2/customize/sass/" target="_blank" rel="noopener">customization</a>.</p><a id="more"></a><h2 id="Use-JavaScript-plugins"><a href="#Use-JavaScript-plugins" class="headerlink" title="Use JavaScript plugins"></a>Use JavaScript plugins</h2><p>Bootstrap provides JS plugins to enable interactive components, such as Modal, Toast, etc. There are two ways of using these plugins: through <code>data</code> attributes, or create instances programatically. Let’s take <a href="https://getbootstrap.com/docs/5.2/components/modal/" target="_blank" rel="noopener">Modal</a> for an example.</p><h3 id="Through-data-attributes"><a href="#Through-data-attributes" class="headerlink" title="Through data attributes"></a>Through <code>data</code> attributes</h3><p>First, you need to import the Bootstrap JS. In the following example, we import the individual Modal plugin. You can also import the full Bootstrap JS using <code>import &#39;bootstrap&#39;</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;script setup lang=&quot;ts&quot;&gt;</span><br><span class="line">import &apos;bootstrap/js/dist/modal&apos;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;template&gt;</span><br><span class="line">  &lt;button type=&quot;button&quot; class=&quot;btn btn-primary&quot; data-bs-toggle=&quot;modal&quot; data-bs-target=&quot;#exampleModal&quot;&gt;</span><br><span class="line">    Launch demo modal 1</span><br><span class="line">  &lt;/button&gt;</span><br><span class="line"></span><br><span class="line">  &lt;div class=&quot;modal fade&quot; id=&quot;exampleModal&quot; tabindex=&quot;-1&quot; aria-labelledby=&quot;exampleModalLabel&quot; aria-hidden=&quot;true&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;modal-dialog&quot;&gt;</span><br><span class="line">      &lt;div class=&quot;modal-content&quot;&gt;</span><br><span class="line">        &lt;div class=&quot;modal-header&quot;&gt;</span><br><span class="line">          &lt;h5 class=&quot;modal-title&quot; id=&quot;exampleModalLabel&quot;&gt;Modal title&lt;/h5&gt;</span><br><span class="line">          &lt;button type=&quot;button&quot; class=&quot;btn-close&quot; data-bs-dismiss=&quot;modal&quot; aria-label=&quot;Close&quot;&gt;&lt;/button&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">        &lt;div class=&quot;modal-body&quot;&gt;</span><br><span class="line">          Woo-hoo, you&apos;re reading this text in a modal!</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">        &lt;div class=&quot;modal-footer&quot;&gt;</span><br><span class="line">          &lt;button type=&quot;button&quot; class=&quot;btn btn-secondary&quot; data-bs-dismiss=&quot;modal&quot;&gt;Close&lt;/button&gt;</span><br><span class="line">          &lt;button type=&quot;button&quot; class=&quot;btn btn-primary&quot;&gt;Save changes&lt;/button&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&lt;/template&gt;</span><br></pre></td></tr></table></figure><p>When the <em>Launch</em> button is clicked, <code>data-bs-toggle</code> tells Bootstrap to show or hide a modal dialog with the element ID indicated by <code>data-bs-target</code>. When the <em>Close</em> button is clicked, <code>data-bs-dismiss</code> indicates hiding the dialog that contains this button. <code>data</code> attribute is simple, but not flexible. In practice, we tend to use JS instance instead.</p><h3 id="Through-JS-instances"><a href="#Through-JS-instances" class="headerlink" title="Through JS instances"></a>Through JS instances</h3><p>From the Bootstrap document, we see the following instruction:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> myModalAlternative = <span class="keyword">new</span> bootstrap.Modal(<span class="string">'#myModal'</span>, options)</span><br></pre></td></tr></table></figure><p>It creates a <code>Modal</code> instance on a DOM element with the ID <code>myModal</code>, and then we can call the <code>show</code> or <code>hide</code> methods on it. In Vue, we need to replace the element ID with a <a href="https://vuejs.org/guide/essentials/template-refs.html" target="_blank" rel="noopener">Template Ref</a>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&lt;script setup lang=&quot;ts&quot;&gt;</span><br><span class="line">import &#123; ref, onMounted &#125; from &apos;vue&apos;</span><br><span class="line">import &#123; Modal &#125; from &apos;bootstrap&apos;</span><br><span class="line"></span><br><span class="line">const modalRef = ref&lt;HTMLElement | null&gt;(null)</span><br><span class="line">let modal: Modal</span><br><span class="line">onMounted(() =&gt; &#123;</span><br><span class="line">  if (modalRef.value) &#123;</span><br><span class="line">    modal = new Modal(modalRef.value)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">function launchDemoModal() &#123;</span><br><span class="line">  modal.show()</span><br><span class="line">&#125;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;template&gt;</span><br><span class="line">  &lt;button type=&quot;button&quot; class=&quot;btn btn-primary&quot; @click=&quot;launchDemoModal&quot;&gt;</span><br><span class="line">    Launch demo modal 2</span><br><span class="line">  &lt;/button&gt;</span><br><span class="line"></span><br><span class="line">  &lt;div class=&quot;modal fade&quot; tabindex=&quot;-1&quot; aria-hidden=&quot;true&quot; ref=&quot;modalRef&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;modal-dialog&quot;&gt;</span><br><span class="line">      &lt;div class=&quot;modal-content&quot;&gt;</span><br><span class="line">        &lt;div class=&quot;modal-header&quot;&gt;</span><br><span class="line">          &lt;h5 class=&quot;modal-title&quot;&gt;Modal title&lt;/h5&gt;</span><br><span class="line">          &lt;button type=&quot;button&quot; class=&quot;btn-close&quot; data-bs-dismiss=&quot;modal&quot; aria-label=&quot;Close&quot;&gt;&lt;/button&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">        &lt;div class=&quot;modal-body&quot;&gt;</span><br><span class="line">          Woo-hoo, you&apos;re reading this text in a modal!</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">        &lt;div class=&quot;modal-footer&quot;&gt;</span><br><span class="line">          &lt;button type=&quot;button&quot; class=&quot;btn btn-secondary&quot; data-bs-dismiss=&quot;modal&quot;&gt;Close&lt;/button&gt;</span><br><span class="line">          &lt;button type=&quot;button&quot; class=&quot;btn btn-primary&quot;&gt;Save changes&lt;/button&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&lt;/template&gt;</span><br></pre></td></tr></table></figure><p>The <code>modalRef</code> will be set by Vue when component is mounted, at that time we create the Modal instance with the passed-in DOM element. Note <code>data-bs-dimiss</code> still works in this example.</p><h3 id="Write-a-custom-component"><a href="#Write-a-custom-component" class="headerlink" title="Write a custom component"></a>Write a custom component</h3><p>If you need to use Modal in different places, it is better to wrap it in a component. Create a <code>components/Modal.vue</code> file and put the following code in it:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">&lt;script setup lang=&quot;ts&quot;&gt;</span><br><span class="line">import &#123; ref, onMounted, watch &#125; from &apos;vue&apos;</span><br><span class="line">import &#123; Modal &#125; from &apos;bootstrap&apos;</span><br><span class="line"></span><br><span class="line">const props = defineProps&lt;&#123;</span><br><span class="line">  modelValue: boolean</span><br><span class="line">  title: string</span><br><span class="line">&#125;&gt;()</span><br><span class="line"></span><br><span class="line">const emit = defineEmits&lt;&#123;</span><br><span class="line">  (e: &apos;update:modelValue&apos;, modelValue: boolean): void</span><br><span class="line">&#125;&gt;()</span><br><span class="line"></span><br><span class="line">const modalRef = ref&lt;HTMLElement | null&gt;(null)</span><br><span class="line">let modal: Modal</span><br><span class="line">onMounted(() =&gt; &#123;</span><br><span class="line">  if (modalRef.value) &#123;</span><br><span class="line">    modal = new Modal(modalRef.value)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">watch(() =&gt; props.modelValue, (modelValue) =&gt; &#123;</span><br><span class="line">  if (modelValue) &#123;</span><br><span class="line">    modal.show()</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    modal.hide()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">function close() &#123;</span><br><span class="line">  emit(&apos;update:modelValue&apos;, false)</span><br><span class="line">&#125;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;template&gt;</span><br><span class="line">  &lt;div class=&quot;modal fade&quot; tabindex=&quot;-1&quot; aria-hidden=&quot;true&quot; ref=&quot;modalRef&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;modal-dialog&quot;&gt;</span><br><span class="line">      &lt;div class=&quot;modal-content&quot;&gt;</span><br><span class="line">        &lt;div class=&quot;modal-header&quot;&gt;</span><br><span class="line">          &lt;h5 class=&quot;modal-title&quot;&gt;&#123;&#123; title &#125;&#125;&lt;/h5&gt;</span><br><span class="line">          &lt;button type=&quot;button&quot; class=&quot;btn-close&quot; aria-label=&quot;Close&quot; @click=&quot;close&quot;&gt;&lt;/button&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">        &lt;div class=&quot;modal-body&quot;&gt;</span><br><span class="line">          &lt;slot /&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">        &lt;div class=&quot;modal-footer&quot;&gt;</span><br><span class="line">          &lt;slot name=&quot;footer&quot; /&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&lt;/template&gt;</span><br></pre></td></tr></table></figure><p>We use <code>v-model</code> to control the visibility of the modal dialog. By watching the value of <code>modelValue</code> property, we call corresponding methods on the Modal instance. Also we have replaced the <code>data-bs-dismiss</code> with a function that changes the value of <code>modelValue</code>, because that should be the single source of truth of the modal state.</p><p>Use this component in a demo view:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&lt;script setup lang=&quot;ts&quot;&gt;</span><br><span class="line">import &#123; ref &#125; from &apos;vue&apos;</span><br><span class="line">import Modal from &apos;../components/Modal.vue&apos;</span><br><span class="line"></span><br><span class="line">const dialogVisible = ref(false)</span><br><span class="line"></span><br><span class="line">function launchDemoModal() &#123;</span><br><span class="line">  dialogVisible.value = true</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function closeModal() &#123;</span><br><span class="line">  dialogVisible.value = false</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function saveChanges() &#123;</span><br><span class="line">  closeModal()</span><br><span class="line">  alert(&apos;Changes saved.&apos;)</span><br><span class="line">&#125;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;template&gt;</span><br><span class="line">  &lt;button type=&quot;button&quot; class=&quot;btn btn-primary&quot; @click=&quot;launchDemoModal&quot;&gt;</span><br><span class="line">    Launch demo modal 3</span><br><span class="line">  &lt;/button&gt;</span><br><span class="line"></span><br><span class="line">  &lt;Modal v-model=&quot;dialogVisible&quot; title=&quot;Modal title&quot;&gt;</span><br><span class="line">    Woo-hoo, you&apos;re reading this text in a modal!</span><br><span class="line">    &lt;template #footer&gt;</span><br><span class="line">      &lt;button type=&quot;button&quot; class=&quot;btn btn-secondary&quot; @click=&quot;closeModal&quot;&gt;Close&lt;/button&gt;</span><br><span class="line">      &lt;button type=&quot;button&quot; class=&quot;btn btn-primary&quot; @click=&quot;saveChanges&quot;&gt;Save changes&lt;/button&gt;</span><br><span class="line">    &lt;/template&gt;</span><br><span class="line">  &lt;/Modal&gt;</span><br><span class="line">&lt;/template&gt;</span><br></pre></td></tr></table></figure><p>Check out the <a href="https://vuejs.org/guide/essentials/component-basics.html" target="_blank" rel="noopener">Vue document</a> to learn about component, slot, v-model, etc. Code examples can be found on <a href="https://github.com/jizhang/blog-demo/tree/master/bootstrap-vue3" target="_blank" rel="noopener">GitHub</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bootstrap V5 and Vue 3.x have been released for a while, but the widely used BootstrapVue library is still based on Bootstrap V4 and Vue 2.x. A &lt;a href=&quot;https://github.com/bootstrap-vue/bootstrap-vue/issues/5196&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;new version&lt;/a&gt; of BootstrapVue is under development, and there is an alternative project &lt;a href=&quot;https://cdmoro.github.io/bootstrap-vue-3/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BootstrapVue 3&lt;/a&gt; in alpha version. However, since Bootstrap is mainly a CSS framework, and it has dropped jQuery dependency in V5, it is not that difficult to integrate into a Vue 3.x project on your own. In this article, we will go through the steps of creating such a project.&lt;/p&gt;
&lt;h2 id=&quot;Create-Vite-project&quot;&gt;&lt;a href=&quot;#Create-Vite-project&quot; class=&quot;headerlink&quot; title=&quot;Create Vite project&quot;&gt;&lt;/a&gt;Create Vite project&lt;/h2&gt;&lt;p&gt;The recommended way of using Vue 3.x is with Vite. Install &lt;code&gt;yarn&lt;/code&gt; and create from the &lt;code&gt;vue-ts&lt;/code&gt; template:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yarn create vite bootstrap-vue3 --template vue-ts&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cd bootstrap-vue3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;yarn install&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;yarn dev&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;Add-Bootstrap-dependencies&quot;&gt;&lt;a href=&quot;#Add-Bootstrap-dependencies&quot; class=&quot;headerlink&quot; title=&quot;Add Bootstrap dependencies&quot;&gt;&lt;/a&gt;Add Bootstrap dependencies&lt;/h2&gt;&lt;p&gt;Bootstrap is published on npm, and it has an extra dependency Popper, so let’s install them both:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yarn add bootstrap @popperjs/core&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;You may also need the type definitions:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yarn add -D @types/bootstrap&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;Use-Bootstrap-CSS&quot;&gt;&lt;a href=&quot;#Use-Bootstrap-CSS&quot; class=&quot;headerlink&quot; title=&quot;Use Bootstrap CSS&quot;&gt;&lt;/a&gt;Use Bootstrap CSS&lt;/h2&gt;&lt;p&gt;Just add a line to your &lt;code&gt;App.vue&lt;/code&gt; file and you are free to use Bootstrap CSS:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;lt;script setup lang=&amp;quot;ts&amp;quot;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;import &amp;apos;bootstrap/dist/css/bootstrap.min.css&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;lt;template&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;lt;button type=&amp;quot;button&amp;quot; class=&amp;quot;btn btn-primary&amp;quot;&amp;gt;Primary&amp;lt;/button&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;lt;/template&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;You can also use Sass for further &lt;a href=&quot;https://getbootstrap.com/docs/5.2/customize/sass/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;customization&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="vue" scheme="http://shzhangji.com/tags/vue/"/>
    
      <category term="bootstrap" scheme="http://shzhangji.com/tags/bootstrap/"/>
    
      <category term="vite" scheme="http://shzhangji.com/tags/vite/"/>
    
  </entry>
  
  <entry>
    <title>Migrate from hexo-deployer-git to GitHub Actions</title>
    <link href="http://shzhangji.com/blog/2022/06/03/migrate-from-hexo-deployer-git-to-github-actions/"/>
    <id>http://shzhangji.com/blog/2022/06/03/migrate-from-hexo-deployer-git-to-github-actions/</id>
    <published>2022-06-03T06:34:18.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>Create <code>.github/workflows/pages.yml</code> in your <code>master</code> branch:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">Update</span> <span class="string">gh-pages</span></span><br><span class="line"></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line"><span class="attr">  push:</span></span><br><span class="line"><span class="attr">    branches:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">master</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line"><span class="attr">  deploy:</span></span><br><span class="line"><span class="attr">    runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line"><span class="attr">    steps:</span></span><br><span class="line"><span class="attr">      - uses:</span> <span class="string">actions/checkout@v3</span></span><br><span class="line"><span class="attr">      - uses:</span> <span class="string">actions/setup-node@v3</span></span><br><span class="line"><span class="attr">        with:</span></span><br><span class="line"><span class="attr">          node-version:</span> <span class="string">"12.22"</span></span><br><span class="line"><span class="attr">          cache:</span> <span class="string">yarn</span></span><br><span class="line"><span class="attr">      - run:</span> <span class="string">yarn</span> <span class="string">install</span></span><br><span class="line"><span class="attr">      - run:</span> <span class="string">yarn</span> <span class="string">build</span></span><br><span class="line"><span class="attr">      - uses:</span> <span class="string">peaceiris/actions-gh-pages@v3</span></span><br><span class="line"><span class="attr">        with:</span></span><br><span class="line"><span class="attr">          github_token:</span> <span class="string">$&#123;&#123;</span> <span class="string">secrets.GITHUB_TOKEN</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">          publish_dir:</span> <span class="string">./public</span></span><br></pre></td></tr></table></figure><p>Go to GitHub repo’s Settings &gt; Pages, change source branch to <code>gh-pages</code>.</p><h2 id="How-it-works"><a href="#How-it-works" class="headerlink" title="How it works"></a>How it works</h2><a id="more"></a><p>Previously with <a href="https://github.com/hexojs/hexo-deployer-git" target="_blank" rel="noopener"><code>hexo-deployer-git</code></a> plugin, we generate the static site locally and push those files to github’s master branch, which will be deployed to GitHub Pages server. The config in <code>_config.yml</code> is as simple as:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">git</span></span><br><span class="line"><span class="attr">  repo:</span> <span class="string">git@github.com:jizhang/jizhang.github.com</span></span><br></pre></td></tr></table></figure><p>Now with GitHub Actions, a CI/CD platform available to public repositories, the build process can be triggered on remote servers whenever master branch is updated. Hexo provides an <a href="https://hexo.io/docs/github-pages" target="_blank" rel="noopener">official document</a> on how to setup the workflow, but it turns out the configuration can be a little bit simpler, thanks to the new versions of <code>actions</code> (we’ll cover it later).</p><p>A workflow is a sequence of jobs to build, test, and deploy our code. Here we only need one job named <code>deploy</code> to generate the static files and push to a branch.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">jobs:</span></span><br><span class="line"><span class="attr">  deploy:</span></span><br><span class="line"><span class="attr">    runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line"><span class="attr">    steps:</span> <span class="string">[]</span></span><br></pre></td></tr></table></figure><p>A job consists of steps that either run a shell command or invoke an <code>action</code> to execute a common task. For instance, we have defined two steps to install node dependencies and build the static site:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">steps:</span></span><br><span class="line"><span class="attr">  - run:</span> <span class="string">yarn</span> <span class="string">install</span></span><br><span class="line"><span class="attr">  - run:</span> <span class="string">yarn</span> <span class="string">build</span></span><br></pre></td></tr></table></figure><p>Make sure you have the following scripts in <code>package.json</code>. Newer version of hexo already has them.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"hexo-site"</span>,</span><br><span class="line">  <span class="attr">"private"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="attr">"scripts"</span>: &#123;</span><br><span class="line">    <span class="attr">"start"</span>: <span class="string">"hexo server --draft"</span>,</span><br><span class="line">    <span class="attr">"build"</span>: <span class="string">"hexo generate"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>But where does the node environment come from? First, the job <code>runs-on</code> a specified platform, which is <code>ubuntu-latest</code> here, and <code>uses</code> the <a href="https://github.com/actions/setup-node" target="_blank" rel="noopener"><code>setup-node</code></a> action to prepare the node environment, <code>yarn</code> command, as well as the cache facility.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">steps:</span></span><br><span class="line"><span class="attr">  - uses:</span> <span class="string">actions/setup-node@v3</span></span><br><span class="line"><span class="attr">    with:</span></span><br><span class="line"><span class="attr">      node-version:</span> <span class="string">"12.22"</span></span><br><span class="line"><span class="attr">      cache:</span> <span class="string">yarn</span></span><br></pre></td></tr></table></figure><p>Under the hood, it searches for a local cache of the specific node version, where github provides last three LTS versions, or it falls back to downloading from the official site. The <code>yarn</code> package manager is pre-bundled by github, or you need a separate step to install it.</p><p>When it comes to caching the downloaded packages, <code>setup-node</code> action utilizes <a href="https://github.com/actions/cache" target="_blank" rel="noopener"><code>actions/cache</code></a>. It caches the global package data, i.e. <code>~/.cache/yarn/v6</code> folder, instead of <code>node_modules</code>, so that cache can be shared between different node versions. <code>setup-node</code> generates a cache key in the form of <code>node-cache-Linux-yarn-${hash(yarn.lock)}</code>. See more about caching on <a href="https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows" target="_blank" rel="noopener">GitHub Docs</a>.</p><p>The static site is generated in <code>public</code> folder, and we need to push them into the <code>gh-pages</code> branch. There is an action <a href="https://github.com/peaceiris/actions-gh-pages" target="_blank" rel="noopener"><code>peaceiris/actions-gh-pages</code></a> that already covers this.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">steps:</span></span><br><span class="line"><span class="attr">  - uses:</span> <span class="string">peaceiris/actions-gh-pages@v3</span></span><br><span class="line"><span class="attr">    with:</span></span><br><span class="line"><span class="attr">      github_token:</span> <span class="string">$&#123;&#123;</span> <span class="string">secrets.GITHUB_TOKEN</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">      publish_dir:</span> <span class="string">./public</span></span><br></pre></td></tr></table></figure><p>It first clones the <code>gh-pages</code> branch into work directory, overwrites it with the files in <code>public</code> folder, commits and pushes to the remote branch. The <code>GITHUB_TOKEN</code> is provided by GitHub Actions, with adequate permissions.</p><p>Last but not least, this workflow needs to be triggered on the <code>push</code> event of the <code>master</code> branch:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">on:</span></span><br><span class="line"><span class="attr">  push:</span></span><br><span class="line"><span class="attr">    branches:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><p>Here is a screenshot of this workflow:</p><p><img src="/images/use-github-actions.png" alt="Use GitHub Actions"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;TL-DR&quot;&gt;&lt;a href=&quot;#TL-DR&quot; class=&quot;headerlink&quot; title=&quot;TL;DR&quot;&gt;&lt;/a&gt;TL;DR&lt;/h2&gt;&lt;p&gt;Create &lt;code&gt;.github/workflows/pages.yml&lt;/code&gt; in your &lt;code&gt;master&lt;/code&gt; branch:&lt;/p&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;Update&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;gh-pages&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;on:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;  push:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;    branches:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;      -&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;master&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;jobs:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;  deploy:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;    runs-on:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;ubuntu-latest&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;    steps:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;      - uses:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;actions/checkout@v3&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;      - uses:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;actions/setup-node@v3&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;        with:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;          node-version:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;12.22&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;          cache:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;yarn&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;      - run:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;yarn&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;install&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;      - run:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;yarn&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;build&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;      - uses:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;peaceiris/actions-gh-pages@v3&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;        with:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;          github_token:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;$&amp;#123;&amp;#123;&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;secrets.GITHUB_TOKEN&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&amp;#125;&amp;#125;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;          publish_dir:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;./public&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Go to GitHub repo’s Settings &amp;gt; Pages, change source branch to &lt;code&gt;gh-pages&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;How-it-works&quot;&gt;&lt;a href=&quot;#How-it-works&quot; class=&quot;headerlink&quot; title=&quot;How it works&quot;&gt;&lt;/a&gt;How it works&lt;/h2&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="hexo" scheme="http://shzhangji.com/tags/hexo/"/>
    
      <category term="github" scheme="http://shzhangji.com/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>Deploy Flink Job Cluster on Kubernetes</title>
    <link href="http://shzhangji.com/blog/2019/08/24/deploy-flink-job-cluster-on-kubernetes/"/>
    <id>http://shzhangji.com/blog/2019/08/24/deploy-flink-job-cluster-on-kubernetes/</id>
    <published>2019-08-24T11:33:22.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a> is the trending container orchestration system that can be used to host various applications from web services to data processing jobs. Applications are packaged in self-contained, yet light-weight containers, and we declare how they should be deployed, how they scale, and how they expose as services. <a href="https://flink.apache.org/" target="_blank" rel="noopener">Flink</a> is also a trending distributed computing framework that can run on a variety of platforms, including Kubernetes. Combining them will bring us robust and scalable deployments of data processing jobs, and more safely Flink can share a Kubernetes cluster with other services.</p><p><img src="/images/flink-on-kubernetes.png" alt="Flink on Kubernetes"></p><p>When deploying Flink on Kubernetes, there are two options, session cluster and job cluster. Session cluster is like running a standalone Flink cluster on k8s that can accept multiple jobs and is suitable for short running tasks or ad-hoc queries. Job cluster, on the other hand, deploys a full set of Flink cluster for each individual job. We build container image for each job, and provide it with dedicated resources, so that jobs have less chance interfering with other, and can scale out independently. So this article will illustrate how to run a Flink job cluster on Kubernetes, the steps are:</p><ul><li>Compile and package the Flink job jar.</li><li>Build a Docker image containing the Flink runtime and the job jar.</li><li>Create a Kubernetes Job for Flink JobManager.</li><li>Create a Kubernetes Service for this Job.</li><li>Create a Kubernetes Deployment for Flink TaskManagers.</li><li>Enable Flink JobManager HA with ZooKeeper.</li><li>Correctly stop and resume Flink job with SavePoint facility.</li></ul><a id="more"></a><h2 id="Kubernetes-Playground"><a href="#Kubernetes-Playground" class="headerlink" title="Kubernetes Playground"></a>Kubernetes Playground</h2><p>In case you do not already have a Kubernetes environment, one can easily setup a local playground with <a href="https://github.com/kubernetes/minikube" target="_blank" rel="noopener">minikube</a>. Take MacOS for an example:</p><ul><li>Install <a href="https://www.virtualbox.org" target="_blank" rel="noopener">VirtualBox</a>, since minikube will setup a k8s cluster inside a virtual machine.</li><li>Download the <a href="https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64" target="_blank" rel="noopener">minikube binary</a>, making it executable and accessible from PATH.</li><li>Execute <code>minikube start</code>, it will download the virtual machine image, kubelet and kubeadm facilities, install and verify the k8s cluster. If you have trouble accessing the internet, setup a proxy and <a href="https://minikube.sigs.k8s.io/docs/reference/networking/proxy/" target="_blank" rel="noopener">tell minikube to use it</a>.</li><li>Download and install the <a href="https://storage.googleapis.com/kubernetes-release/release/v1.15.0/bin/darwin/amd64/kubectl" target="_blank" rel="noopener">kubectl binary</a>. Minikube has configured kubectl to point to the installed k8s cluster, so one can execute <code>kubectl get pods -A</code> to see the running system pods.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   kube-apiserver-minikube            1/1     Running   0          16m</span><br><span class="line">kube-system   etcd-minikube                      1/1     Running   0          15m</span><br><span class="line">kube-system   coredns-5c98db65d4-d4t2h           1/1     Running   0          17m</span><br></pre></td></tr></table></figure><h2 id="Flink-Streaming-Job"><a href="#Flink-Streaming-Job" class="headerlink" title="Flink Streaming Job"></a>Flink Streaming Job</h2><p>Let us create a simple streaming job, that reads data from socket, and prints the count of words every 5 seconds. The following code is taken from <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/datastream_api.html#example-program" target="_blank" rel="noopener">Flink doc</a>, and a full Maven project can be found on <a href="https://github.com/jizhang/flink-on-kubernetes" target="_blank" rel="noopener">GitHub</a>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env</span><br><span class="line">    .socketTextStream(<span class="string">"192.168.99.1"</span>, <span class="number">9999</span>)</span><br><span class="line">    .flatMap(<span class="keyword">new</span> Splitter())</span><br><span class="line">    .keyBy(<span class="number">0</span>)</span><br><span class="line">    .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">    .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">dataStream.print();</span><br></pre></td></tr></table></figure><p>IP <code>192.168.99.1</code> allows container to access services running on minikube host. For this example to work, you need to run <code>nc -lk 9999</code> on your host before creating the JobManager pod.</p><p>Run <code>mvn clean package</code>, and the compiled job jar can be found in <code>target/flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar</code>.</p><h2 id="Build-Docker-Image"><a href="#Build-Docker-Image" class="headerlink" title="Build Docker Image"></a>Build Docker Image</h2><p>Flink provides an official docker image on <a href="https://hub.docker.com/_/flink" target="_blank" rel="noopener">DockerHub</a>. We can use it as the base image and add job jar into it. Besides, in recent Flink distribution, the Hadoop binary is not included anymore, so we need to add Hadoop jar as well. Take a quick look at the base image’s <a href="https://github.com/docker-flink/docker-flink/blob/master/1.8/scala_2.12-debian/Dockerfile" target="_blank" rel="noopener">Dockerfile</a>, it does the following tasks:</p><ul><li>Create from OpenJDK 1.8 base image.</li><li>Install Flink into <code>/opt/flink</code>.</li><li>Add <code>flink</code> user and group.</li><li>Configure the entry point, which we will override in k8s deployments.</li></ul><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> openjdk:<span class="number">8</span>-jre</span><br><span class="line"><span class="keyword">ENV</span> FLINK_HOME=/opt/flink</span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> <span class="variable">$FLINK_HOME</span></span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> useradd flink &amp;&amp; \</span></span><br><span class="line"><span class="bash">  wget -O flink.tgz <span class="string">"<span class="variable">$FLINK_TGZ_URL</span>"</span> &amp;&amp; \</span></span><br><span class="line"><span class="bash">  tar -xf flink.tgz</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">"/docker-entrypoint.sh"</span>]</span></span><br></pre></td></tr></table></figure><p>Based on it, we create a new Dockerfile:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> flink:<span class="number">1.8</span>.<span class="number">1</span>-scala_2.<span class="number">12</span></span><br><span class="line"><span class="keyword">ARG</span> hadoop_jar</span><br><span class="line"><span class="keyword">ARG</span> job_jar</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --chown=flink:flink <span class="variable">$hadoop_jar</span> <span class="variable">$job_jar</span> <span class="variable">$FLINK_HOME</span>/lib/</span></span><br><span class="line"><span class="keyword">USER</span> flink</span><br></pre></td></tr></table></figure><p>Before building the image, you need to install Docker CLI and point it to the docker service inside minikube:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ brew install docker</span><br><span class="line">$ <span class="built_in">eval</span> $(minikube docker-env)</span><br></pre></td></tr></table></figure><p>Then, download the <a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-7.0/flink-shaded-hadoop-2-uber-2.8.3-7.0.jar" target="_blank" rel="noopener">Hadoop uber jar</a>, and execute the following commands:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /path/to/Dockerfile</span><br><span class="line">$ cp /path/to/flink-shaded-hadoop-2-uber-2.8.3-7.0.jar hadoop.jar</span><br><span class="line">$ cp /path/to/flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar job.jar</span><br><span class="line">$ docker build --build-arg hadoop_jar=hadoop.jar --build-arg job_jar=job.jar --tag flink-on-kubernetes:0.0.1 .</span><br></pre></td></tr></table></figure><p>Now we have a local docker image that is ready to be deployed.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker image ls</span><br><span class="line">REPOSITORY           TAG    IMAGE ID      CREATED         SIZE</span><br><span class="line">flink-on-kubernetes  0.0.1  505d2f11cc57  10 seconds ago  618MB</span><br></pre></td></tr></table></figure><h2 id="Deploy-JobManager"><a href="#Deploy-JobManager" class="headerlink" title="Deploy JobManager"></a>Deploy JobManager</h2><p>First, we create a k8s Job for Flink JobManager. Job and Deployment both create and manage Pods to do some work. The difference is Job will quit if the Pod finishes successfully, based on the exit code, while Deployment only quits when asked to. This feature enables us to cancel the Flink job manually, without worrying Deployment restarts the JobManager by mistake.</p><p>Here’s the <a href="https://github.com/jizhang/flink-on-kubernetes/blob/master/docker/jobmanager.yml" target="_blank" rel="noopener"><code>jobmanager.yml</code></a>:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        instance:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      restartPolicy:</span> <span class="string">OnFailure</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink-on-kubernetes:0.0.1</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">["/opt/flink/bin/standalone-job.sh"]</span></span><br><span class="line"><span class="attr">        args:</span> <span class="string">["start-foreground",</span></span><br><span class="line">               <span class="string">"-Djobmanager.rpc.address=$&#123;JOB&#125;-jobmanager"</span><span class="string">,</span></span><br><span class="line">               <span class="string">"-Dparallelism.default=1"</span><span class="string">,</span></span><br><span class="line">               <span class="string">"-Dblob.server.port=6124"</span><span class="string">,</span></span><br><span class="line">               <span class="string">"-Dqueryable-state.server.ports=6125"</span><span class="string">]</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">ui</span></span><br></pre></td></tr></table></figure><ul><li><code>${JOB}</code> can be replaced by <code>envsubst</code>, so that config files can be reused by different jobs.</li><li>Container’s entry point is changed to <code>standalone-job.sh</code>. It will start the JobManager in foreground, scan the class path for a <code>Main-Class</code> as the job entry point, or you can specify the full class name via <code>-j</code> option. Then, this job is automatically submitted to the cluster.</li><li>JobManager’s RPC address is the k8s <a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies" target="_blank" rel="noopener">Service</a>‘s name, which we will create later. Other containers can access JobManager via this host name.</li><li>Blob server and queryable state server’s ports are by default random. We change them to fixed ports for easy exposure.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> JOB=flink-on-kubernetes</span><br><span class="line">$ envsubst &lt;jobmanager.yml | kubectl create -f -</span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">flink-on-kubernetes-jobmanager-kc4kq   1/1     Running   0          2m26s</span><br></pre></td></tr></table></figure><p>Next, we expose this JobManager as k8s Service, so that TaskManagers can register to it.</p><p><code>service.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">    instance:</span> <span class="string">$&#123;JOB&#125;-jobmanager</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ui</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">8081</span></span><br></pre></td></tr></table></figure><p><code>type: NodePort</code> is necessary because we also want to interact with this JobManager outside the k8s cluster.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ envsubst &lt;service.yml | kubectl create -f -</span><br><span class="line">$ kubectl get service</span><br><span class="line">NAME                             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                      AGE</span><br><span class="line">flink-on-kubernetes-jobmanager   NodePort    10.109.78.143   &lt;none&gt;        6123:31476/TCP,6124:32268/TCP,6125:31602/TCP,8081:31254/TCP  15m</span><br></pre></td></tr></table></figure><p>We can see Flink dashboard is exposed on port 31254 on the virtual machine. Minikube provides a command to retrieve the full url of a service.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ minikube service <span class="variable">$JOB</span>-jobmanager --url</span><br><span class="line">http://192.168.99.108:31476</span><br><span class="line">http://192.168.99.108:32268</span><br><span class="line">http://192.168.99.108:31602</span><br><span class="line">http://192.168.99.108:31254</span><br></pre></td></tr></table></figure><h2 id="Deploy-TaskManager"><a href="#Deploy-TaskManager" class="headerlink" title="Deploy TaskManager"></a>Deploy TaskManager</h2><p><code>taskmanager.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">$&#123;JOB&#125;-taskmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">      instance:</span> <span class="string">$&#123;JOB&#125;-taskmanager</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        instance:</span> <span class="string">$&#123;JOB&#125;-taskmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink-on-kubernetes:0.0.1</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">["/opt/flink/bin/taskmanager.sh"]</span></span><br><span class="line"><span class="attr">        args:</span> <span class="string">["start-foreground",</span> <span class="string">"-Djobmanager.rpc.address=$&#123;JOB&#125;-jobmanager"</span><span class="string">]</span></span><br></pre></td></tr></table></figure><p>Change the number of <code>replicas</code> to add more TaskManagers. The <code>taskmanager.numberOfTaskSlots</code> is set to <code>1</code> in this image, which is recommended because we should let k8s handle the scaling.</p><p>Now the job cluster is running, try typing something into the <code>nc</code> console:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ nc -lk 9999</span><br><span class="line">hello world</span><br><span class="line">hello flink</span><br></pre></td></tr></table></figure><p>Open another terminal and tail the TaskManager’s standard output:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs -f -l instance=<span class="variable">$JOB</span>-taskmanager</span><br><span class="line">(hello,2)</span><br><span class="line">(flink,1)</span><br><span class="line">(world,1)</span><br></pre></td></tr></table></figure><h2 id="Configure-JobManager-HA"><a href="#Configure-JobManager-HA" class="headerlink" title="Configure JobManager HA"></a>Configure JobManager HA</h2><p>While TaskManager can achieve high availability by increasing the replicas of the Deployment, JobManager is still a single point of failure. Flink comes with an <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/jobmanager_high_availability.html" target="_blank" rel="noopener">HA solution</a> with the help of ZooKeeper and a distributed file system like HDFS. In a standalone cluster, multiple JobManagers are started and one of them is elected as leader. In YARN or Kubernetes deployment, only one JobManager instance is required. The cluster’s meta info is stored in ZooKeeper, and checkpoint data are stored in HDFS. When JobManager is down, Kubernetes will restart the container, and the new JobManager will restore the last checkpoint and resume the job.</p><p>To enable JobManager HA, change the start command of both JobManager and TaskManager:</p><p><code>jobmanager-ha.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command:</span> <span class="string">["/opt/flink/bin/standalone-job.sh"]</span></span><br><span class="line"><span class="attr">args:</span> <span class="string">["start-foreground",</span></span><br><span class="line">       <span class="string">"-Djobmanager.rpc.address=$&#123;JOB&#125;-jobmanager"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dparallelism.default=1"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dblob.server.port=6124"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dqueryable-state.server.ports=6125"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability=zookeeper"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.zookeeper.quorum=192.168.99.1:2181"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.zookeeper.path.root=/flink"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.cluster-id=/$&#123;JOB&#125;"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.storageDir=hdfs://192.168.99.1:9000/flink/recovery"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.jobmanager.port=6123"</span><span class="string">,</span></span><br><span class="line">       <span class="string">]</span></span><br></pre></td></tr></table></figure><p><code>taskmanager-ha.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command:</span> <span class="string">["/opt/flink/bin/taskmanager.sh"]</span></span><br><span class="line"><span class="attr">args:</span> <span class="string">["start-foreground",</span></span><br><span class="line">       <span class="string">"-Dhigh-availability=zookeeper"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.zookeeper.quorum=192.168.99.1:2181"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.zookeeper.path.root=/flink"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.cluster-id=/$&#123;JOB&#125;"</span><span class="string">,</span></span><br><span class="line">       <span class="string">"-Dhigh-availability.storageDir=hdfs://192.168.99.1:9000/flink/recovery"</span><span class="string">,</span></span><br><span class="line">       <span class="string">]</span></span><br></pre></td></tr></table></figure><ul><li>Prepare a ZooKeeper and HDFS environment on minikube host, so that Flink containers can access them via <code>192.168.99.1:2181</code> and <code>192.168.99.1:9000</code>.</li><li>Cluster meta data will be stored under <code>/flink/${JOB}</code> in ZooKeeper.</li><li>Checkpoint data is stored under <code>/flink/recovery</code> in HDFS. Make sure you create the <code>/flink</code> directory with proper permission.</li><li>The <code>jobmanager.rpc.address</code> property is removed from TaskManager’s arguments because the RPC host and port of JobManager will be fetched from ZooKeeper. The RPC port is by default random, so we changed to a fixed port via <code>high-availability.jobmanager.port</code>, which is exposed in k8s Service.</li></ul><h2 id="Manage-Flink-Job"><a href="#Manage-Flink-Job" class="headerlink" title="Manage Flink Job"></a>Manage Flink Job</h2><p>We can interact with Flink cluster via RESTful API. It is the same port as Flink Dashboard. Install Flink binaries on your host machine, and pass <code>-m</code> argument to point to the JobManager in k8s:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flink list -m 192.168.99.108:30206</span><br><span class="line">------------------ Running/Restarting Jobs -------------------</span><br><span class="line">24.08.2019 12:50:28 : 00000000000000000000000000000000 : Window WordCount (RUNNING)</span><br><span class="line">--------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>In HA mode, Flink job ID is by default <code>00000000000000000000000000000000</code>. We can use this ID to cancel Flink job with <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/savepoints.html" target="_blank" rel="noopener">SavePoint</a>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flink cancel -m 192.168.99.108:30206 -s hdfs://192.168.99.1:9000/flink/savepoints/ 00000000000000000000000000000000</span><br><span class="line">Cancelled job 00000000000000000000000000000000. Savepoint stored <span class="keyword">in</span> hdfs://192.168.99.1:9000/flink/savepoints/savepoint-000000-f776c8e50a0c.</span><br></pre></td></tr></table></figure><p>And the k8s Job is now in completed status:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get job</span><br><span class="line">NAME                             COMPLETIONS   DURATION   AGE</span><br><span class="line">flink-on-kubernetes-jobmanager   1/1           4m40s      7m14s</span><br></pre></td></tr></table></figure><p>To re-submit the job, we need to delete them first:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete job <span class="variable">$JOB</span>-jobmanager</span><br><span class="line">$ kubectl delete deployment <span class="variable">$JOB</span>-taskmanager</span><br></pre></td></tr></table></figure><p>Then add a command argument to <code>jobmanager-savepoint.yml</code>:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command:</span> <span class="string">["/opt/flink/bin/standalone-job.sh"]</span></span><br><span class="line"><span class="attr">args:</span> <span class="string">["start-foreground",</span></span><br><span class="line">       <span class="string">...</span></span><br><span class="line">       <span class="string">"--fromSavepoint"</span><span class="string">,</span> <span class="string">"$&#123;SAVEPOINT&#125;"</span><span class="string">,</span></span><br><span class="line">       <span class="string">]</span></span><br></pre></td></tr></table></figure><p>Start this job from the SavePoint:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> SAVEPOINT=hdfs://192.168.99.1:9000/flink/savepoints/savepoint-000000-f776c8e50a0c</span><br><span class="line">$ envsubst &lt;jobmanager-savepoint.yml | kubectl create -f -</span><br></pre></td></tr></table></figure><p>One note on SavePoint, it has to be used with HA mode, because the <code>--fromSavepoint</code> argument will be passed to <code>standalone-job.sh</code> every time Kubernetes tries to restart a failed JobManager. With HA mode enabled, the new JobManager will first restore from the CheckPoint, ignoring the SavePoint.</p><h3 id="Scale-Flink-Job"><a href="#Scale-Flink-Job" class="headerlink" title="Scale Flink Job"></a>Scale Flink Job</h3><p>There are two ways to scale a Flink job. One is manually restarting it with a different <code>parallelism.default</code> config, which can be found in <code>jobmanager.yml</code>. Another way is using the <code>bin/flink modify</code> command. Under the hood, this command cancels the job with a SavePoint, and restarts it with the new parallelism. So for this to work, you need to first set the default SavePoint directory, like:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command:</span> <span class="string">["/opt/flink/bin/standalone-job.sh"]</span></span><br><span class="line"><span class="attr">args:</span> <span class="string">["start-foreground",</span></span><br><span class="line">       <span class="string">...</span></span><br><span class="line">       <span class="string">"-Dstate.savepoints.dir=hdfs://192.168.99.1:9000/flink/savepoints/"</span><span class="string">,</span></span><br><span class="line">       <span class="string">]</span></span><br></pre></td></tr></table></figure><p>Then, add more TaskManagers with <code>kubectl scale</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl scale --replicas=2 deployment/<span class="variable">$JOB</span>-taskmanager</span><br><span class="line">deployment.extensions/flink-on-kubernetes-taskmanager scaled</span><br></pre></td></tr></table></figure><p>And modify the parallelism of the running job:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flink modify 755877434b676ce9dae5cfb533ed7f33 -m 192.168.99.108:30206 -p 2</span><br><span class="line">Modify job 755877434b676ce9dae5cfb533ed7f33.</span><br><span class="line">Rescaled job 755877434b676ce9dae5cfb533ed7f33. Its new parallelism is 2.</span><br></pre></td></tr></table></figure><p>However, due to an <a href="https://issues.apache.org/jira/browse/FLINK-11997" target="_blank" rel="noopener">unresolved issue</a>, we cannot use <code>flink modify</code> to scale an HA job cluster in Kubernetes mode. Use the manual method instead.</p><h2 id="Flink-Native-Support-on-Kubernetes"><a href="#Flink-Native-Support-on-Kubernetes" class="headerlink" title="Flink Native Support on Kubernetes"></a>Flink Native Support on Kubernetes</h2><p>Flink enjoys a very active community that constantly improves its own design (<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="noopener">FLIP-6</a>) to adopt current cloud-native environment. They’ve also noticed the rapid development of Kubernetes, and the native support of Flink on K8s is under development as well. It’s known that Flink can run natively on resource management systems like YARN/Mesos. Take YARN for an instance, Flink will first start an ApplicationMaster as the JobManager, analyze how much resource this job needs, and request YARN ResourceManager for containers to run TaskManager. When the parallelism changes, JobManager will acquire or release containers correspondingly. This kind of active resource management for Kubernetes is under development (<a href="https://issues.apache.org/jira/browse/FLINK-9953" target="_blank" rel="noopener">FLINK-9953</a>). In future, we can deploy Flink cluster on K8s with a simple command.</p><p>Besides, another kind of resource management is also on its way. It’s called reactive container mode (<a href="https://issues.apache.org/jira/browse/FLINK-10407" target="_blank" rel="noopener">FLINK-10407</a>). In short, when the JobManager realizes there are idle TaskManagers, it will automatically scale the job to its maximum parallelism. Thus, we only need to use <code>kubectl scale</code> changing <code>replicas</code>, without executing <code>flink modify</code> later. Such convenient features will be available soon, I believe.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/deployment/kubernetes.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/deployment/kubernetes.html</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/</a></li><li><a href="https://jobs.zalando.com/tech/blog/running-apache-flink-on-kubernetes/" target="_blank" rel="noopener">https://jobs.zalando.com/tech/blog/running-apache-flink-on-kubernetes/</a></li><li><a href="https://www.slideshare.net/tillrohrmann/redesigning-apache-flinks-distributed-architecture-flink-forward-2017" target="_blank" rel="noopener">https://www.slideshare.net/tillrohrmann/redesigning-apache-flinks-distributed-architecture-flink-forward-2017</a></li><li><a href="https://www.slideshare.net/tillrohrmann/future-of-apache-flink-deployments-containers-kubernetes-and-more-flink-forward-2019-sf" target="_blank" rel="noopener">https://www.slideshare.net/tillrohrmann/future-of-apache-flink-deployments-containers-kubernetes-and-more-flink-forward-2019-sf</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://kubernetes.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubernetes&lt;/a&gt; is the trending container orchestration system that can be used to host various applications from web services to data processing jobs. Applications are packaged in self-contained, yet light-weight containers, and we declare how they should be deployed, how they scale, and how they expose as services. &lt;a href=&quot;https://flink.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Flink&lt;/a&gt; is also a trending distributed computing framework that can run on a variety of platforms, including Kubernetes. Combining them will bring us robust and scalable deployments of data processing jobs, and more safely Flink can share a Kubernetes cluster with other services.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/flink-on-kubernetes.png&quot; alt=&quot;Flink on Kubernetes&quot;&gt;&lt;/p&gt;
&lt;p&gt;When deploying Flink on Kubernetes, there are two options, session cluster and job cluster. Session cluster is like running a standalone Flink cluster on k8s that can accept multiple jobs and is suitable for short running tasks or ad-hoc queries. Job cluster, on the other hand, deploys a full set of Flink cluster for each individual job. We build container image for each job, and provide it with dedicated resources, so that jobs have less chance interfering with other, and can scale out independently. So this article will illustrate how to run a Flink job cluster on Kubernetes, the steps are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compile and package the Flink job jar.&lt;/li&gt;
&lt;li&gt;Build a Docker image containing the Flink runtime and the job jar.&lt;/li&gt;
&lt;li&gt;Create a Kubernetes Job for Flink JobManager.&lt;/li&gt;
&lt;li&gt;Create a Kubernetes Service for this Job.&lt;/li&gt;
&lt;li&gt;Create a Kubernetes Deployment for Flink TaskManagers.&lt;/li&gt;
&lt;li&gt;Enable Flink JobManager HA with ZooKeeper.&lt;/li&gt;
&lt;li&gt;Correctly stop and resume Flink job with SavePoint facility.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="flink" scheme="http://shzhangji.com/tags/flink/"/>
    
      <category term="kubernetes" scheme="http://shzhangji.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Hive ACID Transactional Table</title>
    <link href="http://shzhangji.com/blog/2019/06/10/understanding-hive-acid-transactional-table/"/>
    <id>http://shzhangji.com/blog/2019/06/10/understanding-hive-acid-transactional-table/</id>
    <published>2019-06-10T12:40:55.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://hive.apache.org/" target="_blank" rel="noopener">Apache Hive</a> introduced transactions since version 0.13 to fully support ACID semantics on Hive table, including INSERT/UPDATE/DELETE/MERGE statements, streaming data ingestion, etc. In Hive 3.0, this feature is further improved by optimizing the underlying data file structure, reducing constraints on table scheme, and supporting predicate push down and vectorized query. Examples and setup can be found on <a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions" target="_blank" rel="noopener">Hive wiki</a> and other <a href="https://hortonworks.com/tutorial/using-hive-acid-transactions-to-insert-update-and-delete-data/" target="_blank" rel="noopener">tutorials</a>, while this article will focus on how transactional table is saved on HDFS, and take a closer look at the read-write process.</p><h2 id="File-Structure"><a href="#File-Structure" class="headerlink" title="File Structure"></a>File Structure</h2><h3 id="Insert-Data"><a href="#Insert-Data" class="headerlink" title="Insert Data"></a>Insert Data</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employee (<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, salary <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC TBLPROPERTIES (<span class="string">'transactional'</span> = <span class="string">'true'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employee <span class="keyword">VALUES</span></span><br><span class="line">(<span class="number">1</span>, <span class="string">'Jerry'</span>, <span class="number">5000</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">'Tom'</span>,   <span class="number">8000</span>),</span><br><span class="line">(<span class="number">3</span>, <span class="string">'Kate'</span>,  <span class="number">6000</span>);</span><br></pre></td></tr></table></figure><p>An INSERT statement is executed in a single transaction. It will create a <code>delta</code> directory containing information about this transaction and its data.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/employee/delta_0000001_0000001_0000</span><br><span class="line">/user/hive/warehouse/employee/delta_0000001_0000001_0000/_orc_acid_version</span><br><span class="line">/user/hive/warehouse/employee/delta_0000001_0000001_0000/bucket_00000</span><br></pre></td></tr></table></figure><p>The schema of this folder’s name is <code>delta_minWID_maxWID_stmtID</code>, i.e. “delta” prefix, transactional writes’ range (minimum and maximum write ID), and statement ID. In detail:</p><ul><li>All INSERT statements will create a <code>delta</code> directory. UPDATE statement will also create <code>delta</code> directory right after a <code>delete</code> directory. <code>delete</code> directory is prefixed with “delete_delta”.</li><li>Hive will assign a globally unique ID for every transaction, both read and write. For transactional writes like INSERT and DELETE, it will also assign a table-wise unique ID, a.k.a. a write ID. The write ID range will be encoded in the <code>delta</code> and <code>delete</code> directory names.</li><li>Statement ID is used when multiple writes into the same table happen in one transaction.</li></ul><a id="more"></a><p>For its content, <code>_orc_acid_version</code> always contains “2”, indicating this directory is in ACID version 2 format. Compared with previous version, the main difference is that UPDATE now uses split-update technique to support predicate push down and other features (<a href="https://jira.apache.org/jira/browse/HIVE-14035" target="_blank" rel="noopener">HIVE-14035</a>). <code>bucket_00000</code> is the inserted records. Since this table is not bucketed, there is only one file, and it is in <a href="https://orc.apache.org/" target="_blank" rel="noopener">ORC</a> format. We can take a look at its content with <a href="https://orc.apache.org/docs/java-tools.html" target="_blank" rel="noopener">orc-tools</a>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ orc-tools data bucket_00000</span><br><span class="line">&#123;&quot;operation&quot;:0,&quot;originalTransaction&quot;:1,&quot;bucket&quot;:536870912,&quot;rowId&quot;:0,&quot;currentTransaction&quot;:1,&quot;row&quot;:&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;Jerry&quot;,&quot;salary&quot;:5000&#125;&#125;</span><br><span class="line">&#123;&quot;operation&quot;:0,&quot;originalTransaction&quot;:1,&quot;bucket&quot;:536870912,&quot;rowId&quot;:1,&quot;currentTransaction&quot;:1,&quot;row&quot;:&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;Tom&quot;,&quot;salary&quot;:8000&#125;&#125;</span><br><span class="line">&#123;&quot;operation&quot;:0,&quot;originalTransaction&quot;:1,&quot;bucket&quot;:536870912,&quot;rowId&quot;:2,&quot;currentTransaction&quot;:1,&quot;row&quot;:&#123;&quot;id&quot;:3,&quot;name&quot;:&quot;Kate&quot;,&quot;salary&quot;:6000&#125;&#125;</span><br></pre></td></tr></table></figure><p>The file content is displayed in JSON, row-wise. We can see the actual data is in <code>row</code>, while other keys work for transaction mechanism:</p><ul><li><code>operation</code> 0 means INSERT, 1 UPDATE, and 2 DELETE. UPDATE will not appear because of the split-update technique mentioned above.</li><li><code>originalTransaction</code> is the previous write ID. For INSERT, it is the same as <code>currentTransaction</code>. For DELETE, it is the write ID when this record is first created.</li><li><code>bucket</code> is a 32-bit integer defined by <code>BucketCodec</code> class. Their meanings are:<ul><li>bit 1-3: bucket codec version, currently <code>001</code>.</li><li>bit 4: reserved for future.</li><li>bit 5-16: the bucket ID, 0-based. This ID is determined by CLUSTERED BY columns and number of buckets. It matches the <code>bucket_N</code> prefixed files.</li><li>bit 17-20: reserved for future.</li><li>bit 21-32: statement ID.</li><li>For instance, the binary form of <code>536936448</code> is <code>00100000000000010000000000000000</code>, showing it is a version 1 codec, and bucket ID is 1.</li></ul></li><li><code>rowId</code> is the auto-generated unique ID within the transaction and bucket.</li><li><code>currentTransaction</code> is the current write ID.</li><li><code>row</code> contains the actual data. For DELETE, <code>row</code> will be null.</li></ul><p>We can note that the data rows are ordered by (<code>originalTransaction</code>, <code>bucket</code>, <code>rowId</code>), which is essential for the reading process.</p><p>These information can also be viewed by the <code>row__id</code> virtual column:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> row__id, <span class="keyword">id</span>, <span class="keyword">name</span>, salary <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;writeid&quot;:1,&quot;bucketid&quot;:536870912,&quot;rowid&quot;:0&#125;    1       Jerry   5000</span><br><span class="line">&#123;&quot;writeid&quot;:1,&quot;bucketid&quot;:536870912,&quot;rowid&quot;:1&#125;    2       Tom     8000</span><br><span class="line">&#123;&quot;writeid&quot;:1,&quot;bucketid&quot;:536870912,&quot;rowid&quot;:2&#125;    3       Kate    6000</span><br></pre></td></tr></table></figure><h4 id="Streaming-Data-Ingest-V2"><a href="#Streaming-Data-Ingest-V2" class="headerlink" title="Streaming Data Ingest V2"></a>Streaming Data Ingest V2</h4><p>Hive 3.0 also upgrades the former <a href="https://cwiki.apache.org/confluence/display/Hive/Streaming+Data+Ingest+V2" target="_blank" rel="noopener">Streaming API</a>. Now users or third-party tools like Flume can use the ACID feature writing data continuously into Hive table. These operations will also create <code>delta</code> directories. But mutation is no longer supported.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">StreamingConnection connection = HiveStreamingConnection.newBuilder().connect();</span><br><span class="line">connection.beginTransaction();</span><br><span class="line">connection.write(<span class="string">"11,val11,Asia,China"</span>.getBytes());</span><br><span class="line">connection.write(<span class="string">"12,val12,Asia,India"</span>.getBytes());</span><br><span class="line">connection.commitTransaction();</span><br><span class="line">connection.close();</span><br></pre></td></tr></table></figure><h3 id="Update-Data"><a href="#Update-Data" class="headerlink" title="Update Data"></a>Update Data</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> employee <span class="keyword">SET</span> salary = <span class="number">7000</span> <span class="keyword">WHERE</span> <span class="keyword">id</span> = <span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>This statement will first run a query to find out the <code>row__id</code> of the updating records, and then create a <code>delete</code> directory a long with a <code>delta</code> directory:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/employee/delta_0000001_0000001_0000/bucket_00000</span><br><span class="line">/user/hive/warehouse/employee/delete_delta_0000002_0000002_0000/bucket_00000</span><br><span class="line">/user/hive/warehouse/employee/delta_0000002_0000002_0000/bucket_00000</span><br></pre></td></tr></table></figure><p>Content of <code>delete_delta_0000002_0000002_0000/bucket_00000</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;operation&quot;:2,&quot;originalTransaction&quot;:1,&quot;bucket&quot;:536870912,&quot;rowId&quot;:1,&quot;currentTransaction&quot;:2,&quot;row&quot;:null&#125;</span><br></pre></td></tr></table></figure><p>Content of <code>delta_0000002_0000002_0000/bucket_00000</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;operation&quot;:0,&quot;originalTransaction&quot;:2,&quot;bucket&quot;:536870912,&quot;rowId&quot;:0,&quot;currentTransaction&quot;:2,&quot;row&quot;:&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;Tom&quot;,&quot;salary&quot;:7000&#125;&#125;</span><br></pre></td></tr></table></figure><p>DELETE statement works similarly to UPDATE, i.e. find the record but generate only <code>delete</code> directory.</p><h3 id="Merge-Statement"><a href="#Merge-Statement" class="headerlink" title="Merge Statement"></a>Merge Statement</h3><p>MERGE is like MySQL’s INSERT ON UPDATE. It can update target table with a source table. For instance:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employee_update (<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, salary <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> employee_update <span class="keyword">VALUES</span></span><br><span class="line">(<span class="number">2</span>, <span class="string">'Tom'</span>,  <span class="number">7000</span>),</span><br><span class="line">(<span class="number">4</span>, <span class="string">'Mary'</span>, <span class="number">9000</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">MERGE</span> <span class="keyword">INTO</span> employee <span class="keyword">AS</span> a</span><br><span class="line"><span class="keyword">USING</span> employee_update <span class="keyword">AS</span> b <span class="keyword">ON</span> a.id = b.id</span><br><span class="line"><span class="keyword">WHEN</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span> <span class="keyword">UPDATE</span> <span class="keyword">SET</span> salary = b.salary</span><br><span class="line"><span class="keyword">WHEN</span> <span class="keyword">NOT</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span> <span class="keyword">INSERT</span> <span class="keyword">VALUES</span> (b.id, b.name, b.salary);</span><br></pre></td></tr></table></figure><p>This statement will update the salary of Tom, and insert a new row of Mary. WHENs are considered different statements. The INSERT clause generates <code>delta_0000002_0000002_0000</code>, containing the row of Mary, while UPDATE generates <code>delete_delta_0000002_0000002_0001</code> and <code>delta_0000002_0000002_0001</code>, deleting and inserting the row of Tom.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/employee/delta_0000001_0000001_0000</span><br><span class="line">/user/hive/warehouse/employee/delta_0000002_0000002_0000</span><br><span class="line">/user/hive/warehouse/employee/delete_delta_0000002_0000002_0001</span><br><span class="line">/user/hive/warehouse/employee/delta_0000002_0000002_0001</span><br></pre></td></tr></table></figure><h3 id="Compaction"><a href="#Compaction" class="headerlink" title="Compaction"></a>Compaction</h3><p>As time goes, there will be more and more <code>delta</code> and <code>delete</code> directories in the table, which will affect the read performance, since reading is a process of merging the results of valid transactions. Small files are neither friendly to file systems like HDFS. So Hive uses two kinds of compactors, namely minor and major, to merge these directories while preserving the transaction information.</p><p>Minor compaction will merge multiple <code>delta</code> and <code>delete</code> files into one <code>delta</code> and <code>delete</code> file, respectively. The transaction ID will be preserved in folder name as write ID range, as mentioned above, while omitting the statement ID. Compactions will be automatically initiated in Hive metastore process based on some configured thresholds. We can also trigger it manually with the following SQL:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> employee <span class="keyword">COMPACT</span> <span class="string">'minor'</span>;</span><br></pre></td></tr></table></figure><p>Take the result of MERGE statement for an instance. After minor compaction, the folder structure will become:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/employee/delete_delta_0000001_0000002</span><br><span class="line">/user/hive/warehouse/employee/delta_0000001_0000002</span><br></pre></td></tr></table></figure><p>In <code>delta_0000001_0000002/bucket_00000</code>, rows are simply ordered and concatenated, i.e. two rows of Tom will be both included. Minor compact does not delete any data.</p><p>Major compaction, on the other hand, will merge and write the current table into a single directory, with the name <code>base_N</code>, where N is the latest write ID. Deleted data will be removed in major compaction. <code>row_id</code> remains untouched.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse/employee/base_0000002</span><br></pre></td></tr></table></figure><p>Note that after minor or major compaction, the original files will not be deleted immediately. Deletion is carried out by a cleaner thread, so there will be multiple files containing the same transaction data simultaneously. Take this into account when understanding the reading process.</p><h2 id="Reading-Process"><a href="#Reading-Process" class="headerlink" title="Reading Process"></a>Reading Process</h2><p>Now we see three kinds of files in an ACID table, <code>base</code>, <code>delta</code>, and <code>delete</code>. Each contains data rows that can be identified by <code>row__id</code> and sorted by it, too. Reading data from an ACID table is a process of merging these files, and reflecting the result of the last transaction. This process is written in <code>OrcInputFormat</code> and <code>OrcRawRecordMerger</code> class, and it is basically a merge-sort algorithm.</p><p>Take the following files for an instance. This structure can be generated by: insert three rows, do a major compaction, then update two rows. <code>1-0-0-1</code> is short for <code>originalTransaction</code> - <code>bucketId</code> (not encoded) - <code>rowId</code> - <code>currentTransaction</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+----------+    +----------+    +----------+</span><br><span class="line">| base_1   |    | delete_2 |    | delta_2  |</span><br><span class="line">+----------+    +----------+    +----------+</span><br><span class="line">| 1-0-0-1  |    | 1-0-1-2  |    | 2-0-0-2  |</span><br><span class="line">| 1-0-1-1  |    | 1-0-2-2  |    | 2-0-1-2  |</span><br><span class="line">| 1-0-2-1  |    +----------+    +----------+</span><br><span class="line">+----------+</span><br></pre></td></tr></table></figure><p>Merging process:</p><ul><li>Sort rows from all files by (<code>originalTransaction</code>, <code>bucketId</code>, <code>rowId</code>) ascendingly, (<code>currentTransaction</code>) descendingly. i.e.<ul><li><code>1-0-0-1</code></li><li><code>1-0-1-2</code></li><li><code>1-0-1-1</code></li><li>…</li><li><code>2-0-1-2</code></li></ul></li><li>Fetch the first record.</li><li>If the <code>row__id</code> is the same as previous, skip.</li><li>If the operation is DELETE, skip.<ul><li>As a result, for <code>1-0-1-2</code> and <code>1-0-1-1</code>, this row will be skipped.</li></ul></li><li>Otherwise, emit the row.</li><li>Repeat.</li></ul><p>The merging is done in a streaming way. Hive will open all the files, read the first record, and construct a <code>ReaderKey</code> class, storing <code>originalTransaction</code>, <code>bucketId</code>, <code>rowId</code>, and <code>currentTransaction</code>. <code>ReaderKey</code> class implements the <code>Comparable</code> interface, so they can be sorted in an customized order.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RecordIdentifier</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">RecordIdentifier</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> writeId;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> bucketId;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> rowId;</span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">int</span> <span class="title">compareToInternal</span><span class="params">(RecordIdentifier other)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (other == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (writeId != other.writeId) &#123;</span><br><span class="line">      <span class="keyword">return</span> writeId &lt; other.writeId ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (bucketId != other.bucketId) &#123;</span><br><span class="line">      <span class="keyword">return</span> bucketId &lt; other.bucketId ? - <span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (rowId != other.rowId) &#123;</span><br><span class="line">      <span class="keyword">return</span> rowId &lt; other.rowId ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReaderKey</span> <span class="keyword">extends</span> <span class="title">RecordIdentifier</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> currentWriteId;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">boolean</span> isDeleteEvent = <span class="keyword">false</span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(RecordIdentifier other)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sup = compareToInternal(other);</span><br><span class="line">    <span class="keyword">if</span> (sup == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (other.getClass() == ReaderKey.class) &#123;</span><br><span class="line">        ReaderKey oth = (ReaderKey) other;</span><br><span class="line">        <span class="keyword">if</span> (currentWriteId != oth.currentWriteId) &#123;</span><br><span class="line">          <span class="keyword">return</span> currentWriteId &lt; oth.currentWriteId ? +<span class="number">1</span> : -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (isDeleteEvent != oth.isDeleteEvent) &#123;</span><br><span class="line">          <span class="keyword">return</span> isDeleteEvent ? -<span class="number">1</span> : +<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sup;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Then, the <code>ReaderKey</code> and the file handler will be put into a <code>TreeMap</code>, so every time we poll for the first entry, we can get the desired file handler and read data.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrcRawRecordMerger</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> TreeMap&lt;ReaderKey, ReaderPair&gt; readers = <span class="keyword">new</span> TreeMap&lt;&gt;();</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">next</span><span class="params">(RecordIdentifier recordIdentifier, OrcStruct prev)</span> </span>&#123;</span><br><span class="line">    Map.Entry&lt;ReaderKey, ReaderPair&gt; entry = readers.pollFirstEntry();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Select-Files"><a href="#Select-Files" class="headerlink" title="Select Files"></a>Select Files</h3><p>Previously we pointed out that different transaction files may co-exist at the same time, so Hive needs to first select the files that are valid for the latest transaction. For instance, the following directory structure is the result of these operations: two inserts, one minor compact, one major compact, and one delete.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">delta_0000001_0000001_0000</span><br><span class="line">delta_0000002_0000002_0000</span><br><span class="line">delta_0000001_0000002</span><br><span class="line">base_0000002</span><br><span class="line">delete_delta_0000003_0000003_0000</span><br></pre></td></tr></table></figure><p>Filtering process:</p><ul><li>Consult the Hive Metastore to find out the valid write ID list.</li><li>Extract transaction information from files names, including file type, write ID range, and statement ID.</li><li>Select the <code>base</code> file with the maximum valid write ID.</li><li>Sort <code>delta</code> and <code>delete</code> files by write ID range:<ul><li>Smaller <code>minWID</code> orders first;</li><li>If <code>minWID</code> is the same, larger <code>maxWID</code> orders first;</li><li>Otherwise, sort by <code>stmtID</code>; files w/o <code>stmtID</code> orders first.</li></ul></li><li>Use the <code>base</code> file’s write ID as the current write ID, then iterate and filter <code>delta</code> files:<ul><li>If <code>maxWID</code> is larger than the current write ID, keep it, and update the current write ID;</li><li>If write ID range is the same as previous, keep the file, too.</li></ul></li></ul><p>There are some special cases in this process, e.g. no <code>base</code> file, multiple statements, contains original data files, even ACID version 1 files. More details can be found in <code>AcidUtils#getAcidState</code>.</p><h3 id="Parallel-Execution"><a href="#Parallel-Execution" class="headerlink" title="Parallel Execution"></a>Parallel Execution</h3><p>When executing in parallel environment, such as multiple Hadoop mappers, <code>delta</code> files need to be re-organized. In short, <code>base</code> and <code>delta</code> files can be divided into different splits, while all <code>delete</code> files have to be available to all splits. This ensures deleted records will not be emitted.</p><p><img src="/images/hive-acid/parallel-execution.png" alt="Parallel Execution"></p><h3 id="Vectorized-Query"><a href="#Vectorized-Query" class="headerlink" title="Vectorized Query"></a>Vectorized Query</h3><p>For <a href="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution" target="_blank" rel="noopener">vectoried query</a>, Hive will first try to load all <code>delete</code> files into memory and construct an optimized data structure that can be used to filter out deleted rows when processing row batches. If the <code>delete</code> files are too large, it falls back to sort-merge algorithm.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">VectorizedOrcAcidRowBatchReader</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> DeleteEventRegistry deleteEventRegistry;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">static</span> <span class="class"><span class="keyword">interface</span> <span class="title">DeleteEventRegistry</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">findDeletedRecords</span><span class="params">(ColumnVector[] cols, <span class="keyword">int</span> size, BitSet selectedBitSet)</span></span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ColumnizedDeleteEventRegistry</span> <span class="keyword">implements</span> <span class="title">DeleteEventRegistry</span> </span>&#123;&#125;</span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SortMergedDeleteEventRegistry</span> <span class="keyword">implements</span> <span class="title">DeleteEventRegistry</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">next</span><span class="params">(NullWritable key, VectorizedRowBatch value)</span> </span>&#123;</span><br><span class="line">    BitSet selectedBitSet = <span class="keyword">new</span> BitSet(vectorizedRowBatchBase.size);</span><br><span class="line">    <span class="keyword">this</span>.deleteEventRegistry.findDeletedRecords(innerRecordIdColumnVector,</span><br><span class="line">        vectorizedRowBatchBase.size, selectedBitSet);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> setBitIndex = selectedBitSet.nextSetBit(<span class="number">0</span>), selectedItr = <span class="number">0</span>;</span><br><span class="line">        setBitIndex &gt;= <span class="number">0</span>;</span><br><span class="line">        setBitIndex = selectedBitSet.nextSetBit(setBitIndex+<span class="number">1</span>), ++selectedItr) &#123;</span><br><span class="line">      value.selected[selectedItr] = setBitIndex;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Transaction-Management"><a href="#Transaction-Management" class="headerlink" title="Transaction Management"></a>Transaction Management</h2><p>Hive introduced a new lock manager to support transactional tables. <code>DbTxnManager</code> will detect the ACID operations in query plan and contact the Hive Metastore to open and commit new transactions. It also implements the read-write lock mechanism to support normal locking requirements.</p><p><img src="/images/hive-acid/transaction-management.png" alt="Transaction Management"></p><p>The Hive Metastore is responsible for allocating new transaction IDs. This is done in a database transaction so that multiple Metastore instances will not conflict with each other.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TxnHandler</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> List&lt;Long&gt; <span class="title">openTxns</span><span class="params">(Connection dbConn, Statement stmt, OpenTxnRequest rqst)</span> </span>&#123;</span><br><span class="line">    String s = sqlGenerator.addForUpdateClause(<span class="string">"select ntxn_next from NEXT_TXN_ID"</span>);</span><br><span class="line">    s = <span class="string">"update NEXT_TXN_ID set ntxn_next = "</span> + (first + numTxns);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">long</span> i = first; i &lt; first + numTxns; i++) &#123;</span><br><span class="line">      txnIds.add(i);</span><br><span class="line">      rows.add(i + <span class="string">","</span> + quoteChar(TXN_OPEN) + <span class="string">","</span> + now + <span class="string">","</span> + now + <span class="string">","</span></span><br><span class="line">          + quoteString(rqst.getUser()) + <span class="string">","</span> + quoteString(rqst.getHostname()) + <span class="string">","</span> + txnType.getValue());</span><br><span class="line">    &#125;</span><br><span class="line">    List&lt;String&gt; queries = sqlGenerator.createInsertValuesStmt(</span><br><span class="line">        <span class="string">"TXNS (txn_id, txn_state, txn_started, txn_last_heartbeat, txn_user, txn_host, txn_type)"</span>, rows);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions" target="_blank" rel="noopener">Hive Transactions</a></li><li><a href="https://www.slideshare.net/Hadoop_Summit/transactional-operations-in-apache-hive-present-and-future-102803358" target="_blank" rel="noopener">Transactional Operations in Apache Hive</a></li><li><a href="https://orc.apache.org/docs/acid.html" target="_blank" rel="noopener">ORCFile ACID Support</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://hive.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache Hive&lt;/a&gt; introduced transactions since version 0.13 to fully support ACID semantics on Hive table, including INSERT/UPDATE/DELETE/MERGE statements, streaming data ingestion, etc. In Hive 3.0, this feature is further improved by optimizing the underlying data file structure, reducing constraints on table scheme, and supporting predicate push down and vectorized query. Examples and setup can be found on &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hive wiki&lt;/a&gt; and other &lt;a href=&quot;https://hortonworks.com/tutorial/using-hive-acid-transactions-to-insert-update-and-delete-data/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;tutorials&lt;/a&gt;, while this article will focus on how transactional table is saved on HDFS, and take a closer look at the read-write process.&lt;/p&gt;
&lt;h2 id=&quot;File-Structure&quot;&gt;&lt;a href=&quot;#File-Structure&quot; class=&quot;headerlink&quot; title=&quot;File Structure&quot;&gt;&lt;/a&gt;File Structure&lt;/h2&gt;&lt;h3 id=&quot;Insert-Data&quot;&gt;&lt;a href=&quot;#Insert-Data&quot; class=&quot;headerlink&quot; title=&quot;Insert Data&quot;&gt;&lt;/a&gt;Insert Data&lt;/h3&gt;&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;TABLE&lt;/span&gt; employee (&lt;span class=&quot;keyword&quot;&gt;id&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;int&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;string&lt;/span&gt;, salary &lt;span class=&quot;built_in&quot;&gt;int&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;STORED&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AS&lt;/span&gt; ORC TBLPROPERTIES (&lt;span class=&quot;string&quot;&gt;&#39;transactional&#39;&lt;/span&gt; = &lt;span class=&quot;string&quot;&gt;&#39;true&#39;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;INSERT&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;INTO&lt;/span&gt; employee &lt;span class=&quot;keyword&quot;&gt;VALUES&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;Jerry&#39;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5000&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;Tom&#39;&lt;/span&gt;,   &lt;span class=&quot;number&quot;&gt;8000&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;Kate&#39;&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;6000&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;An INSERT statement is executed in a single transaction. It will create a &lt;code&gt;delta&lt;/code&gt; directory containing information about this transaction and its data.&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/user/hive/warehouse/employee/delta_0000001_0000001_0000&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/user/hive/warehouse/employee/delta_0000001_0000001_0000/_orc_acid_version&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/user/hive/warehouse/employee/delta_0000001_0000001_0000/bucket_00000&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;The schema of this folder’s name is &lt;code&gt;delta_minWID_maxWID_stmtID&lt;/code&gt;, i.e. “delta” prefix, transactional writes’ range (minimum and maximum write ID), and statement ID. In detail:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All INSERT statements will create a &lt;code&gt;delta&lt;/code&gt; directory. UPDATE statement will also create &lt;code&gt;delta&lt;/code&gt; directory right after a &lt;code&gt;delete&lt;/code&gt; directory. &lt;code&gt;delete&lt;/code&gt; directory is prefixed with “delete_delta”.&lt;/li&gt;
&lt;li&gt;Hive will assign a globally unique ID for every transaction, both read and write. For transactional writes like INSERT and DELETE, it will also assign a table-wise unique ID, a.k.a. a write ID. The write ID range will be encoded in the &lt;code&gt;delta&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt; directory names.&lt;/li&gt;
&lt;li&gt;Statement ID is used when multiple writes into the same table happen in one transaction.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="hive" scheme="http://shzhangji.com/tags/hive/"/>
    
      <category term="hadoop" scheme="http://shzhangji.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Real-time Exactly-once ETL with Apache Flink</title>
    <link href="http://shzhangji.com/blog/2018/12/23/real-time-exactly-once-etl-with-apache-flink/"/>
    <id>http://shzhangji.com/blog/2018/12/23/real-time-exactly-once-etl-with-apache-flink/</id>
    <published>2018-12-23T13:42:44.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink is another popular big data processing framework, which differs from Apache Spark in that Flink uses stream processing to mimic batch processing and provides sub-second latency along with exactly-once semantics. One of its use cases is to build a real-time data pipeline, move and transform data between different stores. This article will show you how to build such an application, and explain how Flink guarantees its correctness.</p><p><img src="/images/flink/arch.png" alt="Apache Flink"></p><h2 id="Demo-ETL-Application"><a href="#Demo-ETL-Application" class="headerlink" title="Demo ETL Application"></a>Demo ETL Application</h2><p>Let us build a project that extracts data from Kafka and loads them into HDFS. The result files should be stored in bucketed directories according to event time. Source messages are encoded in JSON, and the event time is stored as timestamp. Samples are:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;timestamp&quot;:1545184226.432,&quot;event&quot;:&quot;page_view&quot;,&quot;uuid&quot;:&quot;ac0e50bf-944c-4e2f-bbf5-a34b22718e0c&quot;&#125;</span><br><span class="line">&#123;&quot;timestamp&quot;:1545184602.640,&quot;event&quot;:&quot;adv_click&quot;,&quot;uuid&quot;:&quot;9b220808-2193-44d1-a0e9-09b9743dec55&quot;&#125;</span><br><span class="line">&#123;&quot;timestamp&quot;:1545184608.969,&quot;event&quot;:&quot;thumbs_up&quot;,&quot;uuid&quot;:&quot;b44c3137-4c91-4f36-96fb-80f56561c914&quot;&#125;</span><br></pre></td></tr></table></figure><p>The result directory structure should be:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/user/flink/event_log/dt=20181219/part-0-1</span><br><span class="line">/user/flink/event_log/dt=20181220/part-1-9</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="Create-Project"><a href="#Create-Project" class="headerlink" title="Create Project"></a>Create Project</h3><p>Flink application requires Java 8, and we can create a project from Maven template.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mvn archetype:generate \</span><br><span class="line">  -DarchetypeGroupId=org.apache.flink \</span><br><span class="line">  -DarchetypeArtifactId=flink-quickstart-java \</span><br><span class="line">  -DarchetypeVersion=1.7.0</span><br></pre></td></tr></table></figure><p>Import it into your favorite IDE, and we can see a class named <code>StreamingJob</code>. We will start from there.</p><h3 id="Kafka-Consumer-Source"><a href="#Kafka-Consumer-Source" class="headerlink" title="Kafka Consumer Source"></a>Kafka Consumer Source</h3><p>Flink provides <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/connectors/kafka.html" target="_blank" rel="noopener">native support</a> for consuming messages from Kafka. Choose the right version and add to dependencies:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.10_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>We need some Kafka server to bootstrap this source. For testing, one can follow the Kafka <a href="https://kafka.apache.org/quickstart" target="_blank" rel="noopener">official document</a> to setup a local broker. Create the source and pass the host and topic name.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">FlinkKafkaConsumer010&lt;String&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer010&lt;&gt;(</span><br><span class="line">    <span class="string">"flink_test"</span>, <span class="keyword">new</span> SimpleStringSchema(), props);</span><br><span class="line">DataStream&lt;String&gt; stream = env.addSource(consumer);</span><br></pre></td></tr></table></figure><p>Flink will read data from a local Kafka broker, with topic <code>flink_test</code>, and transform it into simple strings, indicated by <code>SimpleStringSchema</code>. There are other built-in deserialization schema like JSON and Avro, or you can create a custom one.</p><h3 id="Streaming-File-Sink"><a href="#Streaming-File-Sink" class="headerlink" title="Streaming File Sink"></a>Streaming File Sink</h3><p><code>StreamingFileSink</code> is replacing the previous <code>BucketingSink</code> to store data into HDFS in different directories. The key concept here is bucket assigner, which defaults to <code>DateTimeBucketAssigner</code>, that divides messages into timed buckets according to processing time, i.e. the time when messages arrive the operator. But what we want is to divide messages by event time, so we have to write one on our own.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EventTimeBucketAssigner</span> <span class="keyword">implements</span> <span class="title">BucketAssigner</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getBucketId</span><span class="params">(String element, Context context)</span> </span>&#123;</span><br><span class="line">    JsonNode node = mapper.readTree(element);</span><br><span class="line">    <span class="keyword">long</span> date = (<span class="keyword">long</span>) (node.path(<span class="string">"timestamp"</span>).floatValue() * <span class="number">1000</span>);</span><br><span class="line">    String partitionValue = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyyMMdd"</span>).format(<span class="keyword">new</span> Date(date));</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"dt="</span> + partitionValue;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>We create a bucket assigner that ingests a string, decodes with Jackson, extracts the timestamp, and returns the bucket name of this message. Then the sink will know where to put them. Full code can be found on GitHub (<a href="https://github.com/jizhang/flink-sandbox/blob/blog-etl/src/main/java/com/shzhangji/flinksandbox/kafka/EventTimeBucketAssigner.java" target="_blank" rel="noopener">link</a>).</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">StreamingFileSink&lt;String&gt; sink = StreamingFileSink</span><br><span class="line">    .forRowFormat(<span class="keyword">new</span> Path(<span class="string">"/tmp/kafka-loader"</span>), <span class="keyword">new</span> SimpleStringEncoder&lt;String&gt;())</span><br><span class="line">    .withBucketAssigner(<span class="keyword">new</span> EventTimeBucketAssigner())</span><br><span class="line">    .build();</span><br><span class="line">stream.addSink(sink);</span><br></pre></td></tr></table></figure><p>There is also a <code>forBulkFormat</code>, if you prefer storing data in a more compact way like Parquet.</p><p>A note on <code>StreamingFileSink</code> though, it only works with Hadoop 2.7 and above, because it requires the file system supporting <code>truncate</code>, which helps recovering the writing process from the last checkpoint.</p><h3 id="Enable-Checkpointing"><a href="#Enable-Checkpointing" class="headerlink" title="Enable Checkpointing"></a>Enable Checkpointing</h3><p>So far, the application can be put into work by invoking <code>env.execute()</code>, but it only guarantees at-least-once semantics. To achieve exactly-once, we simply turn on Flink’s checkpointing:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">env.enableCheckpointing(<span class="number">60_000</span>);</span><br><span class="line">env.setStateBackend((StateBackend) <span class="keyword">new</span> FsStateBackend(<span class="string">"/tmp/flink/checkpoints"</span>));</span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(</span><br><span class="line">    ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION);</span><br></pre></td></tr></table></figure><p>Checkpoint is Flink’s solution to fault tolerance, which we will cover later. Here we switch the state backend from default <code>MemoryStateBackend</code> to <code>FsStateBackend</code>, that stores state into filesystem like HDFS, instead of in memory, to help surviving job manager failure. Flink also recommends using <code>RocksDBStateBackend</code>, when job state is very large and requires incremental checkpointing.</p><h3 id="Submit-and-Manage-Jobs"><a href="#Submit-and-Manage-Jobs" class="headerlink" title="Submit and Manage Jobs"></a>Submit and Manage Jobs</h3><p>Flink application can be directly run in IDE, or you can setup a local <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/cluster_setup.html" target="_blank" rel="noopener">standalone cluster</a> and submit jobs with Flink CLI:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.shzhangji.flinksandbox.kafka.KafkaLoader target/flink-sandbox-0.1.0.jar</span><br></pre></td></tr></table></figure><p>We can check out the job information in Flink dashboard:</p><p><img src="/images/flink/dashboard.png" alt="Flink Dashboard"></p><h4 id="Cancel-and-Resume-Job-with-Savepoint"><a href="#Cancel-and-Resume-Job-with-Savepoint" class="headerlink" title="Cancel and Resume Job with Savepoint"></a>Cancel and Resume Job with Savepoint</h4><p>To cancel or restart the job, say we want to upgrade the code logic, we need to create a savepoint. A savepoint is like a checkpoint, storing state of the running tasks. But savepoint is usually manually created, for planned backup or upgrade, while checkpoint is managed by Flink to provide fault tolerance. The <code>cancel</code> sub-command accepts <code>-s</code> option to write savepoint into some directory.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flink cancel -s /tmp/flink/savepoints 1253cc85e5c702dbe963dd7d8d279038</span><br><span class="line">Cancelled job 1253cc85e5c702dbe963dd7d8d279038. Savepoint stored in file:/tmp/flink/savepoints/savepoint-1253cc-0df030f4f2ee.</span><br></pre></td></tr></table></figure><p>For our ETL application, savepoint will include current Kafka offsets, in-progress output file names, etc. To resume from a savepoint, pass <code>-s</code> to <code>run</code> sub-command. The application will start from the savepoint, i.e. consume messages right after the saved offsets, without losing or duplicating data.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink run -s /tmp/flink/savepoints/savepoint-1253cc-0df030f4f2ee -c com.shzhangji.flinksandbox.kafka.KafkaLoader target/flink-sandbox-0.1.0.jar</span><br></pre></td></tr></table></figure><h4 id="YARN-Support"><a href="#YARN-Support" class="headerlink" title="YARN Support"></a>YARN Support</h4><p>Running Flink jobs on YARN also uses <code>flink run</code>. Replace the file paths with HDFS prefix, re-package and run the following command:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ export HADOOP_CONF_DIR=/path/to/hadoop/conf</span><br><span class="line">$ bin/flink run -m yarn-cluster -c com.shzhangji.flinksandbox.kafka.KafkaLoader target/flink-sandbox-0.1.0.jar</span><br><span class="line">Submitted application application_1545534487726_0001</span><br></pre></td></tr></table></figure><p>Flink dashboard will run in YARN application master. The returned application ID can be used to manage the jobs through Flink CLI:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink cancel -s hdfs://localhost:9000/tmp/flink/savepoints -yid application_1545534487726_0001 84de00a5e193f26c937f72a9dc97f386</span><br></pre></td></tr></table></figure><h2 id="How-Flink-Guarantees-Exactly-once-Semantics"><a href="#How-Flink-Guarantees-Exactly-once-Semantics" class="headerlink" title="How Flink Guarantees Exactly-once Semantics"></a>How Flink Guarantees Exactly-once Semantics</h2><p>Flink streaming application can be divided into three parts, source, process, and sink. Different sources and sinks, or <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/connectors/guarantees.html" target="_blank" rel="noopener">connectors</a>, give different guarantees, and the Flink stream processing gives either at-least-once or exactly-once semantics, based on whether checkpointing is enabled.</p><h3 id="Stream-Processing-with-Checkpointing"><a href="#Stream-Processing-with-Checkpointing" class="headerlink" title="Stream Processing with Checkpointing"></a>Stream Processing with Checkpointing</h3><p>Flink’s checkpointing mechanism is based on Chandy-Lamport algorithm. It periodically inserts light-weight barriers into data stream, dividing the stream into sets of records. After an operator has processed all records in the current set, a checkpoint is made and sent to the coordinator, i.e. job manager. Then the operator will send this barrier to its down-streams. When all sinks finish checkpointing, this checkpoint is marked as completed, which means all data before the checkpoint has been properly processed, all operator states are saved, and the application can recover from this checkpoint when encountering failures.</p><p><img src="/images/flink/stream-barrier.png" alt="Stream Barrier"></p><p>For operators with multiple up-streams, a technique called stream aligning is applied. If one of the up-streams is delayed, the operator will stop processing data from other up-streams, until the slow one catches up. This guarantees exactly-once semantics of the operator state, but will certainly introduce some latency. Apart from this <code>EXACTLY_ONCE</code> mode of checkpointing, Flink also provides <code>AT_LEAST_ONCE</code> mode, to minimize the delay. One can refer to <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/internals/stream_checkpointing.html" target="_blank" rel="noopener">document</a> for further details.</p><h3 id="Rewindable-Data-Source"><a href="#Rewindable-Data-Source" class="headerlink" title="Rewindable Data Source"></a>Rewindable Data Source</h3><p>When recovering from the last checkpoint, Flink needs to re-fetch some messages, and data source like Kafka supports consuming messages from given offsets. In detail, <code>FlinkKafkaConsumer</code> implements the <code>CheckpointedFunction</code> and stores topic name, partition ID, and offsets in operator state.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaConsumerBase</span> <span class="keyword">implements</span> <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> </span>&#123;</span><br><span class="line">    OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line">    <span class="keyword">this</span>.unionOffsetStates = stateStore.getUnionListState(<span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">        OFFSETS_STATE_NAME,</span><br><span class="line">        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt;() &#123;&#125;)));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">      <span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : unionOffsetStates.get()) &#123;</span><br><span class="line">        restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> </span>&#123;</span><br><span class="line">    unionOffsetStates.clear();</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">  unionOffsetStates.add(Tuple2.of(kafkaTopicPartitionLongEntry.getKey(),</span><br><span class="line">          kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>When resuming a job from a savepoint, you can find the following lines in task manager logs, indicating that the source will consume from the offsets that are restored from checkpoint.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2018-12-23 10:56:47,380 INFO FlinkKafkaConsumerBase</span><br><span class="line">  Consumer subtask 0 will start reading 2 partitions with offsets in restored state:</span><br><span class="line">    &#123;KafkaTopicPartition&#123;topic=&apos;flink_test&apos;, partition=1&#125;=725,</span><br><span class="line">     KafkaTopicPartition&#123;topic=&apos;flink_test&apos;, partition=0&#125;=721&#125;</span><br></pre></td></tr></table></figure><h3 id="Recover-In-progress-Output-Files"><a href="#Recover-In-progress-Output-Files" class="headerlink" title="Recover In-progress Output Files"></a>Recover In-progress Output Files</h3><p>As the application runs, <code>StreamingFileSink</code> will first write to a temporary file, prefixed with dot and suffixed with <code>in-progress</code>. The in-progress files are renamed to normal files according to some <code>RollingPolicy</code>, which defaults to both time-based (60 seconds) and size-based (128 MB). When task failure happens, or job is canceled, the in-progress files are simply closed. During recovery, the sink can retrieve in-progress file names from checkpointed state, truncate the files to a specific length, so that they do not contain any data after the checkpoint, and then the stream processing can resume.</p><p>Take Hadoop file system for instance, the recovering process happens in <code>HadoopRecoverableFsDataOutputStream</code> class constructor. It is invoked with a <code>HadoopFsRecoverable</code> object that contains the temporary file name, target name, and offset. This object is a member of <code>BucketState</code>, which is stored in operator state.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HadoopRecoverableFsDataOutputStream(FileSystem fs, HadoopFsRecoverable recoverable) &#123;</span><br><span class="line">  <span class="keyword">this</span>.tempFile = checkNotNull(recoverable.tempFile());</span><br><span class="line">  truncate(fs, tempFile, recoverable.offset());</span><br><span class="line">  out = fs.append(tempFile);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>Apache Flink builds upon stream processing, state management is considered from day one, and it integrates well with Hadoop ecosystem. All of these make it a very competitive product in big data field. It is under active development, and gradually gains more features like table API, stream SQL, machine learning, etc. Big companies like Alibaba are also using and contributing to this project. It supports a wide range of use-cases, and is definitely worth a try.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Flink is another popular big data processing framework, which differs from Apache Spark in that Flink uses stream processing to mimic batch processing and provides sub-second latency along with exactly-once semantics. One of its use cases is to build a real-time data pipeline, move and transform data between different stores. This article will show you how to build such an application, and explain how Flink guarantees its correctness.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/flink/arch.png&quot; alt=&quot;Apache Flink&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Demo-ETL-Application&quot;&gt;&lt;a href=&quot;#Demo-ETL-Application&quot; class=&quot;headerlink&quot; title=&quot;Demo ETL Application&quot;&gt;&lt;/a&gt;Demo ETL Application&lt;/h2&gt;&lt;p&gt;Let us build a project that extracts data from Kafka and loads them into HDFS. The result files should be stored in bucketed directories according to event time. Source messages are encoded in JSON, and the event time is stored as timestamp. Samples are:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;timestamp&amp;quot;:1545184226.432,&amp;quot;event&amp;quot;:&amp;quot;page_view&amp;quot;,&amp;quot;uuid&amp;quot;:&amp;quot;ac0e50bf-944c-4e2f-bbf5-a34b22718e0c&amp;quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;timestamp&amp;quot;:1545184602.640,&amp;quot;event&amp;quot;:&amp;quot;adv_click&amp;quot;,&amp;quot;uuid&amp;quot;:&amp;quot;9b220808-2193-44d1-a0e9-09b9743dec55&amp;quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;timestamp&amp;quot;:1545184608.969,&amp;quot;event&amp;quot;:&amp;quot;thumbs_up&amp;quot;,&amp;quot;uuid&amp;quot;:&amp;quot;b44c3137-4c91-4f36-96fb-80f56561c914&amp;quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;The result directory structure should be:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/user/flink/event_log/dt=20181219/part-0-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/user/flink/event_log/dt=20181220/part-1-9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="flink" scheme="http://shzhangji.com/tags/flink/"/>
    
      <category term="etl" scheme="http://shzhangji.com/tags/etl/"/>
    
      <category term="kafka" scheme="http://shzhangji.com/tags/kafka/"/>
    
      <category term="java" scheme="http://shzhangji.com/tags/java/"/>
    
      <category term="hdfs" scheme="http://shzhangji.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>Spark DataSource API V2</title>
    <link href="http://shzhangji.com/blog/2018/12/08/spark-datasource-api-v2/"/>
    <id>http://shzhangji.com/blog/2018/12/08/spark-datasource-api-v2/</id>
    <published>2018-12-08T10:23:11.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p>From Spark 1.3, the team introduced a data source API to help quickly integrating various input formats with Spark SQL. But eventually this version of API became insufficient and the team needed to add a lot of internal codes to provide more efficient solutions for Spark SQL data sources. So in Spark 2.3, the second version of data source API is out, which is supposed to overcome the limitations of the previous version. In this article, I will demonstrate how to implement custom data source for Spark SQL in both V1 and V2 API, to help understanding their differences and the new API’s advantages.</p><h2 id="DataSource-V1-API"><a href="#DataSource-V1-API" class="headerlink" title="DataSource V1 API"></a>DataSource V1 API</h2><p>V1 API provides a set of abstract classes and traits. They are located in <a href="https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala" target="_blank" rel="noopener">spark/sql/sources/interfaces.scala</a>. Some basic APIs are:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">RelationProvider</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseRelation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span></span>: <span class="type">SQLContext</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TableScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>A <code>RelationProvider</code> defines a class that can create a relational data source for Spark SQL to manipulate with. It can initialize itself with provided options, such as file path or authentication. <code>BaseRelation</code> is used to define the data schema, which can be loaded from database, Parquet file, or specified by the user. This class also needs to mix-in one of the <code>Scan</code> traits, implements the <code>buildScan</code> method, and returns an RDD.</p><a id="more"></a><h3 id="JdbcSourceV1"><a href="#JdbcSourceV1" class="headerlink" title="JdbcSourceV1"></a>JdbcSourceV1</h3><p>Now we use V1 API to implement a JDBC data source. For simplicity, the table schema is hard coded, and it only supports full table scan. Complete example can be found on GitHub (<a href="https://github.com/jizhang/spark-sandbox/blob/master/src/main/scala/datasource/JdbcExampleV1.scala" target="_blank" rel="noopener">link</a>), while the sample data is in <a href="https://github.com/jizhang/spark-sandbox/blob/master/data/employee.sql" target="_blank" rel="noopener">here</a>.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcSourceV1</span> <span class="keyword">extends</span> <span class="title">RelationProvider</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">JdbcRelationV1</span>(parameters(<span class="string">"url"</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRelationV1</span>(<span class="params">url: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="keyword">with</span> <span class="title">TableScan</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">Seq</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"emp_name"</span>, <span class="type">StringType</span>)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = <span class="keyword">new</span> <span class="type">JdbcRDD</span>(url)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRDD</span>(<span class="params">url: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">RDD</span>[<span class="type">Row</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(): <span class="type">Iterator</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> conn = <span class="type">DriverManager</span>.getConnection(url)</span><br><span class="line">    <span class="keyword">val</span> stmt = conn.prepareStatement(<span class="string">"SELECT * FROM employee"</span>)</span><br><span class="line">    <span class="keyword">val</span> rs = stmt.executeQuery()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">Row</span>] &#123;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = rs.next()</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>: <span class="type">Row</span> = <span class="type">Row</span>(rs.getInt(<span class="string">"id"</span>), rs.getString(<span class="string">"emp_name"</span>))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The actual data reading happens in <code>JdbcRDD#compute</code>. It receives the connection options, possibly with pruned column list and where conditions, executes the query, and returns an iterator of <code>Row</code> objects, correspondent to the defined schema. Now we can create a <code>DataFrame</code> from this custom data source.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read</span><br><span class="line">  .format(<span class="string">"JdbcSourceV2"</span>)</span><br><span class="line">  .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://localhost/spark"</span>)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><p>The outputs are:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = true)</span><br><span class="line"> |-- emp_name: string (nullable = true)</span><br><span class="line"> |-- dep_name: string (nullable = true)</span><br><span class="line"> |-- salary: decimal(7,2) (nullable = true)</span><br><span class="line"> |-- age: decimal(3,0) (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+--------+----------+-------+---+</span><br><span class="line">| id|emp_name|  dep_name| salary|age|</span><br><span class="line">+---+--------+----------+-------+---+</span><br><span class="line">|  1| Matthew|Management|4500.00| 55|</span><br><span class="line">|  2|  Olivia|Management|4400.00| 61|</span><br><span class="line">|  3|   Grace|Management|4000.00| 42|</span><br><span class="line">|  4|     Jim|Production|3700.00| 35|</span><br><span class="line">|  5|   Alice|Production|3500.00| 24|</span><br><span class="line">+---+--------+----------+-------+---+</span><br></pre></td></tr></table></figure><h3 id="Limitations-of-V1-API"><a href="#Limitations-of-V1-API" class="headerlink" title="Limitations of V1 API"></a>Limitations of V1 API</h3><p>As we can see, V1 API is quite straightforward and can meet the initial requirements of Spark SQL use cases. But as Spark moves forward, V1 API starts to show its limitations.</p><h4 id="Coupled-with-Higher-Level-API"><a href="#Coupled-with-Higher-Level-API" class="headerlink" title="Coupled with Higher Level API"></a>Coupled with Higher Level API</h4><p><code>createRelation</code> accepts <code>SQLContext</code> as parameter; <code>buildScan</code> returns <code>RDD</code> of <code>Row</code>; and when implementing writable data source, the <code>insert</code> method accepts <code>DataFrame</code> type.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">InsertableRelation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(data: <span class="type">DataFrame</span>, overwrite: <span class="type">Boolean</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>These classes are of higher level of Spark API, and some of them have already upgraded, like <code>SQLContext</code> is superceded by <code>SparkSession</code>, and <code>DataFrame</code> is now an alias of <code>Dataset[Row]</code>. Data sources should not be required to reflect these changes.</p><h4 id="Hard-to-Add-New-Push-Down-Operators"><a href="#Hard-to-Add-New-Push-Down-Operators" class="headerlink" title="Hard to Add New Push Down Operators"></a>Hard to Add New Push Down Operators</h4><p>Besides <code>TableScan</code>, V1 API provides <code>PrunedScan</code> to eliminate unnecessary columns, and <code>PrunedFilteredScan</code> to push predicates down to data source. In <code>JdbcSourceV1</code>, they are reflected in the SQL statement.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRelationV1</span> <span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="keyword">with</span> <span class="title">PrunedFilteredScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>]) = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">JdbcRDD</span>(requiredColumns, filters)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRDD</span>(<span class="params">columns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>]</span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>() = &#123;</span><br><span class="line">    <span class="keyword">val</span> wheres = filters.flatMap &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">EqualTo</span>(attribute, value) =&gt; <span class="type">Some</span>(<span class="string">s"<span class="subst">$attribute</span> = '<span class="subst">$value</span>'"</span>)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">s"SELECT <span class="subst">$&#123;columns.mkString(", ")&#125;</span> FROM employee WHERE <span class="subst">$&#123;wheres.mkString(" AND ")&#125;</span>"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>What if we need to push down a new operator like <code>limit</code>? It will introduce a whole new set of <code>Scan</code> traits.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">LimitedScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(limit: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PrunedLimitedScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], limit: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PrunedFilteredLimitedScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>], limit: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Hard-to-Pass-Partition-Info"><a href="#Hard-to-Pass-Partition-Info" class="headerlink" title="Hard to Pass Partition Info"></a>Hard to Pass Partition Info</h4><p>For data sources that support partitioning like HDFS and Kafka, V1 API does not provide native support for partitioning and data locality. We need to achieve this by extending the RDD class. For instance, some Kafka topic contains several partitions, and we want the data reading task to be run on the servers where leader brokers reside.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaPartition</span>(<span class="params">partitionId: <span class="type">Int</span>, leaderHost: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Partition</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">index</span></span>: <span class="type">Int</span> = partitionId</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaRDD</span>(<span class="params">sc: <span class="type">SparkContext</span></span>) <span class="keyword">extends</span> <span class="title">RDD</span>[<span class="type">Row</span>](<span class="params">sc, <span class="type">Nil</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = <span class="type">Array</span>(</span><br><span class="line">    <span class="comment">// populate with Kafka PartitionInfo</span></span><br><span class="line">    <span class="type">KafkaPartition</span>(<span class="number">0</span>, <span class="string">"broker_0"</span>),</span><br><span class="line">    <span class="type">KafkaPartition</span>(<span class="number">1</span>, <span class="string">"broker_1"</span>)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Seq</span>(</span><br><span class="line">    split.asInstanceOf[<span class="type">KafkaPartition</span>].leaderHost</span><br><span class="line">  )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Besides, some database like Cassandra distributes data by primary key. If the query pipeline contains grouping on the columns, this information can be used by the optimizer to avoid shuffling. V2 API supports this with a dedicated trait.</p><h4 id="Lack-of-Transactional-Writing"><a href="#Lack-of-Transactional-Writing" class="headerlink" title="Lack of Transactional Writing"></a>Lack of Transactional Writing</h4><p>Spark tasks may fail, and with V1 API there will be partially written data. For file systems like HDFS, we can put a <code>_SUCCESS</code> file in the output directory to indicate if the job finishes successfully, but this process needs to be implemented by users, while V2 API provides explicit interfaces to support transactional writing.</p><h4 id="Lack-of-Columnar-and-Streaming-Support"><a href="#Lack-of-Columnar-and-Streaming-Support" class="headerlink" title="Lack of Columnar and Streaming Support"></a>Lack of Columnar and Streaming Support</h4><p>Columnar data and stream processing are both added to Spark SQL without using V1 API. Current implementations like <code>ParquetFileFormat</code> and <code>KafkaSource</code> are written in dedicated codes with internal APIs. These features are also addressed by V2 API.</p><h2 id="DataSource-V2-API"><a href="#DataSource-V2-API" class="headerlink" title="DataSource V2 API"></a>DataSource V2 API</h2><p>V2 API starts with a marker interface <code>DataSourceV2</code>. The class needs to be mixed-in with either <code>ReadSupport</code> or <code>WriteSupport</code>. <code>ReadSupport</code> interface, for instance, creates a <code>DataSourceReader</code> with initialization options; <code>DataSourceReader</code> reads schema of the data source, and returns a list of <code>DataReaderFactory</code>; the factory will create the actual <code>DataReader</code>, which works like an iterator. Besides, <code>DataSourceReader</code> can mix-in various <code>Support</code> interfaces, to apply query optimizations like operator push-down and columnar scan. For <code>WriteSupport</code> interfaces, the hierarchy is similar. All of them are written in Java for better interoperability.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataSourceV2</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ReadSupport</span> <span class="keyword">extends</span> <span class="title">DataSourceV2</span> </span>&#123;</span><br><span class="line">  <span class="function">DataSourceReader <span class="title">createReader</span><span class="params">(DataSourceOptions options)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line">  <span class="function">StructType <span class="title">readSchema</span><span class="params">()</span></span>;</span><br><span class="line">  List&lt;DataReaderFactory&lt;Row&gt;&gt; createDataReaderFactories();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">SupportsPushDownRequiredColumns</span> <span class="keyword">extends</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">pruneColumns</span><span class="params">(StructType requiredSchema)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataReaderFactory</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function">DataReader&lt;T&gt; <span class="title">createDataReader</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataReader</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">boolean</span> <span class="title">next</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function">T <span class="title">get</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>You may notice that <code>DataSourceReader#createDataReaderFactories</code> still relies on <code>Row</code> class, because currently only <code>Row</code> is supported, and V2 API is still marked as <code>Evolving</code>.</p><h3 id="JdbcSourceV2"><a href="#JdbcSourceV2" class="headerlink" title="JdbcSourceV2"></a>JdbcSourceV2</h3><p>Let us rewrite the JDBC data source with V2 API. The following is an abridged example of full table scan. Complete code can be found on GitHub (<a href="https://github.com/jizhang/spark-sandbox/blob/master/src/main/scala/datasource/JdbcExampleV2.scala" target="_blank" rel="noopener">link</a>).</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataSourceReader</span> <span class="keyword">extends</span> <span class="title">DataSourceReader</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">readSchema</span> </span>= <span class="type">StructType</span>(<span class="type">Seq</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"emp_name"</span>, <span class="type">StringType</span>)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">    <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(url)).asJava</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataReader</span>(<span class="params">url: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">DataReader</span>[<span class="type">Row</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> rs: <span class="type">ResultSet</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>() = &#123;</span><br><span class="line">    <span class="keyword">if</span> (rs == <span class="literal">null</span>) &#123;</span><br><span class="line">      conn = <span class="type">DriverManager</span>.getConnection(url)</span><br><span class="line">      <span class="keyword">val</span> stmt = conn.prepareStatement(<span class="string">"SELECT * FROM employee"</span>)</span><br><span class="line">      rs = stmt.executeQuery()</span><br><span class="line">    &#125;</span><br><span class="line">    rs.next()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span></span>() = <span class="type">Row</span>(rs.getInt(<span class="string">"id"</span>), rs.getString(<span class="string">"emp_name"</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Prune-Columns"><a href="#Prune-Columns" class="headerlink" title="Prune Columns"></a>Prune Columns</h4><p><code>DataSourceReader</code> can mix-in the <code>SupportsPushDownRequiredColumns</code> trait. Spark will invoke the <code>pruneColumns</code> method with required <code>StructType</code>, and <code>DataSourceReader</code> can pass it to underlying <code>DataReader</code>.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataSourceReader</span> <span class="keyword">with</span> <span class="title">SupportsPushDownRequiredColumns</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> requiredSchema = <span class="type">JdbcSourceV2</span>.schema</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pruneColumns</span></span>(requiredSchema: <span class="type">StructType</span>)  = &#123;</span><br><span class="line">    <span class="keyword">this</span>.requiredSchema = requiredSchema</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">    <span class="keyword">val</span> columns = requiredSchema.fields.map(_.name)</span><br><span class="line">    <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(columns)).asJava</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>We can examine the execution plan with <code>df.explain(true)</code>. For instance, the optimized logical plan of query <code>SELECT emp_name, age FROM employee</code> shows column pruning is pushed down to the data source.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">== Analyzed Logical Plan ==</span><br><span class="line">emp_name: string, age: decimal(3,0)</span><br><span class="line">Project [emp_name#1, age#4]</span><br><span class="line">+- SubqueryAlias employee</span><br><span class="line">   +- DataSourceV2Relation [id#0, emp_name#1, dep_name#2, salary#3, age#4], datasource.JdbcDataSourceReader@15ceeb42</span><br><span class="line"></span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">Project [emp_name#1, age#4]</span><br><span class="line">+- DataSourceV2Relation [emp_name#1, age#4], datasource.JdbcDataSourceReader@15ceeb42</span><br></pre></td></tr></table></figure><h4 id="Push-Down-Filters"><a href="#Push-Down-Filters" class="headerlink" title="Push Down Filters"></a>Push Down Filters</h4><p>Similarly, with <code>SupportsPushDownFilters</code>, we can add where conditions to the underlying SQL query.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcDataSourceReader</span> <span class="keyword">with</span> <span class="title">SupportsPushDownFilters</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> filters = <span class="type">Array</span>.empty[<span class="type">Filter</span>]</span><br><span class="line">  <span class="keyword">var</span> wheres = <span class="type">Array</span>.empty[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pushFilters</span></span>(filters: <span class="type">Array</span>[<span class="type">Filter</span>]) = &#123;</span><br><span class="line">    <span class="keyword">val</span> supported = <span class="type">ListBuffer</span>.empty[<span class="type">Filter</span>]</span><br><span class="line">    <span class="keyword">val</span> unsupported = <span class="type">ListBuffer</span>.empty[<span class="type">Filter</span>]</span><br><span class="line">    <span class="keyword">val</span> wheres = <span class="type">ListBuffer</span>.empty[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">    filters.foreach &#123;</span><br><span class="line">      <span class="keyword">case</span> filter: <span class="type">EqualTo</span> =&gt; &#123;</span><br><span class="line">        supported += filter</span><br><span class="line">        wheres += <span class="string">s"<span class="subst">$&#123;filter.attribute&#125;</span> = '<span class="subst">$&#123;filter.value&#125;</span>'"</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">case</span> filter =&gt; unsupported += filter</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.filters = supported.toArray</span><br><span class="line">    <span class="keyword">this</span>.wheres = wheres.toArray</span><br><span class="line">    unsupported.toArray</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pushedFilters</span> </span>= filters</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">    <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(wheres)).asJava</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Multiple-Partitions"><a href="#Multiple-Partitions" class="headerlink" title="Multiple Partitions"></a>Multiple Partitions</h4><p><code>createDataReaderFactories</code> returns a list. Each reader will output data for an RDD partition. Say we want to parallelize the data reading tasks, we can divide the records into two parts, according to primary key ranges.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataReaderFactories</span></span>() = &#123;</span><br><span class="line">  <span class="type">Seq</span>((<span class="number">1</span>, <span class="number">6</span>), (<span class="number">7</span>, <span class="number">11</span>)).map &#123; <span class="keyword">case</span> (minId, maxId) =&gt;</span><br><span class="line">    <span class="keyword">val</span> partition = <span class="string">s"id BETWEEN <span class="subst">$minId</span> AND <span class="subst">$maxId</span>"</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">JdbcDataReaderFactory</span>(partition)</span><br><span class="line">  &#125;.asJava</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Transactional-Write"><a href="#Transactional-Write" class="headerlink" title="Transactional Write"></a>Transactional Write</h3><p>V2 API provides two sets of <code>commit</code> / <code>abort</code> methods to implement transactional writes.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataSourceWriter</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">commit</span><span class="params">(WriterCommitMessage[] messages)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">abort</span><span class="params">(WriterCommitMessage[] messages)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DataWriter</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">write</span><span class="params">(T record)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">  <span class="function">WriterCommitMessage <span class="title">commit</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">abort</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>DataSourceWriter</code> is running on Spark driver, <code>DataWriter</code> on executor. When <code>DataWriter</code> succeeds in writing, it sends commit message to driver, and after <code>DataSourceWriter</code> collects all writers’ commit messages, it will do the final commit. If the writer task fails, <code>abort</code> will be called, and a new task will be retried. When the retries hit the maximum, <code>abort</code> will be called on all tasks.</p><h3 id="Columnar-and-Streaming-Support"><a href="#Columnar-and-Streaming-Support" class="headerlink" title="Columnar and Streaming Support"></a>Columnar and Streaming Support</h3><p>These features are currently still in experimental status and there is no concrete implementation yet. Briefly, <code>DataSourceReader</code> can mix-in <code>SupportsScanColumnarBatch</code> trait and creates <code>DataReaderFactory</code> that handles <code>ColumnarBatch</code>, an interface that Spark uses to represent columnar data. For streaming support, there are <code>MicroBatchReader</code> and <code>ContinuousReader</code> traits. One can refer to the <a href="https://github.com/apache/spark/blob/v2.3.2/sql/core/src/test/scala/org/apache/spark/sql/sources/v2/DataSourceV2Suite.scala" target="_blank" rel="noopener">unit tests</a> for more details.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="http://blog.madhukaraphatak.com/spark-datasource-v2-part-1/" target="_blank" rel="noopener">http://blog.madhukaraphatak.com/spark-datasource-v2-part-1/</a></li><li><a href="https://databricks.com/session/apache-spark-data-source-v2" target="_blank" rel="noopener">https://databricks.com/session/apache-spark-data-source-v2</a></li><li><a href="https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html</a></li><li><a href="https://developer.ibm.com/code/2018/04/16/introducing-apache-spark-data-sources-api-v2/" target="_blank" rel="noopener">https://developer.ibm.com/code/2018/04/16/introducing-apache-spark-data-sources-api-v2/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;From Spark 1.3, the team introduced a data source API to help quickly integrating various input formats with Spark SQL. But eventually this version of API became insufficient and the team needed to add a lot of internal codes to provide more efficient solutions for Spark SQL data sources. So in Spark 2.3, the second version of data source API is out, which is supposed to overcome the limitations of the previous version. In this article, I will demonstrate how to implement custom data source for Spark SQL in both V1 and V2 API, to help understanding their differences and the new API’s advantages.&lt;/p&gt;
&lt;h2 id=&quot;DataSource-V1-API&quot;&gt;&lt;a href=&quot;#DataSource-V1-API&quot; class=&quot;headerlink&quot; title=&quot;DataSource V1 API&quot;&gt;&lt;/a&gt;DataSource V1 API&lt;/h2&gt;&lt;p&gt;V1 API provides a set of abstract classes and traits. They are located in &lt;a href=&quot;https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;spark/sql/sources/interfaces.scala&lt;/a&gt;. Some basic APIs are:&lt;/p&gt;
&lt;figure class=&quot;highlight scala&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;trait&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;RelationProvider&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;createRelation&lt;/span&gt;&lt;/span&gt;(sqlContext: &lt;span class=&quot;type&quot;&gt;SQLContext&lt;/span&gt;, parameters: &lt;span class=&quot;type&quot;&gt;Map&lt;/span&gt;[&lt;span class=&quot;type&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;type&quot;&gt;String&lt;/span&gt;]): &lt;span class=&quot;type&quot;&gt;BaseRelation&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;abstract&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;BaseRelation&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;sqlContext&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;type&quot;&gt;SQLContext&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;schema&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;type&quot;&gt;StructType&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;trait&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;TableScan&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;buildScan&lt;/span&gt;&lt;/span&gt;(): &lt;span class=&quot;type&quot;&gt;RDD&lt;/span&gt;[&lt;span class=&quot;type&quot;&gt;Row&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;A &lt;code&gt;RelationProvider&lt;/code&gt; defines a class that can create a relational data source for Spark SQL to manipulate with. It can initialize itself with provided options, such as file path or authentication. &lt;code&gt;BaseRelation&lt;/code&gt; is used to define the data schema, which can be loaded from database, Parquet file, or specified by the user. This class also needs to mix-in one of the &lt;code&gt;Scan&lt;/code&gt; traits, implements the &lt;code&gt;buildScan&lt;/code&gt; method, and returns an RDD.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="spark" scheme="http://shzhangji.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Flume Source Code: HDFS Sink</title>
    <link href="http://shzhangji.com/blog/2018/10/03/flume-source-code-hdfs-sink/"/>
    <id>http://shzhangji.com/blog/2018/10/03/flume-source-code-hdfs-sink/</id>
    <published>2018-10-03T11:34:11.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p>Sink is the last component of Apache Flume data flow, and it is used to output data into storages like local files, HDFS, ElasticSearch, etc. In this article, I will illustrate how Flume’s HDFS sink works, by analyzing its source code with diagrams.</p><h2 id="Sink-Component-Lifecycle"><a href="#Sink-Component-Lifecycle" class="headerlink" title="Sink Component Lifecycle"></a>Sink Component Lifecycle</h2><p>In the <a href="http://shzhangji.com/blog/2017/10/23/flume-source-code-component-lifecycle/">previous article</a>, we learnt that every Flume component implements <code>LifecycleAware</code> interface, and is started and monitored by <code>LifecycleSupervisor</code>. Sink component is not directly invoked by this supervisor, but wrapped in <code>SinkRunner</code> and <code>SinkProcessor</code> classes. Flume supports three different <a href="https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors" target="_blank" rel="noopener">sink processors</a>, to connect channel and sinks in different semantics. But here we only consider the <code>DefaultSinkProcessor</code>, that accepts only one sink, and we will skip the concept of sink group as well.</p><p><img src="/images/flume/sink-component-lifecycle.png" alt="Sink Component LifeCycle"></p><a id="more"></a><h2 id="HDFS-Sink-Classes"><a href="#HDFS-Sink-Classes" class="headerlink" title="HDFS Sink Classes"></a>HDFS Sink Classes</h2><p>HDFS sink’s source code locates in <code>flume-hdfs-sink</code> sub-module, and is composed of the following classes:</p><p><img src="/images/flume/hdfs-sink-classes.png" alt="HDFS Sink Classes"></p><p><code>HDFSEventSink</code> class implements the lifecycle methods, including <code>configure</code>, <code>start</code>, <code>process</code>, and <code>stop</code>. It maintains a list of <code>BucketWriter</code>, according to the output file paths, and delegates received events to them. With different implementations of <code>HDFSWriter</code>, <code>BucketWriter</code> can append data to either text file, compressed file, or sequence file.</p><h2 id="Configure-and-Start"><a href="#Configure-and-Start" class="headerlink" title="Configure and Start"></a>Configure and Start</h2><p>When Flume configuration file is loaded, <code>configure</code> method is called on every sink component. In <code>HDFSEventSink#configure</code>, it reads properties that are prefixed with <code>hdfs.</code> from the context, provides default values, and does some sanity checks. For instance, <code>batchSize</code> must be greater than 0, <code>codeC</code> must be provided when <code>fileType</code> is <code>CompressedStream</code>, etc. It also initializes a <code>SinkCounter</code> to provide various metrics for monitoring.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">  filePath = Preconditions.checkNotNull(</span><br><span class="line">      context.getString(<span class="string">"hdfs.path"</span>), <span class="string">"hdfs.path is required"</span>);</span><br><span class="line">  rollInterval = context.getLong(<span class="string">"hdfs.rollInterval"</span>, defaultRollInterval);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (sinkCounter == <span class="keyword">null</span>) &#123;</span><br><span class="line">    sinkCounter = <span class="keyword">new</span> SinkCounter(getName());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>SinkProcessor</code> will invoke the <code>HDFSEventSink#start</code> method, in which two thread pools are created. <code>callTimeoutPool</code> is used by <code>BucketWriter#callWithTimeout</code> to limit the time that HDFS calls may take, such as <a href="http://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/fs/FileSystem.html" target="_blank" rel="noopener"><code>FileSystem#create</code></a>, or <a href="https://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/fs/FSDataOutputStream.html" target="_blank" rel="noopener"><code>FSDataOutputStream#hflush</code></a>. <code>timedRollerPool</code> is used to schedule a periodic task to do time-based file rolling, if <code>rollInterval</code> property is provided. More details will be covered in the next section.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  callTimeoutPool = Executors.newFixedThreadPool(threadsPoolSize,</span><br><span class="line">      <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(timeoutName).build());</span><br><span class="line">  timedRollerPool = Executors.newScheduledThreadPool(rollTimerPoolSize,</span><br><span class="line">      <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(rollerName).build());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Process-Events"><a href="#Process-Events" class="headerlink" title="Process Events"></a>Process Events</h2><p>The <code>process</code> method contains the main logic, i.e. pull events from upstream channel and send them to HDFS. Here is the flow chart of this method.</p><p><img src="/images/flume/process-method-flow-chart.png" alt="Process Method Flow Chart"></p><h3 id="Channel-Transaction"><a href="#Channel-Transaction" class="headerlink" title="Channel Transaction"></a>Channel Transaction</h3><p>Codes are wrapped in a channel transaction, with some exception handlings. Take Kafka channel for instance, when transaction begins, it takes events without committing the offset. Only after it successfully writes these events into HDFS, the consumed offset will be sent to Kafka. And in the next transaction, it can consume messages from the new offset.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Channel channel = getChannel();</span><br><span class="line">Transaction transaction = channel.getTransaction();</span><br><span class="line">transaction.begin()</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  event = channel.take();</span><br><span class="line">  bucketWriter.append(event);</span><br><span class="line">  transaction.commit()</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable th) &#123;</span><br><span class="line">  transaction.rollback();</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> EventDeliveryException(th);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  transaction.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Find-or-Create-BucketWriter"><a href="#Find-or-Create-BucketWriter" class="headerlink" title="Find or Create BucketWriter"></a>Find or Create BucketWriter</h3><p><code>BucketWriter</code> corresponds to an HDFS file, and the file path is generated from configuration. For example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a1.sinks.access_log.hdfs.path = /user/flume/access_log/dt=%Y%m%d</span><br><span class="line">a1.sinks.access_log.hdfs.filePrefix = events.%[localhost]</span><br><span class="line">a1.sinks.access_log.hdfs.inUsePrefix = .</span><br><span class="line">a1.sinks.access_log.hdfs.inUseSuffix = .tmp</span><br><span class="line">a1.sinks.access_log.hdfs.rollInterval = 300</span><br><span class="line">a1.sinks.access_log.hdfs.fileType = CompressedStream</span><br><span class="line">a1.sinks.access_log.hdfs.codeC = lzop</span><br></pre></td></tr></table></figure><p>The generated file paths, temporary and final, will be:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/user/flume/access_log/dt=20180925/.events.hostname1.1537848761307.lzo.tmp</span><br><span class="line">/user/flume/access_log/dt=20180925/events.hostname1.1537848761307.lzo</span><br></pre></td></tr></table></figure><p>Placeholders are replaced in <a href="https://flume.apache.org/releases/content/1.4.0/apidocs/org/apache/flume/formatter/output/BucketPath.html" target="_blank" rel="noopener"><code>BucketPath#escapeString</code></a>. It supports three kinds of placeholders:</p><ul><li><code>%{...}</code>: replace with arbitrary header values;</li><li><code>%[...]</code>: currently only supports <code>%[localhost]</code>, <code>%[ip]</code>, and <code>%[fqdn]</code>;</li><li><code>%x</code>: date time patterns, which requires a <code>timestamp</code> entry in headers, or <code>useLocalTimeStamp</code> is enabled.</li></ul><p>And the prefix and suffix is added in <code>BucketWriter#open</code>. <code>counter</code> is the timestamp when this bucket is opened or re-opened, and <code>lzo</code> is the default extension of the configured compression codec.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">String fullFileName = fileName + <span class="string">"."</span> + counter;</span><br><span class="line">fullFileName += fileSuffix;</span><br><span class="line">fullFileName += codeC.getDefaultExtension();</span><br><span class="line">bucketPath = filePath + <span class="string">"/"</span> + inUsePrefix + fullFileName + inUseSuffix;</span><br><span class="line">targetPath = filePath + <span class="string">"/"</span> + fullFileName;</span><br></pre></td></tr></table></figure><p>If no <code>BucketWriter</code> is associated with the file path, a new one will be created. First, it creates an <code>HDFSWriter</code> corresponding to the <code>fileType</code> config. Flume supports three kinds of writers: <code>HDFSSequenceFile</code>, <code>HDFSDataStream</code>, and <code>HDFSCompressedDataStream</code>. They handle the actual writing to HDFS files, and will be assigned to the new <code>BucketWriter</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bucketWriter = sfWriters.get(lookupPath);</span><br><span class="line"><span class="keyword">if</span> (bucketWriter == <span class="keyword">null</span>) &#123;</span><br><span class="line">  hdfsWriter = writerFactory.getWriter(fileType);</span><br><span class="line">  bucketWriter = <span class="keyword">new</span> BucketWriter(hdfsWriter);</span><br><span class="line">  sfWriters.put(lookupPath, bucketWriter);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Append-Data-and-Flush"><a href="#Append-Data-and-Flush" class="headerlink" title="Append Data and Flush"></a>Append Data and Flush</h3><p>Before appending data, <code>BucketWriter</code> will first self-check whether it is opened. If not, it will call its underlying <code>HDFSWriter</code> to open a new file on HDFS filesystem. Take <code>HDFSCompressedDataStream</code> for instance:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(String filePath, CompressionCodec codec)</span> </span>&#123;</span><br><span class="line">  FileSystem hdfs = dstPath.getFileSystem(conf);</span><br><span class="line">  fsOut = hdfs.append(dstPath)</span><br><span class="line">  compressor = CodedPool.getCompressor(codec, conf);</span><br><span class="line">  cmpOut = codec.createOutputStream(fsOut, compressor);</span><br><span class="line">  serializer = EventSerializerFactory.getInstance(serializerType, cmpOut);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IO Exception </span>&#123;</span><br><span class="line">  serializer.write(event);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Flume’s default <code>serializerType</code> is <code>TEXT</code>, i.e. <a href="https://flume.apache.org/releases/content/1.4.0/apidocs/org/apache/flume/serialization/BodyTextEventSerializer.html" target="_blank" rel="noopener">BodyTextEventSerializer</a> that simply writes the event content to the output stream.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  out.write(e.getBody());</span><br><span class="line">  <span class="keyword">if</span> (appendNewline) &#123;</span><br><span class="line">    out.write(<span class="string">'\n'</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>When <code>BucketWriter</code> is about to close or re-open, it calls <code>sync</code> on <code>HDFSWrtier</code>, which in turn calls <code>flush</code> on serializer and underlying output stream.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sync</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  serializer.flush();</span><br><span class="line">  compOut.finish();</span><br><span class="line">  fsOut.flush();</span><br><span class="line">  hflushOrSync(fsOut);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>From Hadoop 0.21.0, the <a href="https://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/fs/Syncable.html" target="_blank" rel="noopener"><code>Syncable#sync</code></a> method is divided into <code>hflush</code> and <code>hsync</code> methods. Former just flushes data out of client’s buffer, while latter guarantees data is synced to disk device. In order to handle both old and new API, Flume will use Java reflection to determine whether <code>hflush</code> exists, or fall back to <code>sync</code>. The <code>flushOrSync</code> method will invoke the right method.</p><h3 id="File-Rotation"><a href="#File-Rotation" class="headerlink" title="File Rotation"></a>File Rotation</h3><p>In HDFS sink, files can be rotated by file size, event count, or time interval. <code>BucketWriter#shouldRotate</code> is called in every <code>append</code>:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">shouldRotate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">boolean</span> doRotate = <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">if</span> ((rollCount &gt; <span class="number">0</span>) &amp;&amp; (rollCount &lt;= eventCounter)) &#123;</span><br><span class="line">    doRotate = <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> ((rollSize &gt; <span class="number">0</span>) &amp;&amp; (rollSize &lt;= processSize)) &#123;</span><br><span class="line">    doRotate = <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> doRotate;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Time-based rolling, on the other hand, is scheduled in the previously mentioned <code>timedRollerPool</code>:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (rollInterval &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    Callable&lt;Void&gt; action = <span class="keyword">new</span> Callable&lt;Void&gt;() &#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> Void <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        close(<span class="keyword">true</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    timedRollFuture = timedRollerPool.schedule(action, rollInterval);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Close-and-Stop"><a href="#Close-and-Stop" class="headerlink" title="Close and Stop"></a>Close and Stop</h2><p>In <code>HDFSEventSink#close</code>, it iterates every <code>BucketWriter</code> and calls its <code>close</code> method, which in turn calls its underlying <code>HDFSWriter</code>‘s <code>close</code> method. What it does is mostly like <code>flush</code> method, but also closes the output stream and invokes some callback functions, like removing current <code>BucketWriter</code> from the <code>sfWriters</code> hash map.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(<span class="keyword">boolean</span> callCloseCallback)</span> </span>&#123;</span><br><span class="line">  writer.close();</span><br><span class="line">  timedRollFuture.cancel(<span class="keyword">false</span>);</span><br><span class="line">  onCloseCallback.run(onCloseCallbackPath);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The <code>onCloseCallback</code> is passed from <code>HDFSEventSink</code> when initializing the <code>BucketWriter</code>:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">WriterCallback closeCallback = <span class="keyword">new</span> WriterCallback() &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(String bucketPath)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">synchronized</span> (sfWritersLock) &#123;</span><br><span class="line">        sfWriters.remove(bucketPath);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">bucketWriter = <span class="keyword">new</span> BucketWriter(lookPath, closeCallback);</span><br></pre></td></tr></table></figure><p>After all <code>BucketWriter</code>s are closed, <code>HDFSEventSink</code> then shutdown the <code>callTimeoutPool</code> and <code>timedRollerPool</code> executer services.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ExecutorService[] toShutdown = &#123; callTimeoutPool, timedRollerPool &#125;;</span><br><span class="line"><span class="keyword">for</span> (ExecutorService execService : toShutdown) &#123;</span><br><span class="line">  execService.shutdown();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://flume.apache.org/FlumeUserGuide.html#hdfs-sink" target="_blank" rel="noopener">https://flume.apache.org/FlumeUserGuide.html#hdfs-sink</a></li><li><a href="https://github.com/apache/flume" target="_blank" rel="noopener">https://github.com/apache/flume</a></li><li><a href="https://data-flair.training/blogs/flume-sink-processors/" target="_blank" rel="noopener">https://data-flair.training/blogs/flume-sink-processors/</a></li><li><a href="http://hadoop-hbase.blogspot.com/2012/05/hbase-hdfs-and-durable-sync.html" target="_blank" rel="noopener">http://hadoop-hbase.blogspot.com/2012/05/hbase-hdfs-and-durable-sync.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Sink is the last component of Apache Flume data flow, and it is used to output data into storages like local files, HDFS, ElasticSearch, etc. In this article, I will illustrate how Flume’s HDFS sink works, by analyzing its source code with diagrams.&lt;/p&gt;
&lt;h2 id=&quot;Sink-Component-Lifecycle&quot;&gt;&lt;a href=&quot;#Sink-Component-Lifecycle&quot; class=&quot;headerlink&quot; title=&quot;Sink Component Lifecycle&quot;&gt;&lt;/a&gt;Sink Component Lifecycle&lt;/h2&gt;&lt;p&gt;In the &lt;a href=&quot;http://shzhangji.com/blog/2017/10/23/flume-source-code-component-lifecycle/&quot;&gt;previous article&lt;/a&gt;, we learnt that every Flume component implements &lt;code&gt;LifecycleAware&lt;/code&gt; interface, and is started and monitored by &lt;code&gt;LifecycleSupervisor&lt;/code&gt;. Sink component is not directly invoked by this supervisor, but wrapped in &lt;code&gt;SinkRunner&lt;/code&gt; and &lt;code&gt;SinkProcessor&lt;/code&gt; classes. Flume supports three different &lt;a href=&quot;https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;sink processors&lt;/a&gt;, to connect channel and sinks in different semantics. But here we only consider the &lt;code&gt;DefaultSinkProcessor&lt;/code&gt;, that accepts only one sink, and we will skip the concept of sink group as well.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/flume/sink-component-lifecycle.png&quot; alt=&quot;Sink Component LifeCycle&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="flume" scheme="http://shzhangji.com/tags/flume/"/>
    
      <category term="java" scheme="http://shzhangji.com/tags/java/"/>
    
      <category term="hdfs" scheme="http://shzhangji.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>How to Avoid NullPointerException</title>
    <link href="http://shzhangji.com/blog/2018/09/20/how-to-avoid-null-pointer-exception/"/>
    <id>http://shzhangji.com/blog/2018/09/20/how-to-avoid-null-pointer-exception/</id>
    <published>2018-09-20T03:25:16.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p><code>NullPointerException</code> happens when you dereference a possible <code>null</code> object without checking it. It’s a common exception that every Java programmer may encounter in daily work. There’re several strategies that can help us avoid this exception, making our codes more robust. In this article, I will list both traditional ways and those with tools and new features introduced by recent version of Java.</p><h2 id="Runtime-Check"><a href="#Runtime-Check" class="headerlink" title="Runtime Check"></a>Runtime Check</h2><p>The most obvious way is to use <code>if (obj == null)</code> to check every variable you need to use, either from function argument, return value, or instance field. When you receive a <code>null</code> object, you can throw a different, more informative exception like <code>IllegalArgumentException</code>. There are some library functions that can make this process easier, like <a href="https://docs.oracle.com/javase/7/docs/api/java/util/Objects.html" target="_blank" rel="noopener"><code>Objects#requireNonNull</code></a>:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testObjects</span><span class="params">(Object arg)</span> </span>&#123;</span><br><span class="line">  Object checked = Objects.requireNonNull(arg, <span class="string">"arg must not be null"</span>);</span><br><span class="line">  checked.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Or use Guava’s <a href="https://github.com/google/guava/wiki/PreconditionsExplained" target="_blank" rel="noopener"><code>Preconditions</code></a> package, which provides all kinds of arguments checking facilities:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testGuava</span><span class="params">(Object arg)</span> </span>&#123;</span><br><span class="line">  Object checked = Preconditions.checkNotNull(arg, <span class="string">"%s must not be null"</span>, <span class="string">"arg"</span>);</span><br><span class="line">  checked.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>We can also let <a href="https://projectlombok.org/features/NonNull" target="_blank" rel="noopener">Lombok</a> generate the check for us, which will throw a more meaningful <code>NullPointerException</code>:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testLombok</span><span class="params">(@NonNull Object arg)</span> </span>&#123;</span><br><span class="line">  arg.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The generated code and exception message are as follows:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testLombokGenerated</span><span class="params">(Object arg)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (arg == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException(<span class="string">"arg is marked @NonNull but is null"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  arg.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This annotation can also be added to a class field, and Lombok will check nullness for every assignment.</p><a id="more"></a><h2 id="Coding-Convention"><a href="#Coding-Convention" class="headerlink" title="Coding Convention"></a>Coding Convention</h2><p>There are some coding conventions we can use to avoid <code>NullPointerException</code>.</p><ul><li>Use methods that already guard against <code>null</code> values, such as <code>String#equals</code>, <code>String#valueOf</code>, and third party libraries that help us check whether string or collection is empty.</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (str != <span class="keyword">null</span> &amp;&amp; str.equals(<span class="string">"text"</span>)) &#123;&#125;</span><br><span class="line"><span class="keyword">if</span> (<span class="string">"text"</span>.equals(str)) &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (obj != <span class="keyword">null</span>) &#123; obj.toString(); &#125;</span><br><span class="line">String.valueOf(obj); <span class="comment">// "null"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// from spring-core</span></span><br><span class="line">StringUtils.isEmpty(str);</span><br><span class="line">CollectionUtils.isEmpty(col);</span><br><span class="line"><span class="comment">// from guava</span></span><br><span class="line">Strings.isNullOrEmpty(str);</span><br><span class="line"><span class="comment">// from commons-collections4</span></span><br><span class="line">CollectionUtils.isEmpty(col);</span><br></pre></td></tr></table></figure><ul><li>If a method accepts nullable value, define two methods with different signatures, so as to make every parameter mandatory.</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">methodA</span><span class="params">(Object arg1)</span> </span>&#123;</span><br><span class="line">  methodB(arg1, <span class="keyword">new</span> Object[<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">methodB</span><span class="params">(Object arg1, Object[] arg2)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (Object obj : arg2) &#123;&#125; <span class="comment">// no null check</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>For return values, if the type is <code>Collection</code>, return an empty collection instead of null; if it’s a single object, consider throwing an exception. This approach is also suggested by <em>Effective Java</em>. Good examples come from Spring’s JdbcTemplate:</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// return new ArrayList&lt;&gt;() when result set is empty</span></span><br><span class="line">jdbcTemplate.queryForList(<span class="string">"SELECT 1"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// throws EmptyResultDataAccessException when record not found</span></span><br><span class="line">jdbcTemplate.queryForObject(<span class="string">"SELECT 1"</span>, Integer.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// works for generics</span></span><br><span class="line"><span class="keyword">public</span> &lt;T&gt; <span class="function">List&lt;T&gt; <span class="title">testReturnCollection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> Collections.emptyList();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Static-Check"><a href="#Static-Check" class="headerlink" title="Static Check"></a>Static Check</h2><p>Java has some static code analysis tools, like Eclipse IDE, SpotBugs, Checker Framework, etc. They can find out program bugs during compilation process. It would be nice to catch <code>NullPointerException</code> as early as possible, and this can be done with annotations like <code>@Nullable</code> and <code>@Nonnull</code>.</p><p>However, nullness check annotations have not been standardized yet. Though there was a <a href="https://jcp.org/en/jsr/detail?id=305" target="_blank" rel="noopener">JSR 305</a> proposed back to Sep. 2006, it has been dormant ever since. A lot of third party libraries provide such annotations, and they are supported by different tools. Some popular candidates are:</p><ul><li><code>javax.annotation.Nonnull</code>, proposed by JSR 305, and its reference implementation is <code>com.google.code.findbugs.jsr305</code>.</li><li><code>org.eclipse.jdt.annotation.NonNull</code>, used by Eclipse IDE to do static nullness check.</li><li><code>edu.umd.cs.findbugs.annotations.NonNull</code>, used by SpotBugs, it depends on <code>jsr305</code>.</li><li><code>org.springframework.lang.NonNull</code>, provided by Spring Framework.</li><li><code>org.checkerframework.checker.nullness.qual.NonNull</code>, used by Checker Framework.</li><li><code>android.support.annotation.NonNull</code>, used by Android Development Toolkit.</li></ul><p>I suggest using a cross IDE solution like SpotBugs or Checker Framework, which also plays nicely with Maven.</p><h3 id="NonNull-and-CheckForNull-with-SpotBugs"><a href="#NonNull-and-CheckForNull-with-SpotBugs" class="headerlink" title="@NonNull and @CheckForNull with SpotBugs"></a><code>@NonNull</code> and <code>@CheckForNull</code> with SpotBugs</h3><p>SpotBugs is the successor of FindBugs. We can use <code>@NonNull</code> and <code>@CheckForNull</code> on method arguments or return values, so as to apply nullness check. Notably, SpotBugs does not respect <code>@Nullable</code>, which is only useful when overriding <code>@ParametersAreNullableByDefault</code>. Use <code>@CheckForNull</code> instead.</p><p>To integrate SpotBugs with Maven and Eclipse, one can refer to its <a href="https://spotbugs.readthedocs.io/en/latest/maven.html" target="_blank" rel="noopener">official document</a>. Make sure you add the <code>spotbugs-annotations</code> package in Maven dependencies, which includes the nullness check annotations.</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.github.spotbugs<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spotbugs-annotations<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Here are the examples of different scenarios.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@NonNull</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Object <span class="title">returnNonNull</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// ERROR: returnNonNull() may return null, but is declared @Nonnull</span></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@CheckForNull</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Object <span class="title">returnNullable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testReturnNullable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Object obj = returnNullable();</span><br><span class="line">  <span class="comment">// ERROR: Possible null pointer dereference due to return value of called method</span></span><br><span class="line">  System.out.println(obj.toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">argumentNonNull</span><span class="params">(@NonNull Object arg)</span> </span>&#123;</span><br><span class="line">  System.out.println(arg.toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testArgumentNonNull</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// ERROR: Null passed for non-null parameter of argumentNonNull(Object)</span></span><br><span class="line">  argumentNonNull(<span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testNullableArgument</span><span class="params">(@CheckForNull Object arg)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// ERROR: arg must be non-null but is marked as nullable</span></span><br><span class="line">  System.out.println(arg.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>For Eclipse users, it is also possible to use its built-in nullness check along with SpotBugs. By default, Eclipse uses annotations under its own package, i.e. <code>org.eclipse.jdt.annotation.Nullable</code>, but we can easily add more annotations.</p><p><img src="/images/java-npe/eclipse.png" alt="Eclipse null analysis"></p><h3 id="NonNull-and-Nullable-with-Checker-Framework"><a href="#NonNull-and-Nullable-with-Checker-Framework" class="headerlink" title="@NonNull and @Nullable with Checker Framework"></a><code>@NonNull</code> and <code>@Nullable</code> with Checker Framework</h3><p>Checker Framework works as a plugin to the <code>javac</code> compiler, to provide type checks, detect and prevent various errors. Follow the <a href="https://checkerframework.org/manual/#maven" target="_blank" rel="noopener">official document</a>, integrate Checker Framework with <code>maven-compiler-plugin</code>, and it will start to work when executing <code>mvn compile</code>. The Nullness Checker supports all kinds of annotations, from JSR 305 to Eclipse built-ins, even <code>lombok.NonNull</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.checkerframework.checker.nullness.qual.Nullable;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Nullable</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Object <span class="title">returnNullable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testReturnNullable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Object obj = returnNullable();</span><br><span class="line">  <span class="comment">// ERROR: dereference of possibly-null reference obj</span></span><br><span class="line">  System.out.println(obj.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>By default, Checker Framework applies <code>@NonNull</code> to all method arguments and return values. The following snippet, without any annotations, cannot pass compilation, either.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Object <span class="title">returnNonNull</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// ERROR: incompatible types in return.</span></span><br><span class="line">  <span class="comment">// found: null, required: @Initialized @NonNull Object.</span></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">argumentNonNull</span><span class="params">(Object arg)</span> </span>&#123;</span><br><span class="line">  System.out.println(arg.toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testArgumentNonNull</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// ERROR: incompatible types in argument.</span></span><br><span class="line">  <span class="comment">// found: null, required: @Initialized @NonNull Object</span></span><br><span class="line">  argumentNonNull(<span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Checker Framework is especially useful for Spring Framework users, because from version 5.x, Spring provides built-in annotations for nullness check, and they are all over the framework code itself, mainly for Kotlin users, but we Java programmers can benefit from them, too. Take <code>StringUtils</code> class for instance, since the whole package is declared <code>@NonNull</code>, those methods with nullable argument and return values are explicitly annotated with <code>@Nullable</code>, so the following code will cause compilation failure.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Defined in spring-core</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">StringUtils</span> </span>&#123;</span><br><span class="line">  <span class="comment">// str inherits @NonNull from top-level package</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">capitalize</span><span class="params">(String str)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Nullable</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getFilename</span><span class="params">(@Nullable String path)</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ERROR: incompatible types in argument. found null, required @NonNull</span></span><br><span class="line">StringUtils.capitalize(<span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">String filename = StringUtils.getFilename(<span class="string">"/path/to/file"</span>);</span><br><span class="line"><span class="comment">// ERROR: dereference of possibly-null reference filename</span></span><br><span class="line">System.out.println(filename.length());</span><br></pre></td></tr></table></figure><h2 id="Optional-Class"><a href="#Optional-Class" class="headerlink" title="Optional Class"></a>Optional Class</h2><p>Java 8 introduces the <code>Optional&lt;T&gt;</code> class that can be used to wrap a nullable return value, instead of returning null or throwing an exception. On the upside, a method that returns <code>Optional</code> explicitly states it may return an empty value, so the invoker must check the presence of the value, and no NPE will be thrown. However, it does introduce more codes, and adds some overhead of object creation. So use it with caution.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Optional&lt;String&gt; opt;</span><br><span class="line"></span><br><span class="line"><span class="comment">// create</span></span><br><span class="line">opt = Optional.empty();</span><br><span class="line">opt = Optional.of(<span class="string">"text"</span>);</span><br><span class="line">opt = Optional.ofNullable(<span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// test &amp; get</span></span><br><span class="line"><span class="keyword">if</span> (opt.isPresent()) &#123;</span><br><span class="line">  opt.get();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// fall back</span></span><br><span class="line">opt.orElse(<span class="string">"default"</span>);</span><br><span class="line">opt.orElseGet(() -&gt; <span class="string">"default"</span>);</span><br><span class="line">opt.orElseThrow(() -&gt; <span class="keyword">new</span> NullPointerException());</span><br><span class="line"></span><br><span class="line"><span class="comment">// operate</span></span><br><span class="line">opt.ifPresent(value -&gt; &#123;</span><br><span class="line">  System.out.println(value);</span><br><span class="line">&#125;);</span><br><span class="line">opt.filter(value -&gt; value.length() &gt; <span class="number">5</span>);</span><br><span class="line">opt.map(value -&gt; value.trim());</span><br><span class="line">opt.flatMap(value -&gt; &#123;</span><br><span class="line">  String trimmed = value.trim();</span><br><span class="line">  <span class="keyword">return</span> trimmed.isEmpty() ? Optional.empty() : Optional.of(trimmed);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>Chaining of methods is a common cause of NPE, but if you have a series of methods that return <code>Optional</code>, you can chain them with <code>flatMap</code>, NPE-freely.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">String zipCode = getUser()</span><br><span class="line">    .flatMap(User::getAddress)</span><br><span class="line">    .flatMap(Address::getZipCode)</span><br><span class="line">    .orElse(<span class="string">""</span>);</span><br></pre></td></tr></table></figure><p>Java 8 <a href="https://www.oracle.com/technetwork/articles/java/ma14-java-se-8-streams-2177646.html" target="_blank" rel="noopener">Stream API</a> also uses optionals to return nullable values. For instance:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">stringList.stream().findFirst().orElse(<span class="string">"default"</span>);</span><br><span class="line">stringList.stream()</span><br><span class="line">    .max(Comparator.naturalOrder())</span><br><span class="line">    .ifPresent(System.out::println);</span><br></pre></td></tr></table></figure><p>Lastly, there are some special optional classes for primitive types, such as <code>OptionalInt</code>, <code>OptionalDouble</code>, etc. Use them whenever you find applicable.</p><h2 id="NPE-in-Other-JVM-Languages"><a href="#NPE-in-Other-JVM-Languages" class="headerlink" title="NPE in Other JVM Languages"></a>NPE in Other JVM Languages</h2><p>Scala provides an <a href="https://www.scala-lang.org/api/current/scala/Option.html" target="_blank" rel="noopener"><code>Option</code></a> class similar to Java 8 <code>Optional</code>. It has two subclasses, <code>Some</code> represents an existing value, and <code>None</code> for empty result.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> opt: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">Some</span>(<span class="string">"text"</span>)</span><br><span class="line">opt.getOrElse(<span class="string">"default"</span>)</span><br></pre></td></tr></table></figure><p>Instead of invoking <code>Option#isEmpty</code>, we can use Scala’s pattern match:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">opt <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Some</span>(text) =&gt; println(text)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt; println(<span class="string">"default"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Scala’s collection operations are very powerful, and <code>Option</code> can be treated as collection, so we can apply <code>filter</code>, <code>map</code>, or for-comprehension to it.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">opt.map(_.trim).filter(_.length &gt; <span class="number">0</span>).map(_.toUpperCase).getOrElse(<span class="string">"DEFAULT"</span>)</span><br><span class="line"><span class="keyword">val</span> upper = <span class="keyword">for</span> &#123;</span><br><span class="line">  text &lt;- opt</span><br><span class="line">  trimmed &lt;- <span class="type">Some</span>(text.trim())</span><br><span class="line">  upper &lt;- <span class="type">Some</span>(trimmed) <span class="keyword">if</span> trimmed.length &gt; <span class="number">0</span></span><br><span class="line">&#125; <span class="keyword">yield</span> upper</span><br><span class="line">upper.getOrElse(<span class="string">"DEFAULT"</span>)</span><br></pre></td></tr></table></figure><p>Kotlin takes another approach. It distinguishes <a href="https://kotlinlang.org/docs/reference/java-interop.html#nullability-annotations" target="_blank" rel="noopener">nullable types and non-null types</a>, and programmers are forced to check nullness before using nullable variables.</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a: String = <span class="string">"text"</span></span><br><span class="line">a = <span class="literal">null</span> <span class="comment">// Error: Null can not be a value of a non-null type String</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> b: String? = <span class="string">"text"</span></span><br><span class="line"><span class="comment">// Error: Only safe (?.) or non-null asserted (!!.) calls are allowed</span></span><br><span class="line"><span class="comment">// on a nullable receiver of type String?</span></span><br><span class="line">println(b.length)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> l: <span class="built_in">Int</span>? = b?.length <span class="comment">// safe call</span></span><br><span class="line">b!!.length <span class="comment">// may throw NPE</span></span><br></pre></td></tr></table></figure><p>When calling Java methods from Kotlin, the compiler does not ensure null-safety, because every object from Java is nullable. But we can use annotations to achieve strict nullness check. Kotlin supports a wide range of <a href="https://kotlinlang.org/docs/reference/java-interop.html#nullability-annotations" target="_blank" rel="noopener">annotations</a>, including those used in Spring Framework, which makes Spring API null-safe in Kotlin.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In all these solutions, I prefer the annotation approach, since it’s effective while less invasive. All public API methods should be annotated <code>@Nullable</code> or <code>@NonNull</code> so that the caller will be forced to do nullness check, making our program NPE free.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://howtodoinjava.com/java/exception-handling/how-to-effectively-handle-nullpointerexception-in-java/" target="_blank" rel="noopener">https://howtodoinjava.com/java/exception-handling/how-to-effectively-handle-nullpointerexception-in-java/</a></li><li><a href="http://jmri.sourceforge.net/help/en/html/doc/Technical/SpotBugs.shtml" target="_blank" rel="noopener">http://jmri.sourceforge.net/help/en/html/doc/Technical/SpotBugs.shtml</a></li><li><a href="https://dzone.com/articles/features-to-avoid-null-reference-exceptions-java-a" target="_blank" rel="noopener">https://dzone.com/articles/features-to-avoid-null-reference-exceptions-java-a</a></li><li><a href="https://medium.com/@fatihcoskun/kotlin-nullable-types-vs-java-optional-988c50853692" target="_blank" rel="noopener">https://medium.com/@fatihcoskun/kotlin-nullable-types-vs-java-optional-988c50853692</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;NullPointerException&lt;/code&gt; happens when you dereference a possible &lt;code&gt;null&lt;/code&gt; object without checking it. It’s a common exception that every Java programmer may encounter in daily work. There’re several strategies that can help us avoid this exception, making our codes more robust. In this article, I will list both traditional ways and those with tools and new features introduced by recent version of Java.&lt;/p&gt;
&lt;h2 id=&quot;Runtime-Check&quot;&gt;&lt;a href=&quot;#Runtime-Check&quot; class=&quot;headerlink&quot; title=&quot;Runtime Check&quot;&gt;&lt;/a&gt;Runtime Check&lt;/h2&gt;&lt;p&gt;The most obvious way is to use &lt;code&gt;if (obj == null)&lt;/code&gt; to check every variable you need to use, either from function argument, return value, or instance field. When you receive a &lt;code&gt;null&lt;/code&gt; object, you can throw a different, more informative exception like &lt;code&gt;IllegalArgumentException&lt;/code&gt;. There are some library functions that can make this process easier, like &lt;a href=&quot;https://docs.oracle.com/javase/7/docs/api/java/util/Objects.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;Objects#requireNonNull&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;testObjects&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(Object arg)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  Object checked = Objects.requireNonNull(arg, &lt;span class=&quot;string&quot;&gt;&quot;arg must not be null&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  checked.toString();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Or use Guava’s &lt;a href=&quot;https://github.com/google/guava/wiki/PreconditionsExplained&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;Preconditions&lt;/code&gt;&lt;/a&gt; package, which provides all kinds of arguments checking facilities:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;testGuava&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(Object arg)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  Object checked = Preconditions.checkNotNull(arg, &lt;span class=&quot;string&quot;&gt;&quot;%s must not be null&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;arg&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  checked.toString();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;We can also let &lt;a href=&quot;https://projectlombok.org/features/NonNull&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Lombok&lt;/a&gt; generate the check for us, which will throw a more meaningful &lt;code&gt;NullPointerException&lt;/code&gt;:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;testLombok&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(@NonNull Object arg)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  arg.toString();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;The generated code and exception message are as follows:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;testLombokGenerated&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(Object arg)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (arg == &lt;span class=&quot;keyword&quot;&gt;null&lt;/span&gt;) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; NullPointerException(&lt;span class=&quot;string&quot;&gt;&quot;arg is marked @NonNull but is null&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  arg.toString();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;This annotation can also be added to a class field, and Lombok will check nullness for every assignment.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="java" scheme="http://shzhangji.com/tags/java/"/>
    
      <category term="spring" scheme="http://shzhangji.com/tags/spring/"/>
    
      <category term="eclipse" scheme="http://shzhangji.com/tags/eclipse/"/>
    
  </entry>
  
  <entry>
    <title>Is It Necessary to Apply ESLint jsx-no-bind Rule?</title>
    <link href="http://shzhangji.com/blog/2018/09/13/is-it-necessary-to-apply-eslint-jsx-no-bind-rule/"/>
    <id>http://shzhangji.com/blog/2018/09/13/is-it-necessary-to-apply-eslint-jsx-no-bind-rule/</id>
    <published>2018-09-13T00:24:00.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p>When using <a href="https://github.com/yannickcr/eslint-plugin-react" target="_blank" rel="noopener">ESLint React plugin</a>, you may find a rule called <a href="https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/jsx-no-bind.md" target="_blank" rel="noopener"><code>jsx-no-bind</code></a>. It prevents you from using <code>.bind</code> or arrow function in a JSX prop. For instance, ESLint will complain about the arrow function in the <code>onClick</code> prop.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ListArrow</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">Component</span> </span>&#123;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;ul&gt;</span><br><span class="line">        &#123;<span class="keyword">this</span>.state.items.map(<span class="function"><span class="params">item</span> =&gt;</span> (</span><br><span class="line">          &lt;li key=&#123;item.id&#125; onClick=&#123;() =&gt; &#123; alert(item.id) &#125;&#125;&gt;&#123;item.text&#125;&lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">        ))&#125;</span></span><br><span class="line"><span class="regexp">      &lt;/u</span>l&gt;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>There’re two reasons why this rule is introduced. First, a new function will be created on every <code>render</code> call, which may increase the frequency of garbage collection. Second, it will disable the pure rendering process, i.e. when you’re using a <code>PureComponent</code>, or implement the <code>shouldComponentUpdate</code> method by yourself with identity comparison, a new function object in the props will cause unnecessary re-render of the component.</p><p>But some people argue that these two reasons are not solid enough to enforce this rule on all projects, especially when the solutions will introduce more codes and decrease readability. In <a href="https://github.com/airbnb/javascript/blob/eslint-config-airbnb-v17.1.0/packages/eslint-config-airbnb/rules/react.js#L93" target="_blank" rel="noopener">Airbnb ESLint preset</a>, the team only bans the usage of <code>.bind</code>, but allows arrow function in both props and refs. I did some googling, and was convinced that this rule is not quite necessary. Someone says it’s premature optimization, and you should measure before you optimize. I agree with that. In the following sections, I will illustrate how arrow function would affect the pure component, what solutions we can use, and talk a little bit about React rendering internals.</p><a id="more"></a><h2 id="Different-Types-of-React-Component"><a href="#Different-Types-of-React-Component" class="headerlink" title="Different Types of React Component"></a>Different Types of React Component</h2><p>The regular way to create a React component is to extend the <code>React.Component</code> class and implement the <code>render</code> method. There is also a built-in <code>React.PureComponent</code>, which implements the life-cycle method <code>shouldComponentUpdate</code> for you. In regular component, this method will always return <code>true</code>, indicating that React should call <code>render</code> whenever the props or states change. <code>PureComponent</code>, on the other hand, does a shallow identity comparison for the props and states to see whether this component should be re-rendered. The following two components behave the same:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PureChild</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">PureComponent</span> </span>&#123;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;div&gt;&#123;<span class="keyword">this</span>.props.message&#125;&lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    )</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">class RegularChild extends React.Component &#123;</span></span><br><span class="line"><span class="regexp">  shouldComponentUpdate(nextProps, nextStates) &#123;</span></span><br><span class="line"><span class="regexp">    return this.props.message !== nextProps.message</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">  render() &#123;</span></span><br><span class="line"><span class="regexp">    return (</span></span><br><span class="line"><span class="regexp">      &lt;div&gt;&#123;this.props.message&#125;&lt;/</span>div&gt;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>When their parent component is re-rendering, they will both check the message content in props and decide whether they should render again. The comparison rule is quite simple in pure component, it iterates the props and states object, compare each others’ members with <code>===</code> equality check. In JavaScript, only primitive types and the very same object will pass this test, for example:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> === <span class="number">1</span></span><br><span class="line"><span class="string">'hello world'</span> === <span class="string">'hello world'</span></span><br><span class="line">[] !== []</span><br><span class="line">(<span class="function"><span class="params">()</span> =&gt;</span> &#123;&#125;) !== <span class="function">(<span class="params">(</span>) =&gt;</span> &#123;&#125;)</span><br></pre></td></tr></table></figure><p>Clearly, arrow functions will fail the test and cause pure component to re-render every time its parent is re-rendered. On the other side, if you do not use pure component, or do props and states check on your own, enabling the <code>jsx-no-bind</code> rule is plain unnecessary.</p><p>Recently another kind of component has become popular, the stateless functional component (SFC). These components’ render results solely depend on their props, so they are like functions that take inputs and produce steady outputs. But under the hood, they are just regular components, i.e. they do not implement <code>shouldComponentUpdate</code>, and you can not implement by yourself either.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> StatelessChild = <span class="function">(<span class="params">props</span>) =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> (</span><br><span class="line">    &lt;div&gt;&#123;props.message&#125;&lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">  )</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="How-to-Fix-jsx-no-bind-Error"><a href="#How-to-Fix-jsx-no-bind-Error" class="headerlink" title="How to Fix jsx-no-bind Error"></a>How to Fix <code>jsx-no-bind</code> Error</h2><p>Arrow functions are usually used in event handlers. If we use normal functions or class methods, <code>this</code> keyword is not bound to the current instance, it is <code>undefined</code>. By using <code>.bind</code> or arrow function, we can access other class methods through <code>this</code>. To fix the <code>jsx-no-bind</code> error while still keeping the handler function bound, we can either bind it in constructor, or use the experimental class property syntax, which can be transformed by <a href="https://babeljs.io/docs/plugins/transform-class-properties/" target="_blank" rel="noopener">Babel</a>. More information can be found in React <a href="https://reactjs.org/docs/handling-events.html" target="_blank" rel="noopener">official document</a>, and here is the <a href="https://github.com/jizhang/jsx-no-bind/blob/master/src/components/NoArgument.js" target="_blank" rel="noopener">gist</a> of different solutions.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="class"><span class="keyword">class</span> <span class="title">NoArgument</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">Component</span> </span>&#123;</span><br><span class="line">  <span class="keyword">constructor</span>() &#123;</span><br><span class="line">    <span class="keyword">this</span>.handleClickBoundA = <span class="keyword">this</span>.handleClickUnbound.bind(<span class="keyword">this</span>)</span><br><span class="line">    <span class="keyword">this</span>.handleClickBoundC = <span class="function"><span class="params">()</span> =&gt;</span> &#123; <span class="keyword">this</span>.setState() &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  handleClickUnbound() &#123; <span class="comment">/* "this" is undefined */</span> &#125;</span><br><span class="line">  handleClickBoundB = <span class="function"><span class="params">()</span> =&gt;</span> &#123; <span class="keyword">this</span>.setState() &#125;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;div&gt;</span><br><span class="line">        <span class="built_in">Error</span>: jsx-no-bind</span><br><span class="line">        &lt;button onClick=&#123;() =&gt; &#123; <span class="keyword">this</span>.setState() &#125;&#125;&gt;ArrowA&lt;<span class="regexp">/button&gt;</span></span><br><span class="line"><span class="regexp">        &lt;button onClick=&#123;() =&gt; &#123; this.handleClickUnbound() &#125;&#125;&gt;ArrowB&lt;/</span>button&gt;</span><br><span class="line">        &lt;button onClick=&#123;<span class="keyword">this</span>.handleClickUnbound.bind(<span class="keyword">this</span>)&#125;&gt;Bind&lt;<span class="regexp">/button&gt;</span></span><br><span class="line"><span class="regexp">        No error:</span></span><br><span class="line"><span class="regexp">        &lt;button onClick=&#123;this.handleClickBoundA&#125;&gt;BoundA&lt;/</span>button&gt;</span><br><span class="line">        &lt;button onClick=&#123;<span class="keyword">this</span>.handleClickBoundB&#125;&gt;BoundB&lt;<span class="regexp">/button&gt;</span></span><br><span class="line"><span class="regexp">        &lt;button onClick=&#123;this.handleClickBoundC&#125;&gt;BoundC&lt;/</span>button&gt;</span><br><span class="line">      &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    )</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br></pre></td></tr></table></figure><p>For handlers that require extra arguments, e.g. a list of clickable items, things will be a little tricky. There’re two possible solutions, one is to create separate component for the item, and pass handler function and argument as props.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Item</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">PureComponent</span> </span>&#123;</span><br><span class="line">  handleClick = <span class="function"><span class="params">()</span> =&gt;</span> &#123; <span class="keyword">this</span>.props.onClick(<span class="keyword">this</span>.props.item.id) &#125;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;li onClick=&#123;<span class="keyword">this</span>.handleClick&#125;&gt;&#123;<span class="keyword">this</span>.props.item.text&#125;&lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">    )</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">export default class ListSeparate extends React.Component &#123;</span></span><br><span class="line"><span class="regexp">  handleClick = (itemId) =&gt; &#123; alert(itemId) &#125;</span></span><br><span class="line"><span class="regexp">  render() &#123;</span></span><br><span class="line"><span class="regexp">    return (</span></span><br><span class="line"><span class="regexp">      &lt;ul&gt;</span></span><br><span class="line"><span class="regexp">        &#123;this.props.items.map(item =&gt; (</span></span><br><span class="line"><span class="regexp">          &lt;Item key=&#123;item.id&#125; item=&#123;item&#125; onClick=&#123;this.handleClick&#125; /</span>&gt;</span><br><span class="line">        ))&#125;</span><br><span class="line">      &lt;<span class="regexp">/ul&gt;</span></span><br><span class="line"><span class="regexp">    )</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br></pre></td></tr></table></figure><p>This is a practice of separation of concerns, because now <code>List</code> only iterates the items, while <code>Item</code> takes care of rendering them. But this also adds a lot of codes, and make them hard to understand. We need to go through several handler functions to see what they actually do, while in the arrow function example, event handlers are co-located with the component, which is encouraged by React community.</p><p>Another approach is to use DOM <a href="https://developer.mozilla.org/en/docs/Web/API/HTMLElement/dataset" target="_blank" rel="noopener"><code>dataset</code></a> property, i.e. store argument in HTML <code>data-*</code> attribute, and retrieve it with <code>event</code> argument in handler functions.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="class"><span class="keyword">class</span> <span class="title">ListDataset</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">Component</span> </span>&#123;</span><br><span class="line">  handleClick = <span class="function">(<span class="params">event</span>) =&gt;</span> &#123; alert(event.target.dataset.itemId) &#125;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;ul&gt;</span><br><span class="line">        &#123;<span class="keyword">this</span>.props.items.map(<span class="function"><span class="params">item</span> =&gt;</span> (</span><br><span class="line">          &lt;li key=&#123;item.id&#125; data-item-id=&#123;item.id&#125; onClick=&#123;<span class="keyword">this</span>.handleClick&#125;&gt;&#123;item.text&#125;&lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">        ))&#125;</span></span><br><span class="line"><span class="regexp">      &lt;/u</span>l&gt;</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Virtual-DOM-and-Reconciliation"><a href="#Virtual-DOM-and-Reconciliation" class="headerlink" title="Virtual DOM and Reconciliation"></a>Virtual DOM and Reconciliation</h2><p>Arrow function will cause pure components unnecessary re-rendering, actually this statement is partly true. React rendering process has several steps. First, call <code>render</code> function that returns a tree of React elements; compare them with the virtual DOM; and then, apply the difference to the real DOM. The latter process is also called <a href="https://reactjs.org/docs/reconciliation.html" target="_blank" rel="noopener">reconciliation</a>. So even if the <code>render</code> function is called multiple times, the resulting tree of elements may not change at all, so there is no DOM manipulation, and that usually costs more time than pure JavaScript code. For components that constantly change, making them pure just adds one more time of unnecessary comparison.</p><p><img src="/images/jsx-no-bind/should-component-update.png" alt="shouldComponentUpdate"></p><p><a href="https://reactjs.org/docs/optimizing-performance.html#shouldcomponentupdate-in-action" target="_blank" rel="noopener">Source</a></p><p>Besides, change of event handlers will probably not cause re-rendering of the real DOM, because React only listens event on the <code>document</code> level. When the <code>onClick</code> event is triggered on the <code>li</code> element, it will bubble up to the top level and get processed by React event management system.</p><p><img src="/images/jsx-no-bind/top-level-delegation.jpg" alt="Top-level Delegation"></p><p><a href="https://levelup.gitconnected.com/how-exactly-does-react-handles-events-71e8b5e359f2" target="_blank" rel="noopener">Source</a></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>To fix <code>jsx-no-bind</code> we need to trade off readability, and yet the performance may not improve as much as we thought. So instead of guessing what may cause performance problem, why not program in a natural way at first, and when there occurs some noticeable performance issues, take measures, and fix them with appropriate techniques.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/jsx-no-bind.md" target="_blank" rel="noopener">https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/jsx-no-bind.md</a></li><li><a href="https://cdb.reacttraining.com/react-inline-functions-and-performance-bdff784f5578" target="_blank" rel="noopener">https://cdb.reacttraining.com/react-inline-functions-and-performance-bdff784f5578</a></li><li><a href="https://maarten.mulders.it/blog/2017/07/no-bind-or-arrow-in-jsx-props-why-how.html" target="_blank" rel="noopener">https://maarten.mulders.it/blog/2017/07/no-bind-or-arrow-in-jsx-props-why-how.html</a></li><li><a href="https://reactjs.org/docs/faq-functions.html#example-passing-params-using-data-attributes" target="_blank" rel="noopener">https://reactjs.org/docs/faq-functions.html#example-passing-params-using-data-attributes</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When using &lt;a href=&quot;https://github.com/yannickcr/eslint-plugin-react&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ESLint React plugin&lt;/a&gt;, you may find a rule called &lt;a href=&quot;https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/jsx-no-bind.md&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;jsx-no-bind&lt;/code&gt;&lt;/a&gt;. It prevents you from using &lt;code&gt;.bind&lt;/code&gt; or arrow function in a JSX prop. For instance, ESLint will complain about the arrow function in the &lt;code&gt;onClick&lt;/code&gt; prop.&lt;/p&gt;
&lt;figure class=&quot;highlight javascript&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ListArrow&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  render() &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; (&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &amp;lt;ul&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#123;&lt;span class=&quot;keyword&quot;&gt;this&lt;/span&gt;.state.items.map(&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;params&quot;&gt;item&lt;/span&gt; =&amp;gt;&lt;/span&gt; (&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;lt;li key=&amp;#123;item.id&amp;#125; onClick=&amp;#123;() =&amp;gt; &amp;#123; alert(item.id) &amp;#125;&amp;#125;&amp;gt;&amp;#123;item.text&amp;#125;&amp;lt;&lt;span class=&quot;regexp&quot;&gt;/li&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;regexp&quot;&gt;        ))&amp;#125;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;regexp&quot;&gt;      &amp;lt;/u&lt;/span&gt;l&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    )&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;There’re two reasons why this rule is introduced. First, a new function will be created on every &lt;code&gt;render&lt;/code&gt; call, which may increase the frequency of garbage collection. Second, it will disable the pure rendering process, i.e. when you’re using a &lt;code&gt;PureComponent&lt;/code&gt;, or implement the &lt;code&gt;shouldComponentUpdate&lt;/code&gt; method by yourself with identity comparison, a new function object in the props will cause unnecessary re-render of the component.&lt;/p&gt;
&lt;p&gt;But some people argue that these two reasons are not solid enough to enforce this rule on all projects, especially when the solutions will introduce more codes and decrease readability. In &lt;a href=&quot;https://github.com/airbnb/javascript/blob/eslint-config-airbnb-v17.1.0/packages/eslint-config-airbnb/rules/react.js#L93&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Airbnb ESLint preset&lt;/a&gt;, the team only bans the usage of &lt;code&gt;.bind&lt;/code&gt;, but allows arrow function in both props and refs. I did some googling, and was convinced that this rule is not quite necessary. Someone says it’s premature optimization, and you should measure before you optimize. I agree with that. In the following sections, I will illustrate how arrow function would affect the pure component, what solutions we can use, and talk a little bit about React rendering internals.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="javascript" scheme="http://shzhangji.com/tags/javascript/"/>
    
      <category term="react" scheme="http://shzhangji.com/tags/react/"/>
    
      <category term="eslint" scheme="http://shzhangji.com/tags/eslint/"/>
    
  </entry>
  
  <entry>
    <title>Serve TensforFlow Estimator with SavedModel</title>
    <link href="http://shzhangji.com/blog/2018/05/14/serve-tensorflow-estimator-with-savedmodel/"/>
    <id>http://shzhangji.com/blog/2018/05/14/serve-tensorflow-estimator-with-savedmodel/</id>
    <published>2018-05-14T01:43:14.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.tensorflow.org/" target="_blank" rel="noopener">TensorFlow</a> is one of the most popular machine learning frameworks that allow us to build various models with minor efforts. There are several ways to utilize these models in production like web service API, and this article will introduce how to make model prediction APIs with TensorFlow’s SavedModel mechanism.</p><p><img src="/images/tf-logo.png" alt></p><h2 id="Iris-DNN-Estimator"><a href="#Iris-DNN-Estimator" class="headerlink" title="Iris DNN Estimator"></a>Iris DNN Estimator</h2><p>First let’s build the famous iris classifier with TensorFlow’s pre-made DNN estimator. Full illustration can be found on TensorFlow’s website (<a href="https://www.tensorflow.org/get_started/premade_estimators" target="_blank" rel="noopener">Premade Estimators</a>), and I create a repository on GitHub (<a href="https://github.com/jizhang/tf-serve/blob/master/iris_dnn.py" target="_blank" rel="noopener"><code>iris_dnn.py</code></a>) for you to fork and work with. Here’s the gist of training the model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">feature_columns = [tf.feature_column.numeric_column(key=key)</span><br><span class="line">                   <span class="keyword">for</span> key <span class="keyword">in</span> train_x.keys()]</span><br><span class="line">classifier = tf.estimator.DNNClassifier(</span><br><span class="line">    feature_columns=feature_columns,</span><br><span class="line">    hidden_units=[<span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">    n_classes=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">classifier.train(</span><br><span class="line">    input_fn=<span class="keyword">lambda</span>: train_input_fn(train_x, train_y, batch_size=BATCH_SIZE),</span><br><span class="line">    steps=STEPS)</span><br><span class="line"></span><br><span class="line">predictions = classifier.predict(</span><br><span class="line">    input_fn=<span class="keyword">lambda</span>: eval_input_fn(predict_x, labels=<span class="literal">None</span>, batch_size=BATCH_SIZE))</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Export-as-SavedModel"><a href="#Export-as-SavedModel" class="headerlink" title="Export as SavedModel"></a>Export as SavedModel</h2><p>TensorFlow provides the <a href="https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators" target="_blank" rel="noopener">SavedModel</a> utility to let us export the trained model for future predicting and serving. <code>Estimator</code> exposes an <code>export_savedmodel</code> method, which requires two arguments: the export directory and a receiver function. Latter defines what kind of input data the exported model accepts. Usually we will use TensorFlow’s <a href="https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/example/example.proto" target="_blank" rel="noopener"><code>Example</code></a> type, which contains the features of one or more items. For instance, an iris data item can be defined as:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Example(</span><br><span class="line">    features=Features(</span><br><span class="line">        feature=&#123;</span><br><span class="line">            <span class="string">'SepalLength'</span>: Feature(float_list=FloatList(value=[<span class="number">5.1</span>])),</span><br><span class="line">            <span class="string">'SepalWidth'</span>: Feature(float_list=FloatList(value=[<span class="number">3.3</span>])),</span><br><span class="line">            <span class="string">'PetalLength'</span>: Feature(float_list=FloatList(value=[<span class="number">1.7</span>])),</span><br><span class="line">            <span class="string">'PetalWidth'</span>: Feature(float_list=FloatList(value=[<span class="number">0.5</span>])),</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>The receiver function needs to be able to parse the incoming serialized <code>Example</code> object into a map of tensors for model to consume. TensorFlow provides some utility functions to help building it. We first transform the <code>feature_columns</code> array into a map of <code>Feature</code> as the parsing specification, and then use it to build the receiver function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [</span></span><br><span class="line"><span class="comment">#     _NumericColumn(key='SepalLength', shape=(1,), dtype=tf.float32),</span></span><br><span class="line"><span class="comment">#     ...</span></span><br><span class="line"><span class="comment"># ]</span></span><br><span class="line">feature_columns = [tf.feature_column.numeric_column(key=key)</span><br><span class="line">                   <span class="keyword">for</span> key <span class="keyword">in</span> train_x.keys()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#     'SepalLength': FixedLenFeature(shape=(1,), dtype=tf.float32),</span></span><br><span class="line"><span class="comment">#     ...</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line">feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build receiver function, and export.</span></span><br><span class="line">serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)</span><br><span class="line">export_dir = classifier.export_savedmodel(<span class="string">'export'</span>, serving_input_receiver_fn)</span><br></pre></td></tr></table></figure><h2 id="Inspect-SavedModel-with-CLI-Tool"><a href="#Inspect-SavedModel-with-CLI-Tool" class="headerlink" title="Inspect SavedModel with CLI Tool"></a>Inspect SavedModel with CLI Tool</h2><p>Each export will create a timestamped directory, containing the information of the trained model.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export/1524907728/saved_model.pb</span><br><span class="line">export/1524907728/variables</span><br><span class="line">export/1524907728/variables/variables.data-00000-of-00001</span><br><span class="line">export/1524907728/variables/variables.index</span><br></pre></td></tr></table></figure><p>TensorFlow provides a command line tool to inspect the exported model, or even run predictions with it.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ saved_model_cli show --dir <span class="built_in">export</span>/1524906774 \</span><br><span class="line">  --tag_set serve --signature_def serving_default</span><br><span class="line">The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">  inputs[<span class="string">'inputs'</span>] tensor_info:</span><br><span class="line">      dtype: DT_STRING</span><br><span class="line">      shape: (-1)</span><br><span class="line">The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">  outputs[<span class="string">'classes'</span>] tensor_info:</span><br><span class="line">      dtype: DT_STRING</span><br><span class="line">      shape: (-1, 3)</span><br><span class="line">  outputs[<span class="string">'scores'</span>] tensor_info:</span><br><span class="line">      dtype: DT_FLOAT</span><br><span class="line">      shape: (-1, 3)</span><br><span class="line">Method name is: tensorflow/serving/classify</span><br><span class="line"></span><br><span class="line">$ saved_model_cli run --dir <span class="built_in">export</span>/1524906774 \</span><br><span class="line">  --tag_set serve --signature_def serving_default \</span><br><span class="line">  --input_examples <span class="string">'inputs=[&#123;"SepalLength":[5.1],"SepalWidth":[3.3],"PetalLength":[1.7],"PetalWidth":[0.5]&#125;]'</span></span><br><span class="line">Result <span class="keyword">for</span> output key classes:</span><br><span class="line">[[b<span class="string">'0'</span> b<span class="string">'1'</span> b<span class="string">'2'</span>]]</span><br><span class="line">Result <span class="keyword">for</span> output key scores:</span><br><span class="line">[[9.9919027e-01 8.0969761e-04 1.2872645e-09]]</span><br></pre></td></tr></table></figure><h2 id="Serve-SavedModel-with-contrib-predictor"><a href="#Serve-SavedModel-with-contrib-predictor" class="headerlink" title="Serve SavedModel with contrib.predictor"></a>Serve SavedModel with <code>contrib.predictor</code></h2><p>In <code>contrib.predictor</code> package, there is a convenient method for us to build a predictor function from exported model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load model from export directory, and make a predict function.</span></span><br><span class="line">predict_fn = tf.contrib.predictor.from_saved_model(export_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test inputs represented by Pandas DataFrame.</span></span><br><span class="line">inputs = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">'SepalLength'</span>: [<span class="number">5.1</span>, <span class="number">5.9</span>, <span class="number">6.9</span>],</span><br><span class="line">    <span class="string">'SepalWidth'</span>: [<span class="number">3.3</span>, <span class="number">3.0</span>, <span class="number">3.1</span>],</span><br><span class="line">    <span class="string">'PetalLength'</span>: [<span class="number">1.7</span>, <span class="number">4.2</span>, <span class="number">5.4</span>],</span><br><span class="line">    <span class="string">'PetalWidth'</span>: [<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">2.1</span>],</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert input data into serialized Example strings.</span></span><br><span class="line">examples = []</span><br><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> inputs.iterrows():</span><br><span class="line">    feature = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> col, value <span class="keyword">in</span> row.iteritems():</span><br><span class="line">        feature[col] = tf.train.Feature(float_list=tf.train.FloatList(value=[value]))</span><br><span class="line">    example = tf.train.Example(</span><br><span class="line">        features=tf.train.Features(</span><br><span class="line">            feature=feature</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">    examples.append(example.SerializeToString())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions.</span></span><br><span class="line">predictions = predict_fn(&#123;<span class="string">'inputs'</span>: examples&#125;)</span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#     'classes': [</span></span><br><span class="line"><span class="comment">#         [b'0', b'1', b'2'],</span></span><br><span class="line"><span class="comment">#         [b'0', b'1', b'2'],</span></span><br><span class="line"><span class="comment">#         [b'0', b'1', b'2']</span></span><br><span class="line"><span class="comment">#     ],</span></span><br><span class="line"><span class="comment">#     'scores': [</span></span><br><span class="line"><span class="comment">#         [9.9826765e-01, 1.7323202e-03, 4.7271198e-15],</span></span><br><span class="line"><span class="comment">#         [2.1470961e-04, 9.9776912e-01, 2.0161823e-03],</span></span><br><span class="line"><span class="comment">#         [4.2676111e-06, 4.8709501e-02, 9.5128632e-01]</span></span><br><span class="line"><span class="comment">#     ]</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><p>We can tidy up the prediction outputs to make the result clearer:</p><table><thead><tr><th>SepalLength</th><th>SepalWidth</th><th>PetalLength</th><th>PetalWidth</th><th>ClassID</th><th>Probability</th></tr></thead><tbody><tr><td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td><td>0</td><td>0.998268</td></tr><tr><td>5.9</td><td>3.0</td><td>4.2</td><td>1.5</td><td>1</td><td>0.997769</td></tr><tr><td>6.9</td><td>3.1</td><td>5.4</td><td>2.1</td><td>2</td><td>0.951286</td></tr></tbody></table><p>Under the hood, <code>from_saved_model</code> uses the <code>saved_model.loader</code> to load the exported model to a TensorFlow session, extract input / output definitions, create necessary tensors and invoke <code>session.run</code> to get results. I write a simple example (<a href="https://github.com/jizhang/tf-serve/blob/master/iris_sess.py" target="_blank" rel="noopener"><code>iris_sess.py</code></a>) of this workflow, or you can refer to TensorFlow’s source code <a href="https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/predictor/saved_model_predictor.py" target="_blank" rel="noopener"><code>saved_model_predictor.py</code></a>. <a href="https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/python/tools/saved_model_cli.py" target="_blank" rel="noopener"><code>saved_model_cli</code></a> also works this way.</p><h2 id="Serve-SavedModel-with-TensorFlow-Serving"><a href="#Serve-SavedModel-with-TensorFlow-Serving" class="headerlink" title="Serve SavedModel with TensorFlow Serving"></a>Serve SavedModel with TensorFlow Serving</h2><p>Finally, let’s see how to use TensorFlow’s side project, <a href="https://www.tensorflow.org/serving/" target="_blank" rel="noopener">TensorFlow Serving</a>, to expose our trained model to the outside world.</p><h3 id="Setup-TensorFlow-ModelServer"><a href="#Setup-TensorFlow-ModelServer" class="headerlink" title="Setup TensorFlow ModelServer"></a>Setup TensorFlow ModelServer</h3><p>TensorFlow server code is written in C++. A convenient way to install it is via package repository. You can follow the <a href="https://www.tensorflow.org/serving/setup" target="_blank" rel="noopener">official document</a>, add the TensorFlow distribution URI, and install the binary:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install tensorflow-model-server</span><br></pre></td></tr></table></figure><p>Then use the following command to start a ModelServer, which will automatically pick up the latest model from the export directory.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tensorflow_model_server --port=9000 --model_base_path=/root/<span class="built_in">export</span></span><br><span class="line">2018-05-14 01:05:12.561 Loading SavedModel with tags: &#123; serve &#125;; from: /root/<span class="built_in">export</span>/1524907728</span><br><span class="line">2018-05-14 01:05:12.639 Successfully loaded servable version &#123;name: default version: 1524907728&#125;</span><br><span class="line">2018-05-14 01:05:12.641 Running ModelServer at 0.0.0.0:9000 ...</span><br></pre></td></tr></table></figure><h3 id="Request-Remote-Model-via-SDK"><a href="#Request-Remote-Model-via-SDK" class="headerlink" title="Request Remote Model via SDK"></a>Request Remote Model via SDK</h3><p>TensorFlow Serving is based on gRPC and Protocol Buffers. So as to make remote procedure calls, we need to install the TensorFlow Serving API, along with its dependencies. Note that TensorFlow only provides client SDK in Python 2.7, but there is a contributed Python 3.x package available on PyPI.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install tensorflow-seving-api-python3==1.7.0</span><br></pre></td></tr></table></figure><p>The procedure is straight forward, we create the connection, assemble some <code>Example</code> instances, send to remote server and get the predictions. Full code can be found in <a href="https://github.com/jizhang/tf-serve/blob/master/iris_remote.py" target="_blank" rel="noopener"><code>iris_remote.py</code></a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create connection, boilerplate of gRPC.</span></span><br><span class="line">channel = implementations.insecure_channel(<span class="string">'127.0.0.1'</span>, <span class="number">9000</span>)</span><br><span class="line">stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get test inputs, and assemble a list of Examples, unserialized.</span></span><br><span class="line">inputs = pd.DateFrame()</span><br><span class="line">examples = [tf.tain.Example() <span class="keyword">for</span> index, row <span class="keyword">in</span> inputs.iterrows()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare RPC request, specify the model name.</span></span><br><span class="line">request = classification_pb2.ClassificationRequest()</span><br><span class="line">request.model_spec.name = <span class="string">'default'</span></span><br><span class="line">request.input.example_list.examples.extend(examples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get response, and tidy up.</span></span><br><span class="line">response = stub.Classify(request, <span class="number">10.0</span>)</span><br><span class="line"><span class="comment"># result &#123;</span></span><br><span class="line"><span class="comment">#   classifications &#123;</span></span><br><span class="line"><span class="comment">#     classes &#123;</span></span><br><span class="line"><span class="comment">#       label: "0"</span></span><br><span class="line"><span class="comment">#       score: 0.998267650604248</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment">#     ...</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment">#   ...</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.tensorflow.org/get_started/premade_estimators" target="_blank" rel="noopener">https://www.tensorflow.org/get_started/premade_estimators</a></li><li><a href="https://www.tensorflow.org/programmers_guide/saved_model" target="_blank" rel="noopener">https://www.tensorflow.org/programmers_guide/saved_model</a></li><li><a href="https://www.tensorflow.org/serving/" target="_blank" rel="noopener">https://www.tensorflow.org/serving/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TensorFlow&lt;/a&gt; is one of the most popular machine learning frameworks that allow us to build various models with minor efforts. There are several ways to utilize these models in production like web service API, and this article will introduce how to make model prediction APIs with TensorFlow’s SavedModel mechanism.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/tf-logo.png&quot; alt&gt;&lt;/p&gt;
&lt;h2 id=&quot;Iris-DNN-Estimator&quot;&gt;&lt;a href=&quot;#Iris-DNN-Estimator&quot; class=&quot;headerlink&quot; title=&quot;Iris DNN Estimator&quot;&gt;&lt;/a&gt;Iris DNN Estimator&lt;/h2&gt;&lt;p&gt;First let’s build the famous iris classifier with TensorFlow’s pre-made DNN estimator. Full illustration can be found on TensorFlow’s website (&lt;a href=&quot;https://www.tensorflow.org/get_started/premade_estimators&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Premade Estimators&lt;/a&gt;), and I create a repository on GitHub (&lt;a href=&quot;https://github.com/jizhang/tf-serve/blob/master/iris_dnn.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;iris_dnn.py&lt;/code&gt;&lt;/a&gt;) for you to fork and work with. Here’s the gist of training the model:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;feature_columns = [tf.feature_column.numeric_column(key=key)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; train_x.keys()]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;classifier = tf.estimator.DNNClassifier(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    feature_columns=feature_columns,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    hidden_units=[&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    n_classes=&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;classifier.train(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    input_fn=&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt;: train_input_fn(train_x, train_y, batch_size=BATCH_SIZE),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    steps=STEPS)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;predictions = classifier.predict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    input_fn=&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt;: eval_input_fn(predict_x, labels=&lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;, batch_size=BATCH_SIZE))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
      <category term="tensorflow" scheme="http://shzhangji.com/tags/tensorflow/"/>
    
      <category term="machine learning" scheme="http://shzhangji.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Connect HBase with Python and Thrift</title>
    <link href="http://shzhangji.com/blog/2018/04/22/connect-hbase-with-python-and-thrift/"/>
    <id>http://shzhangji.com/blog/2018/04/22/connect-hbase-with-python-and-thrift/</id>
    <published>2018-04-22T08:44:12.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://hbase.apache.org/" target="_blank" rel="noopener">Apache HBase</a> is a key-value store in Hadoop ecosystem. It is based on HDFS, and can provide high performance data access on large amount of volume. HBase is written in Java, and has native support for Java clients. But with the help of Thrift and various language bindings, we can access HBase in web services quite easily. This article will describe how to read and write HBase table with Python and Thrift.</p><p><img src="/images/hbase.png" alt></p><h2 id="Generate-Thrift-Class"><a href="#Generate-Thrift-Class" class="headerlink" title="Generate Thrift Class"></a>Generate Thrift Class</h2><p>For anyone who is new to <a href="https://thrift.apache.org/" target="_blank" rel="noopener">Apache Thrift</a>, it provides an IDL (Interface Description Language) to let you describe your service methods and data types and then transform them into different languages. For instance, a Thrift type definition like this:</p><figure class="highlight thrift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TColumn</span> </span>&#123;</span><br><span class="line">  <span class="number">1</span>: <span class="keyword">required</span> <span class="built_in">binary</span> family,</span><br><span class="line">  <span class="number">2</span>: <span class="keyword">optional</span> <span class="built_in">binary</span> qualifier,</span><br><span class="line">  <span class="number">3</span>: <span class="keyword">optional</span> <span class="built_in">i64</span> timestamp</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Will be transformed into the following Python code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TColumn</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, family=None, qualifier=None, timestamp=None,)</span>:</span></span><br><span class="line">        self.family = family</span><br><span class="line">        self.qualifier = qualifier</span><br><span class="line">        self.timestamp = timestamp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(self, iprot)</span>:</span></span><br><span class="line">        iprot.readStructBegin()</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            (fname, ftype, fid) = iprot.readFieldBegin()</span><br><span class="line">            <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(self, oprot)</span>:</span></span><br><span class="line">        oprot.writeStructBegin(<span class="string">'TColumn'</span>)</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="HBase-Thrift-vs-Thrift2"><a href="#HBase-Thrift-vs-Thrift2" class="headerlink" title="HBase Thrift vs Thrift2"></a>HBase Thrift vs Thrift2</h3><p>HBase provides <a href="https://github.com/apache/hbase/tree/master/hbase-thrift/src/main/resources/org/apache/hadoop/hbase" target="_blank" rel="noopener">two versions</a> of Thrift IDL files, and they have two main differences.</p><p>First, <code>thrift2</code> mimics the data types and methods from HBase Java API, which could be more intuitive to use. For instance, constructing a <code>Get</code> operation in Java is:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(<span class="string">"rowkey"</span>));</span><br><span class="line">get.addColumn(Bytes.toBytes(<span class="string">"cf"</span>), Bytes.toBytes(<span class="string">"col1"</span>));</span><br><span class="line">get.addColumn(Bytes.toBytes(<span class="string">"cf"</span>), Bytes.toBytes(<span class="string">"col2"</span>));</span><br></pre></td></tr></table></figure><p>In <code>thrift2</code>, there is a corresponding <code>TGet</code> type:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tget = TGet(</span><br><span class="line">    row=<span class="string">'rowkey'</span>,</span><br><span class="line">    columns=[</span><br><span class="line">        TColumn(family=<span class="string">'cf'</span>, qualifier=<span class="string">'col1'</span>),</span><br><span class="line">        TColumn(family=<span class="string">'cf'</span>, qualifier=<span class="string">'col2'</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>While in <code>thrift</code>, we directly invoke one of the <code>get</code> methods:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">client.getRowWithColumns(</span><br><span class="line">    tableName=<span class="string">'tbl'</span>,</span><br><span class="line">    row=<span class="string">'rowkey'</span>,</span><br><span class="line">    columns=[<span class="string">'cf:col1'</span>, <span class="string">'cf:col2'</span>],</span><br><span class="line">    attributes=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>The second difference is that <code>thrift2</code> lacks the administration interfaces, like <code>createTable</code>, <code>majorCompact</code>, etc. Currently these APIs are still under development, so if you need to use them via Thrift, you will have to fall back to version one.</p><p>After deciding which version we use, now we can download the <code>hbase.thrift</code> file, and generate Python code from it. One note on Thrift version though. Since we will use Python 3.x, which is supported by Thrift 0.10 onwards, so make sure you install the right version. Execute the following command, and you will get several Python files.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ thrift -gen py hbase.thrift</span><br><span class="line">$ find gen-py</span><br><span class="line">gen-py/hbase/__init__.py</span><br><span class="line">gen-py/hbase/constants.py</span><br><span class="line">gen-py/hbase/THBaseService.py</span><br><span class="line">gen-py/hbase/ttypes.py</span><br></pre></td></tr></table></figure><h2 id="Run-HBase-in-Standalone-Mode"><a href="#Run-HBase-in-Standalone-Mode" class="headerlink" title="Run HBase in Standalone Mode"></a>Run HBase in Standalone Mode</h2><p>In case you do not have a running HBase service to test against, you can follow the quick start guide (<a href="https://hbase.apache.org/book.html#quickstart" target="_blank" rel="noopener">link</a>) to download the binaries, do some minor configuration, and then execute the following commands to start a standalone HBase server as well as the Thrift2 server.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/start-hbase.sh</span><br><span class="line">bin/hbase-daemon.sh start thrift2</span><br><span class="line">bin/hbase shell</span><br></pre></td></tr></table></figure><p>Then in the HBase shell, we create a test table and read / write some data.</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; create <span class="string">"tsdata"</span>, NAME =&gt; <span class="string">"cf"</span></span><br><span class="line">&gt; put <span class="string">"tsdata"</span>, <span class="string">"sys.cpu.user:20180421:192.168.1.1"</span>, <span class="string">"cf:1015"</span>, <span class="string">"0.28"</span></span><br><span class="line">&gt; get <span class="string">"tsdata"</span>, <span class="string">"sys.cpu.user:20180421:192.168.1.1"</span></span><br><span class="line">COLUMN                                        CELL</span><br><span class="line"> <span class="symbol">cf:</span><span class="number">1015</span>                                      timestamp=<span class="number">1524277135973</span>, value=<span class="number">0</span>.<span class="number">28</span></span><br><span class="line"><span class="number">1</span> row(s) <span class="keyword">in</span> <span class="number">0</span>.<span class="number">0330</span> seconds</span><br></pre></td></tr></table></figure><h2 id="Connect-to-HBase-via-Thrift2"><a href="#Connect-to-HBase-via-Thrift2" class="headerlink" title="Connect to HBase via Thrift2"></a>Connect to HBase via Thrift2</h2><p>Here is the boilerplate of making a connection to HBase Thrift server. Note that Thrift client is not thread-safe, and it does neither provide connection pooling facility. You may choose to connect on every request, which is actually fast enough, or maintain a pool of connections yourself.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TSocket</span><br><span class="line"><span class="keyword">from</span> thrift.protocol <span class="keyword">import</span> TBinaryProtocol</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TTransport</span><br><span class="line"><span class="keyword">from</span> hbase <span class="keyword">import</span> THBaseService</span><br><span class="line"></span><br><span class="line">transport = TTransport.TBufferedTransport(TSocket.TSocket(<span class="string">'127.0.0.1'</span>, <span class="number">9090</span>))</span><br><span class="line">protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)</span><br><span class="line">client = THBaseService.Client(protocol)</span><br><span class="line">transport.open()</span><br><span class="line"><span class="comment"># perform some operations with "client"</span></span><br><span class="line">transport.close()</span><br></pre></td></tr></table></figure><p>We can test the connection with some basic operations:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hbase.ttypes <span class="keyword">import</span> TPut, TColumnValue, TGet</span><br><span class="line">tput = TPut(</span><br><span class="line">    row=<span class="string">'sys.cpu.user:20180421:192.168.1.1'</span>,</span><br><span class="line">    columnValues=[</span><br><span class="line">        TColumnValue(family=<span class="string">'cf'</span>, qualifier=<span class="string">'1015'</span>, value=<span class="string">'0.28'</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">client.put(<span class="string">'tsdata'</span>, tput)</span><br><span class="line"></span><br><span class="line">tget = TGet(row=<span class="string">'sys.cpu.user:20180421:192.168.1.1'</span>)</span><br><span class="line">tresult = client.get(<span class="string">'tsdata'</span>, tget)</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> tresult.columnValues:</span><br><span class="line">    print(col.qualifier, <span class="string">'='</span>, col.value)</span><br></pre></td></tr></table></figure><h2 id="Thrift2-Data-Types-and-Methods-Overview"><a href="#Thrift2-Data-Types-and-Methods-Overview" class="headerlink" title="Thrift2 Data Types and Methods Overview"></a>Thrift2 Data Types and Methods Overview</h2><p>For a full list of the available APIs, one can directly look into <code>hbase.thrift</code> or <code>hbase/THBaseService.py</code> files. Following is an abridged table of those data types and methods.</p><h3 id="Data-Types"><a href="#Data-Types" class="headerlink" title="Data Types"></a>Data Types</h3><table><thead><tr><th>Class</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td>TColumn</td><td>Represents a column family or a single column.</td><td>TColumn(family=’cf’, qualifier=’gender’)</td></tr><tr><td>TColumnValue</td><td>Column and its value.</td><td>TColumnValue(family=’cf’, qualifier=’gender’, value=’male’)</td></tr><tr><td>TResult</td><td>Query result, a single row. <code>row</code> attribute would be <code>None</code> if no result is found.</td><td>TResult(row=’employee_001’, columnValues=[TColumnValue])</td></tr><tr><td>TGet</td><td>Query a single row.</td><td>TGet(row=’employee_001’, columns=[TColumn])</td></tr><tr><td>TPut</td><td>Mutate a single row.</td><td>TPut(row=’employee_001’, columnValues=[TColumnValue])</td></tr><tr><td>TDelete</td><td>Delete an entire row or only some columns.</td><td>TDelete(row=’employee_001’, columns=[TColumn])</td></tr><tr><td>TScan</td><td>Scan for multiple rows and columns.</td><td>See below.</td></tr></tbody></table><h3 id="THBaseService-Methods"><a href="#THBaseService-Methods" class="headerlink" title="THBaseService Methods"></a>THBaseService Methods</h3><table><thead><tr><th>Method Signature</th><th>Description</th></tr></thead><tbody><tr><td>get(table: str, tget: TGet) -&gt; TResult</td><td>Query a single row.</td></tr><tr><td>getMultiple(table: str, tgets: List[TGet]) -&gt; List[TResult]</td><td>Query multiple rows.</td></tr><tr><td>put(table: str, tput: TPut) -&gt; None</td><td>Mutate a row.</td></tr><tr><td>putMultiple(table: str, tputs: List[TPut]) -&gt; None</td><td>Mutate multiple rows.</td></tr><tr><td>deleteSingle(table: str, tdelete: TDelete) -&gt; None</td><td>Delete a row.</td></tr><tr><td>deleteMultiple(table: str, tdeletes: List[TDelete]) -&gt; None</td><td>Delete multiple rows.</td></tr><tr><td>openScanner(table: str, tscan: TScan) -&gt; int</td><td>Open a scanner, returns scannerId.</td></tr><tr><td>getScannerRows(scannerId: int, numRows: int) -&gt; List[TResult]</td><td>Get scanner rows.</td></tr><tr><td>closeScanner(scannerId: int) -&gt; None</td><td>Close a scanner.</td></tr><tr><td>getScannerResults(table: str, tscan: TScan, numRows: int) -&gt; List[TResult]</td><td>A convenient method to get scan results.</td></tr></tbody></table><h3 id="Scan-Operation-Example"><a href="#Scan-Operation-Example" class="headerlink" title="Scan Operation Example"></a>Scan Operation Example</h3><p>I wrote some example codes on GitHub (<a href="https://github.com/jizhang/python-hbase" target="_blank" rel="noopener">link</a>), and the following is how a <code>Scan</code> operation is made.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scanner_id = client.openScanner(</span><br><span class="line">    table=<span class="string">'tsdata'</span>,</span><br><span class="line">    tscan=TScan(</span><br><span class="line">        startRow=<span class="string">'sys.cpu.user:20180421'</span>,</span><br><span class="line">        stopRow=<span class="string">'sys.cpu.user:20180422'</span>,</span><br><span class="line">        columns=[TColumn(<span class="string">'cf'</span>, <span class="string">'1015'</span>)]</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    num_rows = <span class="number">10</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        tresults = client.getScannerRows(scanner_id, num_rows)</span><br><span class="line">        <span class="keyword">for</span> tresult <span class="keyword">in</span> tresults:</span><br><span class="line">            print(tresult)</span><br><span class="line">        <span class="keyword">if</span> len(tresults) &lt; num_rows:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    client.closeScanner(scanner_id)</span><br></pre></td></tr></table></figure><h2 id="Thrift-Server-High-Availability"><a href="#Thrift-Server-High-Availability" class="headerlink" title="Thrift Server High Availability"></a>Thrift Server High Availability</h2><p>There are several solutions to eliminate the single point of failure of Thrift server. You can either (1) randomly select a server address on the client-side, and fall back to others if failure is detected, (2) setup a proxy facility to load balance the TCP connections, or (3) run individual Thrift server on every client machine, and let client code connects the local Thrift server. Usually we use the second approach, so you may consult your system administrator on that topic.</p><p><img src="/images/hbase-thrift-ha.png" alt></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://blog.cloudera.com/blog/2013/09/how-to-use-the-hbase-thrift-interface-part-1/" target="_blank" rel="noopener">https://blog.cloudera.com/blog/2013/09/how-to-use-the-hbase-thrift-interface-part-1/</a></li><li><a href="https://thrift.apache.org/tutorial/py" target="_blank" rel="noopener">https://thrift.apache.org/tutorial/py</a></li><li><a href="https://yq.aliyun.com/articles/88299" target="_blank" rel="noopener">https://yq.aliyun.com/articles/88299</a></li><li><a href="http://opentsdb.net/docs/build/html/user_guide/backends/hbase.html" target="_blank" rel="noopener">http://opentsdb.net/docs/build/html/user_guide/backends/hbase.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://hbase.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache HBase&lt;/a&gt; is a key-value store in Hadoop ecosystem. It is based on HDFS, and can provide high performance data access on large amount of volume. HBase is written in Java, and has native support for Java clients. But with the help of Thrift and various language bindings, we can access HBase in web services quite easily. This article will describe how to read and write HBase table with Python and Thrift.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/hbase.png&quot; alt&gt;&lt;/p&gt;
&lt;h2 id=&quot;Generate-Thrift-Class&quot;&gt;&lt;a href=&quot;#Generate-Thrift-Class&quot; class=&quot;headerlink&quot; title=&quot;Generate Thrift Class&quot;&gt;&lt;/a&gt;Generate Thrift Class&lt;/h2&gt;&lt;p&gt;For anyone who is new to &lt;a href=&quot;https://thrift.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache Thrift&lt;/a&gt;, it provides an IDL (Interface Description Language) to let you describe your service methods and data types and then transform them into different languages. For instance, a Thrift type definition like this:&lt;/p&gt;
&lt;figure class=&quot;highlight thrift&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;TColumn&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;: &lt;span class=&quot;keyword&quot;&gt;required&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;binary&lt;/span&gt; family,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;: &lt;span class=&quot;keyword&quot;&gt;optional&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;binary&lt;/span&gt; qualifier,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;: &lt;span class=&quot;keyword&quot;&gt;optional&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;i64&lt;/span&gt; timestamp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Will be transformed into the following Python code:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;TColumn&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(object)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, family=None, qualifier=None, timestamp=None,)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.family = family&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.qualifier = qualifier&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.timestamp = timestamp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, iprot)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        iprot.readStructBegin()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            (fname, ftype, fid) = iprot.readFieldBegin()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;comment&quot;&gt;# ...&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, oprot)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        oprot.writeStructBegin(&lt;span class=&quot;string&quot;&gt;&#39;TColumn&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# ...&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
      <category term="hbase" scheme="http://shzhangji.com/tags/hbase/"/>
    
      <category term="thrift" scheme="http://shzhangji.com/tags/thrift/"/>
    
  </entry>
  
  <entry>
    <title>Form Handling in Vuex Strict Mode</title>
    <link href="http://shzhangji.com/blog/2018/04/17/form-handling-in-vuex-strict-mode/"/>
    <id>http://shzhangji.com/blog/2018/04/17/form-handling-in-vuex-strict-mode/</id>
    <published>2018-04-17T06:13:40.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/vue.png" alt></p><p>When handling form inputs in Vue, we usually use <code>v-model</code> to achieve two-way binding. But if we want to put form data into Vuex store, two-way binding becomes a problem, since in <strong>strict mode</strong>, Vuex doesn’t allow state change outside mutation handlers. Take the following snippet for instance, while full code can be found on GitHub (<a href="https://github.com/jizhang/vuex-form" target="_blank" rel="noopener">link</a>).</p><p><code>src/store/table.js</code></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span><br><span class="line">  state: &#123;</span><br><span class="line">    namespaced: <span class="literal">true</span>,</span><br><span class="line">    table: &#123;</span><br><span class="line">      table_name: <span class="string">''</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>src/components/NonStrict.vue</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">b-form-group</span> <span class="attr">label</span>=<span class="string">"Table Name:"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">b-form-input</span> <span class="attr">v-model</span>=<span class="string">"table.table_name"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">b-form-group</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="javascript"><span class="keyword">import</span> &#123; mapState &#125; <span class="keyword">from</span> <span class="string">'vuex'</span></span></span><br><span class="line"></span><br><span class="line"><span class="javascript"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span></span><br><span class="line">  computed: &#123;</span><br><span class="line"><span class="javascript">    ...mapState(<span class="string">'table'</span>, [</span></span><br><span class="line"><span class="javascript">      <span class="string">'table'</span></span></span><br><span class="line">    ])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>When we input something in “Table Name” field, an error will be thrown in browser’s console:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Error: [vuex] Do not mutate vuex store state outside mutation handlers.</span><br><span class="line">    at assert (vuex.esm.js?358c:97)</span><br><span class="line">    at Vue.store._vm.$watch.deep (vuex.esm.js?358c:746)</span><br><span class="line">    at Watcher.run (vue.esm.js?efeb:3233)</span><br></pre></td></tr></table></figure><p>Apart from not using strict mode at all, which is fine if you’re ready to lose some benefits of tracking every mutation to the store, there’re several ways to solve this error. In this article, we’ll explore these solutions, and explain how they work.</p><a id="more"></a><h2 id="Local-Copy"><a href="#Local-Copy" class="headerlink" title="Local Copy"></a>Local Copy</h2><p>The first solution is to copy the form data from Vuex store to local state, do normal two-way binding, and commit to store when user submits the form.</p><p><code>src/components/LocalCopy.vue</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">b-form-input</span> <span class="attr">v-model</span>=<span class="string">"table.table_name"</span> /&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="javascript"><span class="keyword">import</span> _ <span class="keyword">from</span> <span class="string">'lodash'</span></span></span><br><span class="line"></span><br><span class="line"><span class="javascript"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span></span><br><span class="line">  data () &#123;</span><br><span class="line"><span class="javascript">    <span class="keyword">return</span> &#123;</span></span><br><span class="line"><span class="javascript">      table: _.cloneDeep(<span class="keyword">this</span>.$store.state.table.table)</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  methods: &#123;</span><br><span class="line">    handleSubmit (event) &#123;</span><br><span class="line"><span class="javascript">      <span class="keyword">this</span>.$store.commit(<span class="string">'table/setTable'</span>, <span class="keyword">this</span>.table)</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>src/store/table.js</code></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span><br><span class="line">  mutations: &#123;</span><br><span class="line">    setTable (state, payload) &#123;</span><br><span class="line">      state.table = payload</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>There’re two caveats in this solution. One is when you try to update the form after committing to store, you’ll again get “Error: [vuex] Do not mutate vuex store state outside mutation handlers.” It’s because the component’s local copy is assigned into Vuex store. We can modify the <code>setTable</code> mutation to solve it.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">setTable (state, payload) &#123;</span><br><span class="line">  <span class="comment">// assign properties individually</span></span><br><span class="line">  _.assign(state.table, payload)</span><br><span class="line">  <span class="comment">// or, clone the payload</span></span><br><span class="line">  state.table = _.cloneDeep(payload)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Another problem is when other components commit changes to Vuex store’s <code>table</code>, e.g. in a dialog with sub-forms, current component will not be updated. In this case, we’ll need to set a watched property.</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="javascript"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span></span><br><span class="line">  data () &#123;</span><br><span class="line"><span class="javascript">    <span class="keyword">return</span> &#123;</span></span><br><span class="line"><span class="javascript">      table: _.cloneDeep(<span class="keyword">this</span>.$store.state.table.table)</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  computed: &#123;</span><br><span class="line">    storeTable () &#123;</span><br><span class="line"><span class="javascript">      <span class="keyword">return</span> _.cloneDeep(<span class="keyword">this</span>.$store.state.table.table)</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  watch: &#123;</span><br><span class="line">    storeTable (newValue) &#123;</span><br><span class="line"><span class="javascript">      <span class="keyword">this</span>.table = newValue</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>This approach can also bypass the first caveat, because following updates in component’s form will not affect the object inside Vuex store.</p><h2 id="Explicit-Update"><a href="#Explicit-Update" class="headerlink" title="Explicit Update"></a>Explicit Update</h2><p>A ReactJS-like approach is to commit data on input / change event, i.e. use one-way data binding instead of two-way, and let Vuex store become the single source of truth of your application.</p><p><code>src/components/ExplicitUpdate.vue</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">b-form-input</span> <span class="attr">:value</span>=<span class="string">"table.table_name"</span> @<span class="attr">input</span>=<span class="string">"updateTableForm(&#123; table_name: $event &#125;)"</span> /&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="javascript"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span></span><br><span class="line">  computed: &#123;</span><br><span class="line"><span class="javascript">    ...mapState(<span class="string">'table'</span>, [</span></span><br><span class="line"><span class="javascript">      <span class="string">'table'</span></span></span><br><span class="line">    ])</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  methods: &#123;</span><br><span class="line"><span class="javascript">    ...mapMutations(<span class="string">'table'</span>, [</span></span><br><span class="line"><span class="javascript">      <span class="string">'updateTableForm'</span></span></span><br><span class="line">    ])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>src/store/table.js</code></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> table &#123;</span><br><span class="line">  mutations: &#123;</span><br><span class="line">    updateTableForm (state, payload) &#123;</span><br><span class="line">      _.assign(state.table, payload)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This is also the recommended way of form handling in Vuex <a href="https://vuex.vuejs.org/en/forms.html" target="_blank" rel="noopener">doc</a>, and according to Vue’s <a href="https://vuejs.org/v2/guide/forms.html" target="_blank" rel="noopener">doc</a>, <code>v-model</code> is essentially a syntax sugar for updating data on user input events.</p><h2 id="Computed-Property"><a href="#Computed-Property" class="headerlink" title="Computed Property"></a>Computed Property</h2><p>Vue’s computed property supports getter and setter, we can use it as a bridge between Vuex store and component. One limitation is computed property doesn’t support nested property, so we need to make aliases for nested states.</p><p><code>src/components/ComputedProperty.vue</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">b-form-input</span> <span class="attr">v-model</span>=<span class="string">"tableName"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">b-form-select</span> <span class="attr">v-model</span>=<span class="string">"tableCategory"</span> /&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="javascript"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span></span><br><span class="line">  computed: &#123;</span><br><span class="line">    tableName: &#123;</span><br><span class="line"><span class="javascript">      <span class="keyword">get</span> () &#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">return</span> <span class="keyword">this</span>.$store.state.table.table.table_name</span></span><br><span class="line">      &#125;,</span><br><span class="line"><span class="javascript">      <span class="keyword">set</span> (value) &#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">this</span>.updateTableForm(&#123; <span class="attr">table_name</span>: value &#125;)</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    tableCategory: &#123;</span><br><span class="line"><span class="javascript">      <span class="keyword">get</span> () &#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">return</span> <span class="keyword">this</span>.$store.state.table.table.category</span></span><br><span class="line">      &#125;,</span><br><span class="line"><span class="javascript">      <span class="keyword">set</span> (value) &#123;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">this</span>.updateTableForm(&#123; <span class="attr">category</span>: value &#125;)</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  methods: &#123;</span><br><span class="line"><span class="javascript">    ...mapMutations(<span class="string">'table'</span>, [</span></span><br><span class="line"><span class="javascript">      <span class="string">'updateTableForm'</span></span></span><br><span class="line">    ])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>When there’re a lot of fields, it becomes quite verbose to list them all. We may create some utilities for this purpose. First, in Vuex store, we add a common mutation that can set arbitrary state indicated by a lodash-style path.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mutations: &#123;</span><br><span class="line">  myUpdateField (state, payload) &#123;</span><br><span class="line">    <span class="keyword">const</span> &#123; path, value &#125; = payload</span><br><span class="line">    _.set(state, path, value)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Then in component, we write a function that takes alias / path pairs, and creates getter / setter for them.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> mapFields = <span class="function">(<span class="params">namespace, fields</span>) =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> _.mapValues(fields, path =&gt; &#123;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">      <span class="keyword">get</span> () &#123;</span><br><span class="line">        <span class="keyword">return</span> _.get(<span class="keyword">this</span>.$store.state[namespace], path)</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="keyword">set</span> (value) &#123;</span><br><span class="line">        <span class="keyword">this</span>.$store.commit(<span class="string">`<span class="subst">$&#123;namespace&#125;</span>/myUpdateField`</span>, &#123; path, value &#125;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span><br><span class="line">  computed: &#123;</span><br><span class="line">    ...mapFields(<span class="string">'table'</span>, &#123;</span><br><span class="line">      tableName: <span class="string">'table.table_name'</span>,</span><br><span class="line">      tableCategory: <span class="string">'table.category'</span>,</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In fact, someone’s already created a project named <a href="https://github.com/maoberlehner/vuex-map-fields" target="_blank" rel="noopener">vuex-map-fields</a>, whose <code>mapFields</code> utility does exactly the same thing.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://vuex.vuejs.org/en/forms.html" target="_blank" rel="noopener">https://vuex.vuejs.org/en/forms.html</a></li><li><a href="https://ypereirareis.github.io/blog/2017/04/25/vuejs-two-way-data-binding-state-management-vuex-strict-mode/" target="_blank" rel="noopener">https://ypereirareis.github.io/blog/2017/04/25/vuejs-two-way-data-binding-state-management-vuex-strict-mode/</a></li><li><a href="https://markus.oberlehner.net/blog/form-fields-two-way-data-binding-and-vuex/" target="_blank" rel="noopener">https://markus.oberlehner.net/blog/form-fields-two-way-data-binding-and-vuex/</a></li><li><a href="https://forum.vuejs.org/t/vuex-form-best-practices/20084" target="_blank" rel="noopener">https://forum.vuejs.org/t/vuex-form-best-practices/20084</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/vue.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;When handling form inputs in Vue, we usually use &lt;code&gt;v-model&lt;/code&gt; to achieve two-way binding. But if we want to put form data into Vuex store, two-way binding becomes a problem, since in &lt;strong&gt;strict mode&lt;/strong&gt;, Vuex doesn’t allow state change outside mutation handlers. Take the following snippet for instance, while full code can be found on GitHub (&lt;a href=&quot;https://github.com/jizhang/vuex-form&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;link&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;src/store/table.js&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight javascript&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;default&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  state: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    namespaced: &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    table: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      table_name: &lt;span class=&quot;string&quot;&gt;&#39;&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;src/components/NonStrict.vue&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight html&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;b-form-group&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;label&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;Table Name:&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;b-form-input&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;v-model&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;table.table_name&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;b-form-group&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;script&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;javascript&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; &amp;#123; mapState &amp;#125; &lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;vuex&#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;javascript&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;default&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  computed: &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;javascript&quot;&gt;    ...mapState(&lt;span class=&quot;string&quot;&gt;&#39;table&#39;&lt;/span&gt;, [&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;javascript&quot;&gt;      &lt;span class=&quot;string&quot;&gt;&#39;table&#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;script&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;When we input something in “Table Name” field, an error will be thrown in browser’s console:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Error: [vuex] Do not mutate vuex store state outside mutation handlers.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    at assert (vuex.esm.js?358c:97)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    at Vue.store._vm.$watch.deep (vuex.esm.js?358c:746)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    at Watcher.run (vue.esm.js?efeb:3233)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Apart from not using strict mode at all, which is fine if you’re ready to lose some benefits of tracking every mutation to the store, there’re several ways to solve this error. In this article, we’ll explore these solutions, and explain how they work.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="javascript" scheme="http://shzhangji.com/tags/javascript/"/>
    
      <category term="frontend" scheme="http://shzhangji.com/tags/frontend/"/>
    
      <category term="vue" scheme="http://shzhangji.com/tags/vue/"/>
    
      <category term="vuex" scheme="http://shzhangji.com/tags/vuex/"/>
    
  </entry>
  
  <entry>
    <title>Error Handling in RESTful API</title>
    <link href="http://shzhangji.com/blog/2018/04/07/error-handling-in-restful-api/"/>
    <id>http://shzhangji.com/blog/2018/04/07/error-handling-in-restful-api/</id>
    <published>2018-04-07T06:49:19.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/restful-api.png" alt="RESTful API"></p><p>RESTful API is a common tool of building web services, especially in front and back-end separated application. It is based on HTTP protocol, which is simple, text-oriented, and well supported by various languages, browsers or clients. However, REST is not yet standardized, so that the developers need to decide how to design their APIs. One of the decisions is error handling. Should I use HTTP status code? How to handle form validation errors, etc. This article will propose an error handling mechanism for RESTful API, based on my daily work and understanding of this technique.</p><h2 id="Types-of-Errors"><a href="#Types-of-Errors" class="headerlink" title="Types of Errors"></a>Types of Errors</h2><p>I tend to categorize errors into two types, global and local. Global errors include requesting an unknown API url, not being authorized to access this API, or there’s something wrong with the server code, unexpected and fatal. These errors should be caught by the web framework, no customized handling in individual API function.</p><p>Local errors, on the other hand, are closely related to the current API. Examples are form validation, violation of unique constraint, or other expected errors. We need to write specific codes to catch these errors, and raise a global error with message and payload for framework to catch and respond with.</p><p>Flask, for instance, provides a mechanism to catch exceptions globally:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BadRequest</span><span class="params">(Exception)</span>:</span></span><br><span class="line">    <span class="string">"""Custom exception class to be thrown when local error occurs."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, message, status=<span class="number">400</span>, payload=None)</span>:</span></span><br><span class="line">        self.message = message</span><br><span class="line">        self.status = status</span><br><span class="line">        self.payload = payload</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.errorhandler(BadRequest)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_bad_request</span><span class="params">(error)</span>:</span></span><br><span class="line">    <span class="string">"""Catch BadRequest exception globally, serialize into JSON, and respond with 400."""</span></span><br><span class="line">    payload = dict(error.payload <span class="keyword">or</span> ())</span><br><span class="line">    payload[<span class="string">'status'</span>] = error.status</span><br><span class="line">    payload[<span class="string">'message'</span>] = error.message</span><br><span class="line">    <span class="keyword">return</span> jsonify(payload), <span class="number">400</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/person', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">person_post</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Create a new person object and return its ID"""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> request.form.get(<span class="string">'username'</span>):</span><br><span class="line">        <span class="keyword">raise</span> BadRequest(<span class="string">'username cannot be empty'</span>, <span class="number">40001</span>, &#123; <span class="string">'ext'</span>: <span class="number">1</span> &#125;)</span><br><span class="line">    <span class="keyword">return</span> jsonify(last_insert_id=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Error-Response-Payload"><a href="#Error-Response-Payload" class="headerlink" title="Error Response Payload"></a>Error Response Payload</h2><p>When you post to <code>/person</code> with an empty <code>username</code>, it’ll return the following error response:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">HTTP/1.1 400 Bad Request</span><br><span class="line">Content-Type: application/json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;status&quot;: 40001,</span><br><span class="line">  &quot;message&quot;: &quot;username cannot be empty&quot;,</span><br><span class="line">  &quot;ext&quot;: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>There’re several parts in this response: HTTP status code, a custom status code, error message, and some extra information.</p><h3 id="Use-HTTP-Status-Code"><a href="#Use-HTTP-Status-Code" class="headerlink" title="Use HTTP Status Code"></a>Use HTTP Status Code</h3><p>HTTP status code itself provides rich semantics for errors. Generally <code>4xx</code> for client-side error and <code>5xx</code> server-side. Here’s a brief list of commonly used codes:</p><ul><li><code>200</code> Response is OK.</li><li><code>400</code> Bad request, e.g. user posts some in valid data.</li><li><code>401</code> Unauthorized. With <code>Flask-Login</code>, you can decorate a route with <code>@login_required</code>, and if the user hasn’t logged in, <code>401</code> will be returned, and client-side can redirect to login page.</li><li><code>403</code> Access is forbidden.</li><li><code>404</code> Resource not found.</li><li><code>500</code> Internal server error. Usually for unexpected and irrecoverable exceptions on the server-side.</li></ul><h3 id="Custom-Error-Code"><a href="#Custom-Error-Code" class="headerlink" title="Custom Error Code"></a>Custom Error Code</h3><p>When client receives an error, we can either open a global modal dialog to show the message, or handle the errors locally, such as displaying error messages below each form control. For this to work, we need to give these local errors a special coding convention, say <code>400</code> for global error, while <code>40001</code> and <code>40002</code> will trigger different error handlers.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">fetch().then(<span class="function"><span class="params">response</span> =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (response.status == <span class="number">400</span>) &#123; <span class="comment">// http status code</span></span><br><span class="line">    response.json().then(<span class="function"><span class="params">responseJson</span> =&gt;</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (responseJson.status == <span class="number">400</span>) &#123; <span class="comment">// custom error code</span></span><br><span class="line">        <span class="comment">// global error handler</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (responseJson.status == <span class="number">40001</span>) &#123; <span class="comment">// custom error code</span></span><br><span class="line">        <span class="comment">// custom error handler</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h3 id="More-Error-Information"><a href="#More-Error-Information" class="headerlink" title="More Error Information"></a>More Error Information</h3><p>Sometimes it is ideal to return all validation errors in one response, and we can use <code>payload</code> to achieve that.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"status"</span>: <span class="number">40001</span>,</span><br><span class="line">  <span class="string">"message"</span>: <span class="string">"form validation failed"</span></span><br><span class="line">  <span class="string">"errors"</span>: [</span><br><span class="line">    &#123; <span class="string">"name"</span>: <span class="string">"username"</span>, <span class="string">"error"</span>: <span class="string">"username cannot be empty"</span> &#125;,</span><br><span class="line">    &#123; <span class="string">"name"</span>: <span class="string">"password"</span>, <span class="string">"error"</span>: <span class="string">"password minimum length is 6"</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Fetch-API"><a href="#Fetch-API" class="headerlink" title="Fetch API"></a>Fetch API</h2><p>For AJAX request, <a href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API" target="_blank" rel="noopener">Fetch API</a> becomes the standard library. We can wrap it into a function that does proper error handling. Full code can be found in GitHub (<a href="https://github.com/jizhang/rest-error/blob/master/src/request.js" target="_blank" rel="noopener">link</a>).</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">request</span>(<span class="params">url, args, form</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> fetch(url, config)</span><br><span class="line">    .then(<span class="function"><span class="params">response</span> =&gt;</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (response.ok) &#123;</span><br><span class="line">        <span class="keyword">return</span> response.json()</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (response.status === <span class="number">400</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> response.json()</span><br><span class="line">          .then(<span class="function"><span class="params">responseJson</span> =&gt;</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (responseJson.status === <span class="number">400</span>) &#123;</span><br><span class="line">              alert(responseJson.message) <span class="comment">// global error handler</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// let subsequent "catch()" in the Promise chain handle the error</span></span><br><span class="line">            <span class="keyword">throw</span> responseJson</span><br><span class="line">          &#125;, error =&gt; &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RequestError(<span class="number">400</span>)</span><br><span class="line">          &#125;)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// handle predefined HTTP status code respectively</span></span><br><span class="line">      <span class="keyword">switch</span> (response.status) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">401</span>:</span><br><span class="line">          <span class="keyword">break</span> <span class="comment">// redirect to login page</span></span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">          alert(<span class="string">'HTTP Status Code '</span> + response.status)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> RequestError(response.status)</span><br><span class="line">    &#125;, error =&gt; &#123;</span><br><span class="line">      alert(error.message)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> RequestError(<span class="number">0</span>, error.message)</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This method will reject the promise whenever an error happens. Invokers can catch the error and check its <code>status</code>. Here’s an example of combining this approach with MobX and ReactJS:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// MobX Store</span></span><br><span class="line">loginUser = flow(<span class="function"><span class="keyword">function</span>* <span class="title">loginUser</span>(<span class="params">form</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.loading = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// yield may throw an error, i.e. reject this Promise</span></span><br><span class="line">    <span class="keyword">this</span>.userId = <span class="keyword">yield</span> request(<span class="string">'/login'</span>, <span class="literal">null</span>, form)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">this</span>.loading = <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// React Component</span></span><br><span class="line">login = <span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">  userStore.loginUser(<span class="keyword">this</span>.state.form)</span><br><span class="line">    .catch(<span class="function"><span class="params">error</span> =&gt;</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (error.status === <span class="number">40001</span>) &#123;</span><br><span class="line">        <span class="comment">// custom error handler</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://en.wikipedia.org/wiki/Representational_state_transfer" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Representational_state_transfer</a></li><li><a href="https://alidg.me/blog/2016/9/24/rest-api-error-handling" target="_blank" rel="noopener">https://alidg.me/blog/2016/9/24/rest-api-error-handling</a></li><li><a href="https://www.wptutor.io/web/js/generators-coroutines-async-javascript" target="_blank" rel="noopener">https://www.wptutor.io/web/js/generators-coroutines-async-javascript</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/restful-api.png&quot; alt=&quot;RESTful API&quot;&gt;&lt;/p&gt;
&lt;p&gt;RESTful API is a common tool of building web services, especially in front and back-end separated application. It is based on HTTP protocol, which is simple, text-oriented, and well supported by various languages, browsers or clients. However, REST is not yet standardized, so that the developers need to decide how to design their APIs. One of the decisions is error handling. Should I use HTTP status code? How to handle form validation errors, etc. This article will propose an error handling mechanism for RESTful API, based on my daily work and understanding of this technique.&lt;/p&gt;
&lt;h2 id=&quot;Types-of-Errors&quot;&gt;&lt;a href=&quot;#Types-of-Errors&quot; class=&quot;headerlink&quot; title=&quot;Types of Errors&quot;&gt;&lt;/a&gt;Types of Errors&lt;/h2&gt;&lt;p&gt;I tend to categorize errors into two types, global and local. Global errors include requesting an unknown API url, not being authorized to access this API, or there’s something wrong with the server code, unexpected and fatal. These errors should be caught by the web framework, no customized handling in individual API function.&lt;/p&gt;
&lt;p&gt;Local errors, on the other hand, are closely related to the current API. Examples are form validation, violation of unique constraint, or other expected errors. We need to write specific codes to catch these errors, and raise a global error with message and payload for framework to catch and respond with.&lt;/p&gt;
&lt;p&gt;Flask, for instance, provides a mechanism to catch exceptions globally:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;BadRequest&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(Exception)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;Custom exception class to be thrown when local error occurs.&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, message, status=&lt;span class=&quot;number&quot;&gt;400&lt;/span&gt;, payload=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.message = message&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.status = status&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.payload = payload&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@app.errorhandler(BadRequest)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;handle_bad_request&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(error)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;Catch BadRequest exception globally, serialize into JSON, and respond with 400.&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    payload = dict(error.payload &lt;span class=&quot;keyword&quot;&gt;or&lt;/span&gt; ())&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    payload[&lt;span class=&quot;string&quot;&gt;&#39;status&#39;&lt;/span&gt;] = error.status&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    payload[&lt;span class=&quot;string&quot;&gt;&#39;message&#39;&lt;/span&gt;] = error.message&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; jsonify(payload), &lt;span class=&quot;number&quot;&gt;400&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@app.route(&#39;/person&#39;, methods=[&#39;POST&#39;])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;person_post&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;Create a new person object and return its ID&quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; request.form.get(&lt;span class=&quot;string&quot;&gt;&#39;username&#39;&lt;/span&gt;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;raise&lt;/span&gt; BadRequest(&lt;span class=&quot;string&quot;&gt;&#39;username cannot be empty&#39;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;40001&lt;/span&gt;, &amp;#123; &lt;span class=&quot;string&quot;&gt;&#39;ext&#39;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; &amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; jsonify(last_insert_id=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
      <category term="javascript" scheme="http://shzhangji.com/tags/javascript/"/>
    
      <category term="frontend" scheme="http://shzhangji.com/tags/frontend/"/>
    
      <category term="restful" scheme="http://shzhangji.com/tags/restful/"/>
    
  </entry>
  
  <entry>
    <title>Flume Source Code: Component Lifecycle</title>
    <link href="http://shzhangji.com/blog/2017/10/23/flume-source-code-component-lifecycle/"/>
    <id>http://shzhangji.com/blog/2017/10/23/flume-source-code-component-lifecycle/</id>
    <published>2017-10-23T04:57:32.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://flume.apache.org/" target="_blank" rel="noopener">Apache Flume</a> is a real-time ETL tool for data warehouse platform. It consists of different types of components, and during runtime all of them are managed by Flume’s lifecycle and supervisor mechanism. This article will walk you through the source code of Flume’s component lifecycle management.</p><h2 id="Repository-Structure"><a href="#Repository-Structure" class="headerlink" title="Repository Structure"></a>Repository Structure</h2><p>Flume’s source code can be downloaded from GitHub. It’s a Maven project, so we can import it into an IDE for efficient code reading. The following is the main structure of the project:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/flume-ng-node</span><br><span class="line">/flume-ng-code</span><br><span class="line">/flume-ng-sdk</span><br><span class="line">/flume-ng-sources/flume-kafka-source</span><br><span class="line">/flume-ng-channels/flume-kafka-channel</span><br><span class="line">/flume-ng-sinks/flume-hdfs-sink</span><br></pre></td></tr></table></figure><h2 id="Application-Entrance"><a href="#Application-Entrance" class="headerlink" title="Application Entrance"></a>Application Entrance</h2><p>The <code>main</code> entrance of Flume agent is in the <code>org.apache.flume.node.Application</code> class of <code>flume-ng-node</code> module. Following is an abridged source code:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    CommandLineParser parser = <span class="keyword">new</span> GnuParser();</span><br><span class="line">    <span class="keyword">if</span> (isZkConfigured) &#123;</span><br><span class="line">      <span class="keyword">if</span> (reload) &#123;</span><br><span class="line">        PollingZooKeeperConfigurationProvider zookeeperConfigurationProvider;</span><br><span class="line">        components.add(zookeeperConfigurationProvider);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        StaticZooKeeperConfigurationProvider zookeeperConfigurationProvider;</span><br><span class="line">        application.handleConfigurationEvent();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// PropertiesFileConfigurationProvider</span></span><br><span class="line">    &#125;</span><br><span class="line">    application.start();</span><br><span class="line">    Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread(<span class="string">"agent-shutdown-hook"</span>) &#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        appReference.stop();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The process can be illustrated as follows:</p><ol><li>Parse command line arguments with <code>commons-cli</code>, including the Flume agent’s name, configuration method and path.</li><li>Configurations can be provided via properties file or ZooKeeper. Both provider support live-reload, i.e. we can update component settings without restarting the agent.<ul><li>File-based live-reload is implemented by using a background thread that checks the last modification time of the file.</li><li>ZooKeeper-based live-reload is provided by Curator’s <code>NodeCache</code> recipe, which uses ZooKeeper’s <em>watch</em> functionality underneath.</li></ul></li><li>If live-reload is on (by default), configuration providers will add themselves into the application’s component list, and after calling <code>Application#start</code>, a <code>LifecycleSupervisor</code> will start the provider, and trigger the reload event to parse the configuration and load all defined components.</li><li>If live-reload is off, configuration providers will parse the file immediately and start all components, also supervised by <code>LifecycleSupervisor</code>.</li><li>Finally add a JVM shutdown hook by <code>Runtime#addShutdownHook</code>, which in turn invokes <code>Application#stop</code> to shutdown the Flume agent.</li></ol><a id="more"></a><h2 id="Configuration-Reload"><a href="#Configuration-Reload" class="headerlink" title="Configuration Reload"></a>Configuration Reload</h2><p>In <code>PollingPropertiesFileConfigurationProvider</code>, when it detects file changes, it will invoke the <code>AbstractConfigurationProvider#getConfiguration</code> method to parse the configuration file into an <code>MaterializedConfiguration</code> instance, which contains the source, sink, and channel definitions. And then, the polling thread send an event to <code>Application</code> via a Guava’s <code>EventBus</code> instance, which effectively invokes the <code>Application#handleConfigurationEvent</code> method to reload all components.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Application class</span></span><br><span class="line"><span class="meta">@Subscribe</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">handleConfigurationEvent</span><span class="params">(MaterializedConfiguration conf)</span> </span>&#123;</span><br><span class="line">  stopAllComponents();</span><br><span class="line">  startAllComponents(conf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PollingPropertiesFileConfigurationProvider$FileWatcherRunnable</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  eventBus.post(getConfiguration());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Start-Components"><a href="#Start-Components" class="headerlink" title="Start Components"></a>Start Components</h2><p>The starting process lies in <code>Application#startAllComponents</code>. The method accepts a new set of components, starts the <code>Channel</code>s first, followed by <code>Sink</code>s and <code>Source</code>s.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startAllComponents</span><span class="params">(MaterializedConfiguration materializedConfiguration)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.materializedConfiguration = materializedConfiguration;</span><br><span class="line">  <span class="keyword">for</span> (Entry&lt;String, Channel&gt; entry :</span><br><span class="line">      materializedConfiguration.getChannels().entrySet()) &#123;</span><br><span class="line">    supervisor.supervise(entry.getValue(),</span><br><span class="line">        <span class="keyword">new</span> SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//  Wait for all channels to start.</span></span><br><span class="line">  <span class="keyword">for</span> (Channel ch : materializedConfiguration.getChannels().values()) &#123;</span><br><span class="line">    <span class="keyword">while</span> (ch.getLifecycleState() != LifecycleState.START</span><br><span class="line">        &amp;&amp; !supervisor.isComponentInErrorState(ch)) &#123;</span><br><span class="line">      Thread.sleep(<span class="number">500</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Start and supervise sinkds and sources</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The <code>LifecycleSupervisor</code> manages instances that implement <code>LifecycleAware</code> interface. Supervisor will schedule a <code>MonitorRunnable</code> instance with a fixed delay (3 secs), which tries to convert a <code>LifecycleAware</code> instance into its <code>desiredState</code>, by calling <code>LifecycleAware#start</code> or <code>stop</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MonitorRunnable</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!lifecycleAware.getLifecycleState().equals(</span><br><span class="line">        supervisoree.status.desiredState)) &#123;</span><br><span class="line">      <span class="keyword">switch</span> (supervisoree.status.desiredState) &#123;</span><br><span class="line">        <span class="keyword">case</span> START:</span><br><span class="line">          lifecycleAware.start();</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> STOP:</span><br><span class="line">          lifecycleAware.stop();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Stop-Components"><a href="#Stop-Components" class="headerlink" title="Stop Components"></a>Stop Components</h2><p>When JVM is shutting down, the hook invokes <code>Application#stop</code>, which calls <code>LifecycleSupervisor#stop</code>, that first shutdowns the <code>MonitorRunnable</code>s’ executor pool, and changes all components’ desired status to <code>STOP</code>, waiting for them to fully shutdown.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LifecycleSupervisor</span> <span class="keyword">implements</span> <span class="title">LifecycleAware</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    monitorService.shutdown();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">final</span> Entry&lt;LifecycleAware, Supervisoree&gt; entry :</span><br><span class="line">        supervisedProcesses.entrySet()) &#123;</span><br><span class="line">      <span class="keyword">if</span> (entry.getKey().getLifecycleState().equals(LifecycleState.START)) &#123;</span><br><span class="line">        entry.getValue().status.desiredState = LifecycleState.STOP;</span><br><span class="line">        entry.getKey().stop();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Source-and-Source-Runner"><a href="#Source-and-Source-Runner" class="headerlink" title="Source and Source Runner"></a>Source and Source Runner</h2><p>Take <code>KafkaSource</code> for an instance, we shall see how agent supervises source components, and the same thing happens to sinks and channels.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaSource</span> <span class="keyword">extends</span> <span class="title">AbstractPollableSource</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doStart</span><span class="params">()</span> <span class="keyword">throws</span> FlumeException </span>&#123;</span><br><span class="line">    consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, <span class="keyword">byte</span>[]&gt;(kafkaProps);</span><br><span class="line">    it = consumer.poll(<span class="number">1000</span>).iterator();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doStop</span><span class="params">()</span> <span class="keyword">throws</span> FlumeException </span>&#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>KafkaSource</code> is a pollable source, which means it needs a runner thread to constantly poll for more data to process.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PollableSourceRunner</span> <span class="keyword">extends</span> <span class="title">SourceRunner</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    source.start();</span><br><span class="line">    runner = <span class="keyword">new</span> PollingRunner();</span><br><span class="line">    runnerThread = <span class="keyword">new</span> Thread(runner);</span><br><span class="line">    runnerThread.start();</span><br><span class="line">    lifecycleState = LifecycleState.START;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    runnerThread.interrupt();</span><br><span class="line">    runnerThread.join();</span><br><span class="line">    source.stop();</span><br><span class="line">    lifecycleState = LifecycleState.STOP;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">PollingRunner</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">while</span> (!shouldStop.get()) &#123;</span><br><span class="line">        source.process();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Both <code>AbstractPollableSource</code> and <code>SourceRunner</code> are subclass of <code>LifecycleAware</code>, which means they have <code>start</code> and <code>stop</code> methods for supervisor to call. In this case, <code>SourceRunner</code> is the component that Flume agent actually supervises, and <code>PollableSource</code> is instantiated and managed by <code>SourceRunner</code>. Details lie in <code>AbstractConfigurationProvider#loadSources</code>:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">loadSources</span><span class="params">(Map&lt;String, SourceRunner&gt; sourceRunnerMap)</span> </span>&#123;</span><br><span class="line">  Source source = sourceFactory.create();</span><br><span class="line">  Configurables.configure(source, config);</span><br><span class="line">  sourceRunnerMap.put(comp.getComponentName(),</span><br><span class="line">      SourceRunner.forSource(source));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://github.com/apache/flume" target="_blank" rel="noopener">https://github.com/apache/flume</a></li><li><a href="https://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">https://flume.apache.org/FlumeUserGuide.html</a></li><li><a href="https://kafka.apache.org/0100/javadoc/index.html" target="_blank" rel="noopener">https://kafka.apache.org/0100/javadoc/index.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://flume.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache Flume&lt;/a&gt; is a real-time ETL tool for data warehouse platform. It consists of different types of components, and during runtime all of them are managed by Flume’s lifecycle and supervisor mechanism. This article will walk you through the source code of Flume’s component lifecycle management.&lt;/p&gt;
&lt;h2 id=&quot;Repository-Structure&quot;&gt;&lt;a href=&quot;#Repository-Structure&quot; class=&quot;headerlink&quot; title=&quot;Repository Structure&quot;&gt;&lt;/a&gt;Repository Structure&lt;/h2&gt;&lt;p&gt;Flume’s source code can be downloaded from GitHub. It’s a Maven project, so we can import it into an IDE for efficient code reading. The following is the main structure of the project:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-node&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-code&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-sdk&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-sources/flume-kafka-source&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-channels/flume-kafka-channel&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/flume-ng-sinks/flume-hdfs-sink&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;Application-Entrance&quot;&gt;&lt;a href=&quot;#Application-Entrance&quot; class=&quot;headerlink&quot; title=&quot;Application Entrance&quot;&gt;&lt;/a&gt;Application Entrance&lt;/h2&gt;&lt;p&gt;The &lt;code&gt;main&lt;/code&gt; entrance of Flume agent is in the &lt;code&gt;org.apache.flume.node.Application&lt;/code&gt; class of &lt;code&gt;flume-ng-node&lt;/code&gt; module. Following is an abridged source code:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Application&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    CommandLineParser parser = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; GnuParser();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (isZkConfigured) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (reload) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        PollingZooKeeperConfigurationProvider zookeeperConfigurationProvider;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        components.add(zookeeperConfigurationProvider);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &amp;#125; &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        StaticZooKeeperConfigurationProvider zookeeperConfigurationProvider;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        application.handleConfigurationEvent();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125; &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;comment&quot;&gt;// PropertiesFileConfigurationProvider&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    application.start();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Runtime.getRuntime().addShutdownHook(&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Thread(&lt;span class=&quot;string&quot;&gt;&quot;agent-shutdown-hook&quot;&lt;/span&gt;) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;meta&quot;&gt;@Override&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        appReference.stop();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;The process can be illustrated as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Parse command line arguments with &lt;code&gt;commons-cli&lt;/code&gt;, including the Flume agent’s name, configuration method and path.&lt;/li&gt;
&lt;li&gt;Configurations can be provided via properties file or ZooKeeper. Both provider support live-reload, i.e. we can update component settings without restarting the agent.&lt;ul&gt;
&lt;li&gt;File-based live-reload is implemented by using a background thread that checks the last modification time of the file.&lt;/li&gt;
&lt;li&gt;ZooKeeper-based live-reload is provided by Curator’s &lt;code&gt;NodeCache&lt;/code&gt; recipe, which uses ZooKeeper’s &lt;em&gt;watch&lt;/em&gt; functionality underneath.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If live-reload is on (by default), configuration providers will add themselves into the application’s component list, and after calling &lt;code&gt;Application#start&lt;/code&gt;, a &lt;code&gt;LifecycleSupervisor&lt;/code&gt; will start the provider, and trigger the reload event to parse the configuration and load all defined components.&lt;/li&gt;
&lt;li&gt;If live-reload is off, configuration providers will parse the file immediately and start all components, also supervised by &lt;code&gt;LifecycleSupervisor&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Finally add a JVM shutdown hook by &lt;code&gt;Runtime#addShutdownHook&lt;/code&gt;, which in turn invokes &lt;code&gt;Application#stop&lt;/code&gt; to shutdown the Flume agent.&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="flume" scheme="http://shzhangji.com/tags/flume/"/>
    
      <category term="java" scheme="http://shzhangji.com/tags/java/"/>
    
      <category term="source code" scheme="http://shzhangji.com/tags/source-code/"/>
    
  </entry>
  
  <entry>
    <title>Pandas and Tidy Data</title>
    <link href="http://shzhangji.com/blog/2017/09/30/pandas-and-tidy-data/"/>
    <id>http://shzhangji.com/blog/2017/09/30/pandas-and-tidy-data/</id>
    <published>2017-09-30T04:24:32.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p>In the paper <a href="https://www.jstatsoft.org/article/view/v059i10" target="_blank" rel="noopener">Tidy Data</a>, <a href="https://en.wikipedia.org/wiki/Hadley_Wickham" target="_blank" rel="noopener">Dr. Wickham</a> proposed a specific form of data structure: each variable is a column, each observation is a row, and each type of observational unit is a table. He argued that with tidy data, data analysts can manipulate, model, and visualize data more easily and effectively. He lists <em>five common data structures</em> that are untidy, and demonstrates how to use <a href="https://github.com/hadley/tidy-data/" target="_blank" rel="noopener">R language</a> to tidy them. In this article, we’ll use Python and Pandas to achieve the same tidiness.</p><p>Source code and demo data can be found on GitHub (<a href="https://github.com/jizhang/pandas-tidy-data" target="_blank" rel="noopener">link</a>), and readers are supposed to have Python environment installed, preferably with Anaconda and Spyder IDE.</p><h2 id="Column-headers-are-values-not-variable-names"><a href="#Column-headers-are-values-not-variable-names" class="headerlink" title="Column headers are values, not variable names"></a>Column headers are values, not variable names</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">'data/pew.csv'</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/tidy-data/pew.png" alt="Religion and Income - Pew Forum"></p><p>Column names “&lt;$10k”, “$10-20k” are really income ranges that constitutes a variable. Variables are measurements of attributes, like height, weight, and in this case, income and religion. The values within the table form another variable, frequency. To make <em>each variable a column</em>, we do the following transformation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = df.set_index(<span class="string">'religion'</span>)</span><br><span class="line">df = df.stack()</span><br><span class="line">df.index = df.index.rename(<span class="string">'income'</span>, level=<span class="number">1</span>)</span><br><span class="line">df.name = <span class="string">'frequency'</span></span><br><span class="line">df = df.reset_index()</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/tidy-data/pew-tidy.png" alt="Religion and Income - Tidy"></p><a id="more"></a><p>Here we use the <a href="https://pandas.pydata.org/pandas-docs/stable/reshaping.html" target="_blank" rel="noopener">stack / unstack</a> feature of Pandas MultiIndex objects. <code>stack()</code> will use the column names to form a second level of index, then we do some proper naming and use <code>reset_index()</code> to flatten the table. In line 4 <code>df</code> is actually a Series, since Pandas will automatically convert from a single-column DataFrame.</p><p>Pandas provides another more commonly used method to do the transformation, <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html" target="_blank" rel="noopener"><code>melt()</code></a>. It accepts the following arguments:</p><ul><li><code>frame</code>: the DataFrame to manipulate.</li><li><code>id_vars</code>: columns that stay put.</li><li><code>value_vars</code>: columns that will be transformed to a variable.</li><li><code>var_name</code>: name the newly added variable column.</li><li><code>value_name</code>: name the value column.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'data/pew.csv'</span>)</span><br><span class="line">df = pd.melt(df, id_vars=[<span class="string">'religion'</span>], value_vars=list(df.columns)[<span class="number">1</span>:],</span><br><span class="line">             var_name=<span class="string">'income'</span>, value_name=<span class="string">'frequency'</span>)</span><br><span class="line">df = df.sort_values(by=<span class="string">'religion'</span>)</span><br><span class="line">df.to_csv(<span class="string">'data/pew-tidy.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>This will give the same result. We’ll use <code>melt()</code> method a lot in the following sections.</p><p>Let’s take a look at another form of untidiness that falls in this section:</p><p><img src="/images/tidy-data/billboard.png" alt="Billboard 2000"></p><p>In this dataset, weekly ranks are recorded in separate columns. To answer the question “what’s the rank of ‘Dancing Queen’ in 2000-07-15”, we need to do some calculations with <code>date.entered</code> and the week columns. Let’s transform it into a tidy form:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'data/billboard.csv'</span>)</span><br><span class="line">df = pd.melt(df, id_vars=list(df.columns)[:<span class="number">5</span>], value_vars=list(df.columns)[<span class="number">5</span>:],</span><br><span class="line">             var_name=<span class="string">'week'</span>, value_name=<span class="string">'rank'</span>)</span><br><span class="line">df[<span class="string">'week'</span>] = df[<span class="string">'week'</span>].str[<span class="number">2</span>:].astype(int)</span><br><span class="line">df[<span class="string">'date.entered'</span>] = pd.to_datetime(df[<span class="string">'date.entered'</span>]) + pd.to_timedelta((df[<span class="string">'week'</span>] - <span class="number">1</span>) * <span class="number">7</span>, <span class="string">'d'</span>)</span><br><span class="line">df = df.rename(columns=&#123;<span class="string">'date.entered'</span>: <span class="string">'date'</span>&#125;)</span><br><span class="line">df = df.sort_values(by=[<span class="string">'track'</span>, <span class="string">'date'</span>])</span><br><span class="line">df.to_csv(<span class="string">'data/billboard-intermediate.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/tidy-data/billboard-intermediate.png" alt="Billboard 2000 - Intermediate Tidy"></p><p>We’ve also transformed the <code>date.entered</code> variable into the exact date of that week. Now <code>week</code> becomes a single column that represents a variable. But we can see a lot of duplications in this table, like artist and track. We’ll solve this problem in the fourth section.</p><h2 id="Multiple-variables-stored-in-one-column"><a href="#Multiple-variables-stored-in-one-column" class="headerlink" title="Multiple variables stored in one column"></a>Multiple variables stored in one column</h2><p>Storing variable values in columns is quite common because it makes the data table more compact, and easier to do analysis like cross validation, etc. The following dataset even manages to store two variables in the column, sex and age.</p><p><img src="/images/tidy-data/tb.png" alt="Tuberculosis (TB)"></p><p><code>m</code> stands for <code>male</code>, <code>f</code> for <code>female</code>, and age ranges are <code>0-14</code>, <code>15-24</code>, and so forth. To tidy it, we first melt the columns, use Pandas’ string operation to extract <code>sex</code>, and do a value mapping for the <code>age</code> ranges.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'data/tb.csv'</span>)</span><br><span class="line">df = pd.melt(df, id_vars=[<span class="string">'country'</span>, <span class="string">'year'</span>], value_vars=list(df.columns)[<span class="number">2</span>:],</span><br><span class="line">             var_name=<span class="string">'column'</span>, value_name=<span class="string">'cases'</span>)</span><br><span class="line">df = df[df[<span class="string">'cases'</span>] != <span class="string">'---'</span>]</span><br><span class="line">df[<span class="string">'cases'</span>] = df[<span class="string">'cases'</span>].astype(int)</span><br><span class="line">df[<span class="string">'sex'</span>] = df[<span class="string">'column'</span>].str[<span class="number">0</span>]</span><br><span class="line">df[<span class="string">'age'</span>] = df[<span class="string">'column'</span>].str[<span class="number">1</span>:].map(&#123;</span><br><span class="line">    <span class="string">'014'</span>: <span class="string">'0-14'</span>,</span><br><span class="line">    <span class="string">'1524'</span>: <span class="string">'15-24'</span>,</span><br><span class="line">    <span class="string">'2534'</span>: <span class="string">'25-34'</span>,</span><br><span class="line">    <span class="string">'3544'</span>: <span class="string">'35-44'</span>,</span><br><span class="line">    <span class="string">'4554'</span>: <span class="string">'45-54'</span>,</span><br><span class="line">    <span class="string">'5564'</span>: <span class="string">'55-64'</span>,</span><br><span class="line">    <span class="string">'65'</span>: <span class="string">'65+'</span></span><br><span class="line">&#125;)</span><br><span class="line">df = df[[<span class="string">'country'</span>, <span class="string">'year'</span>, <span class="string">'sex'</span>, <span class="string">'age'</span>, <span class="string">'cases'</span>]]</span><br><span class="line">df.to_csv(<span class="string">'data/tb-tidy.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/tidy-data/tb-tidy.png" alt="Tuberculosis (TB) - Tidy"></p><h2 id="Variables-are-stored-in-both-rows-and-columns"><a href="#Variables-are-stored-in-both-rows-and-columns" class="headerlink" title="Variables are stored in both rows and columns"></a>Variables are stored in both rows and columns</h2><p>This is a temperature dataset collection by a Weather Station named MX17004. Dates are spread in columns which can be melted into one column. <code>tmax</code> and <code>tmin</code> stand for highest and lowest temperatures, and they are really variables of each observational unit, in this case, each day, so we should <code>unstack</code> them into different columns.</p><p><img src="/images/tidy-data/weather.png" alt="Weather Station"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'data/weather.csv'</span>)</span><br><span class="line">df = pd.melt(df, id_vars=[<span class="string">'id'</span>, <span class="string">'year'</span>, <span class="string">'month'</span>, <span class="string">'element'</span>],</span><br><span class="line">             value_vars=list(df.columns)[<span class="number">4</span>:],</span><br><span class="line">             var_name=<span class="string">'date'</span>, value_name=<span class="string">'value'</span>)</span><br><span class="line">df[<span class="string">'date'</span>] = df[<span class="string">'date'</span>].str[<span class="number">1</span>:].astype(<span class="string">'int'</span>)</span><br><span class="line">df[<span class="string">'date'</span>] = df[[<span class="string">'year'</span>, <span class="string">'month'</span>, <span class="string">'date'</span>]].apply(</span><br><span class="line">    <span class="keyword">lambda</span> row: <span class="string">'&#123;:4d&#125;-&#123;:02d&#125;-&#123;:02d&#125;'</span>.format(*row),</span><br><span class="line">    axis=<span class="number">1</span>)</span><br><span class="line">df = df.loc[df[<span class="string">'value'</span>] != <span class="string">'---'</span>, [<span class="string">'id'</span>, <span class="string">'date'</span>, <span class="string">'element'</span>, <span class="string">'value'</span>]]</span><br><span class="line">df = df.set_index([<span class="string">'id'</span>, <span class="string">'date'</span>, <span class="string">'element'</span>])</span><br><span class="line">df = df.unstack()</span><br><span class="line">df.columns = list(df.columns.get_level_values(<span class="string">'element'</span>))</span><br><span class="line">df = df.reset_index()</span><br><span class="line">df.to_csv(<span class="string">'data/weather-tidy.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><p><img src="/images/tidy-data/weather-tidy.png" alt="Weather Station - Tidy"></p><h2 id="Multiple-types-in-one-table"><a href="#Multiple-types-in-one-table" class="headerlink" title="Multiple types in one table"></a>Multiple types in one table</h2><p>In the processed Billboard dataset, we can see duplicates of song tracks, it’s because this table actually contains two types of observational units, song tracks and weekly ranks. To tidy it, we first generate identities for each song track, i.e. <code>id</code>, and then separate them into different tables.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'data/billboard-intermediate.csv'</span>)</span><br><span class="line">df_track = df[[<span class="string">'artist'</span>, <span class="string">'track'</span>, <span class="string">'time'</span>]].drop_duplicates()</span><br><span class="line">df_track.insert(<span class="number">0</span>, <span class="string">'id'</span>, range(<span class="number">1</span>, len(df_track) + <span class="number">1</span>))</span><br><span class="line">df = pd.merge(df, df_track, on=[<span class="string">'artist'</span>, <span class="string">'track'</span>, <span class="string">'time'</span>])</span><br><span class="line">df = df[[<span class="string">'id'</span>, <span class="string">'date'</span>, <span class="string">'rank'</span>]]</span><br><span class="line">df_track.to_csv(<span class="string">'data/billboard-track.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">df.to_csv(<span class="string">'data/billboard-rank.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">print(df_track, <span class="string">'\n\n'</span>, df)</span><br></pre></td></tr></table></figure><p><img src="/images/tidy-data/billboard-track.png" alt="Billboard 2000 - Track"></p><p><img src="/images/tidy-data/billboard-rank.png" alt="Billboard 2000 - Rank"></p><h2 id="One-type-in-multiple-tables"><a href="#One-type-in-multiple-tables" class="headerlink" title="One type in multiple tables"></a>One type in multiple tables</h2><p>Datasets can be separated in two ways, by different values of an variable like year 2000, 2001, location China, Britain, or by different attributes like temperature from one sensor, humidity from another. In the first case, we can write a utility function that walks through the data directory, reads each file, and assigns the filename to a dedicated column. In the end we can combine these DataFrames with <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html" target="_blank" rel="noopener"><code>pd.concat</code></a>. In the latter case, there should be some attribute that can identify the same units, like date, personal ID, etc. We can use <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge.html" target="_blank" rel="noopener"><code>pd.merge</code></a> to join datasets by common keys.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://tomaugspurger.github.io/modern-5-tidy.html" target="_blank" rel="noopener">https://tomaugspurger.github.io/modern-5-tidy.html</a></li><li><a href="https://hackernoon.com/reshaping-data-in-python-fa27dda2ff77" target="_blank" rel="noopener">https://hackernoon.com/reshaping-data-in-python-fa27dda2ff77</a></li><li><a href="http://www.jeannicholashould.com/tidy-data-in-python.html" target="_blank" rel="noopener">http://www.jeannicholashould.com/tidy-data-in-python.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In the paper &lt;a href=&quot;https://www.jstatsoft.org/article/view/v059i10&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Tidy Data&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Hadley_Wickham&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Dr. Wickham&lt;/a&gt; proposed a specific form of data structure: each variable is a column, each observation is a row, and each type of observational unit is a table. He argued that with tidy data, data analysts can manipulate, model, and visualize data more easily and effectively. He lists &lt;em&gt;five common data structures&lt;/em&gt; that are untidy, and demonstrates how to use &lt;a href=&quot;https://github.com/hadley/tidy-data/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;R language&lt;/a&gt; to tidy them. In this article, we’ll use Python and Pandas to achieve the same tidiness.&lt;/p&gt;
&lt;p&gt;Source code and demo data can be found on GitHub (&lt;a href=&quot;https://github.com/jizhang/pandas-tidy-data&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;link&lt;/a&gt;), and readers are supposed to have Python environment installed, preferably with Anaconda and Spyder IDE.&lt;/p&gt;
&lt;h2 id=&quot;Column-headers-are-values-not-variable-names&quot;&gt;&lt;a href=&quot;#Column-headers-are-values-not-variable-names&quot; class=&quot;headerlink&quot; title=&quot;Column headers are values, not variable names&quot;&gt;&lt;/a&gt;Column headers are values, not variable names&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;data/pew.csv&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df.head(&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;img src=&quot;/images/tidy-data/pew.png&quot; alt=&quot;Religion and Income - Pew Forum&quot;&gt;&lt;/p&gt;
&lt;p&gt;Column names “&amp;lt;$10k”, “$10-20k” are really income ranges that constitutes a variable. Variables are measurements of attributes, like height, weight, and in this case, income and religion. The values within the table form another variable, frequency. To make &lt;em&gt;each variable a column&lt;/em&gt;, we do the following transformation:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;df = df.set_index(&lt;span class=&quot;string&quot;&gt;&#39;religion&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df = df.stack()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df.index = df.index.rename(&lt;span class=&quot;string&quot;&gt;&#39;income&#39;&lt;/span&gt;, level=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df.name = &lt;span class=&quot;string&quot;&gt;&#39;frequency&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df = df.reset_index()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df.head(&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;img src=&quot;/images/tidy-data/pew-tidy.png&quot; alt=&quot;Religion and Income - Tidy&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
      <category term="analytics" scheme="http://shzhangji.com/tags/analytics/"/>
    
      <category term="pandas" scheme="http://shzhangji.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>Apache Beam Quick Start with Python</title>
    <link href="http://shzhangji.com/blog/2017/09/12/apache-beam-quick-start-with-python/"/>
    <id>http://shzhangji.com/blog/2017/09/12/apache-beam-quick-start-with-python/</id>
    <published>2017-09-12T13:08:25.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://beam.apache.org/get-started/beam-overview/" target="_blank" rel="noopener">Apache Beam</a> is a big data processing standard created by Google in 2016. It provides unified DSL to process both batch and stream data, and can be executed on popular platforms like Spark, Flink, and of course Google’s commercial product Dataflow. Beam’s model is based on previous works known as <a href="https://web.archive.org/web/20160923141630/https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35650.pdf" target="_blank" rel="noopener">FlumeJava</a> and <a href="https://web.archive.org/web/20160201091359/http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41378.pdf" target="_blank" rel="noopener">Millwheel</a>, and addresses solutions for data processing tasks like ETL, analysis, and <a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101" target="_blank" rel="noopener">stream processing</a>. Currently it provides SDK in two languages, Java and Python. This article will introduce how to use Python to write Beam applications.</p><p><img src="/images/beam/arch.jpg" alt="Apache Beam Pipeline"></p><h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>Apache Beam Python SDK requires Python 2.7.x. You can use <a href="https://github.com/pyenv/pyenv" target="_blank" rel="noopener">pyenv</a> to manage different Python versions, or compile from <a href="https://www.python.org/downloads/source/" target="_blank" rel="noopener">source</a> (make sure you have SSL installed). And then you can install Beam SDK from PyPI, better in a virtual environment:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ virtualenv venv --distribute</span><br><span class="line">$ source venv/bin/activate</span><br><span class="line">(venv) $ pip install apache-beam</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Wordcount-Example"><a href="#Wordcount-Example" class="headerlink" title="Wordcount Example"></a>Wordcount Example</h2><p>Wordcount is the de-facto “Hello World” in big data field, so let’s take a look at how it’s done with Beam:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> apache_beam <span class="keyword">as</span> beam</span><br><span class="line"><span class="keyword">from</span> apache_beam.options.pipeline_options <span class="keyword">import</span> PipelineOptions</span><br><span class="line"><span class="keyword">with</span> beam.Pipeline(options=PipelineOptions()) <span class="keyword">as</span> p:</span><br><span class="line">    lines = p | <span class="string">'Create'</span> &gt;&gt; beam.Create([<span class="string">'cat dog'</span>, <span class="string">'snake cat'</span>, <span class="string">'dog'</span>])</span><br><span class="line">    counts = (</span><br><span class="line">        lines</span><br><span class="line">        | <span class="string">'Split'</span> &gt;&gt; (beam.FlatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">' '</span>))</span><br><span class="line">                      .with_output_types(unicode))</span><br><span class="line">        | <span class="string">'PairWithOne'</span> &gt;&gt; beam.Map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">        | <span class="string">'GroupAndSum'</span> &gt;&gt; beam.CombinePerKey(sum)</span><br><span class="line">    )</span><br><span class="line">    counts | <span class="string">'Print'</span> &gt;&gt; beam.ParDo(<span class="keyword">lambda</span> (w, c): print(<span class="string">'%s: %s'</span> % (w, c)))</span><br></pre></td></tr></table></figure><p>Run the script, you’ll get the counts of difference words:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(venv) $ python wordcount.py</span><br><span class="line">cat: 2</span><br><span class="line">snake: 1</span><br><span class="line">dog: 2</span><br></pre></td></tr></table></figure><p>There’re three fundamental concepts in Apache Beam, namely Pipeline, PCollection, and Transform.</p><ul><li><strong>Pipeline</strong> holds the DAG (Directed Acyclic Graph) of data and process tasks. It’s analogous to MapReduce <code>Job</code> and Storm <code>Topology</code>.</li><li><strong>PCollection</strong> is the data structure to which we apply various operations, like parse, convert, or aggregate. You can think of it as Spark <code>RDD</code>.</li><li>And <strong>Transform</strong> is where your main logic goes. Each transform will take a PCollection in and produce a new PCollection. Beam provides many built-in Transforms, and we’ll cover them later.</li></ul><p>As in this example, <code>Pipeline</code> and <code>PipelineOptions</code> are used to construct a pipeline. Use the <code>with</code> statement so that context manager will invoke <code>Pipeline.run</code> and <code>wait_until_finish</code> automatically.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[Output PCollection] = [Input PCollection] | [Label] &gt;&gt; [Transform]</span><br></pre></td></tr></table></figure><p><code>|</code> is the operator to apply transforms, and each transform can be optionally supplied with a unique label. Transforms can be chained, and we can compose arbitrary shapes of transforms, and at runtime they’ll be represented as DAG.</p><p><code>beam.Create</code> is a transform that creates PCollection from memory data, mainly for testing. Beam has built-in sources and sinks to read and write bounded or unbounded data, and it’s possible to implement our own.</p><p><code>beam.Map</code> is a <em>one-to-one</em> transform, and in this example we convert a word string to a <code>(word, 1)</code> tuple. <code>beam.FlatMap</code> is a combination of <code>Map</code> and <code>Flatten</code>, i.e. we split each line into an array of words, and then flatten these sequences into a single one.</p><p><code>CombinePerKey</code> works on two-element tuples. It groups the tuples by the first element (the key), and apply the provided function to the list of second elements (values). Finally, we use <code>beam.ParDo</code> to print out the counts. This is a rather basic transform, and we’ll discuss it in the following section.</p><h2 id="Input-and-Output"><a href="#Input-and-Output" class="headerlink" title="Input and Output"></a>Input and Output</h2><p>Currently, Beam’s Python SDK has very limited supports for IO. This table (<a href="https://beam.apache.org/documentation/io/built-in/" target="_blank" rel="noopener">source</a>) gives an overview of the available built-in transforms:</p><table><thead><tr><th>Language</th><th>File-based</th><th>Messaging</th><th>Database</th></tr></thead><tbody><tr><td>Java</td><td>HDFS<br>TextIO<br>XML</td><td>AMQP<br>Kafka<br>JMS</td><td>Hive<br>Solr<br>JDBC</td></tr><tr><td>Python</td><td>textio<br>avroio<br>tfrecordio</td><td>-</td><td>Google Big Query<br>Google Cloud Datastore</td></tr></tbody></table><p>The following snippet demonstrates the usage of <code>textio</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lines = p | <span class="string">'Read'</span> &gt;&gt; beam.io.ReadFromText(<span class="string">'/path/to/input-*.csv'</span>)</span><br><span class="line">lines | <span class="string">'Write'</span> &gt;&gt; beam.io.WriteToText(<span class="string">'/path/to/output'</span>, file_name_suffix=<span class="string">'.csv'</span>)</span><br></pre></td></tr></table></figure><p><code>textio</code> is able to read multiple input files by using wildcard or you can flatten PCollections created from difference sources. The outputs are also split into several files due to pipeline’s parallel processing nature.</p><h2 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h2><p>There’re basic transforms and higher-level built-ins. In general, we prefer to use the later so that we can focus on the application logic. The following table lists some commonly used higher-level transforms:</p><table><thead><tr><th>Transform</th><th>Meaning</th></tr></thead><tbody><tr><td>Create(value)</td><td>Creates a PCollection from an iterable.</td></tr><tr><td>Filter(fn)</td><td>Use callable <code>fn</code> to filter out elements.</td></tr><tr><td>Map(fn)</td><td>Use callable <code>fn</code> to do a one-to-one transformation.</td></tr><tr><td>FlatMap(fn)</td><td>Similar to <code>Map</code>, but <code>fn</code> needs to return an iterable of zero or more elements, and these iterables will be flattened into one PCollection.</td></tr><tr><td>Flatten()</td><td>Merge several PCollections into a single one.</td></tr><tr><td>Partition(fn)</td><td>Split a PCollection into several partitions. <code>fn</code> is a <code>PartitionFn</code> or a callable that accepts two arguments - <code>element</code>, <code>num_partitions</code>.</td></tr><tr><td>GroupByKey()</td><td>Works on a PCollection of key/value pairs (two-element tuples), groups by common key, and returns <code>(key, iter&lt;value&gt;)</code> pairs.</td></tr><tr><td>CoGroupByKey()</td><td>Groups results across several PCollections by key. e.g. input <code>(k, v)</code> and <code>(k, w)</code>, output <code>(k, (iter&lt;v&gt;, iter&lt;w&gt;))</code>.</td></tr><tr><td>RemoveDuplicates()</td><td>Get distint values in PCollection.</td></tr><tr><td>CombinePerKey(fn)</td><td>Similar to <code>GroupByKey</code>, but combines the values by a <code>CombineFn</code> or a callable that takes an iterable, such as <code>sum</code>, <code>max</code>.</td></tr><tr><td>CombineGlobally(fn)</td><td>Reduces a PCollection to a single value by applying <code>fn</code>.</td></tr></tbody></table><h3 id="Callable-DoFn-and-ParDo"><a href="#Callable-DoFn-and-ParDo" class="headerlink" title="Callable, DoFn, and ParDo"></a>Callable, DoFn, and ParDo</h3><p>Most transforms accepts a callable as argument. In Python, <a href="https://docs.python.org/2/library/functions.html#callable" target="_blank" rel="noopener">callable</a> can be a function, method, lambda expression, or class instance that has <code>__call__</code> method. Under the hood, Beam will wrap the callable as a <code>DoFn</code>, and all these transforms will invoke <code>ParDo</code>, the lower-level transform, with the <code>DoFn</code>.</p><p>Let’s replace the expression <code>lambda x: x.split(&#39; &#39;)</code> with a <code>DoFn</code> class:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SplitFn</span><span class="params">(beam.DoFn)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, element)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> element.split(<span class="string">' '</span>)</span><br><span class="line"></span><br><span class="line">lines | beam.ParDo(SplitFn())</span><br></pre></td></tr></table></figure><p>The <code>ParDo</code> transform works like <code>FlatMap</code>, except that it only accepts <code>DoFn</code>. In addition to <code>return</code>, we can <code>yield</code> element from <code>process</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SplitAndPairWithOneFn</span><span class="params">(beam.DoFn)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, element)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> element.split(<span class="string">' '</span>):</span><br><span class="line">            <span class="keyword">yield</span> (word, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Combiner-Functions"><a href="#Combiner-Functions" class="headerlink" title="Combiner Functions"></a>Combiner Functions</h3><p>Combiner functions, or <code>CombineFn</code>, are used to reduce a collection of elements into a single value. You can either perform on the entire PCollection (<code>CombineGlobally</code>), or combine the values for each key (<code>CombinePerKey</code>). Beam is capable of wrapping callables into <code>CombinFn</code>. The callable should take an iterable and returns a single value. Since Beam distributes computation to multiple nodes, the combiner function will be invoked multiple times to get partial results, so they ought to be <a href="https://en.wikipedia.org/wiki/Commutative_property" target="_blank" rel="noopener">commutative</a> and <a href="https://en.wikipedia.org/wiki/Associative_property" target="_blank" rel="noopener">associative</a>. <code>sum</code>, <code>min</code>, <code>max</code> are good examples.</p><p>Beam provides some built-in combiners like count, mean, top. Take count for instance, the following two lines are equivalent, they return the total count of lines.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lines | beam.combiners.Count.Globally()</span><br><span class="line">lines | beam.CombineGlobally(beam.combiners.CountCombineFn())</span><br></pre></td></tr></table></figure><p>Other combiners can be found in Beam Python SDK Documentation (<a href="https://beam.apache.org/documentation/sdks/pydoc/2.1.0/apache_beam.transforms.html#module-apache_beam.transforms.combiners" target="_blank" rel="noopener">link</a>). For more complex combiners, we need to subclass the <code>CombinFn</code> and implement four methods. Take the built-in <code>Mean</code> for an example:</p><p><a href="https://github.com/apache/beam/blob/v2.1.0/sdks/python/apache_beam/transforms/combiners.py#L75" target="_blank" rel="noopener"><code>apache_beam/transforms/combiners.py</code></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MeanCombineFn</span><span class="params">(core.CombineFn)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">create_accumulator</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Create a "local" accumulator to track sum and count."""</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add_input</span><span class="params">(self, <span class="params">(sum_, count)</span>, element)</span>:</span></span><br><span class="line">    <span class="string">"""Process the incoming value."""</span></span><br><span class="line">    <span class="keyword">return</span> sum_ + element, count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge_accumulators</span><span class="params">(self, accumulators)</span>:</span></span><br><span class="line">    <span class="string">"""Merge several accumulators into a single one."""</span></span><br><span class="line">    sums, counts = zip(*accumulators)</span><br><span class="line">    <span class="keyword">return</span> sum(sums), sum(counts)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">extract_output</span><span class="params">(self, <span class="params">(sum_, count)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Compute the mean average."""</span></span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">0</span>:</span><br><span class="line">      <span class="keyword">return</span> float(<span class="string">'NaN'</span>)</span><br><span class="line">    <span class="keyword">return</span> sum_ / float(count)</span><br></pre></td></tr></table></figure><h3 id="Composite-Transform"><a href="#Composite-Transform" class="headerlink" title="Composite Transform"></a>Composite Transform</h3><p>Take a look at the <a href="https://github.com/apache/beam/blob/v2.1.0/sdks/python/apache_beam/transforms/combiners.py#L101" target="_blank" rel="noopener">source code</a> of <code>beam.combiners.Count.Globally</code> we used before. It subclasses <code>PTransform</code> and applies some transforms to the PCollection. This forms a sub-graph of DAG, and we call it composite transform. Composite transforms are used to gather relative codes into logical modules, making them easy to understand and maintain.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Count</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Globally</span><span class="params">(ptransform.PTransform)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">expand</span><span class="params">(self, pcoll)</span>:</span></span><br><span class="line">      <span class="keyword">return</span> pcoll | core.CombineGlobally(CountCombineFn())</span><br></pre></td></tr></table></figure><p>More built-in transforms are listed below:</p><table><thead><tr><th>Transform</th><th>Meaning</th></tr></thead><tbody><tr><td>Count.Globally()</td><td>Count the total number of elements.</td></tr><tr><td>Count.PerKey()</td><td>Count number elements of each unique key.</td></tr><tr><td>Count.PerElement()</td><td>Count the occurrences of each element.</td></tr><tr><td>Mean.Globally()</td><td>Compute the average of all elements.</td></tr><tr><td>Mean.PerKey()</td><td>Compute the averages for each key.</td></tr><tr><td>Top.Of(n, reverse)</td><td>Get the top <code>n</code> elements from the PCollection. See also Top.Largest(n), Top.Smallest(n).</td></tr><tr><td>Top.PerKey(n, reverse)</td><td>Get top <code>n</code> elements for each key. See also Top.LargestPerKey(n), Top.SmallestPerKey(n)</td></tr><tr><td>Sample.FixedSizeGlobally(n)</td><td>Get a sample of <code>n</code> elements.</td></tr><tr><td>Sample.FixedSizePerKey(n)</td><td>Get samples from each key.</td></tr><tr><td>ToList()</td><td>Combine to a single list.</td></tr><tr><td>ToDict()</td><td>Combine to a single dict. Works on 2-element tuples.</td></tr></tbody></table><h2 id="Windowing"><a href="#Windowing" class="headerlink" title="Windowing"></a>Windowing</h2><p>When processing event data, such as access log or click stream, there’s an <em>event time</em> property attached to every item, and it’s common to perform aggregation on a per-time-window basis. With Beam, we can define different kinds of windows to divide event data into groups. Windowing can be used in both bounded and unbounded data source. Since current Python SDK only supports bounded source, the following example will work on an offline access log file, but the process can be applied to unbounded source as is.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">64.242.88.10 - - [07/Mar/2004:16:05:49 -0800] &quot;GET /edit HTTP/1.1&quot; 401 12846</span><br><span class="line">64.242.88.10 - - [07/Mar/2004:16:06:51 -0800] &quot;GET /rdiff HTTP/1.1&quot; 200 4523</span><br><span class="line">64.242.88.10 - - [07/Mar/2004:16:10:02 -0800] &quot;GET /hsdivision HTTP/1.1&quot; 200 6291</span><br><span class="line">64.242.88.10 - - [07/Mar/2004:16:11:58 -0800] &quot;GET /view HTTP/1.1&quot; 200 7352</span><br><span class="line">64.242.88.10 - - [07/Mar/2004:16:20:55 -0800] &quot;GET /view HTTP/1.1&quot; 200 5253</span><br></pre></td></tr></table></figure><p><code>logmining.py</code>, full source code can be found on GitHub (<a href="https://github.com/jizhang/hello-beam/blob/master/logmining.py" target="_blank" rel="noopener">link</a>).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">lines = p | <span class="string">'Create'</span> &gt;&gt; beam.io.ReadFromText(<span class="string">'access.log'</span>)</span><br><span class="line">windowed_counts = (</span><br><span class="line">    lines</span><br><span class="line">    | <span class="string">'Timestamp'</span> &gt;&gt; beam.Map(<span class="keyword">lambda</span> x: beam.window.TimestampedValue(</span><br><span class="line">                              x, extract_timestamp(x)))</span><br><span class="line">    | <span class="string">'Window'</span> &gt;&gt; beam.WindowInto(beam.window.SlidingWindows(<span class="number">600</span>, <span class="number">300</span>))</span><br><span class="line">    | <span class="string">'Count'</span> &gt;&gt; (beam.CombineGlobally(beam.combiners.CountCombineFn())</span><br><span class="line">                  .without_defaults())</span><br><span class="line">)</span><br><span class="line">windowed_counts =  windowed_counts | beam.ParDo(PrintWindowFn())</span><br></pre></td></tr></table></figure><p>First of all, we need to add a timestamp to each record. <code>extract_timestamp</code> is a custom function to parse <code>[07/Mar/2004:16:05:49 -0800]</code> as a unix timestamp. <code>TimestampedValue</code> links this timestamp to the record. Then we define a sliding window with the size <em>10 minutes</em> and period <em>5 minutes</em>, which means the first window is <code>[00:00, 00:10)</code>, second window is <code>[00:05, 00:15)</code>, and so forth. All windows have a <em>10 minutes</em> duration, and adjacent windows have a <em>5 minutes</em> shift. Sliding window is different from fixed window, in that the same elements could appear in different windows. The combiner function is a simple count, so the pipeline result of the first five logs will be:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[2004-03-08T00:00:00Z, 2004-03-08T00:10:00Z) @ 2</span><br><span class="line">[2004-03-08T00:05:00Z, 2004-03-08T00:15:00Z) @ 4</span><br><span class="line">[2004-03-08T00:10:00Z, 2004-03-08T00:20:00Z) @ 2</span><br><span class="line">[2004-03-08T00:15:00Z, 2004-03-08T00:25:00Z) @ 1</span><br><span class="line">[2004-03-08T00:20:00Z, 2004-03-08T00:30:00Z) @ 1</span><br></pre></td></tr></table></figure><p>In stream processing for unbounded source, event data will arrive in different order, so we need to deal with late data with Beam’s watermark and trigger facility. This is a rather advanced topic, and the Python SDK has not yet implemented this feature. If you’re interested, please refer to Stream <a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101" target="_blank" rel="noopener">101</a> and <a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102" target="_blank" rel="noopener">102</a> articles.</p><h2 id="Pipeline-Runner"><a href="#Pipeline-Runner" class="headerlink" title="Pipeline Runner"></a>Pipeline Runner</h2><p>As mentioned above, Apache Beam is just a standard that provides SDK and APIs. It’s the pipeline runner that is responsible to execute the workflow graph. The following matrix lists all available runners and their capabilities compared to Beam Model.</p><p><img src="/images/beam/matrix.png" alt="Beam Capability Matrix"></p><p><a href="https://beam.apache.org/documentation/runners/capability-matrix/" target="_blank" rel="noopener">Source</a></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://beam.apache.org/documentation/programming-guide/" target="_blank" rel="noopener">https://beam.apache.org/documentation/programming-guide/</a></li><li><a href="https://beam.apache.org/documentation/sdks/pydoc/2.1.0/" target="_blank" rel="noopener">https://beam.apache.org/documentation/sdks/pydoc/2.1.0/</a></li><li><a href="https://sookocheff.com/post/dataflow/get-to-know-dataflow/" target="_blank" rel="noopener">https://sookocheff.com/post/dataflow/get-to-know-dataflow/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://beam.apache.org/get-started/beam-overview/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache Beam&lt;/a&gt; is a big data processing standard created by Google in 2016. It provides unified DSL to process both batch and stream data, and can be executed on popular platforms like Spark, Flink, and of course Google’s commercial product Dataflow. Beam’s model is based on previous works known as &lt;a href=&quot;https://web.archive.org/web/20160923141630/https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35650.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;FlumeJava&lt;/a&gt; and &lt;a href=&quot;https://web.archive.org/web/20160201091359/http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41378.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Millwheel&lt;/a&gt;, and addresses solutions for data processing tasks like ETL, analysis, and &lt;a href=&quot;https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;stream processing&lt;/a&gt;. Currently it provides SDK in two languages, Java and Python. This article will introduce how to use Python to write Beam applications.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/beam/arch.jpg&quot; alt=&quot;Apache Beam Pipeline&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Installation&quot;&gt;&lt;a href=&quot;#Installation&quot; class=&quot;headerlink&quot; title=&quot;Installation&quot;&gt;&lt;/a&gt;Installation&lt;/h2&gt;&lt;p&gt;Apache Beam Python SDK requires Python 2.7.x. You can use &lt;a href=&quot;https://github.com/pyenv/pyenv&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pyenv&lt;/a&gt; to manage different Python versions, or compile from &lt;a href=&quot;https://www.python.org/downloads/source/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;source&lt;/a&gt; (make sure you have SSL installed). And then you can install Beam SDK from PyPI, better in a virtual environment:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ virtualenv venv --distribute&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ source venv/bin/activate&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;(venv) $ pip install apache-beam&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
      <category term="stream processing" scheme="http://shzhangji.com/tags/stream-processing/"/>
    
      <category term="apache beam" scheme="http://shzhangji.com/tags/apache-beam/"/>
    
      <category term="mapreduce" scheme="http://shzhangji.com/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>Hive Window and Analytical Functions</title>
    <link href="http://shzhangji.com/blog/2017/09/04/hive-window-and-analytical-functions/"/>
    <id>http://shzhangji.com/blog/2017/09/04/hive-window-and-analytical-functions/</id>
    <published>2017-09-04T13:55:23.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p>SQL is one of the major tools of data analysis. It provides filtering, transforming and aggregation functionalities, and we can use it to process big volume of data with the help of Hive and Hadoop. However, legacy SQL does not support operations like grouped ranking and moving average, because the <code>GROUP BY</code> clause can only produce one aggregation result for each group, but not for each row. Fortunately, with the new SQL standard coming, we can use the <code>WINDOW</code> clause to compute aggregations on a set of rows and return the result for each row.</p><p><img src="/images/hive-window/window-stock.png" alt="Moving Average"></p><p>For instance, if we want to calculate the two-day moving average for each stock, we can write the following query:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="string">`date`</span>, <span class="string">`stock`</span>, <span class="string">`close`</span></span><br><span class="line">  ,<span class="keyword">AVG</span>(<span class="string">`close`</span>) <span class="keyword">OVER</span> <span class="string">`w`</span> <span class="keyword">AS</span> <span class="string">`mavg`</span></span><br><span class="line"><span class="keyword">FROM</span> <span class="string">`t_stock`</span></span><br><span class="line"><span class="keyword">WINDOW</span> <span class="string">`w`</span> <span class="keyword">AS</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="string">`stock`</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="string">`date`</span></span><br><span class="line">               <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>)</span><br></pre></td></tr></table></figure><p><code>OVER</code>, <code>WINDOW</code> and <code>ROWS BETWEEN AND</code> are all newly added SQL keywords to support windowing operations. In this query, <code>PARTITION BY</code> and <code>ORDER BY</code> works like <code>GROUP BY</code> and <code>ORDER BY</code> after the <code>WHERE</code> clause, except it doesn’t collapse the rows, but only divides them into non-overlapping partitions to work on. <code>ROWS BETWEEN AND</code> here constructs a <strong>window frame</strong>. In this case, each frame contains the previous row and current row. We’ll discuss more on frames later. Finally, <code>AVG</code> is a window function that computes results on each frame. Note that <code>WINDOW</code> clause can also be directly appended to window function:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">AVG</span>(<span class="string">`close`</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="string">`stock`</span>) <span class="keyword">AS</span> <span class="string">`mavg`</span> <span class="keyword">FROM</span> <span class="string">`t_stock`</span>;</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Window-Query-Concepts"><a href="#Window-Query-Concepts" class="headerlink" title="Window Query Concepts"></a>Window Query Concepts</h2><p><img src="/images/hive-window/concepts.png" alt="Concepts"></p><p><a href="https://en.wikibooks.org/wiki/Structured_Query_Language/Window_functions" target="_blank" rel="noopener">Source</a></p><p>SQL window query introduces three concepts, namely window partition, window frame and window function.</p><p><code>PARTITION</code> clause divides result set into <strong>window partitions</strong> by one or more columns, and the rows within can be optionally sorted by one or more columns. If there’s not <code>PARTITION BY</code>, the entire result set is treated as a single partition; if there’s not <code>ORDER BY</code>, window frames cannot be defined, and all rows within the partition constitutes a single frame.</p><p><strong>Window frame</strong> selects rows from partition for window function to work on. There’re two ways of defining frame in Hive, <code>ROWS</code> AND <code>RANGE</code>. For both types, we define the upper bound and lower bound. For instance, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> selects rows from the beginning of the partition to the current row; <code>SUM(close) RANGE BETWEEN 100 PRECEDING AND 200 FOLLOWING</code> selects rows by the <em>distance</em> from the current row’s value. Say current <code>close</code> is <code>200</code>, and this frame will includes rows whose <code>close</code> values range from <code>100</code> to <code>400</code>, within the partition. All possible combinations of frame definitions are listed as follows, and the default definition is <code>RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br></pre></td></tr></table></figure><p>All <strong>window functions</strong> compute results on the current frame. Hive supports the following functions:</p><ul><li><code>FIRST_VALUE(col)</code>, <code>LAST_VALUE(col)</code> returns the column value of first / last row within the frame;</li><li><code>LEAD(col, n)</code>, <code>LAG(col, n)</code> returns the column value of n-th row before / after current row;</li><li><code>RANK()</code>, <code>ROW_NUMBER()</code> assigns a sequence of the current row within the frame. The difference is <code>RANK()</code> will contain duplicate if there’re identical values.</li><li><code>COUNT()</code>, <code>SUM(col)</code>, <code>MIN(col)</code> works as usual.</li></ul><h2 id="Hive-Query-Examples"><a href="#Hive-Query-Examples" class="headerlink" title="Hive Query Examples"></a>Hive Query Examples</h2><h3 id="Top-K"><a href="#Top-K" class="headerlink" title="Top K"></a>Top K</h3><p>First, let’s create some test data of employee incomes in Hive:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t_employee (<span class="keyword">id</span> <span class="built_in">INT</span>, emp_name <span class="built_in">VARCHAR</span>(<span class="number">20</span>), dep_name <span class="built_in">VARCHAR</span>(<span class="number">20</span>),</span><br><span class="line">salary <span class="built_in">DECIMAL</span>(<span class="number">7</span>, <span class="number">2</span>), age <span class="built_in">DECIMAL</span>(<span class="number">3</span>, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> t_employee <span class="keyword">VALUES</span></span><br><span class="line">( <span class="number">1</span>,  <span class="string">'Matthew'</span>, <span class="string">'Management'</span>,  <span class="number">4500</span>, <span class="number">55</span>),</span><br><span class="line">( <span class="number">2</span>,  <span class="string">'Olivia'</span>,  <span class="string">'Management'</span>,  <span class="number">4400</span>, <span class="number">61</span>),</span><br><span class="line">( <span class="number">3</span>,  <span class="string">'Grace'</span>,   <span class="string">'Management'</span>,  <span class="number">4000</span>, <span class="number">42</span>),</span><br><span class="line">( <span class="number">4</span>,  <span class="string">'Jim'</span>,     <span class="string">'Production'</span>,  <span class="number">3700</span>, <span class="number">35</span>),</span><br><span class="line">( <span class="number">5</span>,  <span class="string">'Alice'</span>,   <span class="string">'Production'</span>,  <span class="number">3500</span>, <span class="number">24</span>),</span><br><span class="line">( <span class="number">6</span>,  <span class="string">'Michael'</span>, <span class="string">'Production'</span>,  <span class="number">3600</span>, <span class="number">28</span>),</span><br><span class="line">( <span class="number">7</span>,  <span class="string">'Tom'</span>,     <span class="string">'Production'</span>,  <span class="number">3800</span>, <span class="number">35</span>),</span><br><span class="line">( <span class="number">8</span>,  <span class="string">'Kevin'</span>,   <span class="string">'Production'</span>,  <span class="number">4000</span>, <span class="number">52</span>),</span><br><span class="line">( <span class="number">9</span>,  <span class="string">'Elvis'</span>,   <span class="string">'Service'</span>,     <span class="number">4100</span>, <span class="number">40</span>),</span><br><span class="line">(<span class="number">10</span>,  <span class="string">'Sophia'</span>,  <span class="string">'Sales'</span>,       <span class="number">4300</span>, <span class="number">36</span>),</span><br><span class="line">(<span class="number">11</span>,  <span class="string">'Samantha'</span>,<span class="string">'Sales'</span>,       <span class="number">4100</span>, <span class="number">38</span>);</span><br></pre></td></tr></table></figure><p>We can use the <code>RANK()</code> function to find out who earns the most within each department:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> dep_name, emp_name, salary</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">  <span class="keyword">SELECT</span></span><br><span class="line">    dep_name, emp_name, salary</span><br><span class="line">    ,<span class="keyword">RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> dep_name <span class="keyword">ORDER</span> <span class="keyword">BY</span> salary <span class="keyword">DESC</span>) <span class="keyword">AS</span> rnk</span><br><span class="line">  <span class="keyword">FROM</span> t_employee</span><br><span class="line">) a</span><br><span class="line"><span class="keyword">WHERE</span> rnk = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>Normally when there’s duplicates, <code>RANK()</code> returns the same value for each row and <em>skip</em> the next sequence number. Use <code>DENSE_RANK()</code> if you want consecutive ranks.</p><h3 id="Cumulative-Distribution"><a href="#Cumulative-Distribution" class="headerlink" title="Cumulative Distribution"></a>Cumulative Distribution</h3><p>We can calculate the cumulative distribution of salaries among all departments. For example, salary <code>4000</code>‘s cumulative distribution is <code>0.55</code>, which means 55% people’s salaries are less or equal to <code>4000</code>. To calculate this, we first count the frequencies of every salary, and do a cumulative summing:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  salary</span><br><span class="line">  ,<span class="keyword">SUM</span>(cnt) <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary)</span><br><span class="line">  / <span class="keyword">SUM</span>(cnt) <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span></span><br><span class="line">                   <span class="keyword">AND</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span>)</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">  <span class="keyword">SELECT</span> salary, <span class="keyword">count</span>(*) <span class="keyword">AS</span> cnt</span><br><span class="line">  <span class="keyword">FROM</span> t_employee</span><br><span class="line">  <span class="keyword">GROUP</span> <span class="keyword">BY</span> salary</span><br><span class="line">) a;</span><br></pre></td></tr></table></figure><p>This can also be done with Hive’s <code>CUME_DIST()</code> window function. There’s another <code>PERCENT_RANK()</code> function, which computes the rank of the salary as percentage.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  salary</span><br><span class="line">  ,<span class="keyword">CUME_DIST</span>() <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary) <span class="keyword">AS</span> pct_cum</span><br><span class="line">  ,<span class="keyword">PERCENT_RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary) <span class="keyword">AS</span> pct_rank</span><br><span class="line"><span class="keyword">FROM</span> t_employee;</span><br></pre></td></tr></table></figure><p><img src="/images/hive-window/employee-pct.png" alt="Cumulative Distribution"></p><h3 id="Clickstream-Sessionization"><a href="#Clickstream-Sessionization" class="headerlink" title="Clickstream Sessionization"></a>Clickstream Sessionization</h3><p>We can divide click events into different sessions by setting a <em>timeout</em>, in this case 30 minutes, and assign an id to each session:</p><p><img src="/images/hive-window/clickstream.png" alt="Click Stream"></p><p>First, in subquery <code>b</code>, we use the <code>LAG(col)</code> function to calculate the time difference between current row and previous row, and if it’s more than 30 minutes, a new session is marked. Then we do a cumulative sum of the <code>new_session</code> field so that each session will get an incremental sequence.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  ipaddress, clicktime</span><br><span class="line">  ,<span class="keyword">SUM</span>(<span class="keyword">IF</span>(new_session, <span class="number">1</span>, <span class="number">0</span>)) <span class="keyword">OVER</span> x + <span class="number">1</span> <span class="keyword">AS</span> sessionid</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">  <span class="keyword">SELECT</span></span><br><span class="line">    ipaddress, clicktime, ts</span><br><span class="line">    ,ts - LAG(ts) <span class="keyword">OVER</span> w &gt; <span class="number">1800</span> <span class="keyword">AS</span> new_session</span><br><span class="line">  <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> *, <span class="keyword">UNIX_TIMESTAMP</span>(clicktime) <span class="keyword">AS</span> ts</span><br><span class="line">    <span class="keyword">FROM</span> t_clickstream</span><br><span class="line">  ) a</span><br><span class="line">  <span class="keyword">WINDOW</span> w <span class="keyword">AS</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> ipaddress <span class="keyword">ORDER</span> <span class="keyword">BY</span> ts)</span><br><span class="line">) b</span><br><span class="line"><span class="keyword">WINDOW</span> x <span class="keyword">AS</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> ipaddress <span class="keyword">ORDER</span> <span class="keyword">BY</span> ts);</span><br></pre></td></tr></table></figure><h2 id="Implementation-Detail"><a href="#Implementation-Detail" class="headerlink" title="Implementation Detail"></a>Implementation Detail</h2><p>Briefly speaking, window query consists of two steps: divide records into partitions, and evaluate window functions on each of them. The partitioning process is intuitive in map-reduce paradigm, since Hadoop will take care of the shuffling and sorting. However, ordinary UDAF can only return one row for each group, but in window query, there need to be a <em>table in, table out</em> contract. So the community introduced Partitioned Table Function (PTF) into Hive.</p><p>PTF, as the name suggests, works on partitions, and inputs / outputs a set of table rows. The following sequence diagram lists the major classes of PTF mechanism. <code>PTFOperator</code> reads data from sorted source and create input partitions; <code>WindowTableFunction</code> manages window frames, invokes window functions (UDAF), and writes the results to output partitions.</p><p><img src="/images/hive-window/window-sequence.png" alt="PTF Sequence Diagram"></p><p>The HIVE-896 ticket (<a href="https://issues.apache.org/jira/browse/HIVE-896" target="_blank" rel="noopener">link</a>) contains discussions on introducing analytical window functions into Hive, and this slide (<a href="https://www.slideshare.net/Hadoop_Summit/analytical-queries-with-hive" target="_blank" rel="noopener">link</a>), authored by one of the committers, explains how they implemented and merged PTF into Hive.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics</a></li><li><a href="https://github.com/hbutani/SQLWindowing" target="_blank" rel="noopener">https://github.com/hbutani/SQLWindowing</a></li><li><a href="https://content.pivotal.io/blog/time-series-analysis-1-introduction-to-window-functions" target="_blank" rel="noopener">https://content.pivotal.io/blog/time-series-analysis-1-introduction-to-window-functions</a></li><li><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SQL is one of the major tools of data analysis. It provides filtering, transforming and aggregation functionalities, and we can use it to process big volume of data with the help of Hive and Hadoop. However, legacy SQL does not support operations like grouped ranking and moving average, because the &lt;code&gt;GROUP BY&lt;/code&gt; clause can only produce one aggregation result for each group, but not for each row. Fortunately, with the new SQL standard coming, we can use the &lt;code&gt;WINDOW&lt;/code&gt; clause to compute aggregations on a set of rows and return the result for each row.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/hive-window/window-stock.png&quot; alt=&quot;Moving Average&quot;&gt;&lt;/p&gt;
&lt;p&gt;For instance, if we want to calculate the two-day moving average for each stock, we can write the following query:&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;string&quot;&gt;`date`&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;`stock`&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;`close`&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  ,&lt;span class=&quot;keyword&quot;&gt;AVG&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;`close`&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;OVER&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`w`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`mavg`&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`t_stock`&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;WINDOW&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`w`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AS&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`stock`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`date`&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               &lt;span class=&quot;keyword&quot;&gt;ROWS&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;PRECEDING&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;CURRENT&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;ROW&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;OVER&lt;/code&gt;, &lt;code&gt;WINDOW&lt;/code&gt; and &lt;code&gt;ROWS BETWEEN AND&lt;/code&gt; are all newly added SQL keywords to support windowing operations. In this query, &lt;code&gt;PARTITION BY&lt;/code&gt; and &lt;code&gt;ORDER BY&lt;/code&gt; works like &lt;code&gt;GROUP BY&lt;/code&gt; and &lt;code&gt;ORDER BY&lt;/code&gt; after the &lt;code&gt;WHERE&lt;/code&gt; clause, except it doesn’t collapse the rows, but only divides them into non-overlapping partitions to work on. &lt;code&gt;ROWS BETWEEN AND&lt;/code&gt; here constructs a &lt;strong&gt;window frame&lt;/strong&gt;. In this case, each frame contains the previous row and current row. We’ll discuss more on frames later. Finally, &lt;code&gt;AVG&lt;/code&gt; is a window function that computes results on each frame. Note that &lt;code&gt;WINDOW&lt;/code&gt; clause can also be directly appended to window function:&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AVG&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;`close`&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;OVER&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`stock`&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`mavg`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`t_stock`&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="analytics" scheme="http://shzhangji.com/tags/analytics/"/>
    
      <category term="sql" scheme="http://shzhangji.com/tags/sql/"/>
    
      <category term="hive" scheme="http://shzhangji.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>An Introduction to stream-lib The Stream Processing Utilities</title>
    <link href="http://shzhangji.com/blog/2017/08/27/an-introduction-to-stream-lib-the-stream-processing-utilities/"/>
    <id>http://shzhangji.com/blog/2017/08/27/an-introduction-to-stream-lib-the-stream-processing-utilities/</id>
    <published>2017-08-27T02:57:24.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p>When processing a large amount of data, certain operations will cost a lot of time and space, such as counting the distinct values, or figuring out the 95th percentile of a sequence of numbers. But sometimes the accuracy is not that important. Maybe you just want a brief summary of the dataset, or it’s a monitoring system, where limited error rate is tolerable. There’re plenty of such algorithms that can trade accuracy with huge saves of time-space. What’s more, most of the data structures can be merged, making it possible to use in stream processing applications. <a href="https://github.com/addthis/stream-lib" target="_blank" rel="noopener"><code>stream-lib</code></a> is a collection of these algorithms. They are Java implementations based on academical research and papers. This artile will give a brief introduction to this utility library.</p><h2 id="Count-Cardinality-with-HyperLogLog"><a href="#Count-Cardinality-with-HyperLogLog" class="headerlink" title="Count Cardinality with HyperLogLog"></a>Count Cardinality with <code>HyperLogLog</code></h2><p>Unique visitors (UV) is the major metric of websites. We usually generate UUIDs for each user and track them by HTTP Cookie, or roughly use the IP address. We can use a <code>HashSet</code> to count the exact value of UV, but that takes a lot of memory. With <code>HyperLogLog</code>, an algorithm for the count-distinct problem, we are able to <a href="https://en.wikipedia.org/wiki/HyperLogLog" target="_blank" rel="noopener">estimate cardinalities of &gt; 10^9 with a typical accuracy of 2%, using 1.5 kB of memory</a>.</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.clearspring.analytics<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>stream<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ICardinality card = <span class="keyword">new</span> HyperLogLog(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i : <span class="keyword">new</span> <span class="keyword">int</span>[] &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span> &#125;) &#123;</span><br><span class="line">    card.offer(i);</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(card.cardinality()); <span class="comment">// 4</span></span><br></pre></td></tr></table></figure><a id="more"></a><p><code>HyperLogLog</code> estimates cardinality by counting the leading zeros of each member’s binary value. If the maximum count is <code>n</code>, the cardinality is <code>2^n</code>. There’re some key points in this algorithm. First, members needs to be uniformly distributed, which we can use a hash function to achieve. <code>stream-lib</code> uses <a href="https://en.wikipedia.org/wiki/MurmurHash" target="_blank" rel="noopener">MurmurHash</a>, a simple, fast, and well distributed hash function, that is used in lots of hash-based lookup algorithms. Second, to decrease the variance of the result, set members are splitted into subsets, and the final result is the harmonic mean of all subsets’ cardinality. The integer argument that we passed to <code>HyperLogLog</code> constructor is the number of bits that it’ll use to split subsets, and the accuracy can be derived from this formula: <code>1.04/sqrt(2^log2m)</code>.</p><p><code>HyperLogLog</code> is an extension of <code>LogLog</code> algorithm, and the <code>HyperLogLogPlus</code> makes some more improvements. For instance, it uses a 64 bit hash function to remove the correction factor that adjusts hash collision; for small cardinality, it applies an empirical bias correction; and it also supports growing from a sparse data strucutre of registers (holding subsets) to a dense one. These algorithms are all included in <code>stream-lib</code></p><h2 id="Test-Membership-with-BloomFilter"><a href="#Test-Membership-with-BloomFilter" class="headerlink" title="Test Membership with BloomFilter"></a>Test Membership with <code>BloomFilter</code></h2><p><img src="/images/stream-lib/bloom-filter.jpg" alt="Bloom Filter"></p><p><code>BloomFilter</code> is a widely used data structure to test whether a set contains a certain member. The key is it will give false positive result, but never false negative. For example, Chrome maintains a malicious URLs in local storage, and it’s a bloom filter. When typing a new URL, if the filter says it’s not malicious, then it’s definitely not. But if the filter says it is in the set, then Chrome needs to contact the remote server for further confirmation.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Filter filter = <span class="keyword">new</span> BloomFilter(<span class="number">100</span>, <span class="number">0.01</span>);</span><br><span class="line">filter.add(<span class="string">"google.com"</span>);</span><br><span class="line">filter.add(<span class="string">"twitter.com"</span>);</span><br><span class="line">filter.add(<span class="string">"facebook.com"</span>);</span><br><span class="line">System.out.println(filter.isPresent(<span class="string">"bing.com"</span>)); <span class="comment">// false</span></span><br></pre></td></tr></table></figure><p>The contruction process of a bloom filter is faily simple:</p><ul><li>Create a bit array of <code>n</code> bits. In Java, we can use the <a href="https://docs.oracle.com/javase/8/docs/api/java/util/BitSet.html" target="_blank" rel="noopener"><code>BitSet</code></a> class.</li><li>Apply <code>k</code> number of hash functions to the incoming value, and set the corresponding bits to true.</li><li>When testing a membership, apply those hash functions and get the bits’ values:<ul><li>If every bit hits, the value might be in the set, with a False Positive Probability (FPP);</li><li>If not all bits hit, the value is definitely not in the set.</li></ul></li></ul><p>Again, those hash functions need to be uniformly distributed, and pairwise independent. Murmur hash meets the criteria. The FPP can be calculated by this formula: <code>(1-e^(-kn/m))^k</code>. This page (<a href="https://llimllib.github.io/bloomfilter-tutorial/" target="_blank" rel="noopener">link</a>) provides an online visualization of bloom filter. Other use cases are: anti-spam in email service, non-existent rows detection in Cassandra and HBase, and Squid also uses it to do <a href="https://wiki.squid-cache.org/SquidFaq/CacheDigests" target="_blank" rel="noopener">cache digest</a>.</p><h2 id="Top-k-Elements-with-CountMinSketch"><a href="#Top-k-Elements-with-CountMinSketch" class="headerlink" title="Top-k Elements with CountMinSketch"></a>Top-k Elements with <code>CountMinSketch</code></h2><p><img src="/images/stream-lib/count-min-sketch.png" alt="Count Min Sketch"></p><p><a href="https://stackoverflow.com/a/35356116/1030720" target="_blank" rel="noopener">Source</a></p><p><a href="https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch" target="_blank" rel="noopener"><code>CountMinSketch</code></a> is a “sketching” algorithm that uses minimal space to track frequencies of incoming events. We can for example find out the top K tweets streaming out of Twitter, or count the most visited pages of a website. The “sketch” can be used to estimate these frequencies, with some loss of accuracy, of course.</p><p>The following snippet shows how to use <code>stream-lib</code> to get the top three animals in the <code>List</code>:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; animals;</span><br><span class="line">IFrequency freq = <span class="keyword">new</span> CountMinSketch(<span class="number">10</span>, <span class="number">5</span>, <span class="number">0</span>);</span><br><span class="line">Map&lt;String, Long&gt; top = Collections.emptyMap();</span><br><span class="line"><span class="keyword">for</span> (String animal : animals) &#123;</span><br><span class="line">    freq.add(animal, <span class="number">1</span>);</span><br><span class="line">    top = Stream.concat(top.keySet().stream(), Stream.of(animal)).distinct()</span><br><span class="line">              .map(a -&gt; <span class="keyword">new</span> SimpleEntry&lt;String, Long&gt;(a, freq.estimateCount(a)))</span><br><span class="line">              .sorted(Comparator.comparing(SimpleEntry&lt;String, Long&gt;::getValue).reversed())</span><br><span class="line">              .limit(<span class="number">3</span>)</span><br><span class="line">              .collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">System.out.println(top); <span class="comment">// &#123;rabbit=25, bird=45, spider=35&#125;</span></span><br></pre></td></tr></table></figure><p><code>CountMinSketch#estimateCount</code> is a <em>point query</em> that asks for the count of an event. Since the “sketch” cannot remeber the exact events, we need to store them else where.</p><p>The data structure of count-min sketch is similar to bloom filter, instead of one bit array of <code>w</code> bits, it uses <code>d</code> number of them, so as to form a <code>d x w</code> matrix. When a value comes, it applies <code>d</code> number of hash functions, and update the corresponding bit in the matrix. These hash functions need only to be <a href="https://en.wikipedia.org/wiki/Pairwise_independence" target="_blank" rel="noopener">pairwise independent</a>, so <code>stream-lib</code> uses a simple yet fast <code>(a*x+b) mod p</code> formula. When doing <em>point query</em>, calculate the hash values, and the smallest value is the frequency.</p><p>The estimation error is <code>ε = e / w</code> while probability of bad estimate is <code>δ = 1 / e ^ d</code>. So we can increase <code>w</code> and / or <code>d</code> to improve the results. Original paper can be found in this <a href="https://web.archive.org/web/20060907232042/http://www.eecs.harvard.edu/~michaelm/CS222/countmin.pdf" target="_blank" rel="noopener">link</a>.</p><h2 id="Histogram-and-Quantile-with-T-Digest"><a href="#Histogram-and-Quantile-with-T-Digest" class="headerlink" title="Histogram and Quantile with T-Digest"></a>Histogram and Quantile with <code>T-Digest</code></h2><p><img src="/images/stream-lib/t-digest.png" alt="T-Digest"></p><p><a href="https://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest" target="_blank" rel="noopener">Source</a></p><p>Median, 95th percentile are common use cases in descriptive statistics. Median for instance is less influenced by outliers than mean, but the calculation is not simple. One needs to track all data, sort them, and then get the final result. With <code>T-Digest</code>, we can agian generate a summarized distribution of the dataset and estimate the quantiles.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Random rand = <span class="keyword">new</span> Random();</span><br><span class="line">List&lt;Double&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">TDigest digest = <span class="keyword">new</span> TDigest(<span class="number">100</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000000</span>; ++i) &#123;</span><br><span class="line">    <span class="keyword">double</span> d = rand.nextDouble();</span><br><span class="line">    data.add(d);</span><br><span class="line">    digest.add(d);</span><br><span class="line">&#125;</span><br><span class="line">Collections.sort(data);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">double</span> q : <span class="keyword">new</span> <span class="keyword">double</span>[] &#123; <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.9</span> &#125;) &#123;</span><br><span class="line">    System.out.println(String.format(<span class="string">"quantile=%.1f digest=%.4f exact=%.4f"</span>,</span><br><span class="line">            q, digest.quantile(q), data.get((<span class="keyword">int</span>) (data.size() * q))));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// quantile=0.1 digest=0.0998 exact=0.1003</span></span><br><span class="line"><span class="comment">// quantile=0.5 digest=0.5009 exact=0.5000</span></span><br><span class="line"><span class="comment">// quantile=0.9 digest=0.8994 exact=0.8998</span></span><br></pre></td></tr></table></figure><p>The <code>T-Digest</code> paper can be found in this <a href="https://raw.githubusercontent.com/tdunning/t-digest/master/docs/t-digest-paper/histo.pdf" target="_blank" rel="noopener">link</a>. In brief, it uses a variant of 1-dimensional k-means clustering mechanism, representing the empirical distribution by retaining the centroids of subsets. Besides, different <code>T-Digest</code> instances can be merged into a larger, more accurate instance, which can be used in parallel processing with ease.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>As we can see, most algorithms tries to save space and time with the cost of slight accuracy. By “sketching” the batch or streaming dataset, we can catch the “interesting” features and give very good estimation, especially when the dataset itself fullfills certain distribution. <code>stream-lib</code> and other opensourced projects certainly ease the process for us end users.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.javadoc.io/doc/com.clearspring.analytics/stream/2.9.5" target="_blank" rel="noopener">https://www.javadoc.io/doc/com.clearspring.analytics/stream/2.9.5</a></li><li><a href="http://www.addthis.com/blog/2011/03/29/new-open-source-stream-summarizing-java-library/" target="_blank" rel="noopener">http://www.addthis.com/blog/2011/03/29/new-open-source-stream-summarizing-java-library/</a></li><li><a href="https://www.mapr.com/blog/some-important-streaming-algorithms-you-should-know-about" target="_blank" rel="noopener">https://www.mapr.com/blog/some-important-streaming-algorithms-you-should-know-about</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When processing a large amount of data, certain operations will cost a lot of time and space, such as counting the distinct values, or figuring out the 95th percentile of a sequence of numbers. But sometimes the accuracy is not that important. Maybe you just want a brief summary of the dataset, or it’s a monitoring system, where limited error rate is tolerable. There’re plenty of such algorithms that can trade accuracy with huge saves of time-space. What’s more, most of the data structures can be merged, making it possible to use in stream processing applications. &lt;a href=&quot;https://github.com/addthis/stream-lib&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;stream-lib&lt;/code&gt;&lt;/a&gt; is a collection of these algorithms. They are Java implementations based on academical research and papers. This artile will give a brief introduction to this utility library.&lt;/p&gt;
&lt;h2 id=&quot;Count-Cardinality-with-HyperLogLog&quot;&gt;&lt;a href=&quot;#Count-Cardinality-with-HyperLogLog&quot; class=&quot;headerlink&quot; title=&quot;Count Cardinality with HyperLogLog&quot;&gt;&lt;/a&gt;Count Cardinality with &lt;code&gt;HyperLogLog&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;Unique visitors (UV) is the major metric of websites. We usually generate UUIDs for each user and track them by HTTP Cookie, or roughly use the IP address. We can use a &lt;code&gt;HashSet&lt;/code&gt; to count the exact value of UV, but that takes a lot of memory. With &lt;code&gt;HyperLogLog&lt;/code&gt;, an algorithm for the count-distinct problem, we are able to &lt;a href=&quot;https://en.wikipedia.org/wiki/HyperLogLog&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;estimate cardinalities of &amp;gt; 10^9 with a typical accuracy of 2%, using 1.5 kB of memory&lt;/a&gt;.&lt;/p&gt;
&lt;figure class=&quot;highlight xml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;com.clearspring.analytics&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;stream&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;2.9.5&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ICardinality card = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; HyperLogLog(&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i : &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;[] &amp;#123; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt; &amp;#125;) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    card.offer(i);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;System.out.println(card.cardinality()); &lt;span class=&quot;comment&quot;&gt;// 4&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="stream processing" scheme="http://shzhangji.com/tags/stream-processing/"/>
    
      <category term="java" scheme="http://shzhangji.com/tags/java/"/>
    
      <category term="algorithm" scheme="http://shzhangji.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Extract Data from MySQL with Binlog and Canal</title>
    <link href="http://shzhangji.com/blog/2017/08/12/extract-data-from-mysql-with-binlog-and-canal/"/>
    <id>http://shzhangji.com/blog/2017/08/12/extract-data-from-mysql-with-binlog-and-canal/</id>
    <published>2017-08-12T11:15:09.000Z</published>
    <updated>2022-06-12T05:01:34.283Z</updated>
    
    <content type="html"><![CDATA[<p>Data extraction is the very first step of an ETL process. We need to load data from external data stores like RDMBS or logging file system, and then we can do cleaning, transformation and summary. In modern website stack, MySQL is the most widely used database, and it’s common to extract data from different instances and load into a central MySQL database, or directly into Hive. There’re several query-based techniques that we can use to do the extraction, including the popular open source software <a href="http://sqoop.apache.org/" target="_blank" rel="noopener">Sqoop</a>, but they are not meant for real-time data ingestion. Binlog, on the other hand, is a real-time data stream that is used to do replication between master and slave instances. With the help of Alibaba’s open sourced <a href="https://github.com/alibaba/canal" target="_blank" rel="noopener">Canal</a> project, we can easily utilize the binlog facility to do data extraction from MySQL database to various destinations.</p><p><img src="/images/canal.png" alt="Canal"></p><h2 id="Canal-Components"><a href="#Canal-Components" class="headerlink" title="Canal Components"></a>Canal Components</h2><p>In brief, Canal simulates itself to be a MySQL slave and dump binlog from master, parse it, and send to downstream sinks. Canal consists of two major components, namely Canal server and Canal client. A Canal server can connect to multiple MySQL instances, and maintains an event queue for each instance. Canal clients can then subscribe to theses queues and receive data changes. The following is a quick start guide to get Canal going.</p><a id="more"></a><h3 id="Configure-MySQL-Master"><a href="#Configure-MySQL-Master" class="headerlink" title="Configure MySQL Master"></a>Configure MySQL Master</h3><p>MySQL binlog is not enabled by default. Locate your <code>my.cnf</code> file and make these changes:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">server-id = 1</span><br><span class="line">log_bin = /path/to/mysql-bin.log</span><br><span class="line">binlog_format = ROW</span><br></pre></td></tr></table></figure><p>Note that <code>binlog_format</code> must be <code>ROW</code>, becuase in <code>STATEMENT</code> or <code>MIXED</code> mode, only SQL statements will be logged and transferred (to save log size), but what we need is full data of the changed rows.</p><p>Slave connects to master via an dedicated account, which must have the global <code>REPLICATION</code> priviledges. We can use the <code>GRANT</code> statement to create the account:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>, <span class="keyword">REPLICATION</span> <span class="keyword">SLAVE</span>, <span class="keyword">REPLICATION</span> <span class="keyword">CLIENT</span></span><br><span class="line"><span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'canal'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'canal'</span>;</span><br></pre></td></tr></table></figure><h3 id="Startup-Canal-Server"><a href="#Startup-Canal-Server" class="headerlink" title="Startup Canal Server"></a>Startup Canal Server</h3><p>Download Canal server from its GitHub Releases page (<a href="https://github.com/alibaba/canal/releases" target="_blank" rel="noopener">link</a>). The config files reside in <code>conf</code> directory. A typical layout is:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">canal.deployer/conf/canal.properties</span><br><span class="line">canal.deployer/conf/instanceA/instance.properties</span><br><span class="line">canal.deployer/conf/instanceB/instance.properties</span><br></pre></td></tr></table></figure><p>In <code>conf/canal.properties</code> there’s the main configuration. <code>canal.port</code> for example defines which port Canal server is listening. <code>instanceA/instance.properties</code> defines the MySQL instance that Canal server will draw binlog from. Important settings are:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># slaveId cannot collide with the server-id in my.cnf</span><br><span class="line">canal.instance.mysql.slaveId = 1234</span><br><span class="line">canal.instance.master.address = 127.0.0.1:3306</span><br><span class="line">canal.instance.dbUsername = canal</span><br><span class="line">canal.instance.dbPassword = canal</span><br><span class="line">canal.instance.connectionCharset = UTF-8</span><br><span class="line"># process all tables from all databases</span><br><span class="line">canal.instance.filter.regex = .*\\..*</span><br></pre></td></tr></table></figure><p>Start the server by <code>sh bin/startup.sh</code>, and you’ll see the following output in <code>logs/example/example.log</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Loading properties file from class path resource [canal.properties]</span><br><span class="line">Loading properties file from class path resource [example/instance.properties]</span><br><span class="line">start CannalInstance for 1-example</span><br><span class="line">[destination = example , address = /127.0.0.1:3306 , EventParser] prepare to find start position just show master status</span><br></pre></td></tr></table></figure><h3 id="Write-Canal-Client"><a href="#Write-Canal-Client" class="headerlink" title="Write Canal Client"></a>Write Canal Client</h3><p>To consume update events from Canal server, we can create a Canal client in our application, specify the instance and tables we’re interested in, and start polling.</p><p>First, add <code>com.alibaba.otter:canal.client</code> dependency to your <code>pom.xml</code>, and construct a Canal client:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">CanalConnector connector = CanalConnectors.newSingleConnector(</span><br><span class="line">        <span class="keyword">new</span> InetSocketAddress(<span class="string">"127.0.0.1"</span>, <span class="number">11111</span>), <span class="string">"example"</span>, <span class="string">""</span>, <span class="string">""</span>);</span><br><span class="line"></span><br><span class="line">connector.connect();</span><br><span class="line">connector.subscribe(<span class="string">".*\\..*"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    Message message = connector.getWithoutAck(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">long</span> batchId = message.getId();</span><br><span class="line">    <span class="keyword">if</span> (batchId == -<span class="number">1</span> || message.getEntries().isEmpty()) &#123;</span><br><span class="line">        Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        printEntries(message.getEntries());</span><br><span class="line">        connector.ack(batchId);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The code is quite similar to consuming from a message queue. The update events are sent in batches, and you can acknowledge every batch after being properly processed.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// printEntries</span></span><br><span class="line">RowChange rowChange = RowChange.parseFrom(entry.getStoreValue());</span><br><span class="line"><span class="keyword">for</span> (RowData rowData : rowChange.getRowDatasList()) &#123;</span><br><span class="line">    <span class="keyword">if</span> (rowChange.getEventType() == EventType.INSERT) &#123;</span><br><span class="line">      printColumns(rowData.getAfterCollumnList());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Every <code>Entry</code> in a message represents a set of row changes with the same event type, e.g. INSERT, UPDATE, or DELETE. For each row, we can get the column data like this:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// printColumns</span></span><br><span class="line">String line = columns.stream()</span><br><span class="line">        .map(column -&gt; column.getName() + <span class="string">"="</span> + column.getValue())</span><br><span class="line">        .collect(Collectors.joining(<span class="string">","</span>));</span><br><span class="line">System.out.println(line);</span><br></pre></td></tr></table></figure><p>Full example can be found on GitHub (<a href="https://github.com/jizhang/java-blog-demo/blob/blog-canal/canal/src/main/java/com/shzhangji/demo/canal/SimpleClient.java" target="_blank" rel="noopener">link</a>).</p><h2 id="Load-into-Data-Warehouse"><a href="#Load-into-Data-Warehouse" class="headerlink" title="Load into Data Warehouse"></a>Load into Data Warehouse</h2><h3 id="RDBMS-with-Batch-Insert"><a href="#RDBMS-with-Batch-Insert" class="headerlink" title="RDBMS with Batch Insert"></a>RDBMS with Batch Insert</h3><p>For DB based data warehouse, we can directly use the <code>REPLACE</code> statement and let the database deduplicates rows by primary key. One concern is the instertion performance, so it’s often necessary to cache the data for a while and do a batch insertion, like:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">REPLACE</span> <span class="keyword">INTO</span> <span class="string">`user`</span> (<span class="string">`id`</span>, <span class="string">`name`</span>, <span class="string">`age`</span>, <span class="string">`updated`</span>) <span class="keyword">VALUES</span></span><br><span class="line">(<span class="number">1</span>, <span class="string">'Jerry'</span>, <span class="number">30</span>, <span class="string">'2017-08-12 16:00:00'</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">'Mary'</span>, <span class="number">28</span>, <span class="string">'2017-08-12 17:00:00'</span>),</span><br><span class="line">(<span class="number">3</span>, <span class="string">'Tom'</span>, <span class="number">36</span>, <span class="string">'2017-08-12 18:00:00'</span>);</span><br></pre></td></tr></table></figure><p>Another approach is to write the extracted data into a delimited text file, then execute a <code>LOAD DATA</code> statement. These files can also be used to import data into Hive. But for both approaches, make sure you escape the string columns properly, so as to avoid insertion errors.</p><h3 id="Hive-based-Warehouse"><a href="#Hive-based-Warehouse" class="headerlink" title="Hive-based Warehouse"></a>Hive-based Warehouse</h3><p>Hive tables are stored on HDFS, which is an append-only file system, so it takes efforts to update data in a previously loaded table. One can use a JOIN-based approach, Hive transaction, or switch to HBase.</p><p>Data can be categorized into base and delta. For example, yesterday’s <code>user</code> table is the base, while today’s updated rows are the delta. Using a <code>FULL OUTER JOIN</code> we can generate the latest snapshot:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="keyword">COALESCE</span>(b.<span class="string">`id`</span>, a.<span class="string">`id`</span>) <span class="keyword">AS</span> <span class="string">`id`</span></span><br><span class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`name`</span>, a.<span class="string">`name`</span>) <span class="keyword">AS</span> <span class="string">`name`</span></span><br><span class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`age`</span>, a.<span class="string">`age`</span>) <span class="keyword">AS</span> <span class="string">`age`</span></span><br><span class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`updated`</span>, a.<span class="string">`updated`</span>) <span class="keyword">AS</span> <span class="string">`updated`</span></span><br><span class="line"><span class="keyword">FROM</span> dw_stage.<span class="string">`user`</span> a</span><br><span class="line"><span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> (</span><br><span class="line">  <span class="comment">-- deduplicate by selecting the latest record</span></span><br><span class="line">  <span class="keyword">SELECT</span> <span class="string">`id`</span>, <span class="string">`name`</span>, <span class="string">`age`</span>, <span class="string">`updated`</span></span><br><span class="line">  <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> *, ROW_NUMBER() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="string">`id`</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="string">`updated`</span> <span class="keyword">DESC</span>) <span class="keyword">AS</span> <span class="string">`n`</span></span><br><span class="line">    <span class="keyword">FROM</span> dw_stage.<span class="string">`user_delta`</span></span><br><span class="line">  ) b</span><br><span class="line">  <span class="keyword">WHERE</span> <span class="string">`n`</span> = <span class="number">1</span></span><br><span class="line">) b</span><br><span class="line"><span class="keyword">ON</span> a.<span class="string">`id`</span> = b.<span class="string">`id`</span>;</span><br></pre></td></tr></table></figure><p>Hive 0.13 introduces transaction and ACID table, 0.14 brings us the <code>INSERT</code>, <code>UPDATE</code> and <code>DELETE</code> statements, and Hive 2.0.0 provides a new <a href="https://cwiki.apache.org/confluence/display/Hive/HCatalog+Streaming+Mutation+API" target="_blank" rel="noopener">Streaming Mutation API</a> that can be used to submit insert/update/delete transactions to Hive tables programmatically. Currently, ACID tables must use ORC file format, and be bucketed by primiary key. Hive will store the mutative operations in delta files. When reading from this table, <code>OrcInputFormat</code> will figure out which record is the latest. The official sample code can be found in the test suite (<a href="https://github.com/apache/hive/blob/master/hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/ExampleUseCase.java" target="_blank" rel="noopener">link</a>).</p><p>And the final approach is to use HBase, which is a key-value store built on HDFS, making it perfect for data updates. Its table can also be used by MapReduce jobs, or you can create an external Hive table that points directly to HBase. More information can be found on the <a href="http://hbase.apache.org/" target="_blank" rel="noopener">official website</a>.</p><h2 id="Initialize-Target-Table"><a href="#Initialize-Target-Table" class="headerlink" title="Initialize Target Table"></a>Initialize Target Table</h2><p>Data extraction is usually on-demand, so there may be already historical data in the source table. One obvious approach is dumping the full table manually and load into destination. Or we can reuse the Canal facility, notify the client to query data from source and do the updates.</p><p>First, we create a helper table in the source database:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`retl_buffer`</span> (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">BIGINT</span> AUTO_INCREMENT PRIMARY <span class="keyword">KEY</span></span><br><span class="line">  ,table_name <span class="built_in">VARCHAR</span>(<span class="number">255</span>)</span><br><span class="line">  ,pk_value <span class="built_in">VARCHAR</span>(<span class="number">255</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>To reload all records in <code>user</code> table:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`retl_buffer`</span> (<span class="string">`table_name`</span>, <span class="string">`pk_value`</span>)</span><br><span class="line"><span class="keyword">SELECT</span> <span class="string">'user'</span>, <span class="string">`id`</span> <span class="keyword">FROM</span> <span class="string">`user`</span>;</span><br></pre></td></tr></table></figure><p>When Canal client receives the <code>RowChange</code> of <code>retl_buffer</code> table, it can extract the table name and primary key value from the record, query the source database, and write to the destination.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="string">"retl_buffer"</span>.equals(entry.getHeader().getTableName())) &#123;</span><br><span class="line">    String tableName = rowData.getAfterColumns(<span class="number">1</span>).getValue();</span><br><span class="line">    String pkValue = rowData.getAfterColumns(<span class="number">2</span>).getValue();</span><br><span class="line">    System.out.println(<span class="string">"SELECT * FROM "</span> + tableName + <span class="string">" WHERE id = "</span> + pkValue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This approach is included in another Alibaba’s project <a href="https://github.com/alibaba/otter/wiki/Manager%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E8%87%AA-%E7%94%B1-%E9%97%A8" target="_blank" rel="noopener">Otter</a>.</p><h2 id="Canal-High-Availability"><a href="#Canal-High-Availability" class="headerlink" title="Canal High Availability"></a>Canal High Availability</h2><ul><li>Canal instances can be supplied with a standby MySQL source, typically in a Master-Master HA scenario. Make sure you turn on the <code>log_slave_updates</code> option in both MySQL instances. Canal uses a dedicated heartbeat check, i.e. update a row periodically to check if current source is alive.</li><li>Canal server itself also supports HA. You’ll need a Zookeeper quorum to enable this feature. Clients will get the current server location from Zookeeper, and the server will record the last binlog offset that has been consumed.</li></ul><p>For more information, please checkout the <a href="https://github.com/alibaba/canal/wiki/AdminGuide" target="_blank" rel="noopener">AdminGuide</a>.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://github.com/alibaba/canal/wiki" target="_blank" rel="noopener">https://github.com/alibaba/canal/wiki</a> (in Chinese)</li><li><a href="https://github.com/alibaba/otter/wiki" target="_blank" rel="noopener">https://github.com/alibaba/otter/wiki</a> (in Chinese)</li><li><a href="https://www.phdata.io/4-strategies-for-updating-hive-tables/" target="_blank" rel="noopener">https://www.phdata.io/4-strategies-for-updating-hive-tables/</a></li><li><a href="https://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/" target="_blank" rel="noopener">https://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Data extraction is the very first step of an ETL process. We need to load data from external data stores like RDMBS or logging file system, and then we can do cleaning, transformation and summary. In modern website stack, MySQL is the most widely used database, and it’s common to extract data from different instances and load into a central MySQL database, or directly into Hive. There’re several query-based techniques that we can use to do the extraction, including the popular open source software &lt;a href=&quot;http://sqoop.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Sqoop&lt;/a&gt;, but they are not meant for real-time data ingestion. Binlog, on the other hand, is a real-time data stream that is used to do replication between master and slave instances. With the help of Alibaba’s open sourced &lt;a href=&quot;https://github.com/alibaba/canal&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Canal&lt;/a&gt; project, we can easily utilize the binlog facility to do data extraction from MySQL database to various destinations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/canal.png&quot; alt=&quot;Canal&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Canal-Components&quot;&gt;&lt;a href=&quot;#Canal-Components&quot; class=&quot;headerlink&quot; title=&quot;Canal Components&quot;&gt;&lt;/a&gt;Canal Components&lt;/h2&gt;&lt;p&gt;In brief, Canal simulates itself to be a MySQL slave and dump binlog from master, parse it, and send to downstream sinks. Canal consists of two major components, namely Canal server and Canal client. A Canal server can connect to multiple MySQL instances, and maintains an event queue for each instance. Canal clients can then subscribe to theses queues and receive data changes. The following is a quick start guide to get Canal going.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="etl" scheme="http://shzhangji.com/tags/etl/"/>
    
      <category term="java" scheme="http://shzhangji.com/tags/java/"/>
    
      <category term="mysql" scheme="http://shzhangji.com/tags/mysql/"/>
    
      <category term="canal" scheme="http://shzhangji.com/tags/canal/"/>
    
  </entry>
  
  <entry>
    <title>How to Extract Event Time in Apache Flume</title>
    <link href="http://shzhangji.com/blog/2017/08/05/how-to-extract-event-time-in-apache-flume/"/>
    <id>http://shzhangji.com/blog/2017/08/05/how-to-extract-event-time-in-apache-flume/</id>
    <published>2017-08-05T07:10:47.000Z</published>
    <updated>2022-06-12T05:01:34.279Z</updated>
    
    <content type="html"><![CDATA[<p>Extracting data from upstream message queues is a common task in ETL. In a Hadoop based data warehouse, we usually use Flume to import event logs from Kafka into HDFS, and then run MapReduce jobs agaist it, or create Hive external tables partitioned by time. One of the keys of this process is to extract the event time from the logs, since real-time data can have time lags, or your system is temporarily offline and need to perform a catch-up. Flume provides various facilities to help us do this job easily.</p><p><img src="/images/flume.png" alt="Apache Flume"></p><h2 id="HDFS-Sink-and-Timestamp-Header"><a href="#HDFS-Sink-and-Timestamp-Header" class="headerlink" title="HDFS Sink and Timestamp Header"></a>HDFS Sink and Timestamp Header</h2><p>Here is a simple HDFS Sink config:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a1.sinks = k1</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /user/flume/ds_alog/dt=%Y%m%d</span><br></pre></td></tr></table></figure><p><code>%Y%m%d</code> is the placeholders supported by this sink. It will use the milliseconds in <code>timestamp</code> header to replace them. Also, HDFS Sink provides <code>hdfs.useLocalTimeStamp</code> option so that it’ll use the local time to replace these placeholders, but this is not what we intend.</p><p>Another sink we could use is the Hive Sink, which directly communicates with Hive metastore and loads data into HDFS as Hive table. It supports both delimited text and JSON serializers, and also requires a <code>timestamp</code> header. But we don’t choose it for the following reasons:</p><ul><li>It doesn’t support regular expression serializer, so we cannot extract columns from arbitrary data format like access logs;</li><li>The columns to be extracted are defined in Hive metastore. Say the upstream events add some new keys in JSON, they will be dropped until Hive table definition is updated. As in data warehouse, it’s better to preserve the original source data for a period of time.</li></ul><a id="more"></a><h2 id="Regex-Extractor-Interceptor"><a href="#Regex-Extractor-Interceptor" class="headerlink" title="Regex Extractor Interceptor"></a>Regex Extractor Interceptor</h2><p>Flume has a mechanism called Interceptor, i.e. some optionally chained operations appended to Source, so as to perform various yet primitive transformation. For instance, the <code>TimestampInterceptor</code> is to add current local timestamp to the event header. In this section, I’ll demonstrate how to extract event time from access logs and JSON serialized logs with the help of interceptors.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0.123 [2017-06-27 09:08:00] GET /</span><br><span class="line">0.234 [2017-06-27 09:08:01] GET /</span><br></pre></td></tr></table></figure><p><a href="http://flume.apache.org/FlumeUserGuide.html#regex-extractor-interceptor" target="_blank" rel="noopener"><code>RegexExtractorInterceptor</code></a> can be used to extract values based on regular expressions. Here’s the config:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = regex_extractor</span><br><span class="line">a1.sources.r1.interceptors.i1.regex = \\[(.*?)\\]</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers = s1</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers.s1.type = org.apache.flume.interceptor.RegexExtractorInterceptorMillisSerializer</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers.s1.name = timestamp</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers.s1.pattern = yyyy-MM-dd HH:mm:ss</span><br></pre></td></tr></table></figure><p>It searches the string with pattern <code>\[(.*?)\]</code>, capture the first sub-pattern as <code>s1</code>, then parse it as a datetime string, and finally store it into headers with the name <code>timestamp</code>.</p><h3 id="Search-And-Replace-Interceptor"><a href="#Search-And-Replace-Interceptor" class="headerlink" title="Search And Replace Interceptor"></a>Search And Replace Interceptor</h3><p>For JSON strings:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"actionTime"</span>:<span class="number">1498525680.023</span>,<span class="attr">"actionType"</span>:<span class="string">"pv"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"actionTime"</span>:<span class="number">1498525681.349</span>,<span class="attr">"actionType"</span>:<span class="string">"pv"</span>&#125;</span><br></pre></td></tr></table></figure><p>We can also extract <code>actionTime</code> with a regular expression, but note that HDFS Sink requires the timestamp in milliseconds, so we have to first convert the timestamp with <code>SearchAndReplaceInterceptor</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r1.interceptors = i1 i2</span><br><span class="line">a1.sources.r1.interceptors.i1.type = search_replace</span><br><span class="line">a1.sources.r1.interceptors.i1.searchPattern = \&quot;actionTime\&quot;:(\\d+)\\.(\\d+)</span><br><span class="line">a1.sources.r1.interceptors.i1.replaceString = \&quot;actionTime\&quot;:$1$2</span><br><span class="line">a1.sources.r1.interceptors.i2.type = regex_extractor</span><br><span class="line">a1.sources.r1.interceptors.i2.regex = \&quot;actionTime\&quot;:(\\d+)</span><br><span class="line">a1.sources.r1.interceptors.i2.serializers = s1</span><br><span class="line">a1.sources.r1.interceptors.i2.serializers.s1.name = timestamp</span><br></pre></td></tr></table></figure><p>There’re two chained interceptors, first one replaces <code>1498525680.023</code> with <code>1498525680023</code> and second extracts <code>actionTime</code> right into headers.</p><h3 id="Custom-Interceptor"><a href="#Custom-Interceptor" class="headerlink" title="Custom Interceptor"></a>Custom Interceptor</h3><p>It’s also possible to write your own interceptor, thus do the extraction and conversion in one step. Your interceptor should implements <code>org.apache.flume.interceptor.Interceptor</code> and then do the job in <code>intercept</code> method. The source code and unit test can be found on GitHub (<a href="https://github.com/jizhang/java-blog-demo/blob/blog-flume/flume/src/main/java/com/shzhangji/demo/flume/ActionTimeInterceptor.java" target="_blank" rel="noopener">link</a>). Please add <code>flume-ng-core</code> to your project dependencies.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ActionTimeInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> ObjectMapper mapper = <span class="keyword">new</span> ObjectMapper();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            JsonNode node = mapper.readTree(<span class="keyword">new</span> ByteArrayInputStream(event.getBody()));</span><br><span class="line">            <span class="keyword">long</span> timestamp = (<span class="keyword">long</span>) (node.get(<span class="string">"actionTime"</span>).getDoubleValue() * <span class="number">1000</span>);</span><br><span class="line">            event.getHeaders().put(<span class="string">"timestamp"</span>, Long.toString(timestamp));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="comment">// no-op</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Use-Kafka-Channel-Directly"><a href="#Use-Kafka-Channel-Directly" class="headerlink" title="Use Kafka Channel Directly"></a>Use Kafka Channel Directly</h2><p>When the upstream is Kafka, and you have control of the message format, you can further eliminate the Source and directly pass data from Kafka to HDFS. The trick is to write messages in <code>AvroFlumeEvent</code> format, so that <a href="http://flume.apache.org/FlumeUserGuide.html#kafka-channel" target="_blank" rel="noopener">Kafka Channel</a> can deserialize them and use the <code>timestamp</code> header within. Otherwise, Kafka channel will parse messages as plain text with no headers, and HDFS sink will complain missing <code>timestamp</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// construct an AvroFlumeEvent, this class can be found in flume-ng-sdk artifact</span></span><br><span class="line">Map&lt;CharSequence, CharSequence&gt; headers = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">headers.put(<span class="string">"timestamp"</span>, <span class="string">"1498525680023"</span>);</span><br><span class="line">String body = <span class="string">"some message"</span>;</span><br><span class="line">AvroFlumeEvent event = <span class="keyword">new</span> AvroFlumeEvent(headers, ByteBuffer.wrap(body.getBytes()));</span><br><span class="line"></span><br><span class="line"><span class="comment">// serialize event with Avro encoder</span></span><br><span class="line">ByteArrayOutputStream out = <span class="keyword">new</span> ByteArrayOutputStream();</span><br><span class="line">BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(out, <span class="keyword">null</span>);</span><br><span class="line">SpecificDatumWriter&lt;AvroFlumeEvent&gt; writer = <span class="keyword">new</span> SpecificDatumWriter&lt;&gt;(AvroFlumeEvent.class);</span><br><span class="line">writer.write(event, encoder);</span><br><span class="line">encoder.flush();</span><br><span class="line"></span><br><span class="line"><span class="comment">// send bytes to Kafka</span></span><br><span class="line">producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, <span class="keyword">byte</span>[]&gt;(<span class="string">"alog"</span>, out.toByteArray()));</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html</a></li><li><a href="https://github.com/apache/flume" target="_blank" rel="noopener">https://github.com/apache/flume</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Extracting data from upstream message queues is a common task in ETL. In a Hadoop based data warehouse, we usually use Flume to import event logs from Kafka into HDFS, and then run MapReduce jobs agaist it, or create Hive external tables partitioned by time. One of the keys of this process is to extract the event time from the logs, since real-time data can have time lags, or your system is temporarily offline and need to perform a catch-up. Flume provides various facilities to help us do this job easily.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/flume.png&quot; alt=&quot;Apache Flume&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;HDFS-Sink-and-Timestamp-Header&quot;&gt;&lt;a href=&quot;#HDFS-Sink-and-Timestamp-Header&quot; class=&quot;headerlink&quot; title=&quot;HDFS Sink and Timestamp Header&quot;&gt;&lt;/a&gt;HDFS Sink and Timestamp Header&lt;/h2&gt;&lt;p&gt;Here is a simple HDFS Sink config:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;a1.sinks = k1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a1.sinks.k1.type = hdfs&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a1.sinks.k1.hdfs.path = /user/flume/ds_alog/dt=%Y%m%d&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;%Y%m%d&lt;/code&gt; is the placeholders supported by this sink. It will use the milliseconds in &lt;code&gt;timestamp&lt;/code&gt; header to replace them. Also, HDFS Sink provides &lt;code&gt;hdfs.useLocalTimeStamp&lt;/code&gt; option so that it’ll use the local time to replace these placeholders, but this is not what we intend.&lt;/p&gt;
&lt;p&gt;Another sink we could use is the Hive Sink, which directly communicates with Hive metastore and loads data into HDFS as Hive table. It supports both delimited text and JSON serializers, and also requires a &lt;code&gt;timestamp&lt;/code&gt; header. But we don’t choose it for the following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It doesn’t support regular expression serializer, so we cannot extract columns from arbitrary data format like access logs;&lt;/li&gt;
&lt;li&gt;The columns to be extracted are defined in Hive metastore. Say the upstream events add some new keys in JSON, they will be dropped until Hive table definition is updated. As in data warehouse, it’s better to preserve the original source data for a period of time.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="etl" scheme="http://shzhangji.com/tags/etl/"/>
    
      <category term="flume" scheme="http://shzhangji.com/tags/flume/"/>
    
      <category term="java" scheme="http://shzhangji.com/tags/java/"/>
    
  </entry>
  
</feed>
