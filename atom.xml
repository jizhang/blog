<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ji ZHANG&#39;s Blog</title>
  <subtitle>If I rest, I rust.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://shzhangji.com/"/>
  <updated>2017-10-23T04:57:32.000Z</updated>
  <id>http://shzhangji.com/</id>
  
  <author>
    <name>Ji ZHANG</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flume Source Code - Component Lifecycle</title>
    <link href="http://shzhangji.com/blog/2017/10/23/flume-source-code-component-lifecycle/"/>
    <id>http://shzhangji.com/blog/2017/10/23/flume-source-code-component-lifecycle/</id>
    <published>2017-10-23T04:57:32.000Z</published>
    <updated>2017-10-23T04:57:32.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://flume.apache.org/" target="_blank" rel="external">Apache Flume</a> is a real-time ETL tool for data warehouse platform. It consists of different types of components, and during runtime all of them are managed by Flume’s lifecycle and supervisor mechanism. This article will walk you through the source code of Flume’s component lifecycle management.</p>
<h2 id="Repository-Structure"><a href="#Repository-Structure" class="headerlink" title="Repository Structure"></a>Repository Structure</h2><p>Flume’s source code can be downloaded from GitHub. It’s a Maven project, so we can import it into an IDE for efficient code reading. The following is the main structure of the project:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">/flume-ng-node</div><div class="line">/flume-ng-code</div><div class="line">/flume-ng-sdk</div><div class="line">/flume-ng-sources/flume-kafka-source</div><div class="line">/flume-ng-channels/flume-kafka-channel</div><div class="line">/flume-ng-sinks/flume-hdfs-sink</div></pre></td></tr></table></figure>
<h2 id="Application-Entrance"><a href="#Application-Entrance" class="headerlink" title="Application Entrance"></a>Application Entrance</h2><p>The <code>main</code> entrance of Flume agent is in the <code>org.apache.flume.node.Application</code> class of <code>flume-ng-node</code> module. Following is an abridged source code:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">    CommandLineParser parser = <span class="keyword">new</span> GnuParser();</div><div class="line">    <span class="keyword">if</span> (isZkConfigured) &#123;</div><div class="line">      <span class="keyword">if</span> (reload) &#123;</div><div class="line">        PollingZooKeeperConfigurationProvider zookeeperConfigurationProvider;</div><div class="line">        components.add(zookeeperConfigurationProvider);</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        StaticZooKeeperConfigurationProvider zookeeperConfigurationProvider;</div><div class="line">        application.handleConfigurationEvent();</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// PropertiesFileConfigurationProvider</span></div><div class="line">    &#125;</div><div class="line">    application.start();</div><div class="line">    Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread(<span class="string">"agent-shutdown-hook"</span>) &#123;</div><div class="line">      <span class="meta">@Override</span></div><div class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">        appReference.stop();</div><div class="line">      &#125;</div><div class="line">    &#125;);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>The process can be illustrated as follows:</p>
<ol>
<li>Parse command line arguments with <code>commons-cli</code>, including the Flume agent’s name, configuration method and path.</li>
<li>Configurations can be provided via properties file or ZooKeeper. Both provider support live-reload, i.e. we can update component settings without restarting the agent.<ul>
<li>File-based live-reload is implemented by using a background thread that checks the last modification time of the file.</li>
<li>ZooKeeper-based live-reload is provided by Curator’s <code>NodeCache</code> recipe, which uses ZooKeeper’s <em>watch</em> functionality underneath.</li>
</ul>
</li>
<li>If live-reload is on (by default), configuration providers will add themselves into the application’s component list, and after calling <code>Application#start</code>, a <code>LifecycleSupervisor</code> will start the provider, and trigger the reload event to parse the configuration and load all defined components.</li>
<li>If live-reload is off, configuration providers will parse the file immediately and start all components, also supervised by <code>LifecycleSupervisor</code>.</li>
<li>Finally add a JVM shutdown hook by <code>Runtime#addShutdownHook</code>, which in turn invokes <code>Application#stop</code> to shutdown the Flume agent.</li>
</ol>
<a id="more"></a>
<h2 id="Configuration-Reload"><a href="#Configuration-Reload" class="headerlink" title="Configuration Reload"></a>Configuration Reload</h2><p>In <code>PollingPropertiesFileConfigurationProvider</code>, when it detects file changes, it will invoke the <code>AbstractConfigurationProvider#getConfiguration</code> method to parse the configuration file into an <code>MaterializedConfiguration</code> instance, which contains the source, sink, and channel definitions. And then, the polling thread send an event to <code>Application</code> via a Guava’s <code>EventBus</code> instance, which effectively invokes the <code>Application#handleConfigurationEvent</code> method to reload all components.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Application class</span></div><div class="line"><span class="meta">@Subscribe</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">handleConfigurationEvent</span><span class="params">(MaterializedConfiguration conf)</span> </span>&#123;</div><div class="line">  stopAllComponents();</div><div class="line">  startAllComponents(conf);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// PollingPropertiesFileConfigurationProvider$FileWatcherRunnable</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">  eventBus.post(getConfiguration());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Start-Components"><a href="#Start-Components" class="headerlink" title="Start Components"></a>Start Components</h2><p>The starting process lies in <code>Application#startAllComponents</code>. The method accepts a new set of components, starts the <code>Channel</code>s first, followed by <code>Sink</code>s and <code>Source</code>s.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startAllComponents</span><span class="params">(MaterializedConfiguration materializedConfiguration)</span> </span>&#123;</div><div class="line">  <span class="keyword">this</span>.materializedConfiguration = materializedConfiguration;</div><div class="line">  <span class="keyword">for</span> (Entry&lt;String, Channel&gt; entry :</div><div class="line">      materializedConfiguration.getChannels().entrySet()) &#123;</div><div class="line">    supervisor.supervise(entry.getValue(),</div><div class="line">        <span class="keyword">new</span> SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//  Wait for all channels to start.</span></div><div class="line">  <span class="keyword">for</span> (Channel ch : materializedConfiguration.getChannels().values()) &#123;</div><div class="line">    <span class="keyword">while</span> (ch.getLifecycleState() != LifecycleState.START</div><div class="line">        &amp;&amp; !supervisor.isComponentInErrorState(ch)) &#123;</div><div class="line">      Thread.sleep(<span class="number">500</span>);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Start and supervise sinkds and sources</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>The <code>LifecycleSupervisor</code> manages instances that implement <code>LifecycleAware</code> interface. Supervisor will schedule a <code>MonitorRunnable</code> instance with a fixed delay (3 secs), which tries to convert a <code>LifecycleAware</code> instance into its <code>desiredState</code>, by calling <code>LifecycleAware#start</code> or <code>stop</code>.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MonitorRunnable</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</div><div class="line">  <span class="meta">@Override</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (!lifecycleAware.getLifecycleState().equals(</div><div class="line">        supervisoree.status.desiredState)) &#123;</div><div class="line">      <span class="keyword">switch</span> (supervisoree.status.desiredState) &#123;</div><div class="line">        <span class="keyword">case</span> START:</div><div class="line">          lifecycleAware.start();</div><div class="line">          <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">case</span> STOP:</div><div class="line">          lifecycleAware.stop();</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Stop-Components"><a href="#Stop-Components" class="headerlink" title="Stop Components"></a>Stop Components</h2><p>When JVM is shutting down, the hook invokes <code>Application#stop</code>, which calls <code>LifecycleSupervisor#stop</code>, that first shutdowns the <code>MonitorRunnable</code>s’ executor pool, and changes all components’ desired status to <code>STOP</code>, waiting for them to fully shutdown.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LifecycleSupervisor</span> <span class="keyword">implements</span> <span class="title">LifecycleAware</span> </span>&#123;</div><div class="line">  <span class="meta">@Override</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span> </span>&#123;</div><div class="line">    monitorService.shutdown();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">final</span> Entry&lt;LifecycleAware, Supervisoree&gt; entry :</div><div class="line">        supervisedProcesses.entrySet()) &#123;</div><div class="line">      <span class="keyword">if</span> (entry.getKey().getLifecycleState().equals(LifecycleState.START)) &#123;</div><div class="line">        entry.getValue().status.desiredState = LifecycleState.STOP;</div><div class="line">        entry.getKey().stop();</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Source-and-Source-Runner"><a href="#Source-and-Source-Runner" class="headerlink" title="Source and Source Runner"></a>Source and Source Runner</h2><p>Take <code>KafkaSource</code> for an instance, we shall see how agent supervises source components, and the same thing happens to sinks and channels.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaSource</span> <span class="keyword">extends</span> <span class="title">AbstractPollableSource</span> </span>&#123;</div><div class="line">  <span class="meta">@Override</span></div><div class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doStart</span><span class="params">()</span> <span class="keyword">throws</span> FlumeException </span>&#123;</div><div class="line">    consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, <span class="keyword">byte</span>[]&gt;(kafkaProps);</div><div class="line">    it = consumer.poll(<span class="number">1000</span>).iterator();</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="meta">@Override</span></div><div class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doStop</span><span class="params">()</span> <span class="keyword">throws</span> FlumeException </span>&#123;</div><div class="line">    consumer.close();</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>KafkaSource</code> is a pollable source, which means it needs a runner thread to constantly poll for more data to process.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PollableSourceRunner</span> <span class="keyword">extends</span> <span class="title">SourceRunner</span> </span>&#123;</div><div class="line">  <span class="meta">@Override</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</div><div class="line">    source.start();</div><div class="line">    runner = <span class="keyword">new</span> PollingRunner();</div><div class="line">    runnerThread = <span class="keyword">new</span> Thread(runner);</div><div class="line">    runnerThread.start();</div><div class="line">    lifecycleState = LifecycleState.START;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="meta">@Override</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span> </span>&#123;</div><div class="line">    runnerThread.interrupt();</div><div class="line">    runnerThread.join();</div><div class="line">    source.stop();</div><div class="line">    lifecycleState = LifecycleState.STOP;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">PollingRunner</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">      <span class="keyword">while</span> (!shouldStop.get()) &#123;</div><div class="line">        source.process();</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Both <code>AbstractPollableSource</code> and <code>SourceRunner</code> are subclass of <code>LifecycleAware</code>, which means they have <code>start</code> and <code>stop</code> methods for supervisor to call. In this case, <code>SourceRunner</code> is the component that Flume agent actually supervises, and <code>PollableSource</code> is instantiated and managed by <code>SourceRunner</code>. Details lie in <code>AbstractConfigurationProvider#loadSources</code>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">loadSources</span><span class="params">(Map&lt;String, SourceRunner&gt; sourceRunnerMap)</span> </span>&#123;</div><div class="line">  Source source = sourceFactory.create();</div><div class="line">  Configurables.configure(source, config);</div><div class="line">  sourceRunnerMap.put(comp.getComponentName(),</div><div class="line">      SourceRunner.forSource(source));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a href="https://github.com/apache/flume" target="_blank" rel="external">https://github.com/apache/flume</a></li>
<li><a href="https://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="external">https://flume.apache.org/FlumeUserGuide.html</a></li>
<li><a href="https://kafka.apache.org/0100/javadoc/index.html" target="_blank" rel="external">https://kafka.apache.org/0100/javadoc/index.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://flume.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Apache Flume&lt;/a&gt; is a real-time ETL tool for data warehouse platform. It consists of different types of components, and during runtime all of them are managed by Flume’s lifecycle and supervisor mechanism. This article will walk you through the source code of Flume’s component lifecycle management.&lt;/p&gt;
&lt;h2 id=&quot;Repository-Structure&quot;&gt;&lt;a href=&quot;#Repository-Structure&quot; class=&quot;headerlink&quot; title=&quot;Repository Structure&quot;&gt;&lt;/a&gt;Repository Structure&lt;/h2&gt;&lt;p&gt;Flume’s source code can be downloaded from GitHub. It’s a Maven project, so we can import it into an IDE for efficient code reading. The following is the main structure of the project:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;/flume-ng-node&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;/flume-ng-code&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;/flume-ng-sdk&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;/flume-ng-sources/flume-kafka-source&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;/flume-ng-channels/flume-kafka-channel&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;/flume-ng-sinks/flume-hdfs-sink&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;Application-Entrance&quot;&gt;&lt;a href=&quot;#Application-Entrance&quot; class=&quot;headerlink&quot; title=&quot;Application Entrance&quot;&gt;&lt;/a&gt;Application Entrance&lt;/h2&gt;&lt;p&gt;The &lt;code&gt;main&lt;/code&gt; entrance of Flume agent is in the &lt;code&gt;org.apache.flume.node.Application&lt;/code&gt; class of &lt;code&gt;flume-ng-node&lt;/code&gt; module. Following is an abridged source code:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Application&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    CommandLineParser parser = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; GnuParser();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (isZkConfigured) &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (reload) &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        PollingZooKeeperConfigurationProvider zookeeperConfigurationProvider;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        components.add(zookeeperConfigurationProvider);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &amp;#125; &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        StaticZooKeeperConfigurationProvider zookeeperConfigurationProvider;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        application.handleConfigurationEvent();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#125; &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &lt;span class=&quot;comment&quot;&gt;// PropertiesFileConfigurationProvider&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    application.start();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    Runtime.getRuntime().addShutdownHook(&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Thread(&lt;span class=&quot;string&quot;&gt;&quot;agent-shutdown-hook&quot;&lt;/span&gt;) &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &lt;span class=&quot;meta&quot;&gt;@Override&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        appReference.stop();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#125;);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;The process can be illustrated as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Parse command line arguments with &lt;code&gt;commons-cli&lt;/code&gt;, including the Flume agent’s name, configuration method and path.&lt;/li&gt;
&lt;li&gt;Configurations can be provided via properties file or ZooKeeper. Both provider support live-reload, i.e. we can update component settings without restarting the agent.&lt;ul&gt;
&lt;li&gt;File-based live-reload is implemented by using a background thread that checks the last modification time of the file.&lt;/li&gt;
&lt;li&gt;ZooKeeper-based live-reload is provided by Curator’s &lt;code&gt;NodeCache&lt;/code&gt; recipe, which uses ZooKeeper’s &lt;em&gt;watch&lt;/em&gt; functionality underneath.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If live-reload is on (by default), configuration providers will add themselves into the application’s component list, and after calling &lt;code&gt;Application#start&lt;/code&gt;, a &lt;code&gt;LifecycleSupervisor&lt;/code&gt; will start the provider, and trigger the reload event to parse the configuration and load all defined components.&lt;/li&gt;
&lt;li&gt;If live-reload is off, configuration providers will parse the file immediately and start all components, also supervised by &lt;code&gt;LifecycleSupervisor&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Finally add a JVM shutdown hook by &lt;code&gt;Runtime#addShutdownHook&lt;/code&gt;, which in turn invokes &lt;code&gt;Application#stop&lt;/code&gt; to shutdown the Flume agent.&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="flume, java, source code" scheme="http://shzhangji.com/tags/flume-java-source-code/"/>
    
  </entry>
  
  <entry>
    <title>Pandas and Tidy Data</title>
    <link href="http://shzhangji.com/blog/2017/09/30/pandas-and-tidy-data/"/>
    <id>http://shzhangji.com/blog/2017/09/30/pandas-and-tidy-data/</id>
    <published>2017-09-30T04:24:32.000Z</published>
    <updated>2017-09-30T04:29:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>In the paper <a href="https://www.jstatsoft.org/article/view/v059i10" target="_blank" rel="external">Tidy Data</a>, <a href="https://en.wikipedia.org/wiki/Hadley_Wickham" target="_blank" rel="external">Dr. Wickham</a> proposed a specific form of data structure: each variable is a column, each observation is a row, and each type of observational unit is a table. He argued that with tidy data, data analysts can manipulate, model, and visualize data more easily and effectively. He lists <em>five common data structures</em> that are untidy, and demonstrates how to use <a href="https://github.com/hadley/tidy-data/" target="_blank" rel="external">R language</a> to tidy them. In this article, we’ll use Python and Pandas to achieve the same tidiness.</p>
<p>Source code and demo data can be found on GitHub (<a href="https://github.com/jizhang/pandas-tidy-data" target="_blank" rel="external">link</a>), and readers are supposed to have Python environment installed, preferably with Anaconda and Spyder IDE.</p>
<h2 id="Column-headers-are-values-not-variable-names"><a href="#Column-headers-are-values-not-variable-names" class="headerlink" title="Column headers are values, not variable names"></a>Column headers are values, not variable names</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">df = pd.read_csv(<span class="string">'data/pew.csv'</span>)</div><div class="line">df.head(<span class="number">10</span>)</div></pre></td></tr></table></figure>
<p><img src="/images/tidy-data/pew.png" alt="Religion and Income - Pew Forum"></p>
<p>Column names “&lt;$10k”, “$10-20k” are really income ranges that constitutes a variable. Variables are measurements of attributes, like height, weight, and in this case, income and religion. The values within the table form another variable, frequency. To make <em>each variable a column</em>, we do the following transformation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">df = df.set_index(<span class="string">'religion'</span>)</div><div class="line">df = df.stack()</div><div class="line">df.index = df.index.rename(<span class="string">'income'</span>, level=<span class="number">1</span>)</div><div class="line">df.name = <span class="string">'frequency'</span></div><div class="line">df = df.reset_index()</div><div class="line">df.head(<span class="number">10</span>)</div></pre></td></tr></table></figure>
<p><img src="/images/tidy-data/pew-tidy.png" alt="Religion and Income - Tidy"></p>
<a id="more"></a>
<p>Here we use the <a href="https://pandas.pydata.org/pandas-docs/stable/reshaping.html" target="_blank" rel="external">stack / unstack</a> feature of Pandas MultiIndex objects. <code>stack()</code> will use the column names to form a second level of index, then we do some proper naming and use <code>reset_index()</code> to flatten the table. In line 4 <code>df</code> is actually a Series, since Pandas will automatically convert from a single-column DataFrame.</p>
<p>Pandas provides another more commonly used method to do the transformation, <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html" target="_blank" rel="external"><code>melt()</code></a>. It accepts the following arguments:</p>
<ul>
<li><code>frame</code>: the DataFrame to manipulate.</li>
<li><code>id_vars</code>: columns that stay put.</li>
<li><code>value_vars</code>: columns that will be transformed to a variable.</li>
<li><code>var_name</code>: name the newly added variable column.</li>
<li><code>value_name</code>: name the value column.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">df = pd.read_csv(<span class="string">'data/pew.csv'</span>)</div><div class="line">df = pd.melt(df, id_vars=[<span class="string">'religion'</span>], value_vars=list(df.columns)[<span class="number">1</span>:],</div><div class="line">             var_name=<span class="string">'income'</span>, value_name=<span class="string">'frequency'</span>)</div><div class="line">df = df.sort_values(by=<span class="string">'religion'</span>)</div><div class="line">df.to_csv(<span class="string">'data/pew-tidy.csv'</span>, index=<span class="keyword">False</span>)</div><div class="line">df.head(<span class="number">10</span>)</div></pre></td></tr></table></figure>
<p>This will give the same result. We’ll use <code>melt()</code> method a lot in the following sections.</p>
<p>Let’s take a look at another form of untidiness that falls in this section:</p>
<p><img src="/images/tidy-data/billboard.png" alt="Billboard 2000"></p>
<p>In this dataset, weekly ranks are recorded in separate columns. To answer the question “what’s the rank of ‘Dancing Queen’ in 2000-07-15”, we need to do some calculations with <code>date.entered</code> and the week columns. Let’s transform it into a tidy form:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">df = pd.read_csv(<span class="string">'data/billboard.csv'</span>)</div><div class="line">df = pd.melt(df, id_vars=list(df.columns)[:<span class="number">5</span>], value_vars=list(df.columns)[<span class="number">5</span>:],</div><div class="line">             var_name=<span class="string">'week'</span>, value_name=<span class="string">'rank'</span>)</div><div class="line">df[<span class="string">'week'</span>] = df[<span class="string">'week'</span>].str[<span class="number">2</span>:].astype(int)</div><div class="line">df[<span class="string">'date.entered'</span>] = pd.to_datetime(df[<span class="string">'date.entered'</span>]) + pd.to_timedelta((df[<span class="string">'week'</span>] - <span class="number">1</span>) * <span class="number">7</span>, <span class="string">'d'</span>)</div><div class="line">df = df.rename(columns=&#123;<span class="string">'date.entered'</span>: <span class="string">'date'</span>&#125;)</div><div class="line">df = df.sort_values(by=[<span class="string">'track'</span>, <span class="string">'date'</span>])</div><div class="line">df.to_csv(<span class="string">'data/billboard-intermediate.csv'</span>, index=<span class="keyword">False</span>)</div><div class="line">df.head(<span class="number">10</span>)</div></pre></td></tr></table></figure>
<p><img src="/images/tidy-data/billboard-intermediate.png" alt="Billboard 2000 - Intermediate Tidy"></p>
<p>We’ve also transformed the <code>date.entered</code> variable into the exact date of that week. Now <code>week</code> becomes a single column that represents a variable. But we can see a lot of duplications in this table, like artist and track. We’ll solve this problem in the fourth section.</p>
<h2 id="Multiple-variables-stored-in-one-column"><a href="#Multiple-variables-stored-in-one-column" class="headerlink" title="Multiple variables stored in one column"></a>Multiple variables stored in one column</h2><p>Storing variable values in columns is quite common because it makes the data table more compact, and easier to do analysis like cross validation, etc. The following dataset even manages to store two variables in the column, sex and age.</p>
<p><img src="/images/tidy-data/tb.png" alt="Tuberculosis (TB)"></p>
<p><code>m</code> stands for <code>male</code>, <code>f</code> for <code>female</code>, and age ranges are <code>0-14</code>, <code>15-24</code>, and so forth. To tidy it, we first melt the columns, use Pandas’ string operation to extract <code>sex</code>, and do a value mapping for the <code>age</code> ranges.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">df = pd.read_csv(<span class="string">'data/tb.csv'</span>)</div><div class="line">df = pd.melt(df, id_vars=[<span class="string">'country'</span>, <span class="string">'year'</span>], value_vars=list(df.columns)[<span class="number">2</span>:],</div><div class="line">             var_name=<span class="string">'column'</span>, value_name=<span class="string">'cases'</span>)</div><div class="line">df = df[df[<span class="string">'cases'</span>] != <span class="string">'---'</span>]</div><div class="line">df[<span class="string">'cases'</span>] = df[<span class="string">'cases'</span>].astype(int)</div><div class="line">df[<span class="string">'sex'</span>] = df[<span class="string">'column'</span>].str[<span class="number">0</span>]</div><div class="line">df[<span class="string">'age'</span>] = df[<span class="string">'column'</span>].str[<span class="number">1</span>:].map(&#123;</div><div class="line">    <span class="string">'014'</span>: <span class="string">'0-14'</span>,</div><div class="line">    <span class="string">'1524'</span>: <span class="string">'15-24'</span>,</div><div class="line">    <span class="string">'2534'</span>: <span class="string">'25-34'</span>,</div><div class="line">    <span class="string">'3544'</span>: <span class="string">'35-44'</span>,</div><div class="line">    <span class="string">'4554'</span>: <span class="string">'45-54'</span>,</div><div class="line">    <span class="string">'5564'</span>: <span class="string">'55-64'</span>,</div><div class="line">    <span class="string">'65'</span>: <span class="string">'65+'</span></div><div class="line">&#125;)</div><div class="line">df = df[[<span class="string">'country'</span>, <span class="string">'year'</span>, <span class="string">'sex'</span>, <span class="string">'age'</span>, <span class="string">'cases'</span>]]</div><div class="line">df.to_csv(<span class="string">'data/tb-tidy.csv'</span>, index=<span class="keyword">False</span>)</div><div class="line">df.head(<span class="number">10</span>)</div></pre></td></tr></table></figure>
<p><img src="/images/tidy-data/tb-tidy.png" alt="Tuberculosis (TB) - Tidy"></p>
<h2 id="Variables-are-stored-in-both-rows-and-columns"><a href="#Variables-are-stored-in-both-rows-and-columns" class="headerlink" title="Variables are stored in both rows and columns"></a>Variables are stored in both rows and columns</h2><p>This is a temperature dataset collection by a Weather Station named MX17004. Dates are spread in columns which can be melted into one column. <code>tmax</code> and <code>tmin</code> stand for highest and lowest temperatures, and they are really variables of each observational unit, in this case, each day, so we should <code>unstack</code> them into different columns.</p>
<p><img src="/images/tidy-data/weather.png" alt="Weather Station"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">df = pd.read_csv(<span class="string">'data/weather.csv'</span>)</div><div class="line">df = pd.melt(df, id_vars=[<span class="string">'id'</span>, <span class="string">'year'</span>, <span class="string">'month'</span>, <span class="string">'element'</span>],</div><div class="line">             value_vars=list(df.columns)[<span class="number">4</span>:],</div><div class="line">             var_name=<span class="string">'date'</span>, value_name=<span class="string">'value'</span>)</div><div class="line">df[<span class="string">'date'</span>] = df[<span class="string">'date'</span>].str[<span class="number">1</span>:].astype(<span class="string">'int'</span>)</div><div class="line">df[<span class="string">'date'</span>] = df[[<span class="string">'year'</span>, <span class="string">'month'</span>, <span class="string">'date'</span>]].apply(</div><div class="line">    <span class="keyword">lambda</span> row: <span class="string">'&#123;:4d&#125;-&#123;:02d&#125;-&#123;:02d&#125;'</span>.format(*row),</div><div class="line">    axis=<span class="number">1</span>)</div><div class="line">df = df.loc[df[<span class="string">'value'</span>] != <span class="string">'---'</span>, [<span class="string">'id'</span>, <span class="string">'date'</span>, <span class="string">'element'</span>, <span class="string">'value'</span>]]</div><div class="line">df = df.set_index([<span class="string">'id'</span>, <span class="string">'date'</span>, <span class="string">'element'</span>])</div><div class="line">df = df.unstack()</div><div class="line">df.columns = list(df.columns.get_level_values(<span class="string">'element'</span>))</div><div class="line">df = df.reset_index()</div><div class="line">df.to_csv(<span class="string">'data/weather-tidy.csv'</span>, index=<span class="keyword">False</span>)</div><div class="line">df</div></pre></td></tr></table></figure>
<p><img src="/images/tidy-data/weather-tidy.png" alt="Weather Station - Tidy"></p>
<h2 id="Multiple-types-in-one-table"><a href="#Multiple-types-in-one-table" class="headerlink" title="Multiple types in one table"></a>Multiple types in one table</h2><p>In the processed Billboard dataset, we can see duplicates of song tracks, it’s because this table actually contains two types of observational units, song tracks and weekly ranks. To tidy it, we first generate identities for each song track, i.e. <code>id</code>, and then separate them into different tables.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">df = pd.read_csv(<span class="string">'data/billboard-intermediate.csv'</span>)</div><div class="line">df_track = df[[<span class="string">'artist'</span>, <span class="string">'track'</span>, <span class="string">'time'</span>]].drop_duplicates()</div><div class="line">df_track.insert(<span class="number">0</span>, <span class="string">'id'</span>, range(<span class="number">1</span>, len(df_track) + <span class="number">1</span>))</div><div class="line">df = pd.merge(df, df_track, on=[<span class="string">'artist'</span>, <span class="string">'track'</span>, <span class="string">'time'</span>])</div><div class="line">df = df[[<span class="string">'id'</span>, <span class="string">'date'</span>, <span class="string">'rank'</span>]]</div><div class="line">df_track.to_csv(<span class="string">'data/billboard-track.csv'</span>, index=<span class="keyword">False</span>)</div><div class="line">df.to_csv(<span class="string">'data/billboard-rank.csv'</span>, index=<span class="keyword">False</span>)</div><div class="line">print(df_track, <span class="string">'\n\n'</span>, df)</div></pre></td></tr></table></figure>
<p><img src="/images/tidy-data/billboard-track.png" alt="Billboard 2000 - Track"></p>
<p><img src="/images/tidy-data/billboard-rank.png" alt="Billboard 2000 - Rank"></p>
<h2 id="One-type-in-multiple-tables"><a href="#One-type-in-multiple-tables" class="headerlink" title="One type in multiple tables"></a>One type in multiple tables</h2><p>Datasets can be separated in two ways, by different values of an variable like year 2000, 2001, location China, Britain, or by different attributes like temperature from one sensor, humidity from another. In the first case, we can write a utility function that walks through the data directory, reads each file, and assigns the filename to a dedicated column. In the end we can combine these DataFrames with <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html" target="_blank" rel="external"><code>pd.concat</code></a>. In the latter case, there should be some attribute that can identify the same units, like date, personal ID, etc. We can use <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge.html" target="_blank" rel="external"><code>pd.merge</code></a> to join datasets by common keys.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://tomaugspurger.github.io/modern-5-tidy.html" target="_blank" rel="external">https://tomaugspurger.github.io/modern-5-tidy.html</a></li>
<li><a href="https://hackernoon.com/reshaping-data-in-python-fa27dda2ff77" target="_blank" rel="external">https://hackernoon.com/reshaping-data-in-python-fa27dda2ff77</a></li>
<li><a href="http://www.jeannicholashould.com/tidy-data-in-python.html" target="_blank" rel="external">http://www.jeannicholashould.com/tidy-data-in-python.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In the paper &lt;a href=&quot;https://www.jstatsoft.org/article/view/v059i10&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Tidy Data&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Hadley_Wickham&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Dr. Wickham&lt;/a&gt; proposed a specific form of data structure: each variable is a column, each observation is a row, and each type of observational unit is a table. He argued that with tidy data, data analysts can manipulate, model, and visualize data more easily and effectively. He lists &lt;em&gt;five common data structures&lt;/em&gt; that are untidy, and demonstrates how to use &lt;a href=&quot;https://github.com/hadley/tidy-data/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;R language&lt;/a&gt; to tidy them. In this article, we’ll use Python and Pandas to achieve the same tidiness.&lt;/p&gt;
&lt;p&gt;Source code and demo data can be found on GitHub (&lt;a href=&quot;https://github.com/jizhang/pandas-tidy-data&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;link&lt;/a&gt;), and readers are supposed to have Python environment installed, preferably with Anaconda and Spyder IDE.&lt;/p&gt;
&lt;h2 id=&quot;Column-headers-are-values-not-variable-names&quot;&gt;&lt;a href=&quot;#Column-headers-are-values-not-variable-names&quot; class=&quot;headerlink&quot; title=&quot;Column headers are values, not variable names&quot;&gt;&lt;/a&gt;Column headers are values, not variable names&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;df = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;data/pew.csv&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;df.head(&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;img src=&quot;/images/tidy-data/pew.png&quot; alt=&quot;Religion and Income - Pew Forum&quot;&gt;&lt;/p&gt;
&lt;p&gt;Column names “&amp;lt;$10k”, “$10-20k” are really income ranges that constitutes a variable. Variables are measurements of attributes, like height, weight, and in this case, income and religion. The values within the table form another variable, frequency. To make &lt;em&gt;each variable a column&lt;/em&gt;, we do the following transformation:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;df = df.set_index(&lt;span class=&quot;string&quot;&gt;&#39;religion&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;df = df.stack()&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;df.index = df.index.rename(&lt;span class=&quot;string&quot;&gt;&#39;income&#39;&lt;/span&gt;, level=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;df.name = &lt;span class=&quot;string&quot;&gt;&#39;frequency&#39;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;df = df.reset_index()&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;df.head(&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;img src=&quot;/images/tidy-data/pew-tidy.png&quot; alt=&quot;Religion and Income - Tidy&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
      <category term="analytics" scheme="http://shzhangji.com/tags/analytics/"/>
    
      <category term="pandas" scheme="http://shzhangji.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>Apache Beam Quick Start with Python</title>
    <link href="http://shzhangji.com/blog/2017/09/12/apache-beam-quick-start-with-python/"/>
    <id>http://shzhangji.com/blog/2017/09/12/apache-beam-quick-start-with-python/</id>
    <published>2017-09-12T13:08:25.000Z</published>
    <updated>2017-09-13T01:44:34.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://beam.apache.org/get-started/beam-overview/" target="_blank" rel="external">Apache Beam</a> is a big data processing standard created by Google in 2016. It provides unified DSL to process both batch and stream data, and can be executed on popular platforms like Spark, Flink, and of course Google’s commercial product Dataflow. Beam’s model is based on previous works known as <a href="https://web.archive.org/web/20160923141630/https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35650.pdf" target="_blank" rel="external">FlumeJava</a> and <a href="https://web.archive.org/web/20160201091359/http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41378.pdf" target="_blank" rel="external">Millwheel</a>, and addresses solutions for data processing tasks like ETL, analysis, and <a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101" target="_blank" rel="external">stream processing</a>. Currently it provides SDK in two languages, Java and Python. This article will introduce how to use Python to write Beam applications.</p>
<p><img src="/images/beam/arch.jpg" alt="Apache Beam Pipeline"></p>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>Apache Beam Python SDK requires Python 2.7.x. You can use <a href="https://github.com/pyenv/pyenv" target="_blank" rel="external">pyenv</a> to manage different Python versions, or compile from <a href="https://www.python.org/downloads/source/" target="_blank" rel="external">source</a> (make sure you have SSL installed). And then you can install Beam SDK from PyPI, better in a virtual environment:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ virtualenv venv --distribute</div><div class="line">$ source venv/bin/activate</div><div class="line">(venv) $ pip install apache-beam</div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="Wordcount-Example"><a href="#Wordcount-Example" class="headerlink" title="Wordcount Example"></a>Wordcount Example</h2><p>Wordcount is the de-facto “Hello World” in big data field, so let’s take a look at how it’s done with Beam:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"><span class="keyword">import</span> apache_beam <span class="keyword">as</span> beam</div><div class="line"><span class="keyword">from</span> apache_beam.options.pipeline_options <span class="keyword">import</span> PipelineOptions</div><div class="line"><span class="keyword">with</span> beam.Pipeline(options=PipelineOptions()) <span class="keyword">as</span> p:</div><div class="line">    lines = p | <span class="string">'Create'</span> &gt;&gt; beam.Create([<span class="string">'cat dog'</span>, <span class="string">'snake cat'</span>, <span class="string">'dog'</span>])</div><div class="line">    counts = (</div><div class="line">        lines</div><div class="line">        | <span class="string">'Split'</span> &gt;&gt; (beam.FlatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">' '</span>))</div><div class="line">                      .with_output_types(unicode))</div><div class="line">        | <span class="string">'PairWithOne'</span> &gt;&gt; beam.Map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</div><div class="line">        | <span class="string">'GroupAndSum'</span> &gt;&gt; beam.CombinePerKey(sum)</div><div class="line">    )</div><div class="line">    counts | <span class="string">'Print'</span> &gt;&gt; beam.ParDo(<span class="keyword">lambda</span> (w, c): print(<span class="string">'%s: %s'</span> % (w, c)))</div></pre></td></tr></table></figure>
<p>Run the script, you’ll get the counts of difference words:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">(venv) $ python wordcount.py</div><div class="line">cat: 2</div><div class="line">snake: 1</div><div class="line">dog: 2</div></pre></td></tr></table></figure>
<p>There’re three fundamental concepts in Apache Beam, namely Pipeline, PCollection, and Transform.</p>
<ul>
<li><strong>Pipeline</strong> holds the DAG (Directed Acyclic Graph) of data and process tasks. It’s analogous to MapReduce <code>Job</code> and Storm <code>Topology</code>.</li>
<li><strong>PCollection</strong> is the data structure to which we apply various operations, like parse, convert, or aggregate. You can think of it as Spark <code>RDD</code>.</li>
<li>And <strong>Transform</strong> is where your main logic goes. Each transform will take a PCollection in and produce a new PCollection. Beam provides many built-in Transforms, and we’ll cover them later.</li>
</ul>
<p>As in this example, <code>Pipeline</code> and <code>PipelineOptions</code> are used to construct a pipeline. Use the <code>with</code> statement so that context manager will invoke <code>Pipeline.run</code> and <code>wait_until_finish</code> automatically.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[Output PCollection] = [Input PCollection] | [Label] &gt;&gt; [Transform]</div></pre></td></tr></table></figure>
<p><code>|</code> is the operator to apply transforms, and each transform can be optionally supplied with a unique label. Transforms can be chained, and we can compose arbitrary shapes of transforms, and at runtime they’ll be represented as DAG.</p>
<p><code>beam.Create</code> is a transform that creates PCollection from memory data, mainly for testing. Beam has built-in sources and sinks to read and write bounded or unbounded data, and it’s possible to implement our own.</p>
<p><code>beam.Map</code> is a <em>one-to-one</em> transform, and in this example we convert a word string to a <code>(word, 1)</code> tuple. <code>beam.FlatMap</code> is a combination of <code>Map</code> and <code>Flatten</code>, i.e. we split each line into an array of words, and then flatten these sequences into a single one.</p>
<p><code>CombinePerKey</code> works on two-element tuples. It groups the tuples by the first element (the key), and apply the provided function to the list of second elements (values). Finally, we use <code>beam.ParDo</code> to print out the counts. This is a rather basic transform, and we’ll discuss it in the following section.</p>
<h2 id="Input-and-Output"><a href="#Input-and-Output" class="headerlink" title="Input and Output"></a>Input and Output</h2><p>Currently, Beam’s Python SDK has very limited supports for IO. This table (<a href="https://beam.apache.org/documentation/io/built-in/" target="_blank" rel="external">source</a>) gives an overview of the available built-in transforms:</p>
<table>
<thead>
<tr>
<th>Language</th>
<th>File-based</th>
<th>Messaging</th>
<th>Database</th>
</tr>
</thead>
<tbody>
<tr>
<td>Java</td>
<td>HDFS<br>TextIO<br>XML</td>
<td>AMQP<br>Kafka<br>JMS</td>
<td>Hive<br>Solr<br>JDBC</td>
</tr>
<tr>
<td>Python</td>
<td>textio<br>avroio<br>tfrecordio</td>
<td>-</td>
<td>Google Big Query<br>Google Cloud Datastore</td>
</tr>
</tbody>
</table>
<p>The following snippet demonstrates the usage of <code>textio</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">lines = p | <span class="string">'Read'</span> &gt;&gt; beam.io.ReadFromText(<span class="string">'/path/to/input-*.csv'</span>)</div><div class="line">lines | <span class="string">'Write'</span> &gt;&gt; beam.io.WriteToText(<span class="string">'/path/to/output'</span>, file_name_suffix=<span class="string">'.csv'</span>)</div></pre></td></tr></table></figure>
<p><code>textio</code> is able to read multiple input files by using wildcard or you can flatten PCollections created from difference sources. The outputs are also split into several files due to pipeline’s parallel processing nature.</p>
<h2 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h2><p>There’re basic transforms and higher-level built-ins. In general, we prefer to use the later so that we can focus on the application logic. The following table lists some commonly used higher-level transforms:</p>
<table>
<thead>
<tr>
<th>Transform</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Create(value)</td>
<td>Creates a PCollection from an iterable.</td>
</tr>
<tr>
<td>Filter(fn)</td>
<td>Use callable <code>fn</code> to filter out elements.</td>
</tr>
<tr>
<td>Map(fn)</td>
<td>Use callable <code>fn</code> to do a one-to-one transformation.</td>
</tr>
<tr>
<td>FlatMap(fn)</td>
<td>Similar to <code>Map</code>, but <code>fn</code> needs to return an iterable of zero or more elements, and these iterables will be flattened into one PCollection.</td>
</tr>
<tr>
<td>Flatten()</td>
<td>Merge several PCollections into a single one.</td>
</tr>
<tr>
<td>Partition(fn)</td>
<td>Split a PCollection into several partitions. <code>fn</code> is a <code>PartitionFn</code> or a callable that accepts two arguments - <code>element</code>, <code>num_partitions</code>.</td>
</tr>
<tr>
<td>GroupByKey()</td>
<td>Works on a PCollection of key/value pairs (two-element tuples), groups by common key, and returns <code>(key, iter&lt;value&gt;)</code> pairs.</td>
</tr>
<tr>
<td>CoGroupByKey()</td>
<td>Groups results across several PCollections by key. e.g. input <code>(k, v)</code> and <code>(k, w)</code>, output <code>(k, (iter&lt;v&gt;, iter&lt;w&gt;))</code>.</td>
</tr>
<tr>
<td>RemoveDuplicates()</td>
<td>Get distint values in PCollection.</td>
</tr>
<tr>
<td>CombinePerKey(fn)</td>
<td>Similar to <code>GroupByKey</code>, but combines the values by a <code>CombineFn</code> or a callable that takes an iterable, such as <code>sum</code>, <code>max</code>.</td>
</tr>
<tr>
<td>CombineGlobally(fn)</td>
<td>Reduces a PCollection to a single value by applying <code>fn</code>.</td>
</tr>
</tbody>
</table>
<h3 id="Callable-DoFn-and-ParDo"><a href="#Callable-DoFn-and-ParDo" class="headerlink" title="Callable, DoFn, and ParDo"></a>Callable, DoFn, and ParDo</h3><p>Most transforms accepts a callable as argument. In Python, <a href="https://docs.python.org/2/library/functions.html#callable" target="_blank" rel="external">callable</a> can be a function, method, lambda expression, or class instance that has <code>__call__</code> method. Under the hood, Beam will wrap the callable as a <code>DoFn</code>, and all these transforms will invoke <code>ParDo</code>, the lower-level transform, with the <code>DoFn</code>.</p>
<p>Let’s replace the expression <code>lambda x: x.split(&#39; &#39;)</code> with a <code>DoFn</code> class:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SplitFn</span><span class="params">(beam.DoFn)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, element)</span>:</span></div><div class="line">        <span class="keyword">return</span> element.split(<span class="string">' '</span>)</div><div class="line"></div><div class="line">lines | beam.ParDo(SplitFn())</div></pre></td></tr></table></figure>
<p>The <code>ParDo</code> transform works like <code>FlatMap</code>, except that it only accepts <code>DoFn</code>. In addition to <code>return</code>, we can <code>yield</code> element from <code>process</code> method:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SplitAndPairWithOneFn</span><span class="params">(beam.DoFn)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, element)</span>:</span></div><div class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> element.split(<span class="string">' '</span>):</div><div class="line">            <span class="keyword">yield</span> (word, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<h3 id="Combiner-Functions"><a href="#Combiner-Functions" class="headerlink" title="Combiner Functions"></a>Combiner Functions</h3><p>Combiner functions, or <code>CombineFn</code>, are used to reduce a collection of elements into a single value. You can either perform on the entire PCollection (<code>CombineGlobally</code>), or combine the values for each key (<code>CombinePerKey</code>). Beam is capable of wrapping callables into <code>CombinFn</code>. The callable should take an iterable and returns a single value. Since Beam distributes computation to multiple nodes, the combiner function will be invoked multiple times to get partial results, so they ought to be <a href="https://en.wikipedia.org/wiki/Commutative_property" target="_blank" rel="external">commutative</a> and <a href="https://en.wikipedia.org/wiki/Associative_property" target="_blank" rel="external">associative</a>. <code>sum</code>, <code>min</code>, <code>max</code> are good examples.</p>
<p>Beam provides some built-in combiners like count, mean, top. Take count for instance, the following two lines are equivalent, they return the total count of lines.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">lines | beam.combiners.Count.Globally()</div><div class="line">lines | beam.CombineGlobally(beam.combiners.CountCombineFn())</div></pre></td></tr></table></figure>
<p>Other combiners can be found in Beam Python SDK Documentation (<a href="https://beam.apache.org/documentation/sdks/pydoc/2.1.0/apache_beam.transforms.html#module-apache_beam.transforms.combiners" target="_blank" rel="external">link</a>). For more complex combiners, we need to subclass the <code>CombinFn</code> and implement four methods. Take the built-in <code>Mean</code> for an example:</p>
<p><a href="https://github.com/apache/beam/blob/v2.1.0/sdks/python/apache_beam/transforms/combiners.py#L75" target="_blank" rel="external"><code>apache_beam/transforms/combiners.py</code></a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MeanCombineFn</span><span class="params">(core.CombineFn)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">create_accumulator</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="string">"""Create a "local" accumulator to track sum and count."""</span></div><div class="line">    <span class="keyword">return</span> (<span class="number">0</span>, <span class="number">0</span>)</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add_input</span><span class="params">(self, <span class="params">(sum_, count)</span>, element)</span>:</span></div><div class="line">    <span class="string">"""Process the incoming value."""</span></div><div class="line">    <span class="keyword">return</span> sum_ + element, count + <span class="number">1</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge_accumulators</span><span class="params">(self, accumulators)</span>:</span></div><div class="line">    <span class="string">"""Merge several accumulators into a single one."""</span></div><div class="line">    sums, counts = zip(*accumulators)</div><div class="line">    <span class="keyword">return</span> sum(sums), sum(counts)</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">extract_output</span><span class="params">(self, <span class="params">(sum_, count)</span>)</span>:</span></div><div class="line">    <span class="string">"""Compute the mean average."""</span></div><div class="line">    <span class="keyword">if</span> count == <span class="number">0</span>:</div><div class="line">      <span class="keyword">return</span> float(<span class="string">'NaN'</span>)</div><div class="line">    <span class="keyword">return</span> sum_ / float(count)</div></pre></td></tr></table></figure>
<h3 id="Composite-Transform"><a href="#Composite-Transform" class="headerlink" title="Composite Transform"></a>Composite Transform</h3><p>Take a look at the <a href="https://github.com/apache/beam/blob/v2.1.0/sdks/python/apache_beam/transforms/combiners.py#L101" target="_blank" rel="external">source code</a> of <code>beam.combiners.Count.Globally</code> we used before. It subclasses <code>PTransform</code> and applies some transforms to the PCollection. This forms a sub-graph of DAG, and we call it composite transform. Composite transforms are used to gather relative codes into logical modules, making them easy to understand and maintain.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Count</span><span class="params">(object)</span>:</span></div><div class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Globally</span><span class="params">(ptransform.PTransform)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">expand</span><span class="params">(self, pcoll)</span>:</span></div><div class="line">      <span class="keyword">return</span> pcoll | core.CombineGlobally(CountCombineFn())</div></pre></td></tr></table></figure>
<p>More built-in transforms are listed below:</p>
<table>
<thead>
<tr>
<th>Transform</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Count.Globally()</td>
<td>Count the total number of elements.</td>
</tr>
<tr>
<td>Count.PerKey()</td>
<td>Count number elements of each unique key.</td>
</tr>
<tr>
<td>Count.PerElement()</td>
<td>Count the occurrences of each element.</td>
</tr>
<tr>
<td>Mean.Globally()</td>
<td>Compute the average of all elements.</td>
</tr>
<tr>
<td>Mean.PerKey()</td>
<td>Compute the averages for each key.</td>
</tr>
<tr>
<td>Top.Of(n, reverse)</td>
<td>Get the top <code>n</code> elements from the PCollection. See also Top.Largest(n), Top.Smallest(n).</td>
</tr>
<tr>
<td>Top.PerKey(n, reverse)</td>
<td>Get top <code>n</code> elements for each key. See also Top.LargestPerKey(n), Top.SmallestPerKey(n)</td>
</tr>
<tr>
<td>Sample.FixedSizeGlobally(n)</td>
<td>Get a sample of <code>n</code> elements.</td>
</tr>
<tr>
<td>Sample.FixedSizePerKey(n)</td>
<td>Get samples from each key.</td>
</tr>
<tr>
<td>ToList()</td>
<td>Combine to a single list.</td>
</tr>
<tr>
<td>ToDict()</td>
<td>Combine to a single dict. Works on 2-element tuples.</td>
</tr>
</tbody>
</table>
<h2 id="Windowing"><a href="#Windowing" class="headerlink" title="Windowing"></a>Windowing</h2><p>When processing event data, such as access log or click stream, there’s an <em>event time</em> property attached to every item, and it’s common to perform aggregation on a per-time-window basis. With Beam, we can define different kinds of windows to divide event data into groups. Windowing can be used in both bounded and unbounded data source. Since current Python SDK only supports bounded source, the following example will work on an offline access log file, but the process can be applied to unbounded source as is.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">64.242.88.10 - - [07/Mar/2004:16:05:49 -0800] &quot;GET /edit HTTP/1.1&quot; 401 12846</div><div class="line">64.242.88.10 - - [07/Mar/2004:16:06:51 -0800] &quot;GET /rdiff HTTP/1.1&quot; 200 4523</div><div class="line">64.242.88.10 - - [07/Mar/2004:16:10:02 -0800] &quot;GET /hsdivision HTTP/1.1&quot; 200 6291</div><div class="line">64.242.88.10 - - [07/Mar/2004:16:11:58 -0800] &quot;GET /view HTTP/1.1&quot; 200 7352</div><div class="line">64.242.88.10 - - [07/Mar/2004:16:20:55 -0800] &quot;GET /view HTTP/1.1&quot; 200 5253</div></pre></td></tr></table></figure>
<p><code>logmining.py</code>, full source code can be found on GitHub (<a href="https://github.com/jizhang/hello-beam/blob/master/logmining.py" target="_blank" rel="external">link</a>).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">lines = p | <span class="string">'Create'</span> &gt;&gt; beam.io.ReadFromText(<span class="string">'access.log'</span>)</div><div class="line">windowed_counts = (</div><div class="line">    lines</div><div class="line">    | <span class="string">'Timestamp'</span> &gt;&gt; beam.Map(<span class="keyword">lambda</span> x: beam.window.TimestampedValue(</div><div class="line">                              x, extract_timestamp(x)))</div><div class="line">    | <span class="string">'Window'</span> &gt;&gt; beam.WindowInto(beam.window.SlidingWindows(<span class="number">600</span>, <span class="number">300</span>))</div><div class="line">    | <span class="string">'Count'</span> &gt;&gt; (beam.CombineGlobally(beam.combiners.CountCombineFn())</div><div class="line">                  .without_defaults())</div><div class="line">)</div><div class="line">windowed_counts =  windowed_counts | beam.ParDo(PrintWindowFn())</div></pre></td></tr></table></figure>
<p>First of all, we need to add a timestamp to each record. <code>extract_timestamp</code> is a custom function to parse <code>[07/Mar/2004:16:05:49 -0800]</code> as a unix timestamp. <code>TimestampedValue</code> links this timestamp to the record. Then we define a sliding window with the size <em>10 minutes</em> and period <em>5 minutes</em>, which means the first window is <code>[00:00, 00:10)</code>, second window is <code>[00:05, 00:15)</code>, and so forth. All windows have a <em>10 minutes</em> duration, and adjacent windows have a <em>5 minutes</em> shift. Sliding window is different from fixed window, in that the same elements could appear in different windows. The combiner function is a simple count, so the pipeline result of the first five logs will be:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[2004-03-08T00:00:00Z, 2004-03-08T00:10:00Z) @ 2</div><div class="line">[2004-03-08T00:05:00Z, 2004-03-08T00:15:00Z) @ 4</div><div class="line">[2004-03-08T00:10:00Z, 2004-03-08T00:20:00Z) @ 2</div><div class="line">[2004-03-08T00:15:00Z, 2004-03-08T00:25:00Z) @ 1</div><div class="line">[2004-03-08T00:20:00Z, 2004-03-08T00:30:00Z) @ 1</div></pre></td></tr></table></figure>
<p>In stream processing for unbounded source, event data will arrive in different order, so we need to deal with late data with Beam’s watermark and trigger facility. This is a rather advanced topic, and the Python SDK has not yet implemented this feature. If you’re interested, please refer to Stream <a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101" target="_blank" rel="external">101</a> and <a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102" target="_blank" rel="external">102</a> articles.</p>
<h2 id="Pipeline-Runner"><a href="#Pipeline-Runner" class="headerlink" title="Pipeline Runner"></a>Pipeline Runner</h2><p>As mentioned above, Apache Beam is just a standard that provides SDK and APIs. It’s the pipeline runner that is responsible to execute the workflow graph. The following matrix lists all available runners and their capabilities compared to Beam Model.</p>
<p><img src="/images/beam/matrix.png" alt="Beam Capability Matrix"></p>
<p><a href="https://beam.apache.org/documentation/runners/capability-matrix/" target="_blank" rel="external">Source</a></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://beam.apache.org/documentation/programming-guide/" target="_blank" rel="external">https://beam.apache.org/documentation/programming-guide/</a></li>
<li><a href="https://beam.apache.org/documentation/sdks/pydoc/2.1.0/" target="_blank" rel="external">https://beam.apache.org/documentation/sdks/pydoc/2.1.0/</a></li>
<li><a href="https://sookocheff.com/post/dataflow/get-to-know-dataflow/" target="_blank" rel="external">https://sookocheff.com/post/dataflow/get-to-know-dataflow/</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://beam.apache.org/get-started/beam-overview/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Apache Beam&lt;/a&gt; is a big data processing standard created by Google in 2016. It provides unified DSL to process both batch and stream data, and can be executed on popular platforms like Spark, Flink, and of course Google’s commercial product Dataflow. Beam’s model is based on previous works known as &lt;a href=&quot;https://web.archive.org/web/20160923141630/https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35650.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;FlumeJava&lt;/a&gt; and &lt;a href=&quot;https://web.archive.org/web/20160201091359/http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41378.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Millwheel&lt;/a&gt;, and addresses solutions for data processing tasks like ETL, analysis, and &lt;a href=&quot;https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;stream processing&lt;/a&gt;. Currently it provides SDK in two languages, Java and Python. This article will introduce how to use Python to write Beam applications.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/beam/arch.jpg&quot; alt=&quot;Apache Beam Pipeline&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Installation&quot;&gt;&lt;a href=&quot;#Installation&quot; class=&quot;headerlink&quot; title=&quot;Installation&quot;&gt;&lt;/a&gt;Installation&lt;/h2&gt;&lt;p&gt;Apache Beam Python SDK requires Python 2.7.x. You can use &lt;a href=&quot;https://github.com/pyenv/pyenv&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;pyenv&lt;/a&gt; to manage different Python versions, or compile from &lt;a href=&quot;https://www.python.org/downloads/source/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;source&lt;/a&gt; (make sure you have SSL installed). And then you can install Beam SDK from PyPI, better in a virtual environment:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;$ virtualenv venv --distribute&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;$ source venv/bin/activate&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;(venv) $ pip install apache-beam&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
      <category term="stream processing" scheme="http://shzhangji.com/tags/stream-processing/"/>
    
      <category term="apache beam" scheme="http://shzhangji.com/tags/apache-beam/"/>
    
      <category term="mapreduce" scheme="http://shzhangji.com/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>Hive Window and Analytical Functions</title>
    <link href="http://shzhangji.com/blog/2017/09/04/hive-window-and-analytical-functions/"/>
    <id>http://shzhangji.com/blog/2017/09/04/hive-window-and-analytical-functions/</id>
    <published>2017-09-04T13:55:23.000Z</published>
    <updated>2017-09-06T02:19:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>SQL is one of the major tools of data analysis. It provides filtering, transforming and aggregation functionalities, and we can use it to process big volume of data with the help of Hive and Hadoop. However, legacy SQL does not support operations like grouped ranking and moving average, because the <code>GROUP BY</code> clause can only produce one aggregation result for each group, but not for each row. Fortunately, with the new SQL standard coming, we can use the <code>WINDOW</code> clause to compute aggregations on a set of rows and return the result for each row.</p>
<p><img src="/images/hive-window/window-stock.png" alt="Moving Average"></p>
<p>For instance, if we want to calculate the two-day moving average for each stock, we can write the following query:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span></div><div class="line">  <span class="string">`date`</span>, <span class="string">`stock`</span>, <span class="string">`close`</span></div><div class="line">  ,<span class="keyword">AVG</span>(<span class="string">`close`</span>) <span class="keyword">OVER</span> <span class="string">`w`</span> <span class="keyword">AS</span> <span class="string">`mavg`</span></div><div class="line"><span class="keyword">FROM</span> <span class="string">`t_stock`</span></div><div class="line">WINDOW <span class="string">`w`</span> <span class="keyword">AS</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="string">`stock`</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="string">`date`</span></div><div class="line">               <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>)</div></pre></td></tr></table></figure>
<p><code>OVER</code>, <code>WINDOW</code> and <code>ROWS BETWEEN AND</code> are all newly added SQL keywords to support windowing operations. In this query, <code>PARTITION BY</code> and <code>ORDER BY</code> works like <code>GROUP BY</code> and <code>ORDER BY</code> after the <code>WHERE</code> clause, except it doesn’t collapse the rows, but only divides them into non-overlapping partitions to work on. <code>ROWS BETWEEN AND</code> here constructs a <strong>window frame</strong>. In this case, each frame contains the previous row and current row. We’ll discuss more on frames later. Finally, <code>AVG</code> is a window function that computes results on each frame. Note that <code>WINDOW</code> clause can also be directly appended to window function:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> <span class="keyword">AVG</span>(<span class="string">`close`</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="string">`stock`</span>) <span class="keyword">AS</span> <span class="string">`mavg`</span> <span class="keyword">FROM</span> <span class="string">`t_stock`</span>;</div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="Window-Query-Concepts"><a href="#Window-Query-Concepts" class="headerlink" title="Window Query Concepts"></a>Window Query Concepts</h2><p><img src="/images/hive-window/concepts.png" alt="Concepts"></p>
<p><a href="https://en.wikibooks.org/wiki/Structured_Query_Language/Window_functions" target="_blank" rel="external">Source</a></p>
<p>SQL window query introduces three concepts, namely window partition, window frame and window function.</p>
<p><code>PARTITION</code> clause divides result set into <strong>window partitions</strong> by one or more columns, and the rows within can be optionally sorted by one or more columns. If there’s not <code>PARTITION BY</code>, the entire result set is treated as a single partition; if there’s not <code>ORDER BY</code>, window frames cannot be defined, and all rows within the partition constitutes a single frame.</p>
<p><strong>Window frame</strong> selects rows from partition for window function to work on. There’re two ways of defining frame in Hive, <code>ROWS</code> AND <code>RANGE</code>. For both types, we define the upper bound and lower bound. For instance, <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> selects rows from the beginning of the partition to the current row; <code>SUM(close) RANGE BETWEEN 100 PRECEDING AND 200 FOLLOWING</code> selects rows by the <em>distance</em> from the current row’s value. Say current <code>close</code> is <code>200</code>, and this frame will includes rows whose <code>close</code> values range from <code>100</code> to <code>400</code>, within the partition. All possible combinations of frame definitions are listed as follows, and the default definition is <code>RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</div><div class="line">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</div><div class="line">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</div></pre></td></tr></table></figure>
<p>All <strong>window functions</strong> compute results on the current frame. Hive supports the following functions:</p>
<ul>
<li><code>FIRST_VALUE(col)</code>, <code>LAST_VALUE(col)</code> returns the column value of first / last row within the frame;</li>
<li><code>LEAD(col, n)</code>, <code>LAG(col, n)</code> returns the column value of n-th row before / after current row;</li>
<li><code>RANK()</code>, <code>ROW_NUMBER()</code> assigns a sequence of the current row within the frame. The difference is <code>RANK()</code> will contain duplicate if there’re identical values.</li>
<li><code>COUNT()</code>, <code>SUM(col)</code>, <code>MIN(col)</code> works as usual.</li>
</ul>
<h2 id="Hive-Query-Examples"><a href="#Hive-Query-Examples" class="headerlink" title="Hive Query Examples"></a>Hive Query Examples</h2><h3 id="Top-K"><a href="#Top-K" class="headerlink" title="Top K"></a>Top K</h3><p>First, let’s create some test data of employee incomes in Hive:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t_employee (<span class="keyword">id</span> <span class="built_in">INT</span>, emp_name <span class="built_in">VARCHAR</span>(<span class="number">20</span>), dep_name <span class="built_in">VARCHAR</span>(<span class="number">20</span>),</div><div class="line">salary <span class="built_in">DECIMAL</span>(<span class="number">7</span>, <span class="number">2</span>), age <span class="built_in">DECIMAL</span>(<span class="number">3</span>, <span class="number">0</span>));</div><div class="line"></div><div class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> t_employee <span class="keyword">VALUES</span></div><div class="line">( <span class="number">1</span>,  <span class="string">'Matthew'</span>, <span class="string">'Management'</span>,  <span class="number">4500</span>, <span class="number">55</span>),</div><div class="line">( <span class="number">2</span>,  <span class="string">'Olivia'</span>,  <span class="string">'Management'</span>,  <span class="number">4400</span>, <span class="number">61</span>),</div><div class="line">( <span class="number">3</span>,  <span class="string">'Grace'</span>,   <span class="string">'Management'</span>,  <span class="number">4000</span>, <span class="number">42</span>),</div><div class="line">( <span class="number">4</span>,  <span class="string">'Jim'</span>,     <span class="string">'Production'</span>,  <span class="number">3700</span>, <span class="number">35</span>),</div><div class="line">( <span class="number">5</span>,  <span class="string">'Alice'</span>,   <span class="string">'Production'</span>,  <span class="number">3500</span>, <span class="number">24</span>),</div><div class="line">( <span class="number">6</span>,  <span class="string">'Michael'</span>, <span class="string">'Production'</span>,  <span class="number">3600</span>, <span class="number">28</span>),</div><div class="line">( <span class="number">7</span>,  <span class="string">'Tom'</span>,     <span class="string">'Production'</span>,  <span class="number">3800</span>, <span class="number">35</span>),</div><div class="line">( <span class="number">8</span>,  <span class="string">'Kevin'</span>,   <span class="string">'Production'</span>,  <span class="number">4000</span>, <span class="number">52</span>),</div><div class="line">( <span class="number">9</span>,  <span class="string">'Elvis'</span>,   <span class="string">'Service'</span>,     <span class="number">4100</span>, <span class="number">40</span>),</div><div class="line">(<span class="number">10</span>,  <span class="string">'Sophia'</span>,  <span class="string">'Sales'</span>,       <span class="number">4300</span>, <span class="number">36</span>),</div><div class="line">(<span class="number">11</span>,  <span class="string">'Samantha'</span>,<span class="string">'Sales'</span>,       <span class="number">4100</span>, <span class="number">38</span>);</div></pre></td></tr></table></figure>
<p>We can use the <code>RANK()</code> function to find out who earns the most within each department:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> dep_name, emp_name, salary</div><div class="line"><span class="keyword">FROM</span> (</div><div class="line">  <span class="keyword">SELECT</span></div><div class="line">    dep_name, emp_name, salary</div><div class="line">    ,<span class="keyword">RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> dep_name <span class="keyword">ORDER</span> <span class="keyword">BY</span> salary <span class="keyword">DESC</span>) <span class="keyword">AS</span> rnk</div><div class="line">  <span class="keyword">FROM</span> t_employee</div><div class="line">) a</div><div class="line"><span class="keyword">WHERE</span> rnk = <span class="number">1</span>;</div></pre></td></tr></table></figure>
<p>Normally when there’s duplicates, <code>RANK()</code> returns the same value for each row and <em>skip</em> the next sequence number. Use <code>DENSE_RANK()</code> if you want consecutive ranks.</p>
<h3 id="Cumulative-Distribution"><a href="#Cumulative-Distribution" class="headerlink" title="Cumulative Distribution"></a>Cumulative Distribution</h3><p>We can calculate the cumulative distribution of salaries among all departments. For example, salary <code>4000</code>‘s cumulative distribution is <code>0.55</code>, which means 55% people’s salaries are less or equal to <code>4000</code>. To calculate this, we first count the frequencies of every salary, and do a cumulative summing:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span></div><div class="line">  salary</div><div class="line">  ,<span class="keyword">SUM</span>(cnt) <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary)</div><div class="line">  / <span class="keyword">SUM</span>(cnt) <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span></div><div class="line">                   <span class="keyword">AND</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span>)</div><div class="line"><span class="keyword">FROM</span> (</div><div class="line">  <span class="keyword">SELECT</span> salary, <span class="keyword">count</span>(*) <span class="keyword">AS</span> cnt</div><div class="line">  <span class="keyword">FROM</span> t_employee</div><div class="line">  <span class="keyword">GROUP</span> <span class="keyword">BY</span> salary</div><div class="line">) a;</div></pre></td></tr></table></figure>
<p>This can also be done with Hive’s <code>CUME_DIST()</code> window function. There’s another <code>PERCENT_RANK()</code> function, which computes the rank of the salary as percentage.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span></div><div class="line">  salary</div><div class="line">  ,<span class="keyword">CUME_DIST</span>() <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary) <span class="keyword">AS</span> pct_cum</div><div class="line">  ,<span class="keyword">PERCENT_RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> salary) <span class="keyword">AS</span> pct_rank</div><div class="line"><span class="keyword">FROM</span> t_employee;</div></pre></td></tr></table></figure>
<p><img src="/images/hive-window/employee-pct.png" alt="Cumulative Distribution"></p>
<h3 id="Clickstream-Sessionization"><a href="#Clickstream-Sessionization" class="headerlink" title="Clickstream Sessionization"></a>Clickstream Sessionization</h3><p>We can divide click events into different sessions by setting a <em>timeout</em>, in this case 30 minutes, and assign an id to each session:</p>
<p><img src="/images/hive-window/clickstream.png" alt="Click Stream"></p>
<p>First, in subquery <code>b</code>, we use the <code>LAG(col)</code> function to calculate the time difference between current row and previous row, and if it’s more than 30 minutes, a new session is marked. Then we do a cumulative sum of the <code>new_session</code> field so that each session will get an incremental sequence.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span></div><div class="line">  ipaddress, clicktime</div><div class="line">  ,<span class="keyword">SUM</span>(<span class="keyword">IF</span>(new_session, <span class="number">1</span>, <span class="number">0</span>)) <span class="keyword">OVER</span> x + <span class="number">1</span> <span class="keyword">AS</span> sessionid</div><div class="line"><span class="keyword">FROM</span> (</div><div class="line">  <span class="keyword">SELECT</span></div><div class="line">    ipaddress, clicktime, ts</div><div class="line">    ,ts - LAG(ts) <span class="keyword">OVER</span> w &gt; <span class="number">1800</span> <span class="keyword">AS</span> new_session</div><div class="line">  <span class="keyword">FROM</span> (</div><div class="line">    <span class="keyword">SELECT</span> *, <span class="keyword">UNIX_TIMESTAMP</span>(clicktime) <span class="keyword">AS</span> ts</div><div class="line">    <span class="keyword">FROM</span> t_clickstream</div><div class="line">  ) a</div><div class="line">  WINDOW w <span class="keyword">AS</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> ipaddress <span class="keyword">ORDER</span> <span class="keyword">BY</span> ts)</div><div class="line">) b</div><div class="line">WINDOW x <span class="keyword">AS</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> ipaddress <span class="keyword">ORDER</span> <span class="keyword">BY</span> ts);</div></pre></td></tr></table></figure>
<h2 id="Implementation-Detail"><a href="#Implementation-Detail" class="headerlink" title="Implementation Detail"></a>Implementation Detail</h2><p>Briefly speaking, window query consists of two steps: divide records into partitions, and evaluate window functions on each of them. The partitioning process is intuitive in map-reduce paradigm, since Hadoop will take care of the shuffling and sorting. However, ordinary UDAF can only return one row for each group, but in window query, there need to be a <em>table in, table out</em> contract. So the community introduced Partitioned Table Function (PTF) into Hive.</p>
<p>PTF, as the name suggests, works on partitions, and inputs / outputs a set of table rows. The following sequence diagram lists the major classes of PTF mechanism. <code>PTFOperator</code> reads data from sorted source and create input partitions; <code>WindowTableFunction</code> manages window frames, invokes window functions (UDAF), and writes the results to output partitions.</p>
<p><img src="/images/hive-window/window-sequence.png" alt="PTF Sequence Diagram"></p>
<p>The HIVE-896 ticket (<a href="https://issues.apache.org/jira/browse/HIVE-896" target="_blank" rel="external">link</a>) contains discussions on introducing analytical window functions into Hive, and this slide (<a href="https://www.slideshare.net/Hadoop_Summit/analytical-queries-with-hive" target="_blank" rel="external">link</a>), authored by one of the committers, explains how they implemented and merged PTF into Hive.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics</a></li>
<li><a href="https://github.com/hbutani/SQLWindowing" target="_blank" rel="external">https://github.com/hbutani/SQLWindowing</a></li>
<li><a href="https://content.pivotal.io/blog/time-series-analysis-1-introduction-to-window-functions" target="_blank" rel="external">https://content.pivotal.io/blog/time-series-analysis-1-introduction-to-window-functions</a></li>
<li><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SQL is one of the major tools of data analysis. It provides filtering, transforming and aggregation functionalities, and we can use it to process big volume of data with the help of Hive and Hadoop. However, legacy SQL does not support operations like grouped ranking and moving average, because the &lt;code&gt;GROUP BY&lt;/code&gt; clause can only produce one aggregation result for each group, but not for each row. Fortunately, with the new SQL standard coming, we can use the &lt;code&gt;WINDOW&lt;/code&gt; clause to compute aggregations on a set of rows and return the result for each row.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/hive-window/window-stock.png&quot; alt=&quot;Moving Average&quot;&gt;&lt;/p&gt;
&lt;p&gt;For instance, if we want to calculate the two-day moving average for each stock, we can write the following query:&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &lt;span class=&quot;string&quot;&gt;`date`&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;`stock`&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;`close`&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ,&lt;span class=&quot;keyword&quot;&gt;AVG&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;`close`&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;OVER&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`w`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`mavg`&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`t_stock`&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;WINDOW &lt;span class=&quot;string&quot;&gt;`w`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AS&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`stock`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`date`&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;               &lt;span class=&quot;keyword&quot;&gt;ROWS&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;PRECEDING&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;CURRENT&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;ROW&lt;/span&gt;)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;OVER&lt;/code&gt;, &lt;code&gt;WINDOW&lt;/code&gt; and &lt;code&gt;ROWS BETWEEN AND&lt;/code&gt; are all newly added SQL keywords to support windowing operations. In this query, &lt;code&gt;PARTITION BY&lt;/code&gt; and &lt;code&gt;ORDER BY&lt;/code&gt; works like &lt;code&gt;GROUP BY&lt;/code&gt; and &lt;code&gt;ORDER BY&lt;/code&gt; after the &lt;code&gt;WHERE&lt;/code&gt; clause, except it doesn’t collapse the rows, but only divides them into non-overlapping partitions to work on. &lt;code&gt;ROWS BETWEEN AND&lt;/code&gt; here constructs a &lt;strong&gt;window frame&lt;/strong&gt;. In this case, each frame contains the previous row and current row. We’ll discuss more on frames later. Finally, &lt;code&gt;AVG&lt;/code&gt; is a window function that computes results on each frame. Note that &lt;code&gt;WINDOW&lt;/code&gt; clause can also be directly appended to window function:&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;AVG&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;`close`&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;OVER&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`stock`&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`mavg`&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;`t_stock`&lt;/span&gt;;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="analytics" scheme="http://shzhangji.com/tags/analytics/"/>
    
      <category term="sql" scheme="http://shzhangji.com/tags/sql/"/>
    
      <category term="hive" scheme="http://shzhangji.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>An Introduction to stream-lib The Stream Processing Utilities</title>
    <link href="http://shzhangji.com/blog/2017/08/27/an-introduction-to-stream-lib-the-stream-processing-utilities/"/>
    <id>http://shzhangji.com/blog/2017/08/27/an-introduction-to-stream-lib-the-stream-processing-utilities/</id>
    <published>2017-08-27T02:57:24.000Z</published>
    <updated>2017-08-28T00:55:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>When processing a large amount of data, certain operations will cost a lot of time and space, such as counting the distinct values, or figuring out the 95th percentile of a sequence of numbers. But sometimes the accuracy is not that important. Maybe you just want a brief summary of the dataset, or it’s a monitoring system, where limited error rate is tolerable. There’re plenty of such algorithms that can trade accuracy with huge saves of time-space. What’s more, most of the data structures can be merged, making it possible to use in stream processing applications. <a href="https://github.com/addthis/stream-lib" target="_blank" rel="external"><code>stream-lib</code></a> is a collection of these algorithms. They are Java implementations based on academical research and papers. This artile will give a brief introduction to this utility library.</p>
<h2 id="Count-Cardinality-with-HyperLogLog"><a href="#Count-Cardinality-with-HyperLogLog" class="headerlink" title="Count Cardinality with HyperLogLog"></a>Count Cardinality with <code>HyperLogLog</code></h2><p>Unique visitors (UV) is the major metric of websites. We usually generate UUIDs for each user and track them by HTTP Cookie, or roughly use the IP address. We can use a <code>HashSet</code> to count the exact value of UV, but that takes a lot of memory. With <code>HyperLogLog</code>, an algorithm for the count-distinct problem, we are able to <a href="https://en.wikipedia.org/wiki/HyperLogLog" target="_blank" rel="external">estimate cardinalities of &gt; 10^9 with a typical accuracy of 2%, using 1.5 kB of memory</a>.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.clearspring.analytics<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>stream<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ICardinality card = <span class="keyword">new</span> HyperLogLog(<span class="number">10</span>);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i : <span class="keyword">new</span> <span class="keyword">int</span>[] &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span> &#125;) &#123;</div><div class="line">    card.offer(i);</div><div class="line">&#125;</div><div class="line">System.out.println(card.cardinality()); <span class="comment">// 4</span></div></pre></td></tr></table></figure>
<a id="more"></a>
<p><code>HyperLogLog</code> estimates cardinality by counting the leading zeros of each member’s binary value. If the maximum count is <code>n</code>, the cardinality is <code>2^n</code>. There’re some key points in this algorithm. First, members needs to be uniformly distributed, which we can use a hash function to achieve. <code>stream-lib</code> uses <a href="https://en.wikipedia.org/wiki/MurmurHash" target="_blank" rel="external">MurmurHash</a>, a simple, fast, and well distributed hash function, that is used in lots of hash-based lookup algorithms. Second, to decrease the variance of the result, set members are splitted into subsets, and the final result is the harmonic mean of all subsets’ cardinality. The integer argument that we passed to <code>HyperLogLog</code> constructor is the number of bits that it’ll use to split subsets, and the accuracy can be derived from this formula: <code>1.04/sqrt(2^log2m)</code>.</p>
<p><code>HyperLogLog</code> is an extension of <code>LogLog</code> algorithm, and the <code>HyperLogLogPlus</code> makes some more improvements. For instance, it uses a 64 bit hash function to remove the correction factor that adjusts hash collision; for small cardinality, it applies an empirical bias correction; and it also supports growing from a sparse data strucutre of registers (holding subsets) to a dense one. These algorithms are all included in <code>stream-lib</code></p>
<h2 id="Test-Membership-with-BloomFilter"><a href="#Test-Membership-with-BloomFilter" class="headerlink" title="Test Membership with BloomFilter"></a>Test Membership with <code>BloomFilter</code></h2><p><img src="/images/stream-lib/bloom-filter.jpg" alt="Bloom Filter"></p>
<p><code>BloomFilter</code> is a widely used data structure to test whether a set contains a certain member. The key is it will give false positive result, but never false negative. For example, Chrome maintains a malicious URLs in local storage, and it’s a bloom filter. When typing a new URL, if the filter says it’s not malicious, then it’s definitely not. But if the filter says it is in the set, then Chrome needs to contact the remote server for further confirmation.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Filter filter = <span class="keyword">new</span> BloomFilter(<span class="number">100</span>, <span class="number">0.01</span>);</div><div class="line">filter.add(<span class="string">"google.com"</span>);</div><div class="line">filter.add(<span class="string">"twitter.com"</span>);</div><div class="line">filter.add(<span class="string">"facebook.com"</span>);</div><div class="line">System.out.println(filter.isPresent(<span class="string">"bing.com"</span>)); <span class="comment">// false</span></div></pre></td></tr></table></figure>
<p>The contruction process of a bloom filter is faily simple:</p>
<ul>
<li>Create a bit array of <code>n</code> bits. In Java, we can use the <a href="https://docs.oracle.com/javase/8/docs/api/java/util/BitSet.html" target="_blank" rel="external"><code>BitSet</code></a> class.</li>
<li>Apply <code>k</code> number of hash functions to the incoming value, and set the corresponding bits to true.</li>
<li>When testing a membership, apply those hash functions and get the bits’ values:<ul>
<li>If every bit hits, the value might be in the set, with a False Positive Probability (FPP);</li>
<li>If not all bits hit, the value is definitely not in the set.</li>
</ul>
</li>
</ul>
<p>Again, those hash functions need to be uniformly distributed, and pairwise independent. Murmur hash meets the criteria. The FPP can be calculated by this formula: <code>(1-e^(-kn/m))^k</code>. This page (<a href="https://llimllib.github.io/bloomfilter-tutorial/" target="_blank" rel="external">link</a>) provides an online visualization of bloom filter. Other use cases are: anti-spam in email service, non-existent rows detection in Cassandra and HBase, and Squid also uses it to do <a href="https://wiki.squid-cache.org/SquidFaq/CacheDigests" target="_blank" rel="external">cache digest</a>.</p>
<h2 id="Top-k-Elements-with-CountMinSketch"><a href="#Top-k-Elements-with-CountMinSketch" class="headerlink" title="Top-k Elements with CountMinSketch"></a>Top-k Elements with <code>CountMinSketch</code></h2><p><img src="/images/stream-lib/count-min-sketch.png" alt="Count Min Sketch"></p>
<p><a href="https://stackoverflow.com/a/35356116/1030720" target="_blank" rel="external">Source</a></p>
<p><a href="https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch" target="_blank" rel="external"><code>CountMinSketch</code></a> is a “sketching” algorithm that uses minimal space to track frequencies of incoming events. We can for example find out the top K tweets streaming out of Twitter, or count the most visited pages of a website. The “sketch” can be used to estimate these frequencies, with some loss of accuracy, of course.</p>
<p>The following snippet shows how to use <code>stream-lib</code> to get the top three animals in the <code>List</code>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">List&lt;String&gt; animals;</div><div class="line">IFrequency freq = <span class="keyword">new</span> CountMinSketch(<span class="number">10</span>, <span class="number">5</span>, <span class="number">0</span>);</div><div class="line">Map&lt;String, Long&gt; top = Collections.emptyMap();</div><div class="line"><span class="keyword">for</span> (String animal : animals) &#123;</div><div class="line">    freq.add(animal, <span class="number">1</span>);</div><div class="line">    top = Stream.concat(top.keySet().stream(), Stream.of(animal)).distinct()</div><div class="line">              .map(a -&gt; <span class="keyword">new</span> SimpleEntry&lt;String, Long&gt;(a, freq.estimateCount(a)))</div><div class="line">              .sorted(Comparator.comparing(SimpleEntry&lt;String, Long&gt;::getValue).reversed())</div><div class="line">              .limit(<span class="number">3</span>)</div><div class="line">              .collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));</div><div class="line">&#125;</div><div class="line"></div><div class="line">System.out.println(top); <span class="comment">// &#123;rabbit=25, bird=45, spider=35&#125;</span></div></pre></td></tr></table></figure>
<p><code>CountMinSketch#estimateCount</code> is a <em>point query</em> that asks for the count of an event. Since the “sketch” cannot remeber the exact events, we need to store them else where.</p>
<p>The data structure of count-min sketch is similar to bloom filter, instead of one bit array of <code>w</code> bits, it uses <code>d</code> number of them, so as to form a <code>d x w</code> matrix. When a value comes, it applies <code>d</code> number of hash functions, and update the corresponding bit in the matrix. These hash functions need only to be <a href="https://en.wikipedia.org/wiki/Pairwise_independence" target="_blank" rel="external">pairwise independent</a>, so <code>stream-lib</code> uses a simple yet fast <code>(a*x+b) mod p</code> formula. When doing <em>point query</em>, calculate the hash values, and the smallest value is the frequency.</p>
<p>The estimation error is <code>ε = e / w</code> while probability of bad estimate is <code>δ = 1 / e ^ d</code>. So we can increase <code>w</code> and / or <code>d</code> to improve the results. Original paper can be found in this <a href="https://web.archive.org/web/20060907232042/http://www.eecs.harvard.edu/~michaelm/CS222/countmin.pdf" target="_blank" rel="external">link</a>.</p>
<h2 id="Histogram-and-Quantile-with-T-Digest"><a href="#Histogram-and-Quantile-with-T-Digest" class="headerlink" title="Histogram and Quantile with T-Digest"></a>Histogram and Quantile with <code>T-Digest</code></h2><p><img src="/images/stream-lib/t-digest.png" alt="T-Digest"></p>
<p><a href="https://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest" target="_blank" rel="external">Source</a></p>
<p>Median, 95th percentile are common use cases in descriptive statistics. Median for instance is less influenced by outliers than mean, but the calculation is not simple. One needs to track all data, sort them, and then get the final result. With <code>T-Digest</code>, we can agian generate a summarized distribution of the dataset and estimate the quantiles.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">Random rand = <span class="keyword">new</span> Random();</div><div class="line">List&lt;Double&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">TDigest digest = <span class="keyword">new</span> TDigest(<span class="number">100</span>);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000000</span>; ++i) &#123;</div><div class="line">    <span class="keyword">double</span> d = rand.nextDouble();</div><div class="line">    data.add(d);</div><div class="line">    digest.add(d);</div><div class="line">&#125;</div><div class="line">Collections.sort(data);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">double</span> q : <span class="keyword">new</span> <span class="keyword">double</span>[] &#123; <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.9</span> &#125;) &#123;</div><div class="line">    System.out.println(String.format(<span class="string">"quantile=%.1f digest=%.4f exact=%.4f"</span>,</div><div class="line">            q, digest.quantile(q), data.get((<span class="keyword">int</span>) (data.size() * q))));</div><div class="line">&#125;</div><div class="line"><span class="comment">// quantile=0.1 digest=0.0998 exact=0.1003</span></div><div class="line"><span class="comment">// quantile=0.5 digest=0.5009 exact=0.5000</span></div><div class="line"><span class="comment">// quantile=0.9 digest=0.8994 exact=0.8998</span></div></pre></td></tr></table></figure>
<p>The <code>T-Digest</code> paper can be found in this <a href="https://raw.githubusercontent.com/tdunning/t-digest/master/docs/t-digest-paper/histo.pdf" target="_blank" rel="external">link</a>. In brief, it uses a variant of 1-dimensional k-means clustering mechanism, representing the empirical distribution by retaining the centroids of subsets. Besides, different <code>T-Digest</code> instances can be merged into a larger, more accurate instance, which can be used in parallel processing with ease.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>As we can see, most algorithms tries to save space and time with the cost of slight accuracy. By “sketching” the batch or streaming dataset, we can catch the “interesting” features and give very good estimation, especially when the dataset itself fullfills certain distribution. <code>stream-lib</code> and other opensourced projects certainly ease the process for us end users.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://www.javadoc.io/doc/com.clearspring.analytics/stream/2.9.5" target="_blank" rel="external">https://www.javadoc.io/doc/com.clearspring.analytics/stream/2.9.5</a></li>
<li><a href="http://www.addthis.com/blog/2011/03/29/new-open-source-stream-summarizing-java-library/" target="_blank" rel="external">http://www.addthis.com/blog/2011/03/29/new-open-source-stream-summarizing-java-library/</a></li>
<li><a href="https://www.mapr.com/blog/some-important-streaming-algorithms-you-should-know-about" target="_blank" rel="external">https://www.mapr.com/blog/some-important-streaming-algorithms-you-should-know-about</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When processing a large amount of data, certain operations will cost a lot of time and space, such as counting the distinct values, or figuring out the 95th percentile of a sequence of numbers. But sometimes the accuracy is not that important. Maybe you just want a brief summary of the dataset, or it’s a monitoring system, where limited error rate is tolerable. There’re plenty of such algorithms that can trade accuracy with huge saves of time-space. What’s more, most of the data structures can be merged, making it possible to use in stream processing applications. &lt;a href=&quot;https://github.com/addthis/stream-lib&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;stream-lib&lt;/code&gt;&lt;/a&gt; is a collection of these algorithms. They are Java implementations based on academical research and papers. This artile will give a brief introduction to this utility library.&lt;/p&gt;
&lt;h2 id=&quot;Count-Cardinality-with-HyperLogLog&quot;&gt;&lt;a href=&quot;#Count-Cardinality-with-HyperLogLog&quot; class=&quot;headerlink&quot; title=&quot;Count Cardinality with HyperLogLog&quot;&gt;&lt;/a&gt;Count Cardinality with &lt;code&gt;HyperLogLog&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;Unique visitors (UV) is the major metric of websites. We usually generate UUIDs for each user and track them by HTTP Cookie, or roughly use the IP address. We can use a &lt;code&gt;HashSet&lt;/code&gt; to count the exact value of UV, but that takes a lot of memory. With &lt;code&gt;HyperLogLog&lt;/code&gt;, an algorithm for the count-distinct problem, we are able to &lt;a href=&quot;https://en.wikipedia.org/wiki/HyperLogLog&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;estimate cardinalities of &amp;gt; 10^9 with a typical accuracy of 2%, using 1.5 kB of memory&lt;/a&gt;.&lt;/p&gt;
&lt;figure class=&quot;highlight xml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;com.clearspring.analytics&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;stream&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;2.9.5&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;ICardinality card = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; HyperLogLog(&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i : &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;[] &amp;#123; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt; &amp;#125;) &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    card.offer(i);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;System.out.println(card.cardinality()); &lt;span class=&quot;comment&quot;&gt;// 4&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="java" scheme="http://shzhangji.com/tags/java/"/>
    
      <category term="stream processing" scheme="http://shzhangji.com/tags/stream-processing/"/>
    
      <category term="algorithm" scheme="http://shzhangji.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Extract Data from MySQL with Binlog and Canal</title>
    <link href="http://shzhangji.com/blog/2017/08/12/extract-data-from-mysql-with-binlog-and-canal/"/>
    <id>http://shzhangji.com/blog/2017/08/12/extract-data-from-mysql-with-binlog-and-canal/</id>
    <published>2017-08-12T11:15:09.000Z</published>
    <updated>2017-08-14T00:37:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>Data extraction is the very first step of an ETL process. We need to load data from external data stores like RDMBS or logging file system, and then we can do cleaning, transformation and summary. In modern website stack, MySQL is the most widely used database, and it’s common to extract data from different instances and load into a central MySQL database, or directly into Hive. There’re several query-based techniques that we can use to do the extraction, including the popular open source software <a href="http://sqoop.apache.org/" target="_blank" rel="external">Sqoop</a>, but they are not meant for real-time data ingestion. Binlog, on the other hand, is a real-time data stream that is used to do replication between master and slave instances. With the help of Alibaba’s open sourced <a href="https://github.com/alibaba/canal" target="_blank" rel="external">Canal</a> project, we can easily utilize the binlog facility to do data extraction from MySQL database to various destinations.</p>
<p><img src="/images/canal.png" alt="Canal"></p>
<h2 id="Canal-Components"><a href="#Canal-Components" class="headerlink" title="Canal Components"></a>Canal Components</h2><p>In brief, Canal simulates itself to be a MySQL slave and dump binlog from master, parse it, and send to downstream sinks. Canal consists of two major components, namely Canal server and Canal client. A Canal server can connect to multiple MySQL instances, and maintains an event queue for each instance. Canal clients can then subscribe to theses queues and receive data changes. The following is a quick start guide to get Canal going.</p>
<a id="more"></a>
<h3 id="Configure-MySQL-Master"><a href="#Configure-MySQL-Master" class="headerlink" title="Configure MySQL Master"></a>Configure MySQL Master</h3><p>MySQL binlog is not enabled by default. Locate your <code>my.cnf</code> file and make these changes:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">server-id = 1</div><div class="line">log_bin = /path/to/mysql-bin.log</div><div class="line">binlog_format = ROW</div></pre></td></tr></table></figure>
<p>Note that <code>binlog_format</code> must be <code>ROW</code>, becuase in <code>STATEMENT</code> or <code>MIXED</code> mode, only SQL statements will be logged and transferred (to save log size), but what we need is full data of the changed rows.</p>
<p>Slave connects to master via an dedicated account, which must have the global <code>REPLICATION</code> priviledges. We can use the <code>GRANT</code> statement to create the account:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>, <span class="keyword">REPLICATION</span> <span class="keyword">SLAVE</span>, <span class="keyword">REPLICATION</span> <span class="keyword">CLIENT</span></div><div class="line"><span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'canal'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'canal'</span>;</div></pre></td></tr></table></figure>
<h3 id="Startup-Canal-Server"><a href="#Startup-Canal-Server" class="headerlink" title="Startup Canal Server"></a>Startup Canal Server</h3><p>Download Canal server from its GitHub Releases page (<a href="https://github.com/alibaba/canal/releases" target="_blank" rel="external">link</a>). The config files reside in <code>conf</code> directory. A typical layout is:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">canal.deployer/conf/canal.properties</div><div class="line">canal.deployer/conf/instanceA/instance.properties</div><div class="line">canal.deployer/conf/instanceB/instance.properties</div></pre></td></tr></table></figure>
<p>In <code>conf/canal.properties</code> there’s the main configuration. <code>canal.port</code> for example defines which port Canal server is listening. <code>instanceA/instance.properties</code> defines the MySQL instance that Canal server will draw binlog from. Important settings are:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># slaveId cannot collide with the server-id in my.cnf</div><div class="line">canal.instance.mysql.slaveId = 1234</div><div class="line">canal.instance.master.address = 127.0.0.1:3306</div><div class="line">canal.instance.dbUsername = canal</div><div class="line">canal.instance.dbPassword = canal</div><div class="line">canal.instance.connectionCharset = UTF-8</div><div class="line"># process all tables from all databases</div><div class="line">canal.instance.filter.regex = .*\\..*</div></pre></td></tr></table></figure>
<p>Start the server by <code>sh bin/startup.sh</code>, and you’ll see the following output in <code>logs/example/example.log</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Loading properties file from class path resource [canal.properties]</div><div class="line">Loading properties file from class path resource [example/instance.properties]</div><div class="line">start CannalInstance for 1-example</div><div class="line">[destination = example , address = /127.0.0.1:3306 , EventParser] prepare to find start position just show master status</div></pre></td></tr></table></figure>
<h3 id="Write-Canal-Client"><a href="#Write-Canal-Client" class="headerlink" title="Write Canal Client"></a>Write Canal Client</h3><p>To consume update events from Canal server, we can create a Canal client in our application, specify the instance and tables we’re interested in, and start polling.</p>
<p>First, add <code>com.alibaba.otter:canal.client</code> dependency to your <code>pom.xml</code>, and construct a Canal client:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">CanalConnector connector = CanalConnectors.newSingleConnector(</div><div class="line">        <span class="keyword">new</span> InetSocketAddress(<span class="string">"127.0.0.1"</span>, <span class="number">11111</span>), <span class="string">"example"</span>, <span class="string">""</span>, <span class="string">""</span>);</div><div class="line"></div><div class="line">connector.connect();</div><div class="line">connector.subscribe(<span class="string">".*\\..*"</span>);</div><div class="line"></div><div class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">    Message message = connector.getWithoutAck(<span class="number">100</span>);</div><div class="line">    <span class="keyword">long</span> batchId = message.getId();</div><div class="line">    <span class="keyword">if</span> (batchId == -<span class="number">1</span> || message.getEntries().isEmpty()) &#123;</div><div class="line">        Thread.sleep(<span class="number">3000</span>);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        printEntries(message.getEntries());</div><div class="line">        connector.ack(batchId);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>The code is quite similar to consuming from a message queue. The update events are sent in batches, and you can acknowledge every batch after being properly processed.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// printEntries</span></div><div class="line">RowChange rowChange = RowChange.parseFrom(entry.getStoreValue());</div><div class="line"><span class="keyword">for</span> (RowData rowData : rowChange.getRowDatasList()) &#123;</div><div class="line">    <span class="keyword">if</span> (rowChange.getEventType() == EventType.INSERT) &#123;</div><div class="line">      printColumns(rowData.getAfterCollumnList());</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Every <code>Entry</code> in a message represents a set of row changes with the same event type, e.g. INSERT, UPDATE, or DELETE. For each row, we can get the column data like this:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// printColumns</span></div><div class="line">String line = columns.stream()</div><div class="line">        .map(column -&gt; column.getName() + <span class="string">"="</span> + column.getValue())</div><div class="line">        .collect(Collectors.joining(<span class="string">","</span>));</div><div class="line">System.out.println(line);</div></pre></td></tr></table></figure>
<p>Full example can be found on GitHub (<a href="https://github.com/jizhang/java-sandbox/blob/blog-canal/src/main/java/com/shzhangji/javasandbox/canal/SimpleClient.java" target="_blank" rel="external">link</a>).</p>
<h2 id="Load-into-Data-Warehouse"><a href="#Load-into-Data-Warehouse" class="headerlink" title="Load into Data Warehouse"></a>Load into Data Warehouse</h2><h3 id="RDBMS-with-Batch-Insert"><a href="#RDBMS-with-Batch-Insert" class="headerlink" title="RDBMS with Batch Insert"></a>RDBMS with Batch Insert</h3><p>For DB based data warehouse, we can directly use the <code>REPLACE</code> statement and let the database deduplicates rows by primary key. One concern is the instertion performance, so it’s often necessary to cache the data for a while and do a batch insertion, like:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">REPLACE</span> <span class="keyword">INTO</span> <span class="string">`user`</span> (<span class="string">`id`</span>, <span class="string">`name`</span>, <span class="string">`age`</span>, <span class="string">`updated`</span>) <span class="keyword">VALUES</span></div><div class="line">(<span class="number">1</span>, <span class="string">'Jerry'</span>, <span class="number">30</span>, <span class="string">'2017-08-12 16:00:00'</span>),</div><div class="line">(<span class="number">2</span>, <span class="string">'Mary'</span>, <span class="number">28</span>, <span class="string">'2017-08-12 17:00:00'</span>),</div><div class="line">(<span class="number">3</span>, <span class="string">'Tom'</span>, <span class="number">36</span>, <span class="string">'2017-08-12 18:00:00'</span>);</div></pre></td></tr></table></figure>
<p>Another approach is to write the extracted data into a delimited text file, then execute a <code>LOAD DATA</code> statement. These files can also be used to import data into Hive. But for both approaches, make sure you escape the string columns properly, so as to avoid insertion errors.</p>
<h3 id="Hive-based-Warehouse"><a href="#Hive-based-Warehouse" class="headerlink" title="Hive-based Warehouse"></a>Hive-based Warehouse</h3><p>Hive tables are stored on HDFS, which is an append-only file system, so it takes efforts to update data in a previously loaded table. One can use a JOIN-based approach, Hive transaction, or switch to HBase.</p>
<p>Data can be categorized into base and delta. For example, yesterday’s <code>user</code> table is the base, while today’s updated rows are the delta. Using a <code>FULL OUTER JOIN</code> we can generate the latest snapshot:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span></div><div class="line">  <span class="keyword">COALESCE</span>(b.<span class="string">`id`</span>, a.<span class="string">`id`</span>) <span class="keyword">AS</span> <span class="string">`id`</span></div><div class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`name`</span>, a.<span class="string">`name`</span>) <span class="keyword">AS</span> <span class="string">`name`</span></div><div class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`age`</span>, a.<span class="string">`age`</span>) <span class="keyword">AS</span> <span class="string">`age`</span></div><div class="line">  ,<span class="keyword">COALESCE</span>(b.<span class="string">`updated`</span>, a.<span class="string">`updated`</span>) <span class="keyword">AS</span> <span class="string">`updated`</span></div><div class="line"><span class="keyword">FROM</span> dw_stage.<span class="string">`user`</span> a</div><div class="line"><span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> (</div><div class="line">  <span class="comment">-- deduplicate by selecting the latest record</span></div><div class="line">  <span class="keyword">SELECT</span> <span class="string">`id`</span>, <span class="string">`name`</span>, <span class="string">`age`</span>, <span class="string">`updated`</span></div><div class="line">  <span class="keyword">FROM</span> (</div><div class="line">    <span class="keyword">SELECT</span> *, ROW_NUMBER() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="string">`id`</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="string">`updated`</span> <span class="keyword">DESC</span>) <span class="keyword">AS</span> <span class="string">`n`</span></div><div class="line">    <span class="keyword">FROM</span> dw_stage.<span class="string">`user_delta`</span></div><div class="line">  ) b</div><div class="line">  <span class="keyword">WHERE</span> <span class="string">`n`</span> = <span class="number">1</span></div><div class="line">) b</div><div class="line"><span class="keyword">ON</span> a.<span class="string">`id`</span> = b.<span class="string">`id`</span>;</div></pre></td></tr></table></figure>
<p>Hive 0.13 introduces transaction and ACID table, 0.14 brings us the <code>INSERT</code>, <code>UPDATE</code> and <code>DELETE</code> statements, and Hive 2.0.0 provides a new <a href="https://cwiki.apache.org/confluence/display/Hive/HCatalog+Streaming+Mutation+API" target="_blank" rel="external">Streaming Mutation API</a> that can be used to submit insert/update/delete transactions to Hive tables programmatically. Currently, ACID tables must use ORC file format, and be bucketed by primiary key. Hive will store the mutative operations in delta files. When reading from this table, <code>OrcInputFormat</code> will figure out which record is the latest. The official sample code can be found in the test suite (<a href="https://github.com/apache/hive/blob/master/hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/ExampleUseCase.java" target="_blank" rel="external">link</a>).</p>
<p>And the final approach is to use HBase, which is a key-value store built on HDFS, making it perfect for data updates. Its table can also be used by MapReduce jobs, or you can create an external Hive table that points directly to HBase. More information can be found on the <a href="http://hbase.apache.org/" target="_blank" rel="external">official website</a>.</p>
<h2 id="Initialize-Target-Table"><a href="#Initialize-Target-Table" class="headerlink" title="Initialize Target Table"></a>Initialize Target Table</h2><p>Data extraction is usually on-demand, so there may be already historical data in the source table. One obvious approach is dumping the full table manually and load into destination. Or we can reuse the Canal facility, notify the client to query data from source and do the updates.</p>
<p>First, we create a helper table in the source database:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`retl_buffer`</span> (</div><div class="line">  <span class="keyword">id</span> <span class="built_in">BIGINT</span> AUTO_INCREMENT PRIMARY <span class="keyword">KEY</span></div><div class="line">  ,table_name <span class="built_in">VARCHAR</span>(<span class="number">255</span>)</div><div class="line">  ,pk_value <span class="built_in">VARCHAR</span>(<span class="number">255</span>)</div><div class="line">);</div></pre></td></tr></table></figure>
<p>To reload all records in <code>user</code> table:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`retl_buffer`</span> (<span class="string">`table_name`</span>, <span class="string">`pk_value`</span>)</div><div class="line"><span class="keyword">SELECT</span> <span class="string">'user'</span>, <span class="string">`id`</span> <span class="keyword">FROM</span> <span class="string">`user`</span>;</div></pre></td></tr></table></figure>
<p>When Canal client receives the <code>RowChange</code> of <code>retl_buffer</code> table, it can extract the table name and primary key value from the record, query the source database, and write to the destination.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (<span class="string">"retl_buffer"</span>.equals(entry.getHeader().getTableName())) &#123;</div><div class="line">    String tableName = rowData.getAfterColumns(<span class="number">1</span>).getValue();</div><div class="line">    String pkValue = rowData.getAfterColumns(<span class="number">2</span>).getValue();</div><div class="line">    System.out.println(<span class="string">"SELECT * FROM "</span> + tableName + <span class="string">" WHERE id = "</span> + pkValue);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>This approach is included in another Alibaba’s project <a href="https://github.com/alibaba/otter/wiki/Manager%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E8%87%AA-%E7%94%B1-%E9%97%A8" target="_blank" rel="external">Otter</a>.</p>
<h2 id="Canal-High-Availability"><a href="#Canal-High-Availability" class="headerlink" title="Canal High Availability"></a>Canal High Availability</h2><ul>
<li>Canal instances can be supplied with a standby MySQL source, typically in a Master-Master HA scenario. Make sure you turn on the <code>log_slave_updates</code> option in both MySQL instances. Canal uses a dedicated heartbeat check, i.e. update a row periodically to check if current source is alive.</li>
<li>Canal server itself also supports HA. You’ll need a Zookeeper quorum to enable this feature. Clients will get the current server location from Zookeeper, and the server will record the last binlog offset that has been consumed.</li>
</ul>
<p>For more information, please checkout the <a href="https://github.com/alibaba/canal/wiki/AdminGuide" target="_blank" rel="external">AdminGuide</a>.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://github.com/alibaba/canal/wiki" target="_blank" rel="external">https://github.com/alibaba/canal/wiki</a> (in Chinese)</li>
<li><a href="https://github.com/alibaba/otter/wiki" target="_blank" rel="external">https://github.com/alibaba/otter/wiki</a> (in Chinese)</li>
<li><a href="https://www.phdata.io/4-strategies-for-updating-hive-tables/" target="_blank" rel="external">https://www.phdata.io/4-strategies-for-updating-hive-tables/</a></li>
<li><a href="https://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/" target="_blank" rel="external">https://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Data extraction is the very first step of an ETL process. We need to load data from external data stores like RDMBS or logging file system, and then we can do cleaning, transformation and summary. In modern website stack, MySQL is the most widely used database, and it’s common to extract data from different instances and load into a central MySQL database, or directly into Hive. There’re several query-based techniques that we can use to do the extraction, including the popular open source software &lt;a href=&quot;http://sqoop.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Sqoop&lt;/a&gt;, but they are not meant for real-time data ingestion. Binlog, on the other hand, is a real-time data stream that is used to do replication between master and slave instances. With the help of Alibaba’s open sourced &lt;a href=&quot;https://github.com/alibaba/canal&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Canal&lt;/a&gt; project, we can easily utilize the binlog facility to do data extraction from MySQL database to various destinations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/canal.png&quot; alt=&quot;Canal&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Canal-Components&quot;&gt;&lt;a href=&quot;#Canal-Components&quot; class=&quot;headerlink&quot; title=&quot;Canal Components&quot;&gt;&lt;/a&gt;Canal Components&lt;/h2&gt;&lt;p&gt;In brief, Canal simulates itself to be a MySQL slave and dump binlog from master, parse it, and send to downstream sinks. Canal consists of two major components, namely Canal server and Canal client. A Canal server can connect to multiple MySQL instances, and maintains an event queue for each instance. Canal clients can then subscribe to theses queues and receive data changes. The following is a quick start guide to get Canal going.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="etl" scheme="http://shzhangji.com/tags/etl/"/>
    
      <category term="mysql" scheme="http://shzhangji.com/tags/mysql/"/>
    
      <category term="canal" scheme="http://shzhangji.com/tags/canal/"/>
    
      <category term="java" scheme="http://shzhangji.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>How to Extract Event Time in Apache Flume</title>
    <link href="http://shzhangji.com/blog/2017/08/05/how-to-extract-event-time-in-apache-flume/"/>
    <id>http://shzhangji.com/blog/2017/08/05/how-to-extract-event-time-in-apache-flume/</id>
    <published>2017-08-05T07:10:47.000Z</published>
    <updated>2017-08-07T05:36:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>Extracting data from upstream message queues is a common task in ETL. In a Hadoop based data warehouse, we usually use Flume to import event logs from Kafka into HDFS, and then run MapReduce jobs agaist it, or create Hive external tables partitioned by time. One of the keys of this process is to extract the event time from the logs, since real-time data can have time lags, or your system is temporarily offline and need to perform a catch-up. Flume provides various facilities to help us do this job easily.</p>
<p><img src="/images/flume.png" alt="Apache Flume"></p>
<h2 id="HDFS-Sink-and-Timestamp-Header"><a href="#HDFS-Sink-and-Timestamp-Header" class="headerlink" title="HDFS Sink and Timestamp Header"></a>HDFS Sink and Timestamp Header</h2><p>Here is a simple HDFS Sink config:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a1.sinks = k1</div><div class="line">a1.sinks.k1.type = hdfs</div><div class="line">a1.sinks.k1.hdfs.path = /user/flume/ds_alog/dt=%Y%m%d</div></pre></td></tr></table></figure>
<p><code>%Y%m%d</code> is the placeholders supported by this sink. It will use the milliseconds in <code>timestamp</code> header to replace them. Also, HDFS Sink provides <code>hdfs.useLocalTimeStamp</code> option so that it’ll use the local time to replace these placeholders, but this is not what we intend.</p>
<p>Another sink we could use is the Hive Sink, which directly communicates with Hive metastore and loads data into HDFS as Hive table. It supports both delimited text and JSON serializers, and also requires a <code>timestamp</code> header. But we don’t choose it for the following reasons:</p>
<ul>
<li>It doesn’t support regular expression serializer, so we cannot extract columns from arbitrary data format like access logs;</li>
<li>The columns to be extracted are defined in Hive metastore. Say the upstream events add some new keys in JSON, they will be dropped until Hive table definition is updated. As in data warehouse, it’s better to preserve the original source data for a period of time.</li>
</ul>
<a id="more"></a>
<h2 id="Regex-Extractor-Interceptor"><a href="#Regex-Extractor-Interceptor" class="headerlink" title="Regex Extractor Interceptor"></a>Regex Extractor Interceptor</h2><p>Flume has a mechanism called Interceptor, i.e. some optionally chained operations appended to Source, so as to perform various yet primitive transformation. For instance, the <code>TimestampInterceptor</code> is to add current local timestamp to the event header. In this section, I’ll demonstrate how to extract event time from access logs and JSON serialized logs with the help of interceptors.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">0.123 [2017-06-27 09:08:00] GET /</div><div class="line">0.234 [2017-06-27 09:08:01] GET /</div></pre></td></tr></table></figure>
<p><a href="http://flume.apache.org/FlumeUserGuide.html#regex-extractor-interceptor" target="_blank" rel="external"><code>RegexExtractorInterceptor</code></a> can be used to extract values based on regular expressions. Here’s the config:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">a1.sources.r1.interceptors = i1</div><div class="line">a1.sources.r1.interceptors.i1.type = regex_extractor</div><div class="line">a1.sources.r1.interceptors.i1.regex = \\[(.*?)\\]</div><div class="line">a1.sources.r1.interceptors.i1.serializers = s1</div><div class="line">a1.sources.r1.interceptors.i1.serializers.s1.type = org.apache.flume.interceptor.RegexExtractorInterceptorMillisSerializer</div><div class="line">a1.sources.r1.interceptors.i1.serializers.s1.name = timestamp</div><div class="line">a1.sources.r1.interceptors.i1.serializers.s1.pattern = yyyy-MM-dd HH:mm:ss</div></pre></td></tr></table></figure>
<p>It searches the string with pattern <code>\[(.*?)\]</code>, capture the first sub-pattern as <code>s1</code>, then parse it as a datetime string, and finally store it into headers with the name <code>timestamp</code>.</p>
<h3 id="Search-And-Replace-Interceptor"><a href="#Search-And-Replace-Interceptor" class="headerlink" title="Search And Replace Interceptor"></a>Search And Replace Interceptor</h3><p>For JSON strings:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="attr">"actionTime"</span>:<span class="number">1498525680.023</span>,<span class="attr">"actionType"</span>:<span class="string">"pv"</span>&#125;</div><div class="line">&#123;<span class="attr">"actionTime"</span>:<span class="number">1498525681.349</span>,<span class="attr">"actionType"</span>:<span class="string">"pv"</span>&#125;</div></pre></td></tr></table></figure>
<p>We can also extract <code>actionTime</code> with a regular expression, but note that HDFS Sink requires the timestamp in milliseconds, so we have to first convert the timestamp with <code>SearchAndReplaceInterceptor</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">a1.sources.r1.interceptors = i1 i2</div><div class="line">a1.sources.r1.interceptors.i1.type = search_replace</div><div class="line">a1.sources.r1.interceptors.i1.searchPattern = \&quot;actionTime\&quot;:(\\d+)\\.(\\d+)</div><div class="line">a1.sources.r1.interceptors.i1.replaceString = \&quot;actionTime\&quot;:$1$2</div><div class="line">a1.sources.r1.interceptors.i2.type = regex_extractor</div><div class="line">a1.sources.r1.interceptors.i2.regex = \&quot;actionTime\&quot;:(\\d+)</div><div class="line">a1.sources.r1.interceptors.i2.serializers = s1</div><div class="line">a1.sources.r1.interceptors.i2.serializers.s1.name = timestamp</div></pre></td></tr></table></figure>
<p>There’re two chained interceptors, first one replaces <code>1498525680.023</code> with <code>1498525680023</code> and second extracts <code>actionTime</code> right into headers.</p>
<h3 id="Custom-Interceptor"><a href="#Custom-Interceptor" class="headerlink" title="Custom Interceptor"></a>Custom Interceptor</h3><p>It’s also possible to write your own interceptor, thus do the extraction and conversion in one step. Your interceptor should implements <code>org.apache.flume.interceptor.Interceptor</code> and then do the job in <code>intercept</code> method. The source code and unit test can be found on GitHub (<a href="https://github.com/jizhang/java-sandbox/blob/blog-flume/src/main/java/com/shzhangji/javasandbox/flume/ActionTimeInterceptor.java" target="_blank" rel="external">link</a>). Please add <code>flume-ng-core</code> to your project dependencies.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ActionTimeInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> ObjectMapper mapper = <span class="keyword">new</span> ObjectMapper();</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            JsonNode node = mapper.readTree(<span class="keyword">new</span> ByteArrayInputStream(event.getBody()));</div><div class="line">            <span class="keyword">long</span> timestamp = (<span class="keyword">long</span>) (node.get(<span class="string">"actionTime"</span>).getDoubleValue() * <span class="number">1000</span>);</div><div class="line">            event.getHeaders().put(<span class="string">"timestamp"</span>, Long.toString(timestamp));</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            <span class="comment">// no-op</span></div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> event;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Use-Kafka-Channel-Directly"><a href="#Use-Kafka-Channel-Directly" class="headerlink" title="Use Kafka Channel Directly"></a>Use Kafka Channel Directly</h2><p>When the upstream is Kafka, and you have control of the message format, you can further eliminate the Source and directly pass data from Kafka to HDFS. The trick is to write messages in <code>AvroFlumeEvent</code> format, so that <a href="http://flume.apache.org/FlumeUserGuide.html#kafka-channel" target="_blank" rel="external">Kafka Channel</a> can deserialize them and use the <code>timestamp</code> header within. Otherwise, Kafka channel will parse messages as plain text with no headers, and HDFS sink will complain missing <code>timestamp</code>.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// construct an AvroFlumeEvent, this class can be found in flume-ng-sdk artifact</span></div><div class="line">Map&lt;CharSequence, CharSequence&gt; headers = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">headers.put(<span class="string">"timestamp"</span>, <span class="string">"1498525680023"</span>);</div><div class="line">String body = <span class="string">"some message"</span>;</div><div class="line">AvroFlumeEvent event = <span class="keyword">new</span> AvroFlumeEvent(headers, ByteBuffer.wrap(body.getBytes()));</div><div class="line"></div><div class="line"><span class="comment">// serialize event with Avro encoder</span></div><div class="line">ByteArrayOutputStream out = <span class="keyword">new</span> ByteArrayOutputStream();</div><div class="line">BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(out, <span class="keyword">null</span>);</div><div class="line">SpecificDatumWriter&lt;AvroFlumeEvent&gt; writer = <span class="keyword">new</span> SpecificDatumWriter&lt;&gt;(AvroFlumeEvent.class);</div><div class="line">writer.write(event, encoder);</div><div class="line">encoder.flush();</div><div class="line"></div><div class="line"><span class="comment">// send bytes to Kafka</span></div><div class="line">producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, <span class="keyword">byte</span>[]&gt;(<span class="string">"alog"</span>, out.toByteArray()));</div></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="external">http://flume.apache.org/FlumeUserGuide.html</a></li>
<li><a href="https://github.com/apache/flume" target="_blank" rel="external">https://github.com/apache/flume</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Extracting data from upstream message queues is a common task in ETL. In a Hadoop based data warehouse, we usually use Flume to import event logs from Kafka into HDFS, and then run MapReduce jobs agaist it, or create Hive external tables partitioned by time. One of the keys of this process is to extract the event time from the logs, since real-time data can have time lags, or your system is temporarily offline and need to perform a catch-up. Flume provides various facilities to help us do this job easily.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/flume.png&quot; alt=&quot;Apache Flume&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;HDFS-Sink-and-Timestamp-Header&quot;&gt;&lt;a href=&quot;#HDFS-Sink-and-Timestamp-Header&quot; class=&quot;headerlink&quot; title=&quot;HDFS Sink and Timestamp Header&quot;&gt;&lt;/a&gt;HDFS Sink and Timestamp Header&lt;/h2&gt;&lt;p&gt;Here is a simple HDFS Sink config:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;a1.sinks = k1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;a1.sinks.k1.type = hdfs&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;a1.sinks.k1.hdfs.path = /user/flume/ds_alog/dt=%Y%m%d&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;%Y%m%d&lt;/code&gt; is the placeholders supported by this sink. It will use the milliseconds in &lt;code&gt;timestamp&lt;/code&gt; header to replace them. Also, HDFS Sink provides &lt;code&gt;hdfs.useLocalTimeStamp&lt;/code&gt; option so that it’ll use the local time to replace these placeholders, but this is not what we intend.&lt;/p&gt;
&lt;p&gt;Another sink we could use is the Hive Sink, which directly communicates with Hive metastore and loads data into HDFS as Hive table. It supports both delimited text and JSON serializers, and also requires a &lt;code&gt;timestamp&lt;/code&gt; header. But we don’t choose it for the following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It doesn’t support regular expression serializer, so we cannot extract columns from arbitrary data format like access logs;&lt;/li&gt;
&lt;li&gt;The columns to be extracted are defined in Hive metastore. Say the upstream events add some new keys in JSON, they will be dropped until Hive table definition is updated. As in data warehouse, it’s better to preserve the original source data for a period of time.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="etl" scheme="http://shzhangji.com/tags/etl/"/>
    
      <category term="java" scheme="http://shzhangji.com/tags/java/"/>
    
      <category term="flume" scheme="http://shzhangji.com/tags/flume/"/>
    
  </entry>
  
  <entry>
    <title>How to Achieve Exactly-Once Semantics in Spark Streaming</title>
    <link href="http://shzhangji.com/blog/2017/07/31/how-to-achieve-exactly-once-semantics-in-spark-streaming/"/>
    <id>http://shzhangji.com/blog/2017/07/31/how-to-achieve-exactly-once-semantics-in-spark-streaming/</id>
    <published>2017-07-31T14:56:07.000Z</published>
    <updated>2017-08-01T00:56:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>Exactly-once semantics is one of the advanced topics of stream processing. To process every message once and only once, in spite of system or network failure, not only the stream processing framework needs to provide such functionality, but also the message delivery system, the output data store, as well as how we implement the processing procedure, altogether can we ensure the exactly-once semantics. In this article, I’ll demonstrate how to use Spark Streaming, with Kafka as data source and MySQL the output storage, to achieve exactly-once stream processing.</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt="Spark Streaming"></p>
<h2 id="An-Introductory-Example"><a href="#An-Introductory-Example" class="headerlink" title="An Introductory Example"></a>An Introductory Example</h2><p>First let’s implement a simple yet complete stream processing application that receive access logs from Kafka, parse and count the errors, then write the errors per minute metric into MySQL database.</p>
<p>Sample access logs:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">2017-07-30 14:09:08 ERROR some message</div><div class="line">2017-07-30 14:09:20 INFO  some message</div><div class="line">2017-07-30 14:10:50 ERROR some message</div></pre></td></tr></table></figure>
<p>Output table, where <code>log_time</code> should be truncated to minutes:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> error_log (</div><div class="line">  log_time datetime primary <span class="keyword">key</span>,</div><div class="line">  log_count <span class="built_in">int</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="number">0</span></div><div class="line">);</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>Scala projects are usually managed by <code>sbt</code> tool. Let’s add the following dependencies into <code>build.sbt</code> file. We’re using Spark 2.2 with Kafka 0.10. The choice of database library is ScalikeJDBC 3.0.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scalaVersion := <span class="string">"2.11.11"</span></div><div class="line"></div><div class="line">libraryDependencies ++= <span class="type">Seq</span>(</div><div class="line">  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-streaming"</span> % <span class="string">"2.2.0"</span> % <span class="string">"provided"</span>,</div><div class="line">  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-streaming-kafka-0-10"</span> % <span class="string">"2.2.0"</span>,</div><div class="line">  <span class="string">"org.scalikejdbc"</span> %% <span class="string">"scalikejdbc"</span> % <span class="string">"3.0.1"</span>,</div><div class="line">  <span class="string">"mysql"</span> % <span class="string">"mysql-connector-java"</span> % <span class="string">"5.1.43"</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>The complete code can be found on GitHub (<a href="https://github.com/jizhang/spark-sandbox/blob/master/src/main/scala/ExactlyOnce.scala" target="_blank" rel="external">link</a>), so here only shows the major parts of the application:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// initialize database connection</span></div><div class="line"><span class="type">ConnectionPool</span>.singleton(<span class="string">"jdbc:mysql://localhost:3306/spark"</span>, <span class="string">"root"</span>, <span class="string">""</span>)</div><div class="line"></div><div class="line"><span class="comment">// create Spark streaming context</span></div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ExactlyOnce"</span>).setIfMissing(<span class="string">"spark.master"</span>, <span class="string">"local[2]"</span>)</div><div class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</div><div class="line"></div><div class="line"><span class="comment">// create Kafka DStream with Direct API</span></div><div class="line"><span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</div><div class="line">   <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</div><div class="line">   <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Seq</span>(<span class="string">"alog"</span>), kafkaParams))</div><div class="line"></div><div class="line">messages.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="comment">// do transformation</span></div><div class="line">  <span class="keyword">val</span> result = rdd.map(_.value)</div><div class="line">    .flatMap(parseLog) <span class="comment">// utility function to parse log line into case class</span></div><div class="line">    .filter(_.level == <span class="string">"ERROR"</span>)</div><div class="line">    .map(log =&gt; log.time.truncatedTo(<span class="type">ChronoUnit</span>.<span class="type">MINUTES</span>) -&gt; <span class="number">1</span>)</div><div class="line">    .reduceByKey(_ + _)</div><div class="line">    .collect()</div><div class="line"></div><div class="line">  <span class="comment">// store result into database</span></div><div class="line">  <span class="type">DB</span>.autoCommit &#123; <span class="keyword">implicit</span> session =&gt;</div><div class="line">    result.foreach &#123; <span class="keyword">case</span> (time, count) =&gt;</div><div class="line">      <span class="string">sql""</span><span class="string">"</span></div><div class="line"><span class="string">      insert into error_log (log_time, log_count)</span></div><div class="line"><span class="string">      value ($&#123;time&#125;, $&#123;count&#125;)</span></div><div class="line"><span class="string">      on duplicate key update log_count = log_count + values(log_count)</span></div><div class="line"><span class="string">      "</span><span class="string">""</span>.update.apply()</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Stream-Processing-Semantics"><a href="#Stream-Processing-Semantics" class="headerlink" title="Stream Processing Semantics"></a>Stream Processing Semantics</h2><p>There’re three semantics in stream processing, namely at-most-once, at-least-once, and exactly-once. In a typical Spark Streaming application, there’re three processing phases: receive data, do transformation, and push outputs. Each phase takes different efforts to achieve different semantics.</p>
<p>For <strong>receiving data</strong>, it largely depends on the data source. For instance, reading files from a fault-tolerant file system like HDFS, gives us exactly-once semantics. For upstream queues that support acknowledgement, e.g. RabbitMQ, we can combine it with Spark’s write ahead logs to achieve at-least-once semantics. For unreliable receivers like <code>socketTextStream</code>, there might be data loss due to worker/driver failure and gives us undefined semantics. Kafka, on the other hand, is offset based, and its direct API can give us exactly-once semantics.</p>
<p>When <strong>transforming data</strong> with Spark’s RDD, we automatically get exactly-once semantics, for RDD is itself immutable, fault-tolerant and deterministically re-computable. As long as the source data is available, and there’s no side effects during transformation, the result will always be the same.</p>
<p><strong>Output operation</strong> by default has at-least-once semantics. The <code>foreachRDD</code> function will execute more than once if there’s worker failure, thus writing same data to external storage multiple times. There’re two approaches to solve this issue, idempotent updates, and transactional updates. They are further discussed in the following sections.</p>
<h2 id="Exactly-once-with-Idempotent-Writes"><a href="#Exactly-once-with-Idempotent-Writes" class="headerlink" title="Exactly-once with Idempotent Writes"></a>Exactly-once with Idempotent Writes</h2><p>If multiple writes produce the same data, then this output operation is idempotent. <code>saveAsTextFile</code> is a typical idempotent update; messages with unique keys can be written to database without duplication. This approach will give us the equivalent exactly-once semantics. Note though it’s usually for map-only procedures, and it requires some setup on Kafka DStream.</p>
<ul>
<li>Set <code>enable.auto.commit</code> to <code>false</code>. By default, Kafka DStream will commit the consumer offsets right after it receives the data. We want to postpone this action unitl the batch is fully processed.</li>
<li>Turn on Spark Streaming’s checkpointing to store Kafka offsets. But if the application code changes, checkpointed data is not reusable. This leads to a second option:</li>
<li>Commit Kafka offsets after outputs. Kafka provides a <code>commitAsync</code> API, and the <code>HasOffsetRanges</code> class can be used to extract offsets from the initial RDD:</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">messages.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</div><div class="line">  rdd.foreachPartition &#123; iter =&gt;</div><div class="line">    <span class="comment">// output to database</span></div><div class="line">  &#125;</div><div class="line">  messages.asInstanceOf[<span class="type">CanCommitOffsets</span>].commitAsync(offsetRanges)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Exactly-once-with-Transactional-Writes"><a href="#Exactly-once-with-Transactional-Writes" class="headerlink" title="Exactly-once with Transactional Writes"></a>Exactly-once with Transactional Writes</h2><p>Transactional updates require a unique identifier. One can generate from batch time, partition id, or Kafka offsets, and then write the result along with the identifier into external storage within a single transaction. This atomic operation gives us exactly-once semantics, and can be applied to both map-only and aggregation procedures.</p>
<p>Usually writing to database should happen in <code>foreachPartition</code>, i.e. in worker nodes. It is true for map-only procedure, because Kafka RDD’s partition is correspondent to Kafka partition, so we can extract each partition’s offset like this:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">messages.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</div><div class="line">  rdd.foreachPartition &#123; iter =&gt;</div><div class="line">    <span class="keyword">val</span> offsetRange = offsetRanges(<span class="type">TaskContext</span>.get.partitionId)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>But for shuffled operations like the error log count example, we need to first collect the result back into driver and then perform the transaction.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">messages.foreachRDD &#123; rdd =&gt;</div><div class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</div><div class="line">  <span class="keyword">val</span> result = processLogs(rdd).collect() <span class="comment">// parse log and count error</span></div><div class="line">  <span class="type">DB</span>.localTx &#123; <span class="keyword">implicit</span> session =&gt;</div><div class="line">    result.foreach &#123; <span class="keyword">case</span> (time, count) =&gt;</div><div class="line">      <span class="comment">// save to error_log table</span></div><div class="line">    &#125;</div><div class="line">    offsetRanges.foreach &#123; offsetRange =&gt;</div><div class="line">      <span class="keyword">val</span> affectedRows = <span class="string">sql""</span><span class="string">"</span></div><div class="line"><span class="string">      update kafka_offset set offset = $&#123;offsetRange.untilOffset&#125;</span></div><div class="line"><span class="string">      where topic = $&#123;topic&#125; and `partition` = $&#123;offsetRange.partition&#125;</span></div><div class="line"><span class="string">      and offset = $&#123;offsetRange.fromOffset&#125;</span></div><div class="line"><span class="string">      "</span><span class="string">""</span>.update.apply()</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (affectedRows != <span class="number">1</span>) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"fail to update offset"</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>If the offsets fail to update, or there’s a duplicate offset range detected by <code>offset != $fromOffset</code>, the whole transaction will rollback, which guarantees the exactly-once semantics.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Exactly-once is a very strong semantics in stream processing, and will inevitably bring some overhead to your application and impact the throughput. It’s also not applicable to <a href="https://github.com/koeninger/kafka-exactly-once/blob/master/src/main/scala/example/Windowed.scala" target="_blank" rel="external">windowed</a> operations. So you need to decide whether it’s necessary to spend such efforts, or weaker semantics even with few data loss will suffice. But surely knowing how to achieve exactly-once is a good chance of learning, and it’s a great fun.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/</a></li>
<li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html</a></li>
<li><a href="http://kafka.apache.org/documentation.html#semantics" target="_blank" rel="external">http://kafka.apache.org/documentation.html#semantics</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Exactly-once semantics is one of the advanced topics of stream processing. To process every message once and only once, in spite of system or network failure, not only the stream processing framework needs to provide such functionality, but also the message delivery system, the output data store, as well as how we implement the processing procedure, altogether can we ensure the exactly-once semantics. In this article, I’ll demonstrate how to use Spark Streaming, with Kafka as data source and MySQL the output storage, to achieve exactly-once stream processing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://spark.apache.org/docs/latest/img/streaming-arch.png&quot; alt=&quot;Spark Streaming&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;An-Introductory-Example&quot;&gt;&lt;a href=&quot;#An-Introductory-Example&quot; class=&quot;headerlink&quot; title=&quot;An Introductory Example&quot;&gt;&lt;/a&gt;An Introductory Example&lt;/h2&gt;&lt;p&gt;First let’s implement a simple yet complete stream processing application that receive access logs from Kafka, parse and count the errors, then write the errors per minute metric into MySQL database.&lt;/p&gt;
&lt;p&gt;Sample access logs:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;2017-07-30 14:09:08 ERROR some message&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017-07-30 14:09:20 INFO  some message&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2017-07-30 14:10:50 ERROR some message&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Output table, where &lt;code&gt;log_time&lt;/code&gt; should be truncated to minutes:&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;table&lt;/span&gt; error_log (&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  log_time datetime primary &lt;span class=&quot;keyword&quot;&gt;key&lt;/span&gt;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  log_count &lt;span class=&quot;built_in&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;null&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;);&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="scala" scheme="http://shzhangji.com/tags/scala/"/>
    
      <category term="spark" scheme="http://shzhangji.com/tags/spark/"/>
    
      <category term="spark streaming" scheme="http://shzhangji.com/tags/spark-streaming/"/>
    
      <category term="kafka" scheme="http://shzhangji.com/tags/kafka/"/>
    
      <category term="stream processing" scheme="http://shzhangji.com/tags/stream-processing/"/>
    
  </entry>
  
  <entry>
    <title>Learn Pandas from a SQL Perspective</title>
    <link href="http://shzhangji.com/blog/2017/07/23/learn-pandas-from-a-sql-perspective/"/>
    <id>http://shzhangji.com/blog/2017/07/23/learn-pandas-from-a-sql-perspective/</id>
    <published>2017-07-23T12:02:50.000Z</published>
    <updated>2017-07-26T01:21:10.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://pandas.pydata.org/" target="_blank" rel="external">Pandas</a> is a widely used data processing tool for Python. Along with NumPy and Matplotlib, it provides in-memory high-performance data munging, analyzing, and visualization capabilities. Although Python is an easy-to-learn programming language, it still takes time to learn Pandas APIs and the idiomatic usages. For data engineer and analysts, SQL is the de-facto standard language of data queries. This article will provide examples of how some common SQL queries can be rewritten with Pandas.</p>
<p>The installation and basic concepts of Pandas is not covered in this post. One can check out the offical documentation, or read the book <a href="https://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1491957662/" target="_blank" rel="external">Python for Data Analysis</a>. And I recommend using the <a href="https://www.continuum.io/downloads" target="_blank" rel="external">Anaconda</a> Python distribution, with <a href="https://pythonhosted.org/spyder/" target="_blank" rel="external">Spyder</a> IDE included. Before diving into the codes, please import Pandas and NumPy as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure>
<h2 id="FROM-Load-Data-into-Memory"><a href="#FROM-Load-Data-into-Memory" class="headerlink" title="FROM - Load Data into Memory"></a><code>FROM</code> - Load Data into Memory</h2><p>First of all, let’s read some data into the workspace (memory). Pandas supports a variety of formats, one of them is CSV. Take the following flight delay dataset for example (<a href="/uploads/flights.csv">link</a>):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">date,delay,distance,origin,destination</div><div class="line">02221605,3,358,BUR,SMF</div><div class="line">01022100,-5,239,HOU,DAL</div><div class="line">03210808,6,288,BWI,ALB</div></pre></td></tr></table></figure>
<p>We can use <code>pd.read_csv</code> to load this file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df = pd.read_csv(<span class="string">'flights.csv'</span>, dtype=&#123;<span class="string">'date'</span>: str&#125;)</div><div class="line">df.head()</div></pre></td></tr></table></figure>
<p>This statement will load <code>flights.csv</code> file into memory, use first line as column names, and try to figure out each column’s type. Since the <code>date</code> column is in <code>%m%d%H%M</code> format, we don’t want to lose the initial <code>0</code> in month, so we pass an explict <code>dtype</code> for it, indicating that this column should stay unparsed.</p>
<a id="more"></a>
<p> <code>df.head</code> is a function to peek the dataset. It accepts a single parameter to limit the rows, much like <code>LIMIT</code> caluse. To perform a <code>LIMIT 10, 100</code>, use <code>df.iloc[10:100]</code>. Besides, IPython defaults to show only 60 rows, but we can increase this limit by:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pd.options.display.max_rows = <span class="number">100</span></div><div class="line">df.iloc[<span class="number">10</span>:<span class="number">100</span>]</div></pre></td></tr></table></figure>
<p>Another common loading technique is reading from database. Pandas also has built-in support:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">conn = pymysql.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>)</div><div class="line">df = pd.read_sql(<span class="string">"""</span></div><div class="line"><span class="string">select `date`, `delay`, `distance`, `origin`, `destination`</span></div><div class="line"><span class="string">from flights limit 1000</span></div><div class="line"><span class="string">"""</span>, conn)</div></pre></td></tr></table></figure>
<p>To save DataFrame into file or database, use <code>pd.to_csv</code> and <code>pd.to_sql</code> respectively.</p>
<h2 id="SELECT-Column-Projection"><a href="#SELECT-Column-Projection" class="headerlink" title="SELECT - Column Projection"></a><code>SELECT</code> - Column Projection</h2><p>The <code>SELECT</code> clause in SQL is used to perform column projection and data transformation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">df[<span class="string">'date'</span>] <span class="comment"># SELECT `date`</span></div><div class="line">df[[<span class="string">'date'</span>, <span class="string">'delay'</span>]] <span class="comment"># SELECT `date`, `delay`</span></div><div class="line">df.loc[<span class="number">10</span>:<span class="number">100</span>, [<span class="string">'date'</span>, <span class="string">'delay'</span>]] <span class="comment"># SELECT `date, `delay` LIMIT 10, 100</span></div></pre></td></tr></table></figure>
<p>SQL provides various functions to transform data, most of them can be replaced by Pandas, or you can simply write one with Python. Here I’ll choose some commonly used functions to illustrate.</p>
<h3 id="String-Functions"><a href="#String-Functions" class="headerlink" title="String Functions"></a>String Functions</h3><p>Pandas string functions can be invoked by DataFrame and Series’ <code>str</code> attribute, e.g. <code>df[&#39;origin&#39;].str.lower()</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT CONCAT(origin, ' to ', destination)</span></div><div class="line">df[<span class="string">'origin'</span>].str.cat(df[<span class="string">'destination'</span>], sep=<span class="string">' to '</span>)</div><div class="line"></div><div class="line">df[<span class="string">'origin'</span>].str.strip() <span class="comment"># TRIM(origin)</span></div><div class="line">df[<span class="string">'origin'</span>].str.len() <span class="comment"># LENGTH(origin)</span></div><div class="line">df[<span class="string">'origin'</span>].str.replace(<span class="string">'a'</span>, <span class="string">'b'</span>) <span class="comment"># REPLACE(origin, 'a', 'b')</span></div><div class="line"></div><div class="line"><span class="comment"># SELECT SUBSTRING(origin, 1, 1)</span></div><div class="line">df[<span class="string">'origin'</span>].str[<span class="number">0</span>:<span class="number">1</span>] <span class="comment"># use Python string indexing</span></div><div class="line"></div><div class="line"><span class="comment"># SELECT SUBSTRING_INDEX(domain, '.', 2)</span></div><div class="line"><span class="comment"># www.example.com -&gt; www.example</span></div><div class="line">df[<span class="string">'domain'</span>].str.split(<span class="string">'.'</span>).str[:<span class="number">2</span>].str.join(<span class="string">'.'</span>)</div><div class="line">df[<span class="string">'domain'</span>].str.extract(<span class="string">r'^([^.]+\.[^.]+)'</span>)</div></pre></td></tr></table></figure>
<p>Pandas also has a feature called broadcast behaviour, i.e. perform operations between lower dimensional data (or scalar value) with higher dimensional data. For instances:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">df[<span class="string">'full_date'</span>] = <span class="string">'2001'</span> + df[<span class="string">'date'</span>] <span class="comment"># CONCAT('2001', `date`)</span></div><div class="line">df[<span class="string">'delay'</span>] / <span class="number">60</span></div><div class="line">df[<span class="string">'delay'</span>].div(<span class="number">60</span>) <span class="comment"># same as above</span></div></pre></td></tr></table></figure>
<p>There’re many other string functions that Pandas support out-of-the-box, and they are quite different, thus more powerful than SQL. For a complete list please check the <a href="https://pandas.pydata.org/pandas-docs/stable/text.html" target="_blank" rel="external">Working with Text Data</a> doc.</p>
<h3 id="Date-Functions"><a href="#Date-Functions" class="headerlink" title="Date Functions"></a>Date Functions</h3><p><code>pd.to_datetime</code> is used to convert various datetime representations to the standard <code>datetime64</code> dtype. <code>dt</code> is a property of datetime/period like Series, from which you can extract information about date and time. Full documentation can be found in <a href="https://pandas.pydata.org/pandas-docs/stable/timeseries.html" target="_blank" rel="external">Time Series / Date functionality</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT STR_TO_DATE(full_date, '%Y%m%d%H%i%s') AS `datetime`</span></div><div class="line">df[<span class="string">'datetime'</span>] = pd.to_datetime(df[<span class="string">'full_date'</span>], format=<span class="string">'%Y%m%d%H%M%S'</span>)</div><div class="line"></div><div class="line"><span class="comment"># SELECT DATE_FORMAT(`datetime`, '%Y-%m-%d')</span></div><div class="line">df[<span class="string">'datetime'</span>].dt.strftime(<span class="string">'%Y-%m-%d'</span>)</div><div class="line"></div><div class="line">df[<span class="string">'datetime'</span>].dt.month <span class="comment"># MONTH(`datetime`)</span></div><div class="line">df[<span class="string">'datetime'</span>].dt.hour <span class="comment"># HOUR(`datetime`)</span></div><div class="line"></div><div class="line"><span class="comment"># SELECT UNIX_TIMESTAMP(`datetime`)</span></div><div class="line">df[<span class="string">'datetime'</span>].view(<span class="string">'int64'</span>) // pd.Timedelta(<span class="number">1</span>, unit=<span class="string">'s'</span>).value</div><div class="line"></div><div class="line"><span class="comment"># SELECT FROM_UNIXTIME(`timestamp`)</span></div><div class="line">pd.to_datetime(df[<span class="string">'timestamp'</span>], unit=<span class="string">'s'</span>)</div><div class="line"></div><div class="line"><span class="comment"># SELECT `datetime` + INTERVAL 1 DAY</span></div><div class="line">df[<span class="string">'datetime'</span>] + pd.Timedelta(<span class="number">1</span>, unit=<span class="string">'D'</span>)</div></pre></td></tr></table></figure>
<h2 id="WHERE-Row-Selection"><a href="#WHERE-Row-Selection" class="headerlink" title="WHERE - Row Selection"></a><code>WHERE</code> - Row Selection</h2><p>For logic operators, Pandas will result in a boolean typed Series, which can be used to filter out rows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">(df[<span class="string">'delay'</span>] &gt; <span class="number">0</span>).head()</div><div class="line"><span class="comment"># 0  True</span></div><div class="line"><span class="comment"># 1 False</span></div><div class="line"><span class="comment"># 2  True</span></div><div class="line"><span class="comment"># dtype: bool</span></div><div class="line"></div><div class="line"><span class="comment"># WHERE delay &gt; 0</span></div><div class="line">df[df[<span class="string">'delay'</span>] &gt; <span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>We can combine multiple conditions with bitwise operators:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># WHERE delay &gt; 0 AND distance &lt;= 500</span></div><div class="line">df[(df[<span class="string">'delay'</span>] &gt; <span class="number">0</span>) &amp; (df[<span class="string">'distance'</span>] &lt;= <span class="number">500</span>)]</div><div class="line"></div><div class="line"><span class="comment"># WHERE delay &gt; 0 OR origin = 'BUR'</span></div><div class="line">df[(df[<span class="string">'delay'</span>] &gt; <span class="number">0</span>) | (df[<span class="string">'origin'</span>] == <span class="string">'BUR'</span>)]</div><div class="line"></div><div class="line"><span class="comment"># WHERE NOT (delay &gt; 0)</span></div><div class="line">df[~(df[<span class="string">'delay'</span>] &gt; <span class="number">0</span>)]</div></pre></td></tr></table></figure>
<p>For <code>IS NULL</code> and <code>IS NOT NULL</code>, we can use the built-in functions:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df[df[<span class="string">'delay'</span>].isnull()] <span class="comment"># delay IS NULL</span></div><div class="line">df[df[<span class="string">'delay'</span>].notnull()] <span class="comment"># delay IS NOT NUL</span></div></pre></td></tr></table></figure>
<p>There’s also a <code>df.query</code> method to write filters as string expression:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df.query(<span class="string">'delay &gt; 0 and distaince &lt;= 500'</span>)</div><div class="line">df.query(<span class="string">'(delay &gt; 0) | (origin == "BUR")'</span>)</div></pre></td></tr></table></figure>
<p>Actually, Pandas provides more powerful functionalities for <a href="https://pandas.pydata.org/pandas-docs/stable/indexing.html" target="_blank" rel="external">Indexing and Selecting Data</a>, and some of them cannot be expressed by SQL. You can find more usages in the docs.</p>
<h2 id="GROUP-BY-Aggregation"><a href="#GROUP-BY-Aggregation" class="headerlink" title="GROUP BY - Aggregation"></a><code>GROUP BY</code> - Aggregation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT origin, COUNT(*) FROM flights GROUP BY origin</span></div><div class="line">df.groupby(<span class="string">'origin'</span>).size()</div><div class="line"><span class="comment"># origin</span></div><div class="line"><span class="comment"># ABQ    22</span></div><div class="line"><span class="comment"># ALB     4</span></div><div class="line"><span class="comment"># AMA     4</span></div><div class="line"><span class="comment"># dtype: int64</span></div></pre></td></tr></table></figure>
<p>There’re two parts in an aggregation statement, the columns to group by and the aggregation function. It’s possible to pass multiple columns to <code>df.groupby</code>, as well as multiple aggregators.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT origin, destination, SUM(delay), AVG(distance)</span></div><div class="line"><span class="comment"># GROUP BY origin, destination</span></div><div class="line">df.groupby([<span class="string">'origin'</span>, <span class="string">'destination'</span>]).agg(&#123;</div><div class="line">    <span class="string">'delay'</span>: np.sum,</div><div class="line">    <span class="string">'distance'</span>: np.mean</div><div class="line">&#125;)</div><div class="line"></div><div class="line"><span class="comment"># SELECT origin, MIN(delay), MAX(delay) GROUP BY origin</span></div><div class="line">df.groupby(<span class="string">'origin'</span>)[<span class="string">'delay'</span>].agg([<span class="string">'min'</span>, <span class="string">'max'</span>])</div></pre></td></tr></table></figure>
<p>We can also group by a function result. More usages can be found in <a href="https://pandas.pydata.org/pandas-docs/stable/groupby.html" target="_blank" rel="external">Group By: split-apply-combine</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT LENGTH(origin), COUNT(*) GROUP BY LENGTH(origin)</span></div><div class="line">df.set_index(<span class="string">'origin'</span>).groupby(len).size()</div></pre></td></tr></table></figure>
<h2 id="ORDER-BY-Sorting-Rows"><a href="#ORDER-BY-Sorting-Rows" class="headerlink" title="ORDER BY - Sorting Rows"></a><code>ORDER BY</code> - Sorting Rows</h2><p>There’re two types of sort, by index and by values. If you are not familiar with the concept index, please refer to Pandas tutorials.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># ORDER BY origin</span></div><div class="line">df.set_index(<span class="string">'origin'</span>).sort_index()</div><div class="line">df.sort_values(by=<span class="string">'origin'</span>)</div><div class="line"></div><div class="line"><span class="comment"># ORDER BY origin ASC, destination DESC</span></div><div class="line">df.sort_values(by=[<span class="string">'origin'</span>, <span class="string">'destination'</span>], ascending=[<span class="keyword">True</span>, <span class="keyword">False</span>])</div></pre></td></tr></table></figure>
<h2 id="JOIN-Merge-DateFrames"><a href="#JOIN-Merge-DateFrames" class="headerlink" title="JOIN - Merge DateFrames"></a><code>JOIN</code> - Merge DateFrames</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># FROM product a LEFT JOIN category b ON a.cid = b.id</span></div><div class="line">pd.merge(df_product, df_category, left_on=<span class="string">'cid'</span>, right_on=<span class="string">'id'</span>, how=<span class="string">'left'</span>)</div></pre></td></tr></table></figure>
<p>If join key is the same, we can use <code>on=[&#39;k1&#39;, &#39;k2&#39;]</code>. The default join method (<code>how</code>) is inner join. Other options are <code>left</code> for left join, <code>right</code> outer join, and <code>outer</code> for full outer join.</p>
<p><code>pd.concat</code> can be used to perform <code>UNION</code>. More usages can be found in <a href="https://pandas.pydata.org/pandas-docs/stable/merging.html" target="_blank" rel="external">Merge, join, and concatenate</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># SELECT * FROM a UNION SELECT * FROM b</span></div><div class="line">pd.concat([df_a, df_b]).drop_duplicates()</div></pre></td></tr></table></figure>
<h1 id="Rank-Within-Groups"><a href="#Rank-Within-Groups" class="headerlink" title="Rank Within Groups"></a>Rank Within Groups</h1><p>Last but not least, it’s common to select top n items within each groups. In MySQL, we have to use variables. In Pandas, we can use the <code>rank</code> function on grouped DataFrame:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">rnk = df.groupby(<span class="string">'origin'</span>)[<span class="string">'delay'</span>].rank(method=<span class="string">'first'</span>, ascending=<span class="keyword">False</span>)</div><div class="line">df.assign(rnk=rnk).query(<span class="string">'rnk &lt;= 3'</span>).sort_values([<span class="string">'origin'</span>, <span class="string">'rnk'</span>])</div></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html" target="_blank" rel="external">https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html</a></li>
<li><a href="http://www.gregreda.com/2013/01/23/translating-sql-to-pandas-part1/" target="_blank" rel="external">http://www.gregreda.com/2013/01/23/translating-sql-to-pandas-part1/</a></li>
<li><a href="http://codingsight.com/pivot-tables-in-mysql/" target="_blank" rel="external">http://codingsight.com/pivot-tables-in-mysql/</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://pandas.pydata.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Pandas&lt;/a&gt; is a widely used data processing tool for Python. Along with NumPy and Matplotlib, it provides in-memory high-performance data munging, analyzing, and visualization capabilities. Although Python is an easy-to-learn programming language, it still takes time to learn Pandas APIs and the idiomatic usages. For data engineer and analysts, SQL is the de-facto standard language of data queries. This article will provide examples of how some common SQL queries can be rewritten with Pandas.&lt;/p&gt;
&lt;p&gt;The installation and basic concepts of Pandas is not covered in this post. One can check out the offical documentation, or read the book &lt;a href=&quot;https://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1491957662/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Python for Data Analysis&lt;/a&gt;. And I recommend using the &lt;a href=&quot;https://www.continuum.io/downloads&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Anaconda&lt;/a&gt; Python distribution, with &lt;a href=&quot;https://pythonhosted.org/spyder/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Spyder&lt;/a&gt; IDE included. Before diving into the codes, please import Pandas and NumPy as follows:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;FROM-Load-Data-into-Memory&quot;&gt;&lt;a href=&quot;#FROM-Load-Data-into-Memory&quot; class=&quot;headerlink&quot; title=&quot;FROM - Load Data into Memory&quot;&gt;&lt;/a&gt;&lt;code&gt;FROM&lt;/code&gt; - Load Data into Memory&lt;/h2&gt;&lt;p&gt;First of all, let’s read some data into the workspace (memory). Pandas supports a variety of formats, one of them is CSV. Take the following flight delay dataset for example (&lt;a href=&quot;/uploads/flights.csv&quot;&gt;link&lt;/a&gt;):&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;date,delay,distance,origin,destination&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;02221605,3,358,BUR,SMF&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;01022100,-5,239,HOU,DAL&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;03210808,6,288,BWI,ALB&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;We can use &lt;code&gt;pd.read_csv&lt;/code&gt; to load this file:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;df = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;flights.csv&#39;&lt;/span&gt;, dtype=&amp;#123;&lt;span class=&quot;string&quot;&gt;&#39;date&#39;&lt;/span&gt;: str&amp;#125;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;df.head()&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;This statement will load &lt;code&gt;flights.csv&lt;/code&gt; file into memory, use first line as column names, and try to figure out each column’s type. Since the &lt;code&gt;date&lt;/code&gt; column is in &lt;code&gt;%m%d%H%M&lt;/code&gt; format, we don’t want to lose the initial &lt;code&gt;0&lt;/code&gt; in month, so we pass an explict &lt;code&gt;dtype&lt;/code&gt; for it, indicating that this column should stay unparsed.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
      <category term="analytics" scheme="http://shzhangji.com/tags/analytics/"/>
    
      <category term="pandas" scheme="http://shzhangji.com/tags/pandas/"/>
    
      <category term="sql" scheme="http://shzhangji.com/tags/sql/"/>
    
  </entry>
  
  <entry>
    <title>Log Tailer with WebSocket and Python</title>
    <link href="http://shzhangji.com/blog/2017/07/15/log-tailer-with-websocket-and-python/"/>
    <id>http://shzhangji.com/blog/2017/07/15/log-tailer-with-websocket-and-python/</id>
    <published>2017-07-15T11:21:03.000Z</published>
    <updated>2017-07-15T11:22:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>Tailing a log file is a common task when we deploy or maintain some software in production. Instead of logging into the server and <code>tail -f</code>, it would be nice if we can tail a log file in the browser. With WebSocket, this can be done easily. In this article, I’ll walk you through a simple <strong>logviewer</strong> (<a href="http://github.com/jizhang/logviewer" target="_blank" rel="external">source</a>) utility that is written in Python.</p>
<p><img src="/images/logviewer-websocket.png" alt="Logviewer with WebSocket"></p>
<h2 id="WebSocket-Intro"><a href="#WebSocket-Intro" class="headerlink" title="WebSocket Intro"></a>WebSocket Intro</h2><p>WebSocket is standard protocol over TCP, that provides full-duplex communication between client and server side, usually a browser and a web server. Before WebSocket, when we want to keep an alive browser-server connection, we choose from long polling, forever frame or Comet techniques. Now that WebSocket is widely supported by major browsers, we can use it to implement web chatroom, games, realtime dashboard, etc. Besides, WebSocket connection can be established by an HTTP upgrade request, and communicate over 80 port, so as to bring minimum impact on existing network facility.</p>
<a id="more"></a>
<h2 id="Python’s-websockets-Package"><a href="#Python’s-websockets-Package" class="headerlink" title="Python’s websockets Package"></a>Python’s <code>websockets</code> Package</h2><p><code>websockets</code> is a Python package that utilize Python’s <code>asyncio</code> to develop WebSocket servers and clients. The package can be installed via <code>pip</code>, and it requires Python 3.3+.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">pip install websockets</div><div class="line"><span class="comment"># For Python 3.3</span></div><div class="line">pip install asyncio</div></pre></td></tr></table></figure>
<p>Following is a simple Echo server:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> asyncio</div><div class="line"><span class="keyword">import</span> websockets</div><div class="line"></div><div class="line"><span class="meta">@asyncio.coroutine</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">echo</span><span class="params">(websocket, path)</span>:</span></div><div class="line">    message = <span class="keyword">yield</span> <span class="keyword">from</span> websocket.recv()</div><div class="line">    print(<span class="string">'recv'</span>, message)</div><div class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> websocket.send(message)</div><div class="line"></div><div class="line">start_server = websockets.serve(echo, <span class="string">'localhost'</span>, <span class="number">8765</span>)</div><div class="line"></div><div class="line">asyncio.get_event_loop().run_until_complete(start_server)</div><div class="line">asyncio.get_event_loop().run_forever()</div></pre></td></tr></table></figure>
<p>Here we use Python’s coroutines to handle client requests. Coroutine enables single-threaded application to run concurrent codes, such as handling socket I/O. Note that Python 3.5 introduced two new keywords for coroutine, <code>async</code> and <code>await</code>, so the Echo server can be rewritten as:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">echo</span><span class="params">(websocket, path)</span>:</span></div><div class="line">    message = <span class="keyword">await</span> websocket.recv()</div><div class="line">    <span class="keyword">await</span> websocket.send(message)</div></pre></td></tr></table></figure>
<p>For client side, we use the built-in <code>WebSocket</code> class. You can simply paste the following code into Chrome’s JavaScript console:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> ws = <span class="keyword">new</span> WebSocket(<span class="string">'ws://localhost:8765'</span>)</div><div class="line">ws.onmessage = <span class="function">(<span class="params">event</span>) =&gt;</span> &#123;</div><div class="line">  <span class="built_in">console</span>.log(event.data)</div><div class="line">&#125;</div><div class="line">ws.onopen = <span class="function"><span class="params">()</span> =&gt;</span> &#123;</div><div class="line">  ws.send(<span class="string">'hello'</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Tail-a-Log-File"><a href="#Tail-a-Log-File" class="headerlink" title="Tail a Log File"></a>Tail a Log File</h2><p>We’ll take the following steps to implement a log viewer:</p>
<ul>
<li>Client opens a WebSocket connection, and puts the file path in the url, like <code>ws://localhost:8765/tmp/build.log?tail=1</code>;</li>
<li>Server parses the file path, along with a flag that indicates whether this is a view once or tail request;</li>
<li>Open file and start sending contents within a for loop.</li>
</ul>
<p>Full code can be found on <a href="https://github.com/jizhang/logviewer" target="_blank" rel="external">GitHub</a>, so here I’ll select some important parts:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@asyncio.coroutine</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">view_log</span><span class="params">(websocket, path)</span>:</span></div><div class="line">    parse_result = urllib.parse.urlparse(path)</div><div class="line">    file_path = os.path.abspath(parse_result.path)</div><div class="line">    query = urllib.parse.parse_qs(parse_result.query)</div><div class="line">    tail = query <span class="keyword">and</span> query[<span class="string">'tail'</span>] <span class="keyword">and</span> query[<span class="string">'tail'</span>][<span class="number">0</span>] == <span class="string">'1'</span></div><div class="line">    <span class="keyword">with</span> open(file_path) <span class="keyword">as</span> f:</div><div class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> websocket.send(f.read())</div><div class="line">        <span class="keyword">if</span> tail:</div><div class="line">            <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">                content = f.read()</div><div class="line">                <span class="keyword">if</span> content:</div><div class="line">                    <span class="keyword">yield</span> <span class="keyword">from</span> websocket.send(content)</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.sleep(<span class="number">1</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> websocket.close()</div></pre></td></tr></table></figure>
<h2 id="Miscellaneous"><a href="#Miscellaneous" class="headerlink" title="Miscellaneous"></a>Miscellaneous</h2><ul>
<li>Sometimes the client browser will not close the connection properly, so it’s necessary to add some heartbeat mechanism. For instance:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> time.time() - last_heartbeat &gt; HEARTBEAT_INTERVAL:</div><div class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> websocket.send(<span class="string">'ping'</span>)</div><div class="line">    pong = <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.wait_for(websocket.recv(), <span class="number">5</span>)</div><div class="line">    <span class="keyword">if</span> pong != <span class="string">'pong'</span>:</div><div class="line">        <span class="keyword">raise</span> Exception(<span class="string">'Ping error'</span>))</div><div class="line">    last_heartbeat = time.time()</div></pre></td></tr></table></figure>
<ul>
<li>Log files may contain ANSI color codes (e.g. logging level). We can use <code>ansi2html</code> package to convert them into HTML:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> ansi2html <span class="keyword">import</span> Ansi2HTMLConverter</div><div class="line">conv = Ansi2HTMLConverter(inline=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">yield</span> <span class="keyword">from</span> websocket.send(conv.convert(content, full=<span class="keyword">False</span>))</div></pre></td></tr></table></figure>
<ul>
<li>It’s also necessary to do some permission checks on the file path. For example, convert to absolute path and do a simple prefix check.</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/WebSocket" target="_blank" rel="external">WebSocket - Wikipedia</a></li>
<li><a href="https://websockets.readthedocs.io/en/stable/intro.html" target="_blank" rel="external">websockets - Get Started</a></li>
<li><a href="https://docs.python.org/3/library/asyncio-task.html" target="_blank" rel="external">Tasks and coroutines</a></li>
<li><a href="https://stackoverflow.com/questions/12523044/how-can-i-tail-a-log-file-in-python" target="_blank" rel="external">How can I tail a log file in Python?</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Tailing a log file is a common task when we deploy or maintain some software in production. Instead of logging into the server and &lt;code&gt;tail -f&lt;/code&gt;, it would be nice if we can tail a log file in the browser. With WebSocket, this can be done easily. In this article, I’ll walk you through a simple &lt;strong&gt;logviewer&lt;/strong&gt; (&lt;a href=&quot;http://github.com/jizhang/logviewer&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;source&lt;/a&gt;) utility that is written in Python.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/logviewer-websocket.png&quot; alt=&quot;Logviewer with WebSocket&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;WebSocket-Intro&quot;&gt;&lt;a href=&quot;#WebSocket-Intro&quot; class=&quot;headerlink&quot; title=&quot;WebSocket Intro&quot;&gt;&lt;/a&gt;WebSocket Intro&lt;/h2&gt;&lt;p&gt;WebSocket is standard protocol over TCP, that provides full-duplex communication between client and server side, usually a browser and a web server. Before WebSocket, when we want to keep an alive browser-server connection, we choose from long polling, forever frame or Comet techniques. Now that WebSocket is widely supported by major browsers, we can use it to implement web chatroom, games, realtime dashboard, etc. Besides, WebSocket connection can be established by an HTTP upgrade request, and communicate over 80 port, so as to bring minimum impact on existing network facility.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
      <category term="websocket" scheme="http://shzhangji.com/tags/websocket/"/>
    
      <category term="ops" scheme="http://shzhangji.com/tags/ops/"/>
    
  </entry>
  
  <entry>
    <title>Build Interactive Report with Crossfilter and dc.js</title>
    <link href="http://shzhangji.com/blog/2017/06/18/build-interactive-report-with-crossfilter-and-dc-js/"/>
    <id>http://shzhangji.com/blog/2017/06/18/build-interactive-report-with-crossfilter-and-dc-js/</id>
    <published>2017-06-18T08:38:01.000Z</published>
    <updated>2017-06-19T01:06:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>When visualizing multidimensional datasets, we often want to connect individual charts together, so that one chart’s filter will apply to all the other charts. We can do it manually, filter data on the server side, and update the rendered charts. Or we can filter data on the client side, and let charts update themselves. With Crossfilter and dc.js, this work becomes simple and intuitive.</p>
<h2 id="Airline-On-time-Performance"><a href="#Airline-On-time-Performance" class="headerlink" title="Airline On-time Performance"></a>Airline On-time Performance</h2><p>Here’s an example taken from Crossfilter’s official website. It’s a flight delay analysis report based on <a href="http://stat-computing.org/dataexpo/2009/" target="_blank" rel="external">ASA Data Expo</a> dataset. And this post will introduce how to use dc.js to build the report. A runnable JSFiddle can be found <a href="https://jsfiddle.net/zjerryj/gjao9sws/" target="_blank" rel="external">here</a>, though the dataset is reduced to 1,000 records.</p>
<p><img src="/images/airline-ontime-performance.png" alt=""></p>
<a id="more"></a>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p><a href="http://crossfilter.github.io/crossfilter/" target="_blank" rel="external">Crossfilter</a> is a JavaScript library to do multidimensional queries on large amount of data in the client’s browser. It can <strong>cross-filter</strong> between different group-by queries, so that query results will be connected and updated automatically. With the help of <a href="https://dc-js.github.io/dc.js/" target="_blank" rel="external">dc.js</a>, also a JavaScript library that provides charting capability, together we can develop high-performance, interactive reports.</p>
<h2 id="Dataset-Dimension-and-Measure"><a href="#Dataset-Dimension-and-Measure" class="headerlink" title="Dataset, Dimension, and Measure"></a>Dataset, Dimension, and Measure</h2><p>There’re several concepts in Crossfilter, namely dataset, dimension, measure. If you come from a data warehouse or analytics background, these are similar to the terms in OLAP Cube.</p>
<ul>
<li>Dataset, or a list of records, is a two dimensional table that contains rows and columns.</li>
<li>Dimension columns are used to do group-bys. They are either categorical, like dates, gender, or represents a range of values, like age range, etc.</li>
<li>Measure columns are used to do aggregations, such as sum, standard deviation, so they are mostly numeric. Examples are income, number of children.</li>
</ul>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> flights = d3.csv.parse(flightsCsv)</div><div class="line"><span class="keyword">let</span> flight = crossfilter(flights)</div><div class="line"><span class="keyword">let</span> hour = flight.dimension(<span class="function">(<span class="params">d</span>) =&gt;</span> d.date.getHours() + d.date.getMinutes() / <span class="number">60</span>)</div><div class="line"><span class="keyword">let</span> hours = hour.group(<span class="built_in">Math</span>.floor)</div></pre></td></tr></table></figure>
<p>Here we create a crossfilter object from a parsed csv data. And we define a dimension that is derived from <code>date</code> column, hour of day represented by float values. Then we group by its integer part. To query the top 3 hours that contains most delays:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hours.top(<span class="number">3</span>)</div><div class="line"><span class="comment">// output</span></div><div class="line">[</div><div class="line">  &#123; <span class="attr">key</span>: <span class="number">13</span>, <span class="attr">value</span>: <span class="number">72</span> &#125;,</div><div class="line">  &#123; <span class="attr">key</span>: <span class="number">20</span>, <span class="attr">value</span>: <span class="number">72</span> &#125;,</div><div class="line">  &#123; <span class="attr">key</span>:  <span class="number">8</span>, <span class="attr">value</span>: <span class="number">71</span> &#125;,</div><div class="line">]</div></pre></td></tr></table></figure>
<h2 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h2><p>Now we can plot the hour of delays in a bar chart:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> hourChart = dc.barChart(<span class="string">'#hour-chart'</span>)</div><div class="line">hourChart</div><div class="line">  .width(<span class="number">350</span>)</div><div class="line">  .height(<span class="number">150</span>)</div><div class="line">  .dimension(hour)</div><div class="line">  .group(hours)</div><div class="line">  .x(d3.scale.linear()</div><div class="line">    .domain([<span class="number">0</span>, <span class="number">24</span>])</div><div class="line">    .rangeRound([<span class="number">0</span>, <span class="number">10</span> * <span class="number">24</span>]))</div><div class="line">  .controlsUseVisibility(<span class="literal">true</span>)</div></pre></td></tr></table></figure>
<p>The corresponding HTML code:</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"hour-chart"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"title"</span>&gt;</span>Time of Day</div><div class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"reset"</span> <span class="attr">href</span>=<span class="string">"javascript:;"</span> <span class="attr">style</span>=<span class="string">"visibility: hidden;"</span>&gt;</span>reset<span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></div></pre></td></tr></table></figure>
<p>We can see that dc.js is highly integrated with crossfilter. We simply pass the dimension objects and do some setup for chart axes. In this example, x axis is hours of the day, and y axis is the count of delayed flights.</p>
<p>Note <code>class=&quot;reset&quot;</code> is used with <code>controlUseVisibility</code>, that provides a <code>reset</code> button. Try to drag on the chart to filter a range of data, and you’ll see how this button is used.</p>
<h2 id="Cross-Filtering"><a href="#Cross-Filtering" class="headerlink" title="Cross Filtering"></a>Cross Filtering</h2><p>We can create other charts, such as a hitogram of arrival delay in minutes. You can find the source code in JSFiddle. When you do some filtering (drag and select), the other charts will be updated simultaneously. It is great when you want to explore the distribution of data combined with filtering conditions. Just declare the relationship, and dc.js will do the rest for you.</p>
<p>There’re many other components like pie chart, table grid, or even customized HTML. But to master these tools, you also need some knowledge of d3.js.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="http://crossfilter.github.io/crossfilter/" target="_blank" rel="external">Crossfilter - Fast Multidimensional Filtering for Coordinated Views</a></li>
<li><a href="https://dc-js.github.io/dc.js/" target="_blank" rel="external">dc.js - Dimensional Charting Javascript Library</a></li>
<li><a href="http://blog.rusty.io/2012/09/17/crossfilter-tutorial/" target="_blank" rel="external">Crossfiler Tutorial</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When visualizing multidimensional datasets, we often want to connect individual charts together, so that one chart’s filter will apply to all the other charts. We can do it manually, filter data on the server side, and update the rendered charts. Or we can filter data on the client side, and let charts update themselves. With Crossfilter and dc.js, this work becomes simple and intuitive.&lt;/p&gt;
&lt;h2 id=&quot;Airline-On-time-Performance&quot;&gt;&lt;a href=&quot;#Airline-On-time-Performance&quot; class=&quot;headerlink&quot; title=&quot;Airline On-time Performance&quot;&gt;&lt;/a&gt;Airline On-time Performance&lt;/h2&gt;&lt;p&gt;Here’s an example taken from Crossfilter’s official website. It’s a flight delay analysis report based on &lt;a href=&quot;http://stat-computing.org/dataexpo/2009/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ASA Data Expo&lt;/a&gt; dataset. And this post will introduce how to use dc.js to build the report. A runnable JSFiddle can be found &lt;a href=&quot;https://jsfiddle.net/zjerryj/gjao9sws/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;, though the dataset is reduced to 1,000 records.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/airline-ontime-performance.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="crossfilter" scheme="http://shzhangji.com/tags/crossfilter/"/>
    
      <category term="dc.js" scheme="http://shzhangji.com/tags/dc-js/"/>
    
      <category term="analytics" scheme="http://shzhangji.com/tags/analytics/"/>
    
  </entry>
  
  <entry>
    <title>Why Use Lodash When ES6 Is Available</title>
    <link href="http://shzhangji.com/blog/2017/03/13/why-use-lodash-when-es6-is-available/"/>
    <id>http://shzhangji.com/blog/2017/03/13/why-use-lodash-when-es6-is-available/</id>
    <published>2017-03-13T14:39:01.000Z</published>
    <updated>2017-03-14T01:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://lodash.com/" target="_blank" rel="external">Lodash</a> is a well-known JavaScript utility library that makes it easy to manipulate arrays and objects, as well as functions, strings, etc. I myself enjoys its functional way to process collections, especially chaining and lazy evaluation. But as <a href="http://www.ecma-international.org/ecma-262/6.0/" target="_blank" rel="external">ECMAScript 2015 Standard</a> (ES6) becomes widely supported by major browsers, and <a href="https://babeljs.io/" target="_blank" rel="external">Babel</a>, the JavaScript compiler that transforms ES6 codes to ES5, plays a major role in today’s frontend development, it seems that most Lodash utilities can be replaced by ES6. But should we? In my opinion, Lodash will remain popular, for it still has lots of useful features that could improve the way of programming.</p>
<h2 id="map-and-Array-map-Are-Different"><a href="#map-and-Array-map-Are-Different" class="headerlink" title="_.map and Array#map Are Different"></a><code>_.map</code> and <code>Array#map</code> Are Different</h2><p><code>_.map</code>, <code>_.reduce</code>, <code>_.filter</code> and <code>_.forEach</code> are frequently used functions when processing collections, and ES6 provides direct support for them:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">_.map([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], (i) =&gt; i + <span class="number">1</span>)</div><div class="line">_.reduce([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], (sum, i) =&gt; sum + i, <span class="number">0</span>)</div><div class="line">_.filter([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], (i) =&gt; i &gt; <span class="number">1</span>)</div><div class="line">_.forEach([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], (i) =&gt; &#123; <span class="built_in">console</span>.log(i) &#125;)</div><div class="line"></div><div class="line"><span class="comment">// becomes</span></div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>].map(<span class="function">(<span class="params">i</span>) =&gt;</span> i + <span class="number">1</span>)</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>].reduce(<span class="function">(<span class="params">sum, i</span>) =&gt;</span> sum + i, <span class="number">0</span>)</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>].filter(<span class="function">(<span class="params">i</span>) =&gt;</span> i &gt; <span class="number">1</span>)</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>].forEach(<span class="function">(<span class="params">i</span>) =&gt;</span> &#123; <span class="built_in">console</span>.log(i) &#125;)</div></pre></td></tr></table></figure>
<p>But Lodash’s <code>_.map</code> is more powerful, in that it works on objects, has iteratee / predicate shorthands, lazy evaluation, guards against null parameter, and has better performance.</p>
<a id="more"></a>
<h3 id="Iterate-over-Objects"><a href="#Iterate-over-Objects" class="headerlink" title="Iterate over Objects"></a>Iterate over Objects</h3><p>To iterate over an object in ES6, there’re several approaches:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (<span class="keyword">let</span> key <span class="keyword">in</span> obj) &#123; <span class="built_in">console</span>.log(obj[key]) &#125;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">let</span> key <span class="keyword">of</span> <span class="built_in">Object</span>.keys(obj)) &#123; <span class="built_in">console</span>.log(obj[key]) &#125;</div><div class="line"><span class="built_in">Object</span>.keys(obj).forEach(<span class="function">(<span class="params">key</span>) =&gt;</span> &#123; <span class="built_in">console</span>.log(obj[key]) &#125;)</div></pre></td></tr></table></figure>
<p>With Lodash, there’s a unified <code>_.forEach</code>, for both array and object:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">_.forEach(obj, (value, key) =&gt; &#123; <span class="built_in">console</span>.log(value) &#125;)</div></pre></td></tr></table></figure>
<p>Although ES6 does <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map" target="_blank" rel="external">provide</a> <code>forEach</code> for the newly added <code>Map</code> type, it takes some effort to first convert an object into a <code>Map</code>:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// http://stackoverflow.com/a/36644532/1030720</span></div><div class="line"><span class="keyword">const</span> buildMap = <span class="function"><span class="params">o</span> =&gt;</span> <span class="built_in">Object</span>.keys(o).reduce(<span class="function">(<span class="params">m, k</span>) =&gt;</span> m.set(k, o[k]), <span class="keyword">new</span> <span class="built_in">Map</span>());</div></pre></td></tr></table></figure>
<h3 id="Iteratee-Predicate-Shorthands"><a href="#Iteratee-Predicate-Shorthands" class="headerlink" title="Iteratee / Predicate Shorthands"></a>Iteratee / Predicate Shorthands</h3><p>To extract some property from an array of objects:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> arr = [&#123; <span class="attr">n</span>: <span class="number">1</span> &#125;, &#123; <span class="attr">n</span>: <span class="number">2</span> &#125;]</div><div class="line"><span class="comment">// ES6</span></div><div class="line">arr.map(<span class="function">(<span class="params">obj</span>) =&gt;</span> obj.n)</div><div class="line"><span class="comment">// Lodash</span></div><div class="line">_.map(arr, <span class="string">'n'</span>)</div></pre></td></tr></table></figure>
<p>This can be more helpful when it comes to complex objects:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> arr = [</div><div class="line">  &#123; <span class="attr">a</span>: [ &#123; <span class="attr">n</span>: <span class="number">1</span> &#125; ]&#125;,</div><div class="line">  &#123; <span class="attr">b</span>: [ &#123; <span class="attr">n</span>: <span class="number">1</span> &#125; ]&#125;</div><div class="line">]</div><div class="line"><span class="comment">// ES6</span></div><div class="line">arr.map(<span class="function">(<span class="params">obj</span>) =&gt;</span> obj.a[<span class="number">0</span>].n) <span class="comment">// TypeError: property 'a' is not defined in arr[1]</span></div><div class="line"><span class="comment">// Lodash</span></div><div class="line">_.map(arr, <span class="string">'a[0].n'</span>) <span class="comment">// =&gt; [1, undefined]</span></div></pre></td></tr></table></figure>
<p>As we can see, Lodash not only provides conveniet shorthands, it also guards against undefined values. For <code>_.filter</code>, there’s also predicate shorthand. Here are some examples from Lodash documentation:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> users = [</div><div class="line">  &#123; <span class="string">'user'</span>: <span class="string">'barney'</span>, <span class="string">'age'</span>: <span class="number">36</span>, <span class="string">'active'</span>: <span class="literal">true</span> &#125;,</div><div class="line">  &#123; <span class="string">'user'</span>: <span class="string">'fred'</span>,   <span class="string">'age'</span>: <span class="number">40</span>, <span class="string">'active'</span>: <span class="literal">false</span> &#125;</div><div class="line">];</div><div class="line"><span class="comment">// ES6</span></div><div class="line">users.filter(<span class="function">(<span class="params">o</span>) =&gt;</span> o.active)</div><div class="line"><span class="comment">// Lodash</span></div><div class="line">_.filter(users, <span class="string">'active'</span>)</div><div class="line">_.filter(users, [<span class="string">'active'</span>, <span class="literal">true</span>])</div><div class="line">_.filter(users, &#123;<span class="string">'active'</span>: <span class="literal">true</span>, <span class="string">'age'</span>: <span class="number">36</span>&#125;)</div></pre></td></tr></table></figure>
<h3 id="Chain-and-Lazy-Evaluation"><a href="#Chain-and-Lazy-Evaluation" class="headerlink" title="Chain and Lazy Evaluation"></a>Chain and Lazy Evaluation</h3><p>Here comes the fun part. Processing collections with chaining, lazy evaluation, along with short, easy-to-test functions, is quite popular these days. Most Lodash functions regarding collections can be chained easily. The following is a wordcount example:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> lines = <span class="string">`</span></div><div class="line"><span class="string">an apple orange the grape</span></div><div class="line"><span class="string">banana an apple melon</span></div><div class="line"><span class="string">an orange banana apple</span></div><div class="line"><span class="string">`</span>.split(<span class="string">'\n'</span>)</div><div class="line"></div><div class="line">_.chain(lines)</div><div class="line">  .flatMap(<span class="function"><span class="params">line</span> =&gt;</span> line.split(<span class="regexp">/\s+/</span>))</div><div class="line">  .filter(<span class="function"><span class="params">word</span> =&gt;</span> word.length &gt; <span class="number">3</span>)</div><div class="line">  .groupBy(_.identity)</div><div class="line">  .mapValues(_.size)</div><div class="line">  .forEach(<span class="function">(<span class="params">count, word</span>) =&gt;</span> &#123; <span class="built_in">console</span>.log(word, count) &#125;)</div><div class="line"></div><div class="line"><span class="comment">// apple 3</span></div><div class="line"><span class="comment">// orange 2</span></div><div class="line"><span class="comment">// grape 1</span></div><div class="line"><span class="comment">// banana 2</span></div><div class="line"><span class="comment">// melon 1</span></div></pre></td></tr></table></figure>
<h2 id="Destructuring-Spread-and-Arrow-Function"><a href="#Destructuring-Spread-and-Arrow-Function" class="headerlink" title="Destructuring, Spread and Arrow Function"></a>Destructuring, Spread and Arrow Function</h2><p>ES6 introduces some useful syntaxes like destructuring, spread and arrow function, which can be used to replace a lot of Lodash functions. For instance:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Lodash</span></div><div class="line">_.head([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment">// =&gt; 1</span></div><div class="line">_.tail([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment">// =&gt; [2, 3]</span></div><div class="line"><span class="comment">// ES6 destructuring syntax</span></div><div class="line"><span class="keyword">const</span> [head, ...tail] = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line"></div><div class="line"><span class="comment">// Lodash</span></div><div class="line"><span class="keyword">let</span> say = _.rest(<span class="function">(<span class="params">who, fruits</span>) =&gt;</span> who + <span class="string">' likes '</span> + fruits.join(<span class="string">','</span>))</div><div class="line">say(<span class="string">'Jerry'</span>, <span class="string">'apple'</span>, <span class="string">'grape'</span>)</div><div class="line"><span class="comment">// ES6 spread syntax</span></div><div class="line">say = <span class="function">(<span class="params">who, ...fruits</span>) =&gt;</span> who + <span class="string">' likes '</span> + fruits.join(<span class="string">','</span>)</div><div class="line">say(<span class="string">'Mary'</span>, <span class="string">'banana'</span>, <span class="string">'orange'</span>)</div><div class="line"></div><div class="line"><span class="comment">// Lodash</span></div><div class="line">_.constant(<span class="number">1</span>)() <span class="comment">// =&gt; 1</span></div><div class="line">_.identity(<span class="number">2</span>) <span class="comment">// =&gt; 2</span></div><div class="line"><span class="comment">// ES6</span></div><div class="line">(<span class="function"><span class="params">x</span> =&gt;</span> (<span class="function"><span class="params">()</span> =&gt;</span> x))(<span class="number">1</span>)() <span class="comment">// =&gt; 1</span></div><div class="line">(<span class="function"><span class="params">x</span> =&gt;</span> x)(<span class="number">2</span>) <span class="comment">// =&gt; 2</span></div><div class="line"></div><div class="line"><span class="comment">// Partial application</span></div><div class="line"><span class="keyword">let</span> add = <span class="function">(<span class="params">a, b</span>) =&gt;</span> a + b</div><div class="line"><span class="comment">// Lodash</span></div><div class="line"><span class="keyword">let</span> add1 = _.partial(add, <span class="number">1</span>)</div><div class="line"><span class="comment">// ES6</span></div><div class="line">add1 = <span class="function"><span class="params">b</span> =&gt;</span> add(<span class="number">1</span>, b)</div><div class="line"></div><div class="line"><span class="comment">// Curry</span></div><div class="line"><span class="comment">// Lodash</span></div><div class="line"><span class="keyword">let</span> curriedAdd = _.curry(add)</div><div class="line"><span class="keyword">let</span> add1 = curriedAdd(<span class="number">1</span>)</div><div class="line"><span class="comment">// ES6</span></div><div class="line">curriedAdd = <span class="function"><span class="params">a</span> =&gt;</span> b =&gt; a + b</div><div class="line">add1 = curriedAdd(<span class="number">1</span>)</div></pre></td></tr></table></figure>
<p>For collection related operations, I prefer Lodash functions for they are more concise and can be chained; for functions that can be rewritten by arrow function, Lodash still seems more simple and clear. And according to some arguments in the <a href="#References">references</a>, the currying, <a href="https://lodash.com/docs/#add" target="_blank" rel="external">operators</a> and <a href="https://github.com/lodash/lodash/wiki/FP-Guide" target="_blank" rel="external">fp style</a> from Lodash are far more useful in scenarios like function composition.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Lodash adds great power to JavaScript language. One can write concise and efficient codes with minor efforts. Besides, Lodash is fully <a href="https://lodash.com/custom-builds" target="_blank" rel="external">modularized</a>. Though some of its functions will eventually deprecate, but I believe it’ll still bring many benifits to developers, while pushing the development of JS language as well.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://www.sitepoint.com/lodash-features-replace-es6/" target="_blank" rel="external">10 Lodash Features You Can Replace with ES6</a></li>
<li><a href="https://derickbailey.com/2016/09/12/does-es6-mean-the-end-of-underscore-lodash/" target="_blank" rel="external">Does ES6 Mean The End Of Underscore / Lodash?</a></li>
<li><a href="https://www.reddit.com/r/javascript/comments/41fq2s/why_should_i_use_lodash_or_rather_what_lodash/" target="_blank" rel="external">Why should I use lodash - reddit</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://lodash.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lodash&lt;/a&gt; is a well-known JavaScript utility library that makes it easy to manipulate arrays and objects, as well as functions, strings, etc. I myself enjoys its functional way to process collections, especially chaining and lazy evaluation. But as &lt;a href=&quot;http://www.ecma-international.org/ecma-262/6.0/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ECMAScript 2015 Standard&lt;/a&gt; (ES6) becomes widely supported by major browsers, and &lt;a href=&quot;https://babeljs.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Babel&lt;/a&gt;, the JavaScript compiler that transforms ES6 codes to ES5, plays a major role in today’s frontend development, it seems that most Lodash utilities can be replaced by ES6. But should we? In my opinion, Lodash will remain popular, for it still has lots of useful features that could improve the way of programming.&lt;/p&gt;
&lt;h2 id=&quot;map-and-Array-map-Are-Different&quot;&gt;&lt;a href=&quot;#map-and-Array-map-Are-Different&quot; class=&quot;headerlink&quot; title=&quot;_.map and Array#map Are Different&quot;&gt;&lt;/a&gt;&lt;code&gt;_.map&lt;/code&gt; and &lt;code&gt;Array#map&lt;/code&gt; Are Different&lt;/h2&gt;&lt;p&gt;&lt;code&gt;_.map&lt;/code&gt;, &lt;code&gt;_.reduce&lt;/code&gt;, &lt;code&gt;_.filter&lt;/code&gt; and &lt;code&gt;_.forEach&lt;/code&gt; are frequently used functions when processing collections, and ES6 provides direct support for them:&lt;/p&gt;
&lt;figure class=&quot;highlight js&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;_.map([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], (i) =&amp;gt; i + &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;_.reduce([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], (sum, i) =&amp;gt; sum + i, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;_.filter([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], (i) =&amp;gt; i &amp;gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;_.forEach([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], (i) =&amp;gt; &amp;#123; &lt;span class=&quot;built_in&quot;&gt;console&lt;/span&gt;.log(i) &amp;#125;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// becomes&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;].map(&lt;span class=&quot;function&quot;&gt;(&lt;span class=&quot;params&quot;&gt;i&lt;/span&gt;) =&amp;gt;&lt;/span&gt; i + &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;].reduce(&lt;span class=&quot;function&quot;&gt;(&lt;span class=&quot;params&quot;&gt;sum, i&lt;/span&gt;) =&amp;gt;&lt;/span&gt; sum + i, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;].filter(&lt;span class=&quot;function&quot;&gt;(&lt;span class=&quot;params&quot;&gt;i&lt;/span&gt;) =&amp;gt;&lt;/span&gt; i &amp;gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;].forEach(&lt;span class=&quot;function&quot;&gt;(&lt;span class=&quot;params&quot;&gt;i&lt;/span&gt;) =&amp;gt;&lt;/span&gt; &amp;#123; &lt;span class=&quot;built_in&quot;&gt;console&lt;/span&gt;.log(i) &amp;#125;)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;But Lodash’s &lt;code&gt;_.map&lt;/code&gt; is more powerful, in that it works on objects, has iteratee / predicate shorthands, lazy evaluation, guards against null parameter, and has better performance.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="lodash" scheme="http://shzhangji.com/tags/lodash/"/>
    
      <category term="javascript" scheme="http://shzhangji.com/tags/javascript/"/>
    
      <category term="frontend" scheme="http://shzhangji.com/tags/frontend/"/>
    
      <category term="es6" scheme="http://shzhangji.com/tags/es6/"/>
    
  </entry>
  
  <entry>
    <title>Process Python Collections with Functional Programming</title>
    <link href="http://shzhangji.com/blog/2017/03/04/process-python-collections-with-functional-programming/"/>
    <id>http://shzhangji.com/blog/2017/03/04/process-python-collections-with-functional-programming/</id>
    <published>2017-03-04T14:32:17.000Z</published>
    <updated>2017-03-09T05:49:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>I develop Spark applications with Scala, and it has a very powerful <a href="http://docs.scala-lang.org/overviews/collections/introduction" target="_blank" rel="external">collection system</a>, in which functional programming is certainly a key. Java 8 also introduces Lambda Expression and Stream API. In JavaScript, there is a <a href="https://lodash.com/" target="_blank" rel="external">Lodash</a> library that provides powerful tools to process arrays and objects. When my primary work language changes to Python, I am wondering if it’s possible to manipulate collections in a FP way, and fortunately Python already provides syntax and tools for functional programming. Though list comprehension is the pythonic way to deal with collections, but the idea and concepts of FP is definitely worth learning.</p>
<h2 id="Wordcount-Example"><a href="#Wordcount-Example" class="headerlink" title="Wordcount Example"></a>Wordcount Example</h2><p>Let’s first write a snippet to count the word occurences from a paragraph, in of course a functional way.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"></div><div class="line"></div><div class="line">content = <span class="string">"""</span></div><div class="line"><span class="string">an apple orange the grape</span></div><div class="line"><span class="string">banana an apple melon</span></div><div class="line"><span class="string">an orange banana apple</span></div><div class="line"><span class="string">"""</span></div><div class="line"></div><div class="line">word_matches = re.finditer(<span class="string">r'\S+'</span>, content)</div><div class="line">words = map(<span class="keyword">lambda</span> m: m.group(<span class="number">0</span>), word_matches)</div><div class="line">fruits = filter(<span class="keyword">lambda</span> s: len(s) &gt; <span class="number">3</span>, words)</div><div class="line">grouped_fruits = itertools.groupby(sorted(fruits))</div><div class="line">fruit_counts = map(<span class="keyword">lambda</span> t: (t[<span class="number">0</span>], len(list(t[<span class="number">1</span>]))), grouped_fruits)</div><div class="line">print(list(fruit_counts))</div></pre></td></tr></table></figure>
<p>Run this example and you’ll get a list of fruits, along with their counts:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[(&apos;apple&apos;, 3), (&apos;banana&apos;, 2), (&apos;grape&apos;, 1), (&apos;melon&apos;, 1), (&apos;orange&apos;, 2)]</div></pre></td></tr></table></figure>
<p>This example includes most aspects of processing collections with FP style. For instance, <code>re.finditer</code> returns an <code>iterator</code> that is lazily evaluated; <code>map</code> and <code>filter</code> are used to do transformations; <code>itertools</code> module provides various functions to cope with iterables; and last but not least, the <code>lambda</code> expression, an easy way to define inline anonymous function. All of them will be described in the following sections.</p>
<a id="more"></a>
<h2 id="Ingredients-of-Functional-Programming"><a href="#Ingredients-of-Functional-Programming" class="headerlink" title="Ingredients of Functional Programming"></a>Ingredients of Functional Programming</h2><p>Python is far from being a functional language, but it provides some basic syntax and tools so that we can choose to write Python in a functional way.</p>
<h3 id="Function-as-First-class-Citizen"><a href="#Function-as-First-class-Citizen" class="headerlink" title="Function as First-class Citizen"></a>Function as First-class Citizen</h3><p>Function is data. It can be assigned to a variable, pass as a parameter to another function, or returned by a function. The later two cases also refers to higher order functions. Python makes it quite easy, you can define and pass around the function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a, b)</span>:</span></div><div class="line">    <span class="keyword">return</span> a + b</div><div class="line"></div><div class="line">add_two = add</div><div class="line">print(add_two(<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># =&gt; 3</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate</span><span class="params">(a, b, operation)</span>:</span></div><div class="line">    <span class="keyword">return</span> operation(a, b)</div><div class="line"></div><div class="line">print(calculate(<span class="number">1</span>, <span class="number">2</span>, add)) <span class="comment"># =&gt; 3</span></div></pre></td></tr></table></figure>
<p>Or generate a new function from a function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_n</span><span class="params">(n)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a)</span>:</span></div><div class="line">        <span class="keyword">return</span> a + n</div><div class="line">    <span class="keyword">return</span> add</div><div class="line"></div><div class="line">add_1 = add_n(<span class="number">1</span>)</div><div class="line">print(add_1(<span class="number">1</span>)) <span class="comment"># =&gt; 2</span></div></pre></td></tr></table></figure>
<p>To use function in <code>map</code>, which applies the function to every element of the iterable:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(list(map(add_1, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))) <span class="comment"># =&gt; [2, 3, 4]</span></div></pre></td></tr></table></figure>
<p>For very short function, we can use lambda expression:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">map(<span class="keyword">lambda</span> a: a + <span class="number">1</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</div></pre></td></tr></table></figure>
<h3 id="Being-Lazy"><a href="#Being-Lazy" class="headerlink" title="Being Lazy"></a>Being Lazy</h3><p>Lazy evaluation means postponing the execution until it’s necessary. It’s a very common optimization strategy in big data transformation, becuase all map-like operations should be chained and assigned to a single task. In Python, there’s iterator, an stateful object that remembers the current element during iteration. Let’s assume <code>calc</code> is a heavy function, and the following two lines differ:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[calc(i) <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</div><div class="line">map(calc, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</div></pre></td></tr></table></figure>
<p>List comprehension is eager-evaluated, while <code>map</code> (from Python 3.x on) returns an iterator. You can use the <code>next</code> global function to fetch the next element, or take the first two results using:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> islice</div><div class="line">list(islice(map(calc, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), <span class="number">2</span>))</div></pre></td></tr></table></figure>
<p>It’s worth mentioning that from Python 3.x on a lot of methods returns iterator instead of concrete list, you can refer to <a href="http://shzhangji.com/blog/2017/01/08/python-2-to-3-quick-guide/#Less-Lists-More-Views">this article</a>.</p>
<h3 id="Purity"><a href="#Purity" class="headerlink" title="Purity"></a>Purity</h3><p>A function is pure if its output only depends on its input, and it has no side-effect, i.e. without changing outer/global variable space. Here’re some examples of pure/non-pure functions:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inc</span><span class="params">(a)</span>:</span> <span class="comment"># pure</span></div><div class="line">    <span class="keyword">return</span> a + <span class="number">1</span></div><div class="line"></div><div class="line">i = <span class="number">0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(a)</span>:</span> <span class="comment"># non-pure</span></div><div class="line">    i = len(a)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">greet</span><span class="params">(name)</span>:</span> <span class="comment"># non-pure, change the console</span></div><div class="line">    print(<span class="string">'hi'</span>, name)</div></pre></td></tr></table></figure>
<p>Purity is a good functional style because:</p>
<ul>
<li>it makes you re-design the functions so that they become shorter;</li>
<li>and short functions are easier to test, have less bugs;</li>
<li>purity also enables parallel execution.</li>
</ul>
<p>In concurrency programming, sharing state, lock, and context switch are all performance killers. Pure functions ensures codes can be executed in parallel without coordination of states, and can be re-executed multiple times safely.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor</div><div class="line">executor = ThreadPoolExecutor(<span class="number">5</span>)</div><div class="line">list(executor.map(add_1, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))</div></pre></td></tr></table></figure>
<h3 id="Function-Composition"><a href="#Function-Composition" class="headerlink" title="Function Composition"></a>Function Composition</h3><p>There’re also topics on combining, currying, partially applying functions, so we can tackle complex problems with small well-defined functions. Python provides <code>decorator</code>, <code>generator</code> syntax, along with <code>functools</code>, <code>operator</code> modules for such tasks. These can be found in Python official documentation.</p>
<h2 id="Chaining-Operations"><a href="#Chaining-Operations" class="headerlink" title="Chaining Operations"></a>Chaining Operations</h2><p><code>map</code>, <code>filter</code>, and functions in <code>itertools</code> cannot be easily chained. We have to nest the function calls or introduce intermediate variables. Luckily, there’s an open-sourced <a href="https://github.com/EntilZha/PyFunctional" target="_blank" rel="external">PyFunctional</a> package that can help us transform or aggregate collections in a funcional way quite fluently.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> functional <span class="keyword">import</span> seq</div><div class="line"></div><div class="line">seq(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)\</div><div class="line">    .map(<span class="keyword">lambda</span> x: x * <span class="number">2</span>)\</div><div class="line">    .filter(<span class="keyword">lambda</span> x: x &gt; <span class="number">4</span>)\</div><div class="line">    .reduce(<span class="keyword">lambda</span> x, y: x + y)</div><div class="line"><span class="comment"># =&gt; 14</span></div></pre></td></tr></table></figure>
<h2 id="List-Comprehension-Or-map"><a href="#List-Comprehension-Or-map" class="headerlink" title="List Comprehension Or map?"></a>List Comprehension Or <code>map</code>?</h2><p>List comprehension and generator expression are the pythonic way of processing collections, and the communiy encourages using list comprehension instead of <code>map</code>, etc. There’s a nice <a href="http://stackoverflow.com/a/6407222/1030720" target="_blank" rel="external">answer</a> on StackOverflow that addresses the following principle: use <code>map</code> only when you already have a function defined. Otherwise just stick to listcomps for it’s more widely accepted. Neverthelss, one should still pay attention to the laziness of various methods.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Processing collections is only one application of functional programming. This program paradigm can be applied to other phases of designing your systems. Further materials like <a href="http://deptinfo.unice.fr/~roy/sicp.pdf" target="_blank" rel="external">SICP</a>, <a href="https://www.manning.com/books/functional-programming-in-scala" target="_blank" rel="external">Functional Programming in Scala</a> are all very informative. Hope you enjoy.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://docs.python.org/3/howto/functional.html" target="_blank" rel="external">Functional Programming HOWTO</a></li>
<li><a href="http://kachayev.github.io/talks/uapycon2012/" target="_blank" rel="external">Functional Programming with Python</a></li>
<li><a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" target="_blank" rel="external">Itertools Recipes</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I develop Spark applications with Scala, and it has a very powerful &lt;a href=&quot;http://docs.scala-lang.org/overviews/collections/introduction&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;collection system&lt;/a&gt;, in which functional programming is certainly a key. Java 8 also introduces Lambda Expression and Stream API. In JavaScript, there is a &lt;a href=&quot;https://lodash.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lodash&lt;/a&gt; library that provides powerful tools to process arrays and objects. When my primary work language changes to Python, I am wondering if it’s possible to manipulate collections in a FP way, and fortunately Python already provides syntax and tools for functional programming. Though list comprehension is the pythonic way to deal with collections, but the idea and concepts of FP is definitely worth learning.&lt;/p&gt;
&lt;h2 id=&quot;Wordcount-Example&quot;&gt;&lt;a href=&quot;#Wordcount-Example&quot; class=&quot;headerlink&quot; title=&quot;Wordcount Example&quot;&gt;&lt;/a&gt;Wordcount Example&lt;/h2&gt;&lt;p&gt;Let’s first write a snippet to count the word occurences from a paragraph, in of course a functional way.&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; re&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; itertools&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;content = &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;an apple orange the grape&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;banana an apple melon&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;an orange banana apple&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;word_matches = re.finditer(&lt;span class=&quot;string&quot;&gt;r&#39;\S+&#39;&lt;/span&gt;, content)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;words = map(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; m: m.group(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;), word_matches)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;fruits = filter(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; s: len(s) &amp;gt; &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, words)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;grouped_fruits = itertools.groupby(sorted(fruits))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;fruit_counts = map(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; t: (t[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], len(list(t[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]))), grouped_fruits)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;print(list(fruit_counts))&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Run this example and you’ll get a list of fruits, along with their counts:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[(&amp;apos;apple&amp;apos;, 3), (&amp;apos;banana&amp;apos;, 2), (&amp;apos;grape&amp;apos;, 1), (&amp;apos;melon&amp;apos;, 1), (&amp;apos;orange&amp;apos;, 2)]&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;This example includes most aspects of processing collections with FP style. For instance, &lt;code&gt;re.finditer&lt;/code&gt; returns an &lt;code&gt;iterator&lt;/code&gt; that is lazily evaluated; &lt;code&gt;map&lt;/code&gt; and &lt;code&gt;filter&lt;/code&gt; are used to do transformations; &lt;code&gt;itertools&lt;/code&gt; module provides various functions to cope with iterables; and last but not least, the &lt;code&gt;lambda&lt;/code&gt; expression, an easy way to define inline anonymous function. All of them will be described in the following sections.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
      <category term="functional programming" scheme="http://shzhangji.com/tags/functional-programming/"/>
    
  </entry>
  
  <entry>
    <title>Difference Between Lodash _.assign and _.assignIn</title>
    <link href="http://shzhangji.com/blog/2017/01/29/difference-between-lodash-assign-and-assignin/"/>
    <id>http://shzhangji.com/blog/2017/01/29/difference-between-lodash-assign-and-assignin/</id>
    <published>2017-01-29T06:18:29.000Z</published>
    <updated>2017-02-16T07:35:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>In Lodash, both <code>_.assign</code> and <code>_.assignIn</code> are ways to copy source objects’ properties into target object. According the <a href="https://lodash.com/docs/" target="_blank" rel="external">documentation</a>, <code>_.assign</code> processes <strong>own enumerable string keyed properties</strong>, while <code>_.assignIn</code> processes both <strong>own and inherited source properties</strong>. There’re also other companion functions like <code>_.forOwn</code> and <code>_.forIn</code>, <code>_.has</code> and <code>_.hasIn</code>. So what’s the difference between them?</p>
<p>In brief, the <code>In</code> in latter methods implies the way <code>for...in</code> loop behaves, which <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for...in" target="_blank" rel="external">iterates all enumerable properties of the object itself and those the object inherits from its constructor’s prototype</a>. JavaScript has an inheritance mechanism called prototype chain. When iterating an object’s properties with <code>for...in</code> or <code>_.forIn</code>, all properties appeared in the object and its prototype are processed, until the prototype resolves to <code>null</code>. Here’s the example code taken from Lodash’s doc:</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">Foo</span>(<span class="params"></span>) </span>&#123; <span class="keyword">this</span>.a = <span class="number">1</span>; &#125;</div><div class="line">Foo.prototype.b = <span class="number">2</span>;</div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">Bar</span>(<span class="params"></span>) </span>&#123; <span class="keyword">this</span>.c = <span class="number">3</span>; &#125;</div><div class="line">Bar.prototype.d = <span class="number">4</span>;</div><div class="line">_.assign(&#123;<span class="attr">a</span>: <span class="number">0</span>&#125;, <span class="keyword">new</span> Foo, <span class="keyword">new</span> Bar); <span class="comment">// =&gt; &#123;a: 1, c: 3&#125;</span></div><div class="line">_.assignIn(&#123;<span class="attr">a</span>: <span class="number">0</span>&#125;, <span class="keyword">new</span> Foo, <span class="keyword">new</span> Bar); <span class="comment">// =&gt; &#123;a:1, b:2, c:3, d:4&#125;</span></div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="How-assign-Picks-Properties"><a href="#How-assign-Picks-Properties" class="headerlink" title="How _.assign Picks Properties"></a>How <code>_.assign</code> Picks Properties</h2><p>Let’s dissect the phrase “own enumerable string-keys properties” into three parts. </p>
<h3 id="Own-Property"><a href="#Own-Property" class="headerlink" title="Own Property"></a>Own Property</h3><p>JavaScript is a prototype-based language, but there’re several ways to simulate class and instance, like object literal, function prototype, <code>Object.create</code>, and the newly added <code>class</code> keyword. In either case, we can use <code>Object.prototype.hasOwnProperty()</code> to determine if the property is inherited or not.</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> foo = <span class="keyword">new</span> Foo();</div><div class="line">foo.hasOwnProperty(<span class="string">'a'</span>); <span class="comment">// =&gt; true</span></div><div class="line"><span class="built_in">Object</span>.prototype.hasOwnProperty.call(foo, <span class="string">'b'</span>); <span class="comment">// =&gt; false</span></div></pre></td></tr></table></figure>
<p><code>Object.getOwnPropertyNames()</code> and <code>Object.keys()</code> can retrieve all properties defined directly in the object, except that <code>Object.keys()</code> only returns enumerable keys (see next section).</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> o1 = &#123;<span class="attr">a</span>: <span class="number">1</span>&#125;;</div><div class="line"><span class="keyword">let</span> o2 = <span class="built_in">Object</span>.create(o1);</div><div class="line">o2.b = <span class="number">2</span>;</div><div class="line"><span class="built_in">Object</span>.getOwnPropertyNames(o2); <span class="comment">// =&gt; ['b']</span></div><div class="line"><span class="built_in">Object</span>.keys(o2); <span class="comment">// =&gt; ['b']</span></div></pre></td></tr></table></figure>
<h3 id="Enumerable-Property"><a href="#Enumerable-Property" class="headerlink" title="Enumerable Property"></a>Enumerable Property</h3><p>Object property can be defined with either data descriptor or accessor descriptor. Among data descriptor options, the <code>enumerable</code> boolean indicates whether this property shows in <code>for...in</code> or <code>Object.keys()</code>. </p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> o = &#123;&#125;;</div><div class="line"><span class="built_in">Object</span>.defineProperty(o, <span class="string">'a'</span>, &#123; <span class="attr">enumerable</span>: <span class="literal">false</span>, <span class="attr">value</span>: <span class="number">1</span> &#125;);</div><div class="line"><span class="built_in">Object</span>.keys(o); <span class="comment">// =&gt; []</span></div><div class="line">o.propertyIsEnumerable(<span class="string">'a'</span>); <span class="comment">// =&gt; false</span></div></pre></td></tr></table></figure>
<p>You can refer to <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/defineProperty" target="_blank" rel="external">Object.defineProperty()</a> for more information.</p>
<h3 id="String-keyed-Property"><a href="#String-keyed-Property" class="headerlink" title="String-keyed Property"></a>String-keyed Property</h3><p>Before ES6, object’s keys are always String. ES6 introduces a new primitive type <a href="https://developer.mozilla.org/en-US/docs/Glossary/Symbol" target="_blank" rel="external">Symbol</a>, which can be used as a key for private property. Symbol property is non-enumerable.</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> s = <span class="built_in">Symbol</span>();</div><div class="line"><span class="keyword">let</span> o = &#123;&#125;;</div><div class="line">o[s] = <span class="number">1</span>;</div><div class="line"><span class="built_in">Object</span>.keys(o); <span class="comment">// =&gt; []</span></div></pre></td></tr></table></figure>
<p>There’s a nice <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Enumerability_and_ownership_of_properties#Detection_Table" target="_blank" rel="external">Detection Table</a> to help you figure out which built-in methods process enumerable or inherited properties.</p>
<h2 id="assign-and-assignIn-Implementation"><a href="#assign-and-assignIn-Implementation" class="headerlink" title="_.assign and _.assignIn Implementation"></a><code>_.assign</code> and <code>_.assignIn</code> Implementation</h2><p>Both methods calls <code>_.keys</code> and <code>_.keysIn</code> respectively. <code>_.keys</code> calls <code>Object.keys()</code> and <code>_.keysIn</code> uses <code>for...in</code> loop. Actually <code>Object.keys()</code> is not difficult to implement. As mentioned above, <code>for...in</code> can be used to retrieve both own and inherited properties, while <code>hasOwnProperty</code> determines whether this property is defined in the object itself.</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">keys</span>(<span class="params">object</span>) </span>&#123;</div><div class="line">  <span class="keyword">let</span> result = [];</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">let</span> key <span class="keyword">in</span> <span class="built_in">Object</span>(object)) &#123;</div><div class="line">    <span class="keyword">if</span> (<span class="built_in">Object</span>.prototype.hasOwnProperty.call(object, key)) &#123;</div><div class="line">      result.push(key);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> result;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>Object.assign()</code> does the same thing as <code>_.assign()</code>. Use Lodash if you need to run your code on older browsers.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/assign" target="_blank" rel="external">Object.assign() - JavaScript | MDN</a></li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Inheritance_and_the_prototype_chain" target="_blank" rel="external">Inheritance and The Prototype Chain</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In Lodash, both &lt;code&gt;_.assign&lt;/code&gt; and &lt;code&gt;_.assignIn&lt;/code&gt; are ways to copy source objects’ properties into target object. According the &lt;a href=&quot;https://lodash.com/docs/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;documentation&lt;/a&gt;, &lt;code&gt;_.assign&lt;/code&gt; processes &lt;strong&gt;own enumerable string keyed properties&lt;/strong&gt;, while &lt;code&gt;_.assignIn&lt;/code&gt; processes both &lt;strong&gt;own and inherited source properties&lt;/strong&gt;. There’re also other companion functions like &lt;code&gt;_.forOwn&lt;/code&gt; and &lt;code&gt;_.forIn&lt;/code&gt;, &lt;code&gt;_.has&lt;/code&gt; and &lt;code&gt;_.hasIn&lt;/code&gt;. So what’s the difference between them?&lt;/p&gt;
&lt;p&gt;In brief, the &lt;code&gt;In&lt;/code&gt; in latter methods implies the way &lt;code&gt;for...in&lt;/code&gt; loop behaves, which &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for...in&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;iterates all enumerable properties of the object itself and those the object inherits from its constructor’s prototype&lt;/a&gt;. JavaScript has an inheritance mechanism called prototype chain. When iterating an object’s properties with &lt;code&gt;for...in&lt;/code&gt; or &lt;code&gt;_.forIn&lt;/code&gt;, all properties appeared in the object and its prototype are processed, until the prototype resolves to &lt;code&gt;null&lt;/code&gt;. Here’s the example code taken from Lodash’s doc:&lt;/p&gt;
&lt;figure class=&quot;highlight javascript&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Foo&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;&lt;/span&gt;) &lt;/span&gt;&amp;#123; &lt;span class=&quot;keyword&quot;&gt;this&lt;/span&gt;.a = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;; &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Foo.prototype.b = &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Bar&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;&lt;/span&gt;) &lt;/span&gt;&amp;#123; &lt;span class=&quot;keyword&quot;&gt;this&lt;/span&gt;.c = &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;; &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Bar.prototype.d = &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;_.assign(&amp;#123;&lt;span class=&quot;attr&quot;&gt;a&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&amp;#125;, &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Foo, &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Bar); &lt;span class=&quot;comment&quot;&gt;// =&amp;gt; &amp;#123;a: 1, c: 3&amp;#125;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;_.assignIn(&amp;#123;&lt;span class=&quot;attr&quot;&gt;a&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&amp;#125;, &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Foo, &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Bar); &lt;span class=&quot;comment&quot;&gt;// =&amp;gt; &amp;#123;a:1, b:2, c:3, d:4&amp;#125;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="lodash" scheme="http://shzhangji.com/tags/lodash/"/>
    
      <category term="javascript" scheme="http://shzhangji.com/tags/javascript/"/>
    
      <category term="frontend" scheme="http://shzhangji.com/tags/frontend/"/>
    
  </entry>
  
  <entry>
    <title>Python 2 to 3 Quick Guide</title>
    <link href="http://shzhangji.com/blog/2017/01/08/python-2-to-3-quick-guide/"/>
    <id>http://shzhangji.com/blog/2017/01/08/python-2-to-3-quick-guide/</id>
    <published>2017-01-08T04:26:54.000Z</published>
    <updated>2017-03-09T05:48:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>Few years ago I was programming Python 2.7, when 3.x was still not an option, because of its backward-incompatibiliy and lack of popular third-party libraries support. But now it’s safe to say Python 3 is <a href="http://py3readiness.org/" target="_blank" rel="external">totally ready</a>, and here’s a list of references for those (including me) who are adopting Python 3 with a 2.x background.</p>
<ol>
<li>All Strings Are Unicode</li>
<li><code>print</code> Becomes a Function</li>
<li>Less Lists More Views</li>
<li>Integer Division Returns Float</li>
<li>Comparison Operators Raises <code>TypeError</code></li>
<li>Set Literal Support</li>
<li>New String Formatting</li>
<li>Exception Handling</li>
<li>Global Function Changes</li>
<li>Renaming Modules and Relative Import</li>
</ol>
<h2 id="All-Strings-Are-Unicode"><a href="#All-Strings-Are-Unicode" class="headerlink" title="All Strings Are Unicode"></a>All Strings Are Unicode</h2><p>When dealing with non-ASCII encodings in Python 2, there’re <code>str</code>, <code>unicode</code>, <code>u&#39;...&#39;</code>, <code>s.encode()</code>, etc. In Python 3, there’re only <strong>text</strong> and <strong>binary data</strong>. The former is <code>str</code>, strings that are always represented in Unicode; the later is <code>bytes</code>, which is just a sequence of byte numbers.</p>
<ul>
<li>Conversion between <code>str</code> and <code>bytes</code>:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># str to bytes</span></div><div class="line"><span class="string">'str'</span>.encode(<span class="string">'UTF-8'</span>)</div><div class="line">bytes(<span class="string">'str'</span>, encoding=<span class="string">'UTF-8'</span>)</div><div class="line"></div><div class="line"><span class="comment"># bytes to str</span></div><div class="line"><span class="string">b'bytes'</span>.decode(<span class="string">'UTF-8'</span>)</div><div class="line">str(<span class="string">b'bytes'</span>, encoding=<span class="string">'UTF-8'</span>)</div></pre></td></tr></table></figure>
<ul>
<li><code>basestring</code> is removed, use <code>str</code> as type: <code>isinstance(s, str)</code></li>
<li><code>bytes</code> is immutable, the corresponding mutable version is <code>bytearray</code>.</li>
<li>The default source file encoding is UTF-8 now.</li>
</ul>
<a id="more"></a>
<h2 id="print-Becomes-a-Function"><a href="#print-Becomes-a-Function" class="headerlink" title="print Becomes a Function"></a><code>print</code> Becomes a Function</h2><p>In Python 2, <code>print</code> is a statement, and now it’s used as a function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span>   <span class="comment"># Old: print a new line</span></div><div class="line">print() <span class="comment"># New</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">'hello'</span>, <span class="string">'world'</span>,          <span class="comment"># Old: trailing comma suppresses new line</span></div><div class="line">print(<span class="string">'hello'</span>, <span class="string">'world'</span>, end=<span class="string">' '</span>) <span class="comment"># New: end defaults to '\n'</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> &gt;&gt;sys.stderr, <span class="string">'error'</span>     <span class="comment"># Old: write to stderr</span></div><div class="line">print(<span class="string">'error'</span>, file=sys.stderr) <span class="comment"># New</span></div></pre></td></tr></table></figure>
<p><code>print</code> function also provides <code>sep</code> and <code>flush</code> parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">'hello'</span>, <span class="string">'world'</span>, sep=<span class="string">','</span>, flush=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># Instead of:</span></div><div class="line"><span class="keyword">print</span> <span class="string">','</span>.join((<span class="string">'hello'</span>, <span class="string">'world'</span>))</div><div class="line">sys.stdout.flush()</div></pre></td></tr></table></figure>
<h2 id="Less-Lists-More-Views"><a href="#Less-Lists-More-Views" class="headerlink" title="Less Lists More Views"></a>Less Lists More Views</h2><p>A lot of well-known methods now return iterators, or ‘views’,  instead of eager-evaluated lists.</p>
<ul>
<li>Dictionary’s <code>keys</code>, <code>items</code>, and <code>values</code> methods, while removing <code>iterkeys</code>, <code>iteritems</code>, and <code>itervalues</code>. For example, when you need a sorted key list:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">k = d.keys(); k.sort() <span class="comment"># Old</span></div><div class="line">k = sorted(d.keys())   <span class="comment"># New</span></div></pre></td></tr></table></figure>
<ul>
<li><code>map</code>, <code>filter</code>, and <code>zip</code>, while removing <code>imap</code> methods in <code>itertools</code> module. To get a concrete list, use list comprehension or the <code>list</code> global function:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[x * <span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</div><div class="line">list(map(<span class="keyword">lambda</span> x: x * <span class="number">2</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))</div></pre></td></tr></table></figure>
<ul>
<li><code>range</code> is now equivalent to <code>xrange</code> in Python 2, the later is removed.</li>
<li>For iterators, the <code>next</code> method is renamed to <code>__next__</code>, and there’s a global <code>next</code> function, which accepts an iterator and calls its <code>__next__</code> method.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">iter([<span class="number">1</span>]).next()     <span class="comment"># Old</span></div><div class="line">iter([<span class="number">1</span>]).__next__() <span class="comment"># New</span></div><div class="line">next(iter([<span class="number">1</span>]))      <span class="comment"># New</span></div></pre></td></tr></table></figure>
<h2 id="Integer-Division-Returns-Float"><a href="#Integer-Division-Returns-Float" class="headerlink" title="Integer Division Returns Float"></a>Integer Division Returns Float</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print 1 / 2   # Old: prints 0</div><div class="line">print 1 / 2.0 # Old: prints 0.5</div><div class="line">print(1 / 2)  # New: prints 0.5</div><div class="line">print(1 // 2) # New: prints 0</div></pre></td></tr></table></figure>
<ul>
<li>There’s no difference between <code>long</code> and <code>int</code> now, use <code>int</code> only.</li>
<li>Octal literals are represented as <code>0o755</code>, instead of <code>0755</code>.</li>
</ul>
<h2 id="Comparison-Operators-Raises-TypeError"><a href="#Comparison-Operators-Raises-TypeError" class="headerlink" title="Comparison Operators Raises TypeError"></a>Comparison Operators Raises <code>TypeError</code></h2><ul>
<li><code>&lt;</code>, <code>&lt;=</code>, <code>&gt;=</code>, <code>&gt;</code> can no longer be used between different types.</li>
<li><code>==</code> and <code>!=</code> remains the same.</li>
<li><code>cmp</code> parameter in <code>sort</code> is removed. Use <code>key</code> to extract a comparison key from each element.</li>
</ul>
<h2 id="Set-Literal-Support"><a href="#Set-Literal-Support" class="headerlink" title="Set Literal Support"></a>Set Literal Support</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">s = set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment"># Old, also valid in Python 3</span></div><div class="line">s = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;      <span class="comment"># New</span></div><div class="line">s = set()          <span class="comment"># Empty set</span></div><div class="line">d = &#123;&#125;             <span class="comment"># Empty dict</span></div></pre></td></tr></table></figure>
<h2 id="New-String-Formatting"><a href="#New-String-Formatting" class="headerlink" title="New String Formatting"></a>New String Formatting</h2><p>Python 3 introduces a new form of string formatting, and it’s also back-ported to Python 2.x. The old <code>%s</code> formatting is still available in 3.x, but the <a href="https://docs.python.org/3/library/string.html#format-string-syntax" target="_blank" rel="external">new format</a> seems more expressive and powerful.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># by position</span></div><div class="line"><span class="string">'&#123;&#125;, &#123;&#125;, &#123;&#125;'</span>.format(<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>)    <span class="comment"># a, b, c</span></div><div class="line"><span class="string">'&#123;2&#125;, &#123;1&#125;, &#123;0&#125;'</span>.format(<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>) <span class="comment"># c, b, a</span></div><div class="line"></div><div class="line"><span class="comment"># by name</span></div><div class="line"><span class="string">'Hello, &#123;name&#125;'</span>.format(name=<span class="string">'Jerry'</span>) <span class="comment"># Hello, Jerry</span></div><div class="line"></div><div class="line"><span class="comment"># by attribute</span></div><div class="line">c = <span class="number">1</span> - <span class="number">2j</span></div><div class="line"><span class="string">'real: &#123;0.real&#125;'</span>.format(c) <span class="comment"># real: 1.0</span></div><div class="line"></div><div class="line"><span class="comment"># by index</span></div><div class="line"><span class="string">'X: &#123;0[0]&#125;, Y: &#123;0[1]&#125;'</span>.format((<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># X: 1, Y: 2</span></div><div class="line"></div><div class="line"><span class="comment"># format number</span></div><div class="line"><span class="string">'&#123;:.2f&#125;'</span>.format(<span class="number">1.2</span>)   <span class="comment"># 1.20</span></div><div class="line"><span class="string">'&#123;:.2%&#125;'</span>.format(<span class="number">0.012</span>) <span class="comment"># 1.20%</span></div><div class="line"><span class="string">'&#123;:,&#125;'</span>.format(<span class="number">1234567</span>) <span class="comment"># 1,234,567</span></div><div class="line"></div><div class="line"><span class="comment"># padding</span></div><div class="line"><span class="string">'&#123;:&gt;05&#125;'</span>.format(<span class="number">1</span>) <span class="comment"># 00001</span></div></pre></td></tr></table></figure>
<p>Furthermore, Python 3.6 introduces literal string interpolation (<a href="https://www.python.org/dev/peps/pep-0498/" target="_blank" rel="external">PEP 498</a>).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">name = <span class="string">'Jerry'</span></div><div class="line">print(<span class="string">f'Hello, <span class="subst">&#123;name&#125;</span>'</span>)</div></pre></td></tr></table></figure>
<h2 id="Exception-Handling"><a href="#Exception-Handling" class="headerlink" title="Exception Handling"></a>Exception Handling</h2><p>Raise and catch exceptions in a more standard way:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Old</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">  <span class="keyword">raise</span> Exception, <span class="string">'message'</span></div><div class="line"><span class="keyword">except</span> Exception, e:</div><div class="line">  tb = sys.exc_info()[<span class="number">2</span>]</div><div class="line"></div><div class="line"><span class="comment"># New</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">  <span class="keyword">raise</span> Exception(<span class="string">'message'</span>)</div><div class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">  tb = e.__traceback__</div></pre></td></tr></table></figure>
<h2 id="Global-Function-Changes"><a href="#Global-Function-Changes" class="headerlink" title="Global Function Changes"></a>Global Function Changes</h2><p>Some global functions are (re)moved to reduce duplication and language cruft.</p>
<ul>
<li><code>reduce</code> is removed, use <code>functools.reduce</code>, or explict <code>for</code> loop instead.</li>
<li><code>apply</code> is removed, use <code>f(*args)</code> instead of <code>apply(f, args)</code>.</li>
<li><code>execfile</code> is removed, use <code>exec(open(fn).read())</code></li>
<li>Removed backticks, use <code>repr</code> instread.</li>
<li><code>raw_input</code> is renamed to <code>input</code>, and the old <code>input</code> behaviour can be achieved by <code>eval(input())</code></li>
</ul>
<h2 id="Renaming-Modules-and-Relative-Import"><a href="#Renaming-Modules-and-Relative-Import" class="headerlink" title="Renaming Modules and Relative Import"></a>Renaming Modules and Relative Import</h2><ul>
<li>Different URL modules are unified into <code>urllib</code> module, e.g.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen, Request</div><div class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</div><div class="line">req = Request(<span class="string">'http://shzhangji.com?'</span> + urlencode(&#123;<span class="string">'t'</span>: <span class="number">1</span>&#125;)</div><div class="line"><span class="keyword">with</span> urlopen(req) <span class="keyword">as</span> f:</div><div class="line">  print(f.read())</div></pre></td></tr></table></figure>
<ul>
<li>Some modules are renamed according to <a href="https://www.python.org/dev/peps/pep-0008" target="_blank" rel="external">PEP 8</a>, such as:<ul>
<li>ConfigParser -&gt; configparser</li>
<li>copy_reg -&gt; copyreg</li>
<li>test.test_support -&gt; test.support</li>
</ul>
</li>
<li>Some modules have both pure Python implementation along with an accelerated version, like StringIO and cStringIO. In Python 3, user should always import the standard module, and fallback would happen automatically.<ul>
<li>StringIO + cStringIO -&gt; io</li>
<li>pickle + cPickle -&gt; pickle</li>
</ul>
</li>
<li>All <code>import</code> forms are interpreted as absolute imports, unless started with <code>.</code>:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> . <span class="keyword">import</span> somemod</div><div class="line"><span class="keyword">from</span> .somemod <span class="keyword">import</span> moremod</div></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://docs.python.org/3/whatsnew/3.0.html" target="_blank" rel="external">What’s New In Python 3.0</a></li>
<li><a href="http://www.diveintopython3.net/porting-code-to-python-3-with-2to3.html" target="_blank" rel="external">Porting Code to Python 3 with 2to3</a></li>
<li><a href="http://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html" target="_blank" rel="external">The key differences between Python 2.7.x and Python 3.x with examples</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Few years ago I was programming Python 2.7, when 3.x was still not an option, because of its backward-incompatibiliy and lack of popular third-party libraries support. But now it’s safe to say Python 3 is &lt;a href=&quot;http://py3readiness.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;totally ready&lt;/a&gt;, and here’s a list of references for those (including me) who are adopting Python 3 with a 2.x background.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;All Strings Are Unicode&lt;/li&gt;
&lt;li&gt;&lt;code&gt;print&lt;/code&gt; Becomes a Function&lt;/li&gt;
&lt;li&gt;Less Lists More Views&lt;/li&gt;
&lt;li&gt;Integer Division Returns Float&lt;/li&gt;
&lt;li&gt;Comparison Operators Raises &lt;code&gt;TypeError&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Set Literal Support&lt;/li&gt;
&lt;li&gt;New String Formatting&lt;/li&gt;
&lt;li&gt;Exception Handling&lt;/li&gt;
&lt;li&gt;Global Function Changes&lt;/li&gt;
&lt;li&gt;Renaming Modules and Relative Import&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;All-Strings-Are-Unicode&quot;&gt;&lt;a href=&quot;#All-Strings-Are-Unicode&quot; class=&quot;headerlink&quot; title=&quot;All Strings Are Unicode&quot;&gt;&lt;/a&gt;All Strings Are Unicode&lt;/h2&gt;&lt;p&gt;When dealing with non-ASCII encodings in Python 2, there’re &lt;code&gt;str&lt;/code&gt;, &lt;code&gt;unicode&lt;/code&gt;, &lt;code&gt;u&amp;#39;...&amp;#39;&lt;/code&gt;, &lt;code&gt;s.encode()&lt;/code&gt;, etc. In Python 3, there’re only &lt;strong&gt;text&lt;/strong&gt; and &lt;strong&gt;binary data&lt;/strong&gt;. The former is &lt;code&gt;str&lt;/code&gt;, strings that are always represented in Unicode; the later is &lt;code&gt;bytes&lt;/code&gt;, which is just a sequence of byte numbers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Conversion between &lt;code&gt;str&lt;/code&gt; and &lt;code&gt;bytes&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# str to bytes&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&#39;str&#39;&lt;/span&gt;.encode(&lt;span class=&quot;string&quot;&gt;&#39;UTF-8&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;bytes(&lt;span class=&quot;string&quot;&gt;&#39;str&#39;&lt;/span&gt;, encoding=&lt;span class=&quot;string&quot;&gt;&#39;UTF-8&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# bytes to str&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;b&#39;bytes&#39;&lt;/span&gt;.decode(&lt;span class=&quot;string&quot;&gt;&#39;UTF-8&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;str(&lt;span class=&quot;string&quot;&gt;b&#39;bytes&#39;&lt;/span&gt;, encoding=&lt;span class=&quot;string&quot;&gt;&#39;UTF-8&#39;&lt;/span&gt;)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;basestring&lt;/code&gt; is removed, use &lt;code&gt;str&lt;/code&gt; as type: &lt;code&gt;isinstance(s, str)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bytes&lt;/code&gt; is immutable, the corresponding mutable version is &lt;code&gt;bytearray&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The default source file encoding is UTF-8 now.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>View Spark Source in Eclipse</title>
    <link href="http://shzhangji.com/blog/2015/09/01/view-spark-source-in-eclipse/"/>
    <id>http://shzhangji.com/blog/2015/09/01/view-spark-source-in-eclipse/</id>
    <published>2015-09-01T10:38:00.000Z</published>
    <updated>2017-03-09T05:48:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>Reading source code is a great way to learn opensource projects. I used to read Java projects’ source code on <a href="http://grepcode.com/" target="_blank" rel="external">GrepCode</a> for it is online and has very nice cross reference features. As for Scala projects such as <a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a>, though its source code can be found on <a href="https://github.com/apache/spark/" target="_blank" rel="external">GitHub</a>, it’s quite necessary to setup an IDE to view the code more efficiently. Here’s a howto of viewing Spark source code in Eclipse.</p>
<h2 id="Install-Eclipse-and-Scala-IDE-Plugin"><a href="#Install-Eclipse-and-Scala-IDE-Plugin" class="headerlink" title="Install Eclipse and Scala IDE Plugin"></a>Install Eclipse and Scala IDE Plugin</h2><p>One can download Eclipse from <a href="http://www.eclipse.org/downloads/" target="_blank" rel="external">here</a>. I recommend the “Eclipse IDE for Java EE Developers”, which contains a lot of daily-used features.</p>
<p><img src="/images/scala-ide.png" alt=""></p>
<p>Then go to Scala IDE’s <a href="http://scala-ide.org/download/current.html" target="_blank" rel="external">official site</a> and install the plugin through update site or zip archive.</p>
<h2 id="Generate-Project-File-with-Maven"><a href="#Generate-Project-File-with-Maven" class="headerlink" title="Generate Project File with Maven"></a>Generate Project File with Maven</h2><p>Spark is mainly built with Maven, so make sure you have Maven installed on your box, and download the latest Spark source code from <a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">here</a>, unarchive it, and execute the following command:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ mvn -am -pl core dependency:resolve eclipse:eclipse</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>This command does a bunch of things. First, it indicates what modules should be built. Spark is a large project with multiple modules. Currently we’re only interested in its core module, so <code>-pl</code> or <code>--projects</code> is used. <code>-am</code> or <code>--also-make</code> tells Maven to build core module’s dependencies as well. We can see the module list in output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[INFO] Scanning for projects...</div><div class="line">[INFO] ------------------------------------------------------------------------</div><div class="line">[INFO] Reactor Build Order:</div><div class="line">[INFO]</div><div class="line">[INFO] Spark Project Parent POM</div><div class="line">[INFO] Spark Launcher Project</div><div class="line">[INFO] Spark Project Networking</div><div class="line">[INFO] Spark Project Shuffle Streaming Service</div><div class="line">[INFO] Spark Project Unsafe</div><div class="line">[INFO] Spark Project Core</div></pre></td></tr></table></figure>
<p><code>dependency:resolve</code> tells Maven to download all dependencies. <code>eclipse:eclipse</code> will generate the <code>.project</code> and <code>.classpath</code> files for Eclipse. But the result is not perfect, both files need some fixes.</p>
<p>Edit <code>core/.classpath</code>, change the following two lines:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">classpathentry</span> <span class="attr">kind</span>=<span class="string">"src"</span> <span class="attr">path</span>=<span class="string">"src/main/scala"</span> <span class="attr">including</span>=<span class="string">"**/*.java"</span>/&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">classpathentry</span> <span class="attr">kind</span>=<span class="string">"src"</span> <span class="attr">path</span>=<span class="string">"src/test/scala"</span> <span class="attr">output</span>=<span class="string">"target/scala-2.10/test-classes"</span> <span class="attr">including</span>=<span class="string">"**/*.java"</span>/&gt;</span></div></pre></td></tr></table></figure>
<p>to</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">classpathentry</span> <span class="attr">kind</span>=<span class="string">"src"</span> <span class="attr">path</span>=<span class="string">"src/main/scala"</span> <span class="attr">including</span>=<span class="string">"**/*.java|**/*.scala"</span>/&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">classpathentry</span> <span class="attr">kind</span>=<span class="string">"src"</span> <span class="attr">path</span>=<span class="string">"src/test/scala"</span> <span class="attr">output</span>=<span class="string">"target/scala-2.10/test-classes"</span> <span class="attr">including</span>=<span class="string">"**/*.java|**/*.scala"</span>/&gt;</span></div></pre></td></tr></table></figure>
<p>Edit <code>core/.project</code>, make it looks like this:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">buildSpec</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">buildCommand</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>org.scala-ide.sdt.core.scalabuilder<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">buildCommand</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">buildSpec</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">natures</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">nature</span>&gt;</span>org.scala-ide.sdt.core.scalanature<span class="tag">&lt;/<span class="name">nature</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">nature</span>&gt;</span>org.eclipse.jdt.core.javanature<span class="tag">&lt;/<span class="name">nature</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">natures</span>&gt;</span></div></pre></td></tr></table></figure>
<p>Now you can import “Existing Projects into Workspace”, including <code>core</code>, <code>launcher</code>, <code>network</code>, and <code>unsafe</code>.</p>
<h2 id="Miscellaneous"><a href="#Miscellaneous" class="headerlink" title="Miscellaneous"></a>Miscellaneous</h2><h3 id="Access-restriction-The-type-‘Unsafe’-is-not-API"><a href="#Access-restriction-The-type-‘Unsafe’-is-not-API" class="headerlink" title="Access restriction: The type ‘Unsafe’ is not API"></a>Access restriction: The type ‘Unsafe’ is not API</h3><p>For module <code>spark-unsafe</code>, Eclipse will report an error “Access restriction: The type ‘Unsafe’ is not API (restriction on required library /path/to/jre/lib/rt.jar”. To fix this, right click the “JRE System Library” entry in Package Explorer, change it to “Workspace default JRE”.</p>
<h3 id="Download-Sources-and-Javadocs"><a href="#Download-Sources-and-Javadocs" class="headerlink" title="Download Sources and Javadocs"></a>Download Sources and Javadocs</h3><p>Add the following entry into pom’s project / build / plugins:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-eclipse-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">downloadSources</span>&gt;</span>true<span class="tag">&lt;/<span class="name">downloadSources</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">downloadJavadocs</span>&gt;</span>true<span class="tag">&lt;/<span class="name">downloadJavadocs</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="build-helper-maven-plugin"><a href="#build-helper-maven-plugin" class="headerlink" title="build-helper-maven-plugin"></a>build-helper-maven-plugin</h3><p>Since Spark is a mixture of Java and Scala code, and the maven-eclipse-plugin only knows about Java source files, so we need to use build-helper-maven-plugin to include the Scala sources, as is described <a href="http://docs.scala-lang.org/tutorials/scala-with-maven.html#integration-with-eclipse-scala-ide24" target="_blank" rel="external">here</a>. Fortunately, Spark’s pom.xml has already included this setting.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="http://docs.scala-lang.org/tutorials/scala-with-maven.html" target="_blank" rel="external">http://docs.scala-lang.org/tutorials/scala-with-maven.html</a></li>
<li><a href="https://wiki.scala-lang.org/display/SIW/ScalaEclipseMaven" target="_blank" rel="external">https://wiki.scala-lang.org/display/SIW/ScalaEclipseMaven</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Reading source code is a great way to learn opensource projects. I used to read Java projects’ source code on &lt;a href=&quot;http://grepcode.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;GrepCode&lt;/a&gt; for it is online and has very nice cross reference features. As for Scala projects such as &lt;a href=&quot;http://spark.apache.org&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Apache Spark&lt;/a&gt;, though its source code can be found on &lt;a href=&quot;https://github.com/apache/spark/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;GitHub&lt;/a&gt;, it’s quite necessary to setup an IDE to view the code more efficiently. Here’s a howto of viewing Spark source code in Eclipse.&lt;/p&gt;
&lt;h2 id=&quot;Install-Eclipse-and-Scala-IDE-Plugin&quot;&gt;&lt;a href=&quot;#Install-Eclipse-and-Scala-IDE-Plugin&quot; class=&quot;headerlink&quot; title=&quot;Install Eclipse and Scala IDE Plugin&quot;&gt;&lt;/a&gt;Install Eclipse and Scala IDE Plugin&lt;/h2&gt;&lt;p&gt;One can download Eclipse from &lt;a href=&quot;http://www.eclipse.org/downloads/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;. I recommend the “Eclipse IDE for Java EE Developers”, which contains a lot of daily-used features.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/scala-ide.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Then go to Scala IDE’s &lt;a href=&quot;http://scala-ide.org/download/current.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;official site&lt;/a&gt; and install the plugin through update site or zip archive.&lt;/p&gt;
&lt;h2 id=&quot;Generate-Project-File-with-Maven&quot;&gt;&lt;a href=&quot;#Generate-Project-File-with-Maven&quot; class=&quot;headerlink&quot; title=&quot;Generate Project File with Maven&quot;&gt;&lt;/a&gt;Generate Project File with Maven&lt;/h2&gt;&lt;p&gt;Spark is mainly built with Maven, so make sure you have Maven installed on your box, and download the latest Spark source code from &lt;a href=&quot;http://spark.apache.org/downloads.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;, unarchive it, and execute the following command:&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;$ mvn -am -pl core dependency:resolve eclipse:eclipse&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="spark" scheme="http://shzhangji.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming Logging Configuration</title>
    <link href="http://shzhangji.com/blog/2015/05/31/spark-streaming-logging-configuration/"/>
    <id>http://shzhangji.com/blog/2015/05/31/spark-streaming-logging-configuration/</id>
    <published>2015-05-31T10:18:00.000Z</published>
    <updated>2017-08-14T04:08:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>Spark Streaming applications tend to run forever, so their log files should be properly handled, to avoid exploding server hard drives. This article will give some practical advices of dealing with these log files, on both Spark on YARN and standalone mode.</p>
<h2 id="Log4j’s-RollingFileAppender"><a href="#Log4j’s-RollingFileAppender" class="headerlink" title="Log4j’s RollingFileAppender"></a>Log4j’s RollingFileAppender</h2><p>Spark uses log4j as logging facility. The default configuraiton is to write all logs into standard error, which is fine for batch jobs. But for streaming jobs, we’d better use rolling-file appender, to cut log files by size and keep only several recent files. Here’s an example:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">log4j.rootLogger=INFO, rolling</div><div class="line"></div><div class="line">log4j.appender.rolling=org.apache.log4j.RollingFileAppender</div><div class="line">log4j.appender.rolling.layout=org.apache.log4j.PatternLayout</div><div class="line">log4j.appender.rolling.layout.conversionPattern=[%d] %p %m (%c)%n</div><div class="line">log4j.appender.rolling.maxFileSize=50MB</div><div class="line">log4j.appender.rolling.maxBackupIndex=5</div><div class="line">log4j.appender.rolling.file=/var/log/spark/$&#123;dm.logging.name&#125;.log</div><div class="line">log4j.appender.rolling.encoding=UTF-8</div><div class="line"></div><div class="line">log4j.logger.org.apache.spark=WARN</div><div class="line">log4j.logger.org.eclipse.jetty=WARN</div><div class="line"></div><div class="line">log4j.logger.com.shzhangji.dm=$&#123;dm.logging.level&#125;</div></pre></td></tr></table></figure>
<p>This means log4j will roll the log file by 50MB and keep only 5 recent files. These files are saved in <code>/var/log/spark</code> directory, with filename picked from system property <code>dm.logging.name</code>. We also set the logging level of our package <code>com.shzhangji.dm</code> according to <code>dm.logging.level</code> property. Another thing to mention is that we set <code>org.apache.spark</code> to level <code>WARN</code>, so as to ignore verbose logs from spark.</p>
<a id="more"></a>
<h2 id="Standalone-Mode"><a href="#Standalone-Mode" class="headerlink" title="Standalone Mode"></a>Standalone Mode</h2><p>In standalone mode, Spark Streaming driver is running on the machine where you submit the job, and each Spark worker node will run an executor for this job. So you need to setup log4j for both driver and executor.</p>
<p>For driver, since it’s a long-running application, we tend to use some process management tools like <a href="http://supervisord.org/" target="_blank" rel="external">supervisor</a> to monitor it. And supervisor itself provides the facility of rolling log files, so we can safely write all logs into standard output when setting up driver’s log4j.</p>
<p>For executor, there’re two approaches. One is using <code>spark.executor.logs.rolling.strategy</code> provided by Spark 1.1 and above. It has both time-based and size-based rolling methods. These log files are stored in Spark’s work directory. You can find more details in the <a href="https://spark.apache.org/docs/1.1.0/configuration.html" target="_blank" rel="external">documentation</a>.</p>
<p>The other approach is to setup log4j manually, when you’re using a legacy version, or want to gain more control on the logging process. Here are the steps:</p>
<ol>
<li>Make sure the logging directory exists on all worker nodes. You can use some provisioning tools like <a href="https://github.com/ansible/ansible" target="_blank" rel="external">ansbile</a> to create them.</li>
<li>Create driver’s and executor’s log4j configuration files, and distribute the executor’s to all worker nodes.</li>
<li>Use the above two files in <code>spark-submit</code> command:</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">spark-submit</div><div class="line">  --master spark://127.0.0.1:7077</div><div class="line">  --driver-java-options &quot;-Dlog4j.configuration=file:/path/to/log4j-driver.properties -Ddm.logging.level=DEBUG&quot;</div><div class="line">  --conf &quot;spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/path/to/log4j-executor.properties -Ddm.logging.name=myapp -Ddm.logging.level=DEBUG&quot;</div><div class="line">  ...</div></pre></td></tr></table></figure>
<h2 id="Spark-on-YARN"><a href="#Spark-on-YARN" class="headerlink" title="Spark on YARN"></a>Spark on YARN</h2><p><a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/index.html" target="_blank" rel="external">YARN</a> is a <strong>resource manager</strong> introduced by Hadoop2. Now we can run differenct computational frameworks on the same cluster, like MapReduce, Spark, Storm, etc. The basic unit of YARN is called container, which represents a certain amount of resource (currently memory and virtual CPU cores). Every container has its working directory, and all related files such as application command (jars) and log files are stored in this directory.</p>
<p>When running Spark on YARN, there is a system property <code>spark.yarn.app.container.log.dir</code> indicating the container’s log directory. We only need to replace one line of the above log4j config:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">log4j.appender.rolling.file=$&#123;spark.yarn.app.container.log.dir&#125;/spark.log</div></pre></td></tr></table></figure>
<p>And these log files can be viewed on YARN’s web UI:</p>
<p><img src="/images/spark/yarn-logs.png" alt=""></p>
<p>The <code>spark-submit</code> command is as following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">spark-submit</div><div class="line">  --master yarn-cluster</div><div class="line">  --files /path/to/log4j-spark.properties</div><div class="line">  --conf &quot;spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j-spark.properties&quot;</div><div class="line">  --conf &quot;spark.executor.extraJavaOptions=-Dlog4j.configuration=log4j-spark.properties&quot;</div><div class="line">  ...</div></pre></td></tr></table></figure>
<p>As you can see, both driver and executor use the same configuration file. That is because in <code>yarn-cluster</code> mode, driver is also run as a container in YARN. In fact, the <code>spark-submit</code> command will just quit after job submission.</p>
<p>If YARN’s <a href="http://zh.hortonworks.com/blog/simplifying-user-logs-management-and-access-in-yarn/" target="_blank" rel="external">log aggregation</a> is enabled, application logs will be saved in HDFS after the job is done. One can use <code>yarn logs</code> command to view the files or browse directly into HDFS directory indicated by <code>yarn.nodemanager.log-dirs</code>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark Streaming applications tend to run forever, so their log files should be properly handled, to avoid exploding server hard drives. This article will give some practical advices of dealing with these log files, on both Spark on YARN and standalone mode.&lt;/p&gt;
&lt;h2 id=&quot;Log4j’s-RollingFileAppender&quot;&gt;&lt;a href=&quot;#Log4j’s-RollingFileAppender&quot; class=&quot;headerlink&quot; title=&quot;Log4j’s RollingFileAppender&quot;&gt;&lt;/a&gt;Log4j’s RollingFileAppender&lt;/h2&gt;&lt;p&gt;Spark uses log4j as logging facility. The default configuraiton is to write all logs into standard error, which is fine for batch jobs. But for streaming jobs, we’d better use rolling-file appender, to cut log files by size and keep only several recent files. Here’s an example:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;log4j.rootLogger=INFO, rolling&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling=org.apache.log4j.RollingFileAppender&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.layout=org.apache.log4j.PatternLayout&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.layout.conversionPattern=[%d] %p %m (%c)%n&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.maxFileSize=50MB&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.maxBackupIndex=5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.file=/var/log/spark/$&amp;#123;dm.logging.name&amp;#125;.log&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.encoding=UTF-8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.logger.org.apache.spark=WARN&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.logger.org.eclipse.jetty=WARN&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.logger.com.shzhangji.dm=$&amp;#123;dm.logging.level&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;This means log4j will roll the log file by 50MB and keep only 5 recent files. These files are saved in &lt;code&gt;/var/log/spark&lt;/code&gt; directory, with filename picked from system property &lt;code&gt;dm.logging.name&lt;/code&gt;. We also set the logging level of our package &lt;code&gt;com.shzhangji.dm&lt;/code&gt; according to &lt;code&gt;dm.logging.level&lt;/code&gt; property. Another thing to mention is that we set &lt;code&gt;org.apache.spark&lt;/code&gt; to level &lt;code&gt;WARN&lt;/code&gt;, so as to ignore verbose logs from spark.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="spark" scheme="http://shzhangji.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch Performance Tips</title>
    <link href="http://shzhangji.com/blog/2015/04/28/elasticsearch-performance-tips/"/>
    <id>http://shzhangji.com/blog/2015/04/28/elasticsearch-performance-tips/</id>
    <published>2015-04-28T15:08:00.000Z</published>
    <updated>2017-09-15T08:59:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>Recently we’re using ElasticSearch as a data backend of our recommendation API, to serve both offline and online computed data to users. Thanks to ElasticSearch’s rich and out-of-the-box functionality, it doesn’t take much trouble to setup the cluster. However, we still encounter some misuse and unwise configurations. So here’s a list of ElasticSearch performance tips that we learned from practice.</p>
<h2 id="Tip-1-Set-Num-of-shards-to-Num-of-nodes"><a href="#Tip-1-Set-Num-of-shards-to-Num-of-nodes" class="headerlink" title="Tip 1 Set Num-of-shards to Num-of-nodes"></a>Tip 1 Set Num-of-shards to Num-of-nodes</h2><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard" target="_blank" rel="external">Shard</a> is the foundation of ElasticSearch’s distribution capability. Every index is splitted into several shards (default 5) and are distributed across cluster nodes. But this capability does not come free. Since data being queried reside in all shards (this behaviour can be changed by <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-routing" target="_blank" rel="external">routing</a>), ElasticSearch has to run this query on every shard, fetch the result, and merge them, like a map-reduce process. So if there’re too many shards, more than the number of cluter nodes, the query will be executed more than once on the same node, and it’ll also impact the merge phase. On the other hand, too few shards will also reduce the performance, for not all nodes are being utilized.</p>
<p>Shards have two roles, primary shard and replica shard. Replica shard serves as a backup to the primary shard. When primary goes down, the replica takes its job. It also helps improving the search and get performance, for these requests can be executed on either primary or replica shard.</p>
<p>Shards can be visualized by <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard" target="_blank" rel="external">elasticsearch-head</a> plugin:</p>
<p><img src="/images/elasticsearch/shards-head.png" alt=""></p>
<p>The <code>cu_docs</code> index has two shards <code>0</code> and <code>1</code>, with <code>number_of_replicas</code> set to 1. Primary shard <code>0</code> (bold bordered) resides in server <code>Leon</code>, and its replica in <code>Pris</code>. They are green becuase all primary shards have enough repicas sitting in different servers, so the cluster is healthy.</p>
<p>Since <code>number_of_shards</code> of an index cannot be changed after creation (while <code>number_of_replicas</code> can), one should choose this config wisely. Here are some suggestions:</p>
<ol>
<li>How many nodes do you have, now and future? If you’re sure you’ll only have 3 nodes, set number of shards to 2 and replicas to 1, so there’ll be 4 shards across 3 nodes. If you’ll add some servers in the future, you can set number of shards to 3, so when the cluster grows to 5 nodes, there’ll be 6 distributed shards.</li>
<li>How big is your index? If it’s small, one shard with one replica will due.</li>
<li>How is the read and write frequency, respectively? If it’s search heavy, setup more relicas.</li>
</ol>
<a id="more"></a>
<h2 id="Tip-2-Tuning-Memory-Usage"><a href="#Tip-2-Tuning-Memory-Usage" class="headerlink" title="Tip 2 Tuning Memory Usage"></a>Tip 2 Tuning Memory Usage</h2><p>ElasticSearch and its backend <a href="http://lucene.apache.org/" target="_blank" rel="external">Lucene</a> are both Java application. There’re various memory tuning settings related to heap and native memory.</p>
<h3 id="Set-Max-Heap-Size-to-Half-of-Total-Memory"><a href="#Set-Max-Heap-Size-to-Half-of-Total-Memory" class="headerlink" title="Set Max Heap Size to Half of Total Memory"></a>Set Max Heap Size to Half of Total Memory</h3><p>Generally speaking, more heap memory leads to better performance. But in ElasticSearch’s case, Lucene also requires a lot of native memory (or off-heap memory), to store index segments and provide fast search performance. But it does not load the files by itself. Instead, it relies on the operating system to cache the segement files in memory.</p>
<p>Say we have 16G memory and set -Xmx to 8G, it doesn’t mean the remaining 8G is wasted. Except for the memory OS preserves for itself, it will cache the frequently accessed disk files in memory automatically, which results in a huge performance gain.</p>
<p>Do not set heap size over 32G though, even you have more than 64G memory. The reason is described in <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops" target="_blank" rel="external">this link</a>.</p>
<p>Also, you should probably set -Xms to 8G as well, to avoid the overhead of heap memory growth.</p>
<h3 id="Disable-Swapping"><a href="#Disable-Swapping" class="headerlink" title="Disable Swapping"></a>Disable Swapping</h3><p>Swapping is a way to move unused program code and data to disk so as to provide more space for running applications and file caching. It also provides a buffer for the system to recover from memory exhaustion. But for critical application like ElasticSearch, being swapped is definitely a performance killer.</p>
<p>There’re several ways to disable swapping, and our choice is setting <code>bootstrap.mlockall</code> to true. This tells ElasticSearch to lock its memory space in RAM so that OS will not swap it out. One can confirm this setting via <code>http://localhost:9200/_nodes/process?pretty</code>.</p>
<p>If ElasticSearch is not started as root (and it probably shouldn’t), this setting may not take effect. For Ubuntu server, one needs to add <code>&lt;user&gt; hard memlock unlimited</code> to <code>/etc/security/limits.conf</code>, and run <code>ulimit -l unlimited</code> before starting ElasticSearch process.</p>
<h3 id="Increase-mmap-Counts"><a href="#Increase-mmap-Counts" class="headerlink" title="Increase mmap Counts"></a>Increase <code>mmap</code> Counts</h3><p>ElasticSearch uses memory mapped files, and the default <code>mmap</code> counts is low. Add <code>vm.max_map_count=262144</code> to <code>/etc/sysctl.conf</code>, run <code>sysctl -p /etc/sysctl.conf</code> as root, and then restart ElasticSearch.</p>
<h2 id="Tip-3-Setup-a-Cluster-with-Unicast"><a href="#Tip-3-Setup-a-Cluster-with-Unicast" class="headerlink" title="Tip 3 Setup a Cluster with Unicast"></a>Tip 3 Setup a Cluster with Unicast</h2><p>ElasticSearch has two options to form a cluster, multicast and unicast. The former is suitable when you have a large group of servers and a well configured network. But we found unicast more concise and less error-prone.</p>
<p>Here’s an example of using unicast:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">node.name: &quot;NODE-1&quot;</div><div class="line">discovery.zen.ping.multicast.enabled: false</div><div class="line">discovery.zen.ping.unicast.hosts: [&quot;node-1.example.com&quot;, &quot;node-2.example.com&quot;, &quot;node-3.example.com&quot;]</div><div class="line">discovery.zen.minimum_master_nodes: 2</div></pre></td></tr></table></figure>
<p>The <code>discovery.zen.minimum_master_nodes</code> setting is a way to prevent split-brain symptom, i.e. more than one node thinks itself the master of the cluster. And for this setting to work, you should have an odd number of nodes, and set this config to <code>ceil(num_of_nodes / 2)</code>. In the above cluster, you can lose at most one node. It’s much like a quorum in <a href="http://zookeeper.apache.org" target="_blank" rel="external">Zookeeper</a>.</p>
<h2 id="Tip-4-Disable-Unnecessary-Features"><a href="#Tip-4-Disable-Unnecessary-Features" class="headerlink" title="Tip 4 Disable Unnecessary Features"></a>Tip 4 Disable Unnecessary Features</h2><p>ElasticSearch is a full-featured search engine, but you should always tailor it to your own needs. Here’s a brief list:</p>
<ul>
<li>Use corrent index type. There’re <code>index</code>, <code>not_analyzed</code>, and <code>no</code>. If you don’t need to search the field, set it to <code>no</code>; if you only search for full match, use <code>not_analyzed</code>.</li>
<li>For search-only fields, set <code>store</code> to false.</li>
<li>Disable <code>_all</code> field, if you always know which field to search.</li>
<li>Disable <code>_source</code> fields, if documents are big and you don’t need the update capability.</li>
<li>If you have a document key, set this field in <code>_id</code> - <code>path</code>, instead of index the field twice.</li>
<li>Set <code>index.refresh_interval</code> to a larger number (default 1s), if you don’t need near-realtime search. It’s also an important option in bulk-load operation described below.</li>
</ul>
<h2 id="Tip-5-Use-Bulk-Operations"><a href="#Tip-5-Use-Bulk-Operations" class="headerlink" title="Tip 5 Use Bulk Operations"></a>Tip 5 Use Bulk Operations</h2><p><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/bulk.html" target="_blank" rel="external">Bulk is cheaper</a></p>
<ul>
<li>Bulk Read<ul>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html" target="_blank" rel="external">Multi Get</a> to retrieve multiple documents by a list of ids.</li>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html" target="_blank" rel="external">Scroll</a> to search a large number of documents.</li>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/1.4/msearch.html" target="_blank" rel="external">MultiSearch api</a> to run search requests in parallel.</li>
</ul>
</li>
<li>Bulk Write<ul>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/1.4/bulk.html" target="_blank" rel="external">Bulk API</a> to index, update, delete multiple documents.</li>
<li>Alter <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html" target="_blank" rel="external">index aliases</a> simultaneously.</li>
</ul>
</li>
<li>Bulk Load: when initially building a large index, do the following,<ul>
<li>Set <code>number_of_relicas</code> to 0, so no relicas will be created;</li>
<li>Set <code>index.refresh_interval</code> to -1, disabling nrt search;</li>
<li>Bulk build the documents;</li>
<li>Call <code>optimize</code> on the index, so newly built docs are available for search;</li>
<li>Reset replicas and refresh interval, let ES cluster recover to green.</li>
</ul>
</li>
</ul>
<h2 id="Miscellaneous"><a href="#Miscellaneous" class="headerlink" title="Miscellaneous"></a>Miscellaneous</h2><ul>
<li>File descriptors: system default is too small for ES, set it to 64K will be OK. If <code>ulimit -n 64000</code> does not work, you need to add <code>&lt;user&gt; hard nofile 64000</code> to <code>/etc/security/limits.conf</code>, just like the <code>memlock</code> setting mentioned above.</li>
<li>When using ES client library, it will create a lot of worker threads according to the number of processors. Sometimes it’s not necessary. This behaviour can be changed by setting <code>processors</code> to a lower value like 2:</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> settings = <span class="type">ImmutableSettings</span>.settingsBuilder()</div><div class="line">    .put(<span class="string">"cluster.name"</span>, <span class="string">"elasticsearch"</span>)</div><div class="line">    .put(<span class="string">"processors"</span>, <span class="number">2</span>)</div><div class="line">    .build()</div><div class="line"><span class="keyword">val</span> uri = <span class="type">ElasticsearchClientUri</span>(<span class="string">"elasticsearch://127.0.0.1:9300"</span>)</div><div class="line"><span class="type">ElasticClient</span>.remote(settings, uri)</div></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html</a></li>
<li><a href="http://cpratt.co/how-many-shards-should-elasticsearch-indexes-have/" target="_blank" rel="external">http://cpratt.co/how-many-shards-should-elasticsearch-indexes-have/</a></li>
<li><a href="https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing" target="_blank" rel="external">https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing</a></li>
<li><a href="https://www.loggly.com/blog/nine-tips-configuring-elasticsearch-for-high-performance/" target="_blank" rel="external">https://www.loggly.com/blog/nine-tips-configuring-elasticsearch-for-high-performance/</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Recently we’re using ElasticSearch as a data backend of our recommendation API, to serve both offline and online computed data to users. Thanks to ElasticSearch’s rich and out-of-the-box functionality, it doesn’t take much trouble to setup the cluster. However, we still encounter some misuse and unwise configurations. So here’s a list of ElasticSearch performance tips that we learned from practice.&lt;/p&gt;
&lt;h2 id=&quot;Tip-1-Set-Num-of-shards-to-Num-of-nodes&quot;&gt;&lt;a href=&quot;#Tip-1-Set-Num-of-shards-to-Num-of-nodes&quot; class=&quot;headerlink&quot; title=&quot;Tip 1 Set Num-of-shards to Num-of-nodes&quot;&gt;&lt;/a&gt;Tip 1 Set Num-of-shards to Num-of-nodes&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Shard&lt;/a&gt; is the foundation of ElasticSearch’s distribution capability. Every index is splitted into several shards (default 5) and are distributed across cluster nodes. But this capability does not come free. Since data being queried reside in all shards (this behaviour can be changed by &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-routing&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;routing&lt;/a&gt;), ElasticSearch has to run this query on every shard, fetch the result, and merge them, like a map-reduce process. So if there’re too many shards, more than the number of cluter nodes, the query will be executed more than once on the same node, and it’ll also impact the merge phase. On the other hand, too few shards will also reduce the performance, for not all nodes are being utilized.&lt;/p&gt;
&lt;p&gt;Shards have two roles, primary shard and replica shard. Replica shard serves as a backup to the primary shard. When primary goes down, the replica takes its job. It also helps improving the search and get performance, for these requests can be executed on either primary or replica shard.&lt;/p&gt;
&lt;p&gt;Shards can be visualized by &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;elasticsearch-head&lt;/a&gt; plugin:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/elasticsearch/shards-head.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;cu_docs&lt;/code&gt; index has two shards &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, with &lt;code&gt;number_of_replicas&lt;/code&gt; set to 1. Primary shard &lt;code&gt;0&lt;/code&gt; (bold bordered) resides in server &lt;code&gt;Leon&lt;/code&gt;, and its replica in &lt;code&gt;Pris&lt;/code&gt;. They are green becuase all primary shards have enough repicas sitting in different servers, so the cluster is healthy.&lt;/p&gt;
&lt;p&gt;Since &lt;code&gt;number_of_shards&lt;/code&gt; of an index cannot be changed after creation (while &lt;code&gt;number_of_replicas&lt;/code&gt; can), one should choose this config wisely. Here are some suggestions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How many nodes do you have, now and future? If you’re sure you’ll only have 3 nodes, set number of shards to 2 and replicas to 1, so there’ll be 4 shards across 3 nodes. If you’ll add some servers in the future, you can set number of shards to 3, so when the cluster grows to 5 nodes, there’ll be 6 distributed shards.&lt;/li&gt;
&lt;li&gt;How big is your index? If it’s small, one shard with one replica will due.&lt;/li&gt;
&lt;li&gt;How is the read and write frequency, respectively? If it’s search heavy, setup more relicas.&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="elasticsearch" scheme="http://shzhangji.com/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Use WebJars in Scalatra Project</title>
    <link href="http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project/"/>
    <id>http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project/</id>
    <published>2014-05-27T09:44:00.000Z</published>
    <updated>2017-03-09T05:48:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>As I’m working with my first <a href="http://www.scalatra.org/" target="_blank" rel="external">Scalatra</a> project, I automatically think of using <a href="http://www.webjars.org/" target="_blank" rel="external">WebJars</a> to manage Javascript library dependencies, since it’s more convenient and seems like a good practice. Though there’s no <a href="http://www.webjars.org/documentation" target="_blank" rel="external">official support</a> for Scalatra framework, the installation process is not very complex. But this doesn’t mean I didn’t spend much time on this. I’m still a newbie to Scala, and there’s only a few materials on this subject.</p>
<h2 id="Add-WebJars-Dependency-in-SBT-Build-File"><a href="#Add-WebJars-Dependency-in-SBT-Build-File" class="headerlink" title="Add WebJars Dependency in SBT Build File"></a>Add WebJars Dependency in SBT Build File</h2><p>Scalatra uses <code>.scala</code> configuration file instead of <code>.sbt</code>, so let’s add dependency into <code>project/build.scala</code>. Take <a href="http://dojotoolkit.org/" target="_blank" rel="external">Dojo</a> for example.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">DwExplorerBuild</span> <span class="keyword">extends</span> <span class="title">Build</span> </span>&#123;</div><div class="line">  ...</div><div class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> project = <span class="type">Project</span> (</div><div class="line">    ...</div><div class="line">    settings = <span class="type">Defaults</span>.defaultSettings ++ <span class="type">ScalatraPlugin</span>.scalatraWithJRebel ++ scalateSettings ++ <span class="type">Seq</span>(</div><div class="line">      ...</div><div class="line">      libraryDependencies ++= <span class="type">Seq</span>(</div><div class="line">        ...</div><div class="line">        <span class="string">"org.webjars"</span> % <span class="string">"dojo"</span> % <span class="string">"1.9.3"</span></div><div class="line">      ),</div><div class="line">      ...</div><div class="line">    )</div><div class="line">  )</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>To view this dependency in Eclipse, you need to run <code>sbt eclipse</code> again. In the <em>Referenced Libraries</em> section, you can see a <code>dojo-1.9.3.jar</code>, and the library lies in <code>META-INF/resources/webjars/</code>.</p>
<a id="more"></a>
<h2 id="Add-a-Route-for-WebJars-Resources"><a href="#Add-a-Route-for-WebJars-Resources" class="headerlink" title="Add a Route for WebJars Resources"></a>Add a Route for WebJars Resources</h2><p>Find the <code>ProjectNameStack.scala</code> file and add the following lines at the bottom of the trait:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">trait</span> <span class="title">ProjectNameStack</span> <span class="keyword">extends</span> <span class="title">ScalatraServlet</span> <span class="keyword">with</span> <span class="title">ScalateSupport</span> </span>&#123;</div><div class="line">  ...</div><div class="line">  get(<span class="string">"/webjars/*"</span>) &#123;</div><div class="line">    <span class="keyword">val</span> resourcePath = <span class="string">"/META-INF/resources/webjars/"</span> + params(<span class="string">"splat"</span>)</div><div class="line">    <span class="type">Option</span>(getClass.getResourceAsStream(resourcePath)) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(inputStream) =&gt; &#123;</div><div class="line">        contentType = servletContext.getMimeType(resourcePath)</div><div class="line">        <span class="type">IOUtil</span>.loadBytes(inputStream)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; resourceNotFound()</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>That’s it!</strong> Now you can refer to the WebJars resources in views, like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#set (title)</div><div class="line">Hello, Dojo!</div><div class="line">#end</div><div class="line"></div><div class="line">&lt;div id=&quot;greeting&quot;&gt;&lt;/div&gt;</div><div class="line"></div><div class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;$&#123;uri(&quot;/webjars/dojo/1.9.3/dojo/dojo.js&quot;)&#125;&quot; data-dojo-config=&quot;async: true&quot;&gt;&lt;/script&gt;</div><div class="line">&lt;script type=&quot;text/javascript&quot;&gt;</div><div class="line">require([</div><div class="line">    &apos;dojo/dom&apos;,</div><div class="line">    &apos;dojo/dom-construct&apos;</div><div class="line">], function (dom, domConstruct) &#123;</div><div class="line">    var greetingNode = dom.byId(&apos;greeting&apos;);</div><div class="line">    domConstruct.place(&apos;&lt;i&gt;Dojo!&lt;/i&gt;&apos;, greetingNode);</div><div class="line">&#125;);</div><div class="line">&lt;/script&gt;</div></pre></td></tr></table></figure>
<h3 id="Some-Explanations-on-This-Route"><a href="#Some-Explanations-on-This-Route" class="headerlink" title="Some Explanations on This Route"></a>Some Explanations on This Route</h3><ul>
<li><code>/webjars/*</code> is a <a href="http://www.scalatra.org/2.2/guides/http/routes.html#toc_233" target="_blank" rel="external">Wildcards</a> and <code>params(&quot;splat&quot;)</code> is to extract the asterisk part.</li>
<li><code>resourcePath</code> points to the WebJars resources in the jar file, as we saw in Eclipse. It is then fetched as an <code>InputStream</code> with <code>getResourceAsStream()</code>.</li>
<li><code>servletContext.getMimeType()</code> is a handy method to determine the content type of the requested resource, instead of parsing it by ourselves. I find this in SpringMVC’s <a href="http://grepcode.com/file/repo1.maven.org/maven2/org.springframework/spring-webmvc/3.2.7.RELEASE/org/springframework/web/servlet/resource/ResourceHttpRequestHandler.java#ResourceHttpRequestHandler.handleRequest%28javax.servlet.http.HttpServletRequest%2Cjavax.servlet.http.HttpServletResponse%29" target="_blank" rel="external">ResourceHttpRequestHandler</a>.</li>
<li><code>IOUtil</code> is a utiliy class that comes with <a href="http://scalate.fusesource.org/" target="_blank" rel="external">Scalate</a>, so don’t forget to import it first.</li>
</ul>
<p>At first I tried to figure out whether Scalatra provides a conveniet way to serve static files in classpath, I failed. So I decided to serve them by my own, and <a href="https://gist.github.com/laurilehmijoki/4483113" target="_blank" rel="external">this gist</a> was very helpful.</p>
<p>Anyway, I’ve spent more than half a day to solve this problem, and it turned out to be a very challenging yet interesting way to learn a new language, new framework, and new tools. Keep moving!</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;As I’m working with my first &lt;a href=&quot;http://www.scalatra.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Scalatra&lt;/a&gt; project, I automatically think of using &lt;a href=&quot;http://www.webjars.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;WebJars&lt;/a&gt; to manage Javascript library dependencies, since it’s more convenient and seems like a good practice. Though there’s no &lt;a href=&quot;http://www.webjars.org/documentation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;official support&lt;/a&gt; for Scalatra framework, the installation process is not very complex. But this doesn’t mean I didn’t spend much time on this. I’m still a newbie to Scala, and there’s only a few materials on this subject.&lt;/p&gt;
&lt;h2 id=&quot;Add-WebJars-Dependency-in-SBT-Build-File&quot;&gt;&lt;a href=&quot;#Add-WebJars-Dependency-in-SBT-Build-File&quot; class=&quot;headerlink&quot; title=&quot;Add WebJars Dependency in SBT Build File&quot;&gt;&lt;/a&gt;Add WebJars Dependency in SBT Build File&lt;/h2&gt;&lt;p&gt;Scalatra uses &lt;code&gt;.scala&lt;/code&gt; configuration file instead of &lt;code&gt;.sbt&lt;/code&gt;, so let’s add dependency into &lt;code&gt;project/build.scala&lt;/code&gt;. Take &lt;a href=&quot;http://dojotoolkit.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Dojo&lt;/a&gt; for example.&lt;/p&gt;
&lt;figure class=&quot;highlight scala&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;DwExplorerBuild&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Build&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;lazy&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;val&lt;/span&gt; project = &lt;span class=&quot;type&quot;&gt;Project&lt;/span&gt; (&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    settings = &lt;span class=&quot;type&quot;&gt;Defaults&lt;/span&gt;.defaultSettings ++ &lt;span class=&quot;type&quot;&gt;ScalatraPlugin&lt;/span&gt;.scalatraWithJRebel ++ scalateSettings ++ &lt;span class=&quot;type&quot;&gt;Seq&lt;/span&gt;(&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      libraryDependencies ++= &lt;span class=&quot;type&quot;&gt;Seq&lt;/span&gt;(&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;string&quot;&gt;&quot;org.webjars&quot;&lt;/span&gt; % &lt;span class=&quot;string&quot;&gt;&quot;dojo&quot;&lt;/span&gt; % &lt;span class=&quot;string&quot;&gt;&quot;1.9.3&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      ),&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    )&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  )&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;To view this dependency in Eclipse, you need to run &lt;code&gt;sbt eclipse&lt;/code&gt; again. In the &lt;em&gt;Referenced Libraries&lt;/em&gt; section, you can see a &lt;code&gt;dojo-1.9.3.jar&lt;/code&gt;, and the library lies in &lt;code&gt;META-INF/resources/webjars/&lt;/code&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="scala" scheme="http://shzhangji.com/tags/scala/"/>
    
      <category term="scalatra" scheme="http://shzhangji.com/tags/scalatra/"/>
    
      <category term="webjars" scheme="http://shzhangji.com/tags/webjars/"/>
    
  </entry>
  
  <entry>
    <title>Generate Auto-increment Id in Map-reduce Job</title>
    <link href="http://shzhangji.com/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job/"/>
    <id>http://shzhangji.com/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job/</id>
    <published>2013-10-31T01:35:00.000Z</published>
    <updated>2017-07-15T10:58:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>In DBMS world, it’s easy to generate a unique, auto-increment id, using MySQL’s <a href="http://dev.mysql.com/doc/refman/5.1/en/example-auto-increment.html" target="_blank" rel="external">AUTO_INCREMENT attribute</a> on a primary key or MongoDB’s <a href="http://docs.mongodb.org/manual/tutorial/create-an-auto-incrementing-field/" target="_blank" rel="external">Counters Collection</a> pattern. But when it comes to a distributed, parallel processing framework, like Hadoop Map-reduce, it is not that straight forward. The best solution to identify every record in such framework is to use UUID. But when an integer id is required, it’ll take some steps.</p>
<h2 id="Solution-A-Single-Reducer"><a href="#Solution-A-Single-Reducer" class="headerlink" title="Solution A: Single Reducer"></a>Solution A: Single Reducer</h2><p>This is the most obvious and simple one, just use the following code to specify reducer numbers to 1:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">job.setNumReduceTasks(<span class="number">1</span>);</div></pre></td></tr></table></figure>
<p>And also obvious, there are several demerits:</p>
<ol>
<li>All mappers output will be copied to one task tracker.</li>
<li>Only one process is working on shuffel &amp; sort.</li>
<li>When producing output, there’s also only one process.</li>
</ol>
<p>The above is not a problem for small data sets, or at least small mapper outputs. And it is also the approach that Pig and Hive use when they need to perform a total sort. But when hitting a certain threshold, the sort and copy phase will become very slow and unacceptable.</p>
<a id="more"></a>
<h2 id="Solution-B-Increment-by-Number-of-Tasks"><a href="#Solution-B-Increment-by-Number-of-Tasks" class="headerlink" title="Solution B: Increment by Number of Tasks"></a>Solution B: Increment by Number of Tasks</h2><p>Inspired by a <a href="http://mail-archives.apache.org/mod_mbox/hadoop-common-user/200904.mbox/%3C49E13557.7090504@domaintools.com%3E" target="_blank" rel="external">mailing list</a> that is quite hard to find, which is inspired by MySQL master-master setup (with auto_increment_increment and auto_increment_offset), there’s a brilliant way to generate a globally unique integer id across mappers or reducers. Let’s take mapper for example:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">long</span> id;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">int</span> increment;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException,</span></div><div class="line"><span class="function">            InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>.setup(context);</div><div class="line"></div><div class="line">        id = context.getTaskAttemptID().getTaskID().getId();</div><div class="line">        increment = context.getConfiguration().getInt(<span class="string">"mapred.map.tasks"</span>, <span class="number">0</span>);</div><div class="line">        <span class="keyword">if</span> (increment == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"mapred.map.tasks is zero"</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></div><div class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        id += increment;</div><div class="line">        context.write(<span class="keyword">new</span> LongWritable(id),</div><div class="line">                <span class="keyword">new</span> Text(String.format(<span class="string">"%d, %s"</span>, key.get(), value.toString())));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>The basic idea is simple:</p>
<ol>
<li>Set the initial id to current tasks’s id.</li>
<li>When mapping each row, increment the id by the number of tasks.</li>
</ol>
<p>It’s also applicable to reducers.</p>
<h2 id="Solution-C-Sorted-Auto-increment-Id"><a href="#Solution-C-Sorted-Auto-increment-Id" class="headerlink" title="Solution C: Sorted Auto-increment Id"></a>Solution C: Sorted Auto-increment Id</h2><p>Here’s a real senario: we have several log files pulled from different machines, and we want to identify each row by an auto-increment id, and they should be in time sequence order.</p>
<p>We know Hadoop has a sort phase, so we can use timestamp as the mapper output key, and the framework will do the trick. But the sorting thing happends in one reducer (partition, in fact), so when using multiple reducer tasks, the result is not in total order. To achieve this, we can use the <a href="http://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.html" target="_blank" rel="external">TotalOrderPartitioner</a>.</p>
<p>How about the incremental id? Even though the outputs are in total order, Solution B is not applicable here. So we take another approach: seperate the job in two phases, use the reducer to do sorting <em>and</em> counting, then use the second mapper to generate the id.</p>
<p>Here’s what we gonna do:</p>
<ol>
<li>Use TotalOrderPartitioner, and generate the partition file.</li>
<li>Parse logs in mapper A, use time as the output key.</li>
<li>Let the framework do partitioning and sorting.</li>
<li>Count records in reducer, write it with <a href="http://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.html" target="_blank" rel="external">MultipleOutput</a>.</li>
<li>In mapper B, use count as offset, and increment by 1.</li>
</ol>
<p>To simplify the situation, we assume to have the following inputs and outputs:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"> Input       Output</div><div class="line"></div><div class="line">11:00 a     1 11:00 a</div><div class="line">12:00 b     2 11:01 aa</div><div class="line">13:00 c     3 11:02 aaa</div><div class="line"></div><div class="line">11:01 aa    4 12:00 b</div><div class="line">12:01 bb    5 12:01 bb</div><div class="line">13:01 cc    6 12:02 bbb</div><div class="line"></div><div class="line">11:02 aaa   7 13:00 c</div><div class="line">12:02 bbb   8 13:01 cc</div><div class="line">13:02 ccc   9 13:02 ccc</div></pre></td></tr></table></figure>
<h3 id="Generate-Partition-File"><a href="#Generate-Partition-File" class="headerlink" title="Generate Partition File"></a>Generate Partition File</h3><p>To use TotalOrderpartitioner, we need a partition file (i.e. boundaries) to tell the partitioner how to partition the mapper outputs. Usually we’ll use <a href="https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapreduce/lib/partition/InputSampler.RandomSampler.html" target="_blank" rel="external">InputSampler.RandomSampler</a> class, but this time let’s use a manual partition file.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">SequenceFile.Writer writer = <span class="keyword">new</span> SequenceFile.Writer(fs, getConf(), partition,</div><div class="line">        Text.class, NullWritable.class);</div><div class="line">Text key = <span class="keyword">new</span> Text();</div><div class="line">NullWritable value = NullWritable.get();</div><div class="line">key.set(<span class="string">"12:00"</span>);</div><div class="line">writer.append(key, value);</div><div class="line">key.set(<span class="string">"13:00"</span>);</div><div class="line">writer.append(key, value);</div><div class="line">writer.close();</div></pre></td></tr></table></figure>
<p>So basically, the partitioner will partition the mapper outputs into three parts, the first part will be less than “12:00”, seceond part [“12:00”, “13:00”), thrid [“13:00”, ).</p>
<p>And then, indicate the job to use this partition file:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">job.setPartitionerClass(TotalOrderPartitioner.class);</div><div class="line">otalOrderPartitioner.setPartitionFile(job.getConfiguration(), partition);</div><div class="line"></div><div class="line"><span class="comment">// The number of reducers should equal the number of partitions.</span></div><div class="line">job.setNumReduceTasks(<span class="number">3</span>);</div></pre></td></tr></table></figure>
<h3 id="Use-MutipleOutputs"><a href="#Use-MutipleOutputs" class="headerlink" title="Use MutipleOutputs"></a>Use MutipleOutputs</h3><p>In the reducer, we need to note down the row count of this partition, to do that, we’ll need the MultipleOutputs class, which let use output multiple result files apart from the default “part-r-xxxxx”. The reducer’s code is as following:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JobReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> MultipleOutputs&lt;NullWritable, Text&gt; mos;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">long</span> count;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span></span></div><div class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>.setup(context);</div><div class="line">        mos = <span class="keyword">new</span> MultipleOutputs&lt;NullWritable, Text&gt;(context);</div><div class="line">        count = <span class="number">0</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span></span></div><div class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">for</span> (Text value : values) &#123;</div><div class="line">            context.write(NullWritable.get(), value);</div><div class="line">            ++count;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span></span></div><div class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>.cleanup(context);</div><div class="line">        mos.write(<span class="string">"count"</span>, NullWritable.get(), <span class="keyword">new</span> LongWritable(count));</div><div class="line">        mos.close();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>There’re several things to pay attention to:</p>
<ol>
<li>MultipleOutputs is declared as class member, defined in Reducer#setup method, and must be closed at Reducer#cleanup (otherwise the file will be empty).</li>
<li>When instantiating MultipleOutputs class, the generic type needs to be the same as reducer’s output key/value class.</li>
<li>In order to use a different output key/value class, additional setup needs to be done at job definition:</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Job job = <span class="keyword">new</span> Job(getConf());</div><div class="line">MultipleOutputs.addNamedOutput(job, <span class="string">"count"</span>, SequenceFileOutputFormat.class,</div><div class="line">    NullWritable.class, LongWritable.class);</div></pre></td></tr></table></figure>
<p>For example, if the output folder is “/tmp/total-sort/“, there’ll be the following files when job is done:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">/tmp/total-sort/count-r-00001</div><div class="line">/tmp/total-sort/count-r-00002</div><div class="line">/tmp/total-sort/count-r-00003</div><div class="line">/tmp/total-sort/part-r-00001</div><div class="line">/tmp/total-sort/part-r-00002</div><div class="line">/tmp/total-sort/part-r-00003</div></pre></td></tr></table></figure>
<h3 id="Pass-Start-Ids-to-Mapper"><a href="#Pass-Start-Ids-to-Mapper" class="headerlink" title="Pass Start Ids to Mapper"></a>Pass Start Ids to Mapper</h3><p>When the second mapper processes the inputs, we want them to know the initial id of its partition, which can be calculated from the <code>count-*</code> files we produce before. To pass this information, we can use the job’s Configuration object.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Read and calculate the start id from those row-count files.</span></div><div class="line">Map&lt;String, Long&gt; startIds = <span class="keyword">new</span> HashMap&lt;String, Long&gt;();</div><div class="line"><span class="keyword">long</span> startId = <span class="number">1</span>;</div><div class="line">FileSystem fs = FileSystem.get(getConf());</div><div class="line"><span class="keyword">for</span> (FileStatus file : fs.listStatus(countPath)) &#123;</div><div class="line"></div><div class="line">    Path path = file.getPath();</div><div class="line">    String name = path.getName();</div><div class="line">    <span class="keyword">if</span> (!name.startsWith(<span class="string">"count-"</span>)) &#123;</div><div class="line">        <span class="keyword">continue</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    startIds.put(name.substring(name.length() - <span class="number">5</span>), startId);</div><div class="line"></div><div class="line">    SequenceFile.Reader reader = <span class="keyword">new</span> SequenceFile.Reader(fs, path, getConf());</div><div class="line">    NullWritable key = NullWritable.get();</div><div class="line">    LongWritable value = <span class="keyword">new</span> LongWritable();</div><div class="line">    <span class="keyword">if</span> (!reader.next(key, value)) &#123;</div><div class="line">        <span class="keyword">continue</span>;</div><div class="line">    &#125;</div><div class="line">    startId += value.get();</div><div class="line">    reader.close();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Serialize the map and pass it to Configuration.</span></div><div class="line">job.getConfiguration().set(<span class="string">"startIds"</span>, Base64.encodeBase64String(</div><div class="line">        SerializationUtils.serialize((Serializable) startIds)));</div><div class="line"></div><div class="line"><span class="comment">// Recieve it in Mapper#setup</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMapperB</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> Map&lt;String, Long&gt; startIds;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">long</span> startId;</div><div class="line"></div><div class="line">    <span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span></span></div><div class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>.setup(context);</div><div class="line">        startIds = (Map&lt;String, Long&gt;) SerializationUtils.deserialize(</div><div class="line">                Base64.decodeBase64(context.getConfiguration().get(<span class="string">"startIds"</span>)));</div><div class="line">        String name = ((FileSplit) context.getInputSplit()).getPath().getName();</div><div class="line">        startId = startIds.get(name.substring(name.length() - <span class="number">5</span>));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(NullWritable key, Text value, Context context)</span></span></div><div class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        context.write(<span class="keyword">new</span> LongWritable(startId++), value);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Set-the-Input-Non-splitable"><a href="#Set-the-Input-Non-splitable" class="headerlink" title="Set the Input Non-splitable"></a>Set the Input Non-splitable</h3><p>When the file is bigger than a block or so (depending on some configuration entries), Hadoop will split it, which is not good for us. So let’s define a new InputFormat class to disable the splitting behaviour:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">NonSplitableSequence</span> <span class="keyword">extends</span> <span class="title">SequenceFileInputFormat</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isSplitable</span><span class="params">(JobContext context, Path filename)</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// use it</span></div><div class="line">job.setInputFormatClass(NonSplitableSequence.class);</div></pre></td></tr></table></figure>
<p>And that’s it, we are able to generate a unique, auto-increment id for a sorted collection, with Hadoop’s parallel computing capability. The process is rather complicated, which requires several techniques about Hadoop. It’s worthwhile to dig.</p>
<p>A workable example can be found in my <a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/AutoIncrementId2Job.java" target="_blank" rel="external">Github repository</a>. If you have some more straight-forward approach, please do let me know.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In DBMS world, it’s easy to generate a unique, auto-increment id, using MySQL’s &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.1/en/example-auto-increment.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;AUTO_INCREMENT attribute&lt;/a&gt; on a primary key or MongoDB’s &lt;a href=&quot;http://docs.mongodb.org/manual/tutorial/create-an-auto-incrementing-field/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Counters Collection&lt;/a&gt; pattern. But when it comes to a distributed, parallel processing framework, like Hadoop Map-reduce, it is not that straight forward. The best solution to identify every record in such framework is to use UUID. But when an integer id is required, it’ll take some steps.&lt;/p&gt;
&lt;h2 id=&quot;Solution-A-Single-Reducer&quot;&gt;&lt;a href=&quot;#Solution-A-Single-Reducer&quot; class=&quot;headerlink&quot; title=&quot;Solution A: Single Reducer&quot;&gt;&lt;/a&gt;Solution A: Single Reducer&lt;/h2&gt;&lt;p&gt;This is the most obvious and simple one, just use the following code to specify reducer numbers to 1:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;job.setNumReduceTasks(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;);&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;And also obvious, there are several demerits:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;All mappers output will be copied to one task tracker.&lt;/li&gt;
&lt;li&gt;Only one process is working on shuffel &amp;amp; sort.&lt;/li&gt;
&lt;li&gt;When producing output, there’s also only one process.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The above is not a problem for small data sets, or at least small mapper outputs. And it is also the approach that Pig and Hive use when they need to perform a total sort. But when hitting a certain threshold, the sort and copy phase will become very slow and unacceptable.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
  </entry>
  
</feed>
