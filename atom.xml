<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ji ZHANG&#39;s Blog</title>
  <subtitle>If I rest, I rust.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://shzhangji.com/"/>
  <updated>2017-01-13T01:00:25.000Z</updated>
  <id>http://shzhangji.com/</id>
  
  <author>
    <name>Ji ZHANG</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python 2 to 3 Quick Guide</title>
    <link href="http://shzhangji.com/blog/2017/01/08/python-2-to-3-quick-guide/"/>
    <id>http://shzhangji.com/blog/2017/01/08/python-2-to-3-quick-guide/</id>
    <published>2017-01-08T04:26:54.000Z</published>
    <updated>2017-01-13T01:00:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>Few years ago I was programming Python 2.7, when 3.x was still not an option, because of its backward-incompatibiliy and lack of popular third-party libraries support. But now it’s safe to say Python 3 is <a href="http://py3readiness.org/" target="_blank" rel="external">totally ready</a>, and here’s a list of references for those (including me) who are adopting Python 3 with a 2.x background.</p>
<ol>
<li>All Strings Are Unicode</li>
<li><code>print</code> Becomes a Function</li>
<li>Less Lists More Views</li>
<li>Integer Division Returns Float</li>
<li>Comparison Operators Raises <code>TypeError</code></li>
<li>Set Literal Support</li>
<li>New String Formatting</li>
<li>Exception Handling</li>
<li>Global Function Changes</li>
<li>Renaming Modules and Relative Import</li>
</ol>
<h2 id="All-Strings-Are-Unicode"><a href="#All-Strings-Are-Unicode" class="headerlink" title="All Strings Are Unicode"></a>All Strings Are Unicode</h2><p>When dealing with non-ASCII encodings in Python 2, there’re <code>str</code>, <code>unicode</code>, <code>u&#39;...&#39;</code>, <code>s.encode()</code>, etc. In Python 3, there’re only <strong>text</strong> and <strong>binary data</strong>. The former is <code>str</code>, strings that are always represented in Unicode; the later is <code>bytes</code>, which is just a sequence of byte numbers.</p>
<ul>
<li>Conversion between <code>str</code> and <code>bytes</code>:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># str to bytes</span></div><div class="line"><span class="string">'str'</span>.encode(<span class="string">'UTF-8'</span>)</div><div class="line">bytes(<span class="string">'str'</span>, encoding=<span class="string">'UTF-8'</span>)</div><div class="line"></div><div class="line"><span class="comment"># bytes to str</span></div><div class="line"><span class="string">b'bytes'</span>.decode(<span class="string">'UTF-8'</span>)</div><div class="line">str(<span class="string">b'bytes'</span>, encoding=<span class="string">'UTF-8'</span>)</div></pre></td></tr></table></figure>
<ul>
<li><code>basestring</code> is removed, use <code>str</code> as type: <code>isinstance(s, str)</code></li>
<li><code>bytes</code> is immutable, the corresponding mutable version is <code>bytearray</code>.</li>
<li>The default source file encoding is UTF-8 now.</li>
</ul>
<a id="more"></a>
<h2 id="print-Becomes-a-Function"><a href="#print-Becomes-a-Function" class="headerlink" title="print Becomes a Function"></a><code>print</code> Becomes a Function</h2><p>In Python 2, <code>print</code> is a statement, and now it’s used as a function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span>   <span class="comment"># Old: print a new line</span></div><div class="line">print() <span class="comment"># New</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">'hello'</span>, <span class="string">'world'</span>,          <span class="comment"># Old: trailing comma suppresses new line</span></div><div class="line">print(<span class="string">'hello'</span>, <span class="string">'world'</span>, end=<span class="string">' '</span>) <span class="comment"># New: end defaults to '\n'</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> &gt;&gt;sys.stderr, <span class="string">'error'</span>     <span class="comment"># Old: write to stderr</span></div><div class="line">print(<span class="string">'error'</span>, file=sys.stderr) <span class="comment"># New</span></div></pre></td></tr></table></figure>
<p><code>print</code> function also provides <code>sep</code> and <code>flush</code> parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">'hello'</span>, <span class="string">'world'</span>, sep=<span class="string">','</span>, flush=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># Instead of:</span></div><div class="line"><span class="keyword">print</span> <span class="string">','</span>.join((<span class="string">'hello'</span>, <span class="string">'world'</span>))</div><div class="line">sys.stdout.flush()</div></pre></td></tr></table></figure>
<h2 id="Less-Lists-More-Views"><a href="#Less-Lists-More-Views" class="headerlink" title="Less Lists More Views"></a>Less Lists More Views</h2><p>A lot of well-known methods now return iterators, or ‘views’,  instead of eager-evaluated lists. </p>
<ul>
<li>Dictionary’s <code>keys</code>, <code>items</code>, and <code>values</code> methods, while removing <code>iterkeys</code>, <code>iteritems</code>, and <code>itervalues</code>. For example, when you need a sorted key list:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">k = d.keys(); k.sort() <span class="comment"># Old</span></div><div class="line">k = sorted(d.keys())   <span class="comment"># New</span></div></pre></td></tr></table></figure>
<ul>
<li><code>map</code>, <code>filter</code>, and <code>zip</code>, while removing <code>imap</code> methods in <code>itertools</code> module. To get a concrete list, use list comprehension or the <code>list</code> global function:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[x * <span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</div><div class="line">list(map(<span class="keyword">lambda</span> x: x * <span class="number">2</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))</div></pre></td></tr></table></figure>
<ul>
<li><code>range</code> is now equivalent to <code>xrange</code> in Python 2, the later is removed.</li>
<li>For iterators, the <code>next</code> method is renamed to <code>__next__</code>, and there’s a global <code>next</code> function, which accepts an iterator and calls its <code>__next__</code> method.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">iter([<span class="number">1</span>]).next()     <span class="comment"># Old</span></div><div class="line">iter([<span class="number">1</span>]).__next__() <span class="comment"># New</span></div><div class="line">next(iter([<span class="number">1</span>]))      <span class="comment"># New</span></div></pre></td></tr></table></figure>
<h2 id="Integer-Division-Returns-Float"><a href="#Integer-Division-Returns-Float" class="headerlink" title="Integer Division Returns Float"></a>Integer Division Returns Float</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print 1 / 2   # Old: prints 0</div><div class="line">print 1 / 2.0 # Old: prints 0.5</div><div class="line">print(1 / 2)  # New: prints 0.5</div><div class="line">print(1 // 2) # New: prints 0</div></pre></td></tr></table></figure>
<ul>
<li>There’s no difference between <code>long</code> and <code>int</code> now, use <code>int</code> only.</li>
<li>Octal literals are represented as <code>0o755</code>, instead of <code>0755</code>.</li>
</ul>
<h2 id="Comparison-Operators-Raises-TypeError"><a href="#Comparison-Operators-Raises-TypeError" class="headerlink" title="Comparison Operators Raises TypeError"></a>Comparison Operators Raises <code>TypeError</code></h2><ul>
<li><code>&lt;</code>, <code>&lt;=</code>, <code>&gt;=</code>, <code>&gt;</code> can no longer be used between different types.</li>
<li><code>==</code> and <code>!=</code> remains the same.</li>
<li><code>cmp</code> parameter in <code>sort</code> is removed. Use <code>key</code> to extract a comparison key from each element.</li>
</ul>
<h2 id="Set-Literal-Support"><a href="#Set-Literal-Support" class="headerlink" title="Set Literal Support"></a>Set Literal Support</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">s = set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment"># Old, also valid in Python 3</span></div><div class="line">s = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;      <span class="comment"># New</span></div><div class="line">s = set()          <span class="comment"># Empty set</span></div><div class="line">d = &#123;&#125;             <span class="comment"># Empty dict</span></div></pre></td></tr></table></figure>
<h2 id="New-String-Formatting"><a href="#New-String-Formatting" class="headerlink" title="New String Formatting"></a>New String Formatting</h2><p>Python 3 introduces a new form of string formatting, and it’s also back-ported to Python 2.x. The old <code>%s</code> formatting is still available in 3.x, but the <a href="https://docs.python.org/3/library/string.html#format-string-syntax" target="_blank" rel="external">new format</a> seems more expressive and powerful.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># by position</span></div><div class="line"><span class="string">'&#123;&#125;, &#123;&#125;, &#123;&#125;'</span>.format(<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>)    <span class="comment"># a, b, c</span></div><div class="line"><span class="string">'&#123;2&#125;, &#123;1&#125;, &#123;0&#125;'</span>.format(<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>) <span class="comment"># c, b, a</span></div><div class="line"></div><div class="line"><span class="comment"># by name</span></div><div class="line"><span class="string">'Hello, &#123;name&#125;'</span>.format(name=<span class="string">'Jerry'</span>) <span class="comment"># Hello, Jerry</span></div><div class="line"></div><div class="line"><span class="comment"># by attribute</span></div><div class="line">c = <span class="number">1</span> - <span class="number">2j</span></div><div class="line"><span class="string">'real: &#123;0.real&#125;'</span>.format(c) <span class="comment"># real: 1.0</span></div><div class="line"></div><div class="line"><span class="comment"># by index</span></div><div class="line"><span class="string">'X: &#123;0[0]&#125;, Y: &#123;0[1]&#125;'</span>.format((<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># X: 1, Y: 2</span></div><div class="line"></div><div class="line"><span class="comment"># format number</span></div><div class="line"><span class="string">'&#123;:.2f&#125;'</span>.format(<span class="number">1.2</span>)   <span class="comment"># 1.20</span></div><div class="line"><span class="string">'&#123;:.2%&#125;'</span>.format(<span class="number">0.012</span>) <span class="comment"># 1.20%</span></div><div class="line"><span class="string">'&#123;:,&#125;'</span>.format(<span class="number">1234567</span>) <span class="comment"># 1,234,567</span></div><div class="line"></div><div class="line"><span class="comment"># padding</span></div><div class="line"><span class="string">'&#123;:&gt;05&#125;'</span>.format(<span class="number">1</span>) <span class="comment"># 00001</span></div></pre></td></tr></table></figure>
<p>Furthermore, Python 3.6 introduces literal string interpolation (<a href="https://www.python.org/dev/peps/pep-0498/" target="_blank" rel="external">PEP 498</a>).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">name = <span class="string">'Jerry'</span></div><div class="line">print(f<span class="string">'Hello, &#123;name&#125;'</span>)</div></pre></td></tr></table></figure>
<h2 id="Exception-Handling"><a href="#Exception-Handling" class="headerlink" title="Exception Handling"></a>Exception Handling</h2><p>Raise and catch exceptions in a more standard way:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Old</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">  <span class="keyword">raise</span> Exception, <span class="string">'message'</span></div><div class="line"><span class="keyword">except</span> Exception, e:</div><div class="line">  tb = sys.exc_info()[<span class="number">2</span>]</div><div class="line">  </div><div class="line"><span class="comment"># New</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">  <span class="keyword">raise</span> Exception(<span class="string">'message'</span>)</div><div class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">  tb = e.__traceback__</div></pre></td></tr></table></figure>
<h2 id="Global-Function-Changes"><a href="#Global-Function-Changes" class="headerlink" title="Global Function Changes"></a>Global Function Changes</h2><p>Some global functions are (re)moved to reduce duplication and language cruft.</p>
<ul>
<li><code>reduce</code> is removed, use <code>functools.reduce</code>, or explict <code>for</code> loop instead.</li>
<li><code>apply</code> is removed, use <code>f(*args)</code> instead of <code>apply(f, args)</code>.</li>
<li><code>execfile</code> is removed, use <code>exec(open(fn).read())</code></li>
<li>Removed backticks, use <code>repr</code> instread.</li>
<li><code>raw_input</code> is renamed to <code>input</code>, and the old <code>input</code> behaviour can be achieved by <code>eval(input())</code></li>
</ul>
<h2 id="Renaming-Modules-and-Relative-Import"><a href="#Renaming-Modules-and-Relative-Import" class="headerlink" title="Renaming Modules and Relative Import"></a>Renaming Modules and Relative Import</h2><ul>
<li>Different URL modules are unified into <code>urllib</code> module, e.g.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen, Request</div><div class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</div><div class="line">req = Request(<span class="string">'http://shzhangji.com?'</span> + urlencode(&#123;<span class="string">'t'</span>: <span class="number">1</span>&#125;)</div><div class="line"><span class="keyword">with</span> urlopen(req) <span class="keyword">as</span> f:</div><div class="line">  print(f.read())</div></pre></td></tr></table></figure>
<ul>
<li>Some modules are renamed according to <a href="https://www.python.org/dev/peps/pep-0008" target="_blank" rel="external">PEP 8</a>, such as:<ul>
<li>ConfigParser -&gt; configparser</li>
<li>copy_reg -&gt; copyreg</li>
<li>test.test_support -&gt; test.support</li>
</ul>
</li>
<li>Some modules have both pure Python implementation along with an accelerated version, like StringIO and cStringIO. In Python 3, user should always import the standard module, and fallback would happen automatically.<ul>
<li>StringIO + cStringIO -&gt; io</li>
<li>pickle + cPickle -&gt; pickle</li>
</ul>
</li>
<li>All <code>import</code> forms are interpreted as absolute imports, unless started with <code>.</code>:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> . <span class="keyword">import</span> somemod</div><div class="line"><span class="keyword">from</span> .somemod <span class="keyword">import</span> moremod</div></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://docs.python.org/3/whatsnew/3.0.html" target="_blank" rel="external">What’s New In Python 3.0</a></li>
<li><a href="http://www.diveintopython3.net/porting-code-to-python-3-with-2to3.html" target="_blank" rel="external">Porting Code to Python 3 with 2to3</a></li>
<li><a href="http://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html" target="_blank" rel="external">The key differences between Python 2.7.x and Python 3.x with examples</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Few years ago I was programming Python 2.7, when 3.x was still not an option, because of its backward-incompatibiliy and lack of popular third-party libraries support. But now it’s safe to say Python 3 is &lt;a href=&quot;http://py3readiness.org/&quot;&gt;totally ready&lt;/a&gt;, and here’s a list of references for those (including me) who are adopting Python 3 with a 2.x background.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;All Strings Are Unicode&lt;/li&gt;
&lt;li&gt;&lt;code&gt;print&lt;/code&gt; Becomes a Function&lt;/li&gt;
&lt;li&gt;Less Lists More Views&lt;/li&gt;
&lt;li&gt;Integer Division Returns Float&lt;/li&gt;
&lt;li&gt;Comparison Operators Raises &lt;code&gt;TypeError&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Set Literal Support&lt;/li&gt;
&lt;li&gt;New String Formatting&lt;/li&gt;
&lt;li&gt;Exception Handling&lt;/li&gt;
&lt;li&gt;Global Function Changes&lt;/li&gt;
&lt;li&gt;Renaming Modules and Relative Import&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;All-Strings-Are-Unicode&quot;&gt;&lt;a href=&quot;#All-Strings-Are-Unicode&quot; class=&quot;headerlink&quot; title=&quot;All Strings Are Unicode&quot;&gt;&lt;/a&gt;All Strings Are Unicode&lt;/h2&gt;&lt;p&gt;When dealing with non-ASCII encodings in Python 2, there’re &lt;code&gt;str&lt;/code&gt;, &lt;code&gt;unicode&lt;/code&gt;, &lt;code&gt;u&amp;#39;...&amp;#39;&lt;/code&gt;, &lt;code&gt;s.encode()&lt;/code&gt;, etc. In Python 3, there’re only &lt;strong&gt;text&lt;/strong&gt; and &lt;strong&gt;binary data&lt;/strong&gt;. The former is &lt;code&gt;str&lt;/code&gt;, strings that are always represented in Unicode; the later is &lt;code&gt;bytes&lt;/code&gt;, which is just a sequence of byte numbers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Conversion between &lt;code&gt;str&lt;/code&gt; and &lt;code&gt;bytes&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# str to bytes&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&#39;str&#39;&lt;/span&gt;.encode(&lt;span class=&quot;string&quot;&gt;&#39;UTF-8&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;bytes(&lt;span class=&quot;string&quot;&gt;&#39;str&#39;&lt;/span&gt;, encoding=&lt;span class=&quot;string&quot;&gt;&#39;UTF-8&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# bytes to str&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;b&#39;bytes&#39;&lt;/span&gt;.decode(&lt;span class=&quot;string&quot;&gt;&#39;UTF-8&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;str(&lt;span class=&quot;string&quot;&gt;b&#39;bytes&#39;&lt;/span&gt;, encoding=&lt;span class=&quot;string&quot;&gt;&#39;UTF-8&#39;&lt;/span&gt;)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;basestring&lt;/code&gt; is removed, use &lt;code&gt;str&lt;/code&gt; as type: &lt;code&gt;isinstance(s, str)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bytes&lt;/code&gt; is immutable, the corresponding mutable version is &lt;code&gt;bytearray&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The default source file encoding is UTF-8 now.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="python" scheme="http://shzhangji.com/tags/python/"/>
    
      <category term="english" scheme="http://shzhangji.com/tags/english/"/>
    
  </entry>
  
  <entry>
    <title>开发人员必知的5种开源框架</title>
    <link href="http://shzhangji.com/blog/2016/03/13/top-5-frameworks/"/>
    <id>http://shzhangji.com/blog/2016/03/13/top-5-frameworks/</id>
    <published>2016-03-13T08:25:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>作者：<a href="https://opensource.com/business/15/12/top-5-frameworks" target="_blank" rel="external">John Esposito</a></p>
<p>软件侵吞着世界<a href="http://www.wsj.com/articles/SB10001424053111903480904576512250915629460" target="_blank" rel="external">已经四年多了</a>，但开发人员看待软件的方式稍有不同。我们一直在致力于<a href="http://www.dougengelbart.org/pubs/augment-3906.html" target="_blank" rel="external">解决实际问题</a>，而<a href="http://worrydream.com/refs/Brooks-NoSilverBullet.pdf" target="_blank" rel="external">很少思考软件开发的基石</a>。当问题变得更庞大、解决方案更复杂时，一些实用的、不怎么<a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html" target="_blank" rel="external">产生泄漏</a>的抽象工具就显得越来越重要。</p>
<p>简单地来说，在那些追求生产效率的开发者眼中，<em>框架</em>正在吞食着世界。那究竟是哪些框架、各自又在吞食着哪一部分呢？</p>
<p>开源界的开发框架实在太多了，多到近乎疯狂的地步。我从2015年各种领域的榜单中选取了最受欢迎的5种框架。对于前端框架（我所擅长的领域），我只选取那些真正的客户端框架，这是因为现今的浏览器和移动设备已经具备非常好的性能，越来越多的单页应用（SPA）正在避免和服务端交换数据。</p>
<h2 id="1-展现层：Bootstrap"><a href="#1-展现层：Bootstrap" class="headerlink" title="1. 展现层：Bootstrap"></a>1. 展现层：Bootstrap</h2><p>我们从技术栈的顶端开始看——展现层，这一开发者和普通用户都会接触到的技术。展现层的赢家毫无疑问仍是<a href="http://getbootstrap.com/" target="_blank" rel="external">Bootstrap</a>。Bootstrap的<a href="https://www.google.com/trends/explore#q=%2Fm%2F0j671ln" target="_blank" rel="external">流行度</a>非常之惊人，<a href="https://www.google.com/trends/explore#q=%2Fm%2F0j671ln%2C%20%2Fm%2F0ll4n18%2C%20Material%20Design%20Lite&amp;cmpt=q&amp;tz=Etc%2FGMT%2B5" target="_blank" rel="external">远远甩开</a>了它的老对手<a href="http://foundation.zurb.com/" target="_blank" rel="external">Foundation</a>，以及新星<a href="http://www.getmdl.io/" target="_blank" rel="external">Material Design Lite</a>。在<a href="http://trends.builtwith.com/docinfo/Twitter-Bootstrap" target="_blank" rel="external">BuiltWith</a>上，Bootstrap占据主导地位；而在GitHub上则长期保持<a href="https://github.com/search?q=stars:%3E1&amp;s=stars&amp;type=Repositories" target="_blank" rel="external">Star数</a>和<a href="https://github.com/search?o=desc&amp;q=stars:%3E1&amp;s=forks&amp;type=Repositories" target="_blank" rel="external">Fork数</a>最多的记录。</p>
<p>如今，Bootstrap仍然有着非常活跃的开发社区。8月，Bootstrap发布了<a href="http://v4-alpha.getbootstrap.com/" target="_blank" rel="external">v4</a><a href="http://blog.getbootstrap.com/2015/08/19/bootstrap-4-alpha/" target="_blank" rel="external">内测版</a>，庆祝它的四岁生日。这个版本是对现有功能的<a href="http://v4-alpha.getbootstrap.com/migration/" target="_blank" rel="external">简化和扩充</a>，主要包括：增强可编程性；从Less迁移至Sass；将所有HTML重置代码集中到一个模块；大量自定义样式可直接通过Sass变量指定；所有JavaScript插件都改用ES6重写等。开发团队还开设了<a href="http://themes.getbootstrap.com/" target="_blank" rel="external">官方主题市场</a>，进一步扩充现有的<a href="https://www.google.com/search?q=bootstrap+theme+sites" target="_blank" rel="external">主题生态</a>。</p>
<h2 id="2-网页MVC：AngularJS"><a href="#2-网页MVC：AngularJS" class="headerlink" title="2. 网页MVC：AngularJS"></a>2. 网页MVC：AngularJS</h2><p>随着网页平台技术越来越<a href="https://www.w3.org/blog/news/" target="_blank" rel="external">成熟</a>，开发者们可以远离仍在使用标记语言进行着色的DOM对象，转而面对日渐完善的抽象层进行开发。这一趋势始于现代单页应用（SPA）对XMLHttpRequest的高度依赖，而其中<a href="https://www.google.com/trends/explore#q=%2Fm%2F0j45p7w%2C%20EmberJS%2C%20MeteorJS%2C%20BackboneJS&amp;cmpt=q&amp;tz=Etc%2FGMT%2B5" target="_blank" rel="external">最</a><a href="https://www.pluralsight.com/browse#tab-courses-popular" target="_blank" rel="external">流行</a>的SPA框架当属<a href="https://angularjs.org/" target="_blank" rel="external">AngularJS</a>。</p>
<p>AngularJS有什么特别之处呢？一个词：指令（<a href="https://docs.angularjs.org/guide/directive" target="_blank" rel="external">directive</a>）。一个简单的<code>ng-</code>就能让标签“起死回生”（从静态的标记到动态的JS代码）。依赖注入也是很重要的功能，许多Angular特性都致力于简化维护成本，并进一步从DOM中抽象出来。其基本原则就是将声明式的展现层代码和命令式的领域逻辑充分隔离开来，这种做法对于使用过POM或ORM的人尤为熟悉（我们之中还有人体验过XAML）。这一思想令人振奋，解放了开发者，甚至让人第一眼看上去有些奇怪——因为它赋予了HTML所不该拥有的能力。</p>
<p>有些遗憾的是，AngualrJS的“杀手锏”双向绑定（让视图和模型数据保持一致）将在<a href="https://www.quora.com/Why-is-the-two-way-data-binding-being-dropped-in-Angular-2" target="_blank" rel="external">Angular2</a>中移除，已经<a href="http://angularjs.blogspot.com/2015/11/highlights-from-angularconnect-2015.html" target="_blank" rel="external">临近公测</a>。虽然这一魔法般的特性即将消失，却带来了极大的性能提升，并降低了调试的难度（可以想象一下在悬崖边行走的感觉）。随着单页应用越来越庞大和复杂，这种权衡会变得更有价值。</p>
<a id="more"></a>
<h2 id="3-企业级Java：Spring-Boot"><a href="#3-企业级Java：Spring-Boot" class="headerlink" title="3. 企业级Java：Spring Boot"></a>3. 企业级Java：Spring Boot</h2><p>Java的优点是什么？运行速度快，成熟，完善的类库，庞大的生态环境，一处编译处处执行，活跃的社区等等——除了痛苦的项目起始阶段。即便是最忠实的Java开发者也会转而使用Ruby或Python来快速编写一些只会用到一次的小型脚本（别不承认）。然而，鉴于以上种种原因，Java仍然是企业级应用的首选语言。</p>
<p>这时，Spring Boot出现了，它是模板代码的终结者，有了它，你就能在一条推文中写出一个Java应用程序来：</p>
<p><a href="https://twitter.com/rob_winch/status/364871658483351552" target="_blank" rel="external">https://twitter.com/rob_winch/status/364871658483351552</a></p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// spring run app.groovy</span></div><div class="line"><span class="meta">@Controller</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ThisWillActuallyRun</span> &#123;</span></div><div class="line">  <span class="meta">@RequestMapping</span>(<span class="string">"/"</span>)</div><div class="line">  <span class="meta">@ResponseBody</span></div><div class="line">  String home() &#123;</div><div class="line">    <span class="string">"Hello World!"</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>没有让人不快的XML配置，无需生成别扭的代码。这怎么可能？很简单，Spring Boot在背后做了很多工作，读上面的代码就能看出，框架会自动生成一个内嵌的servlet容器，监听8080端口，处理接收到的请求。这些都无需用户配置，而是遵从Spring Boot的约定。</p>
<p>Spring Boot有多流行？它是目前为止<a href="https://github.com/spring-projects" target="_blank" rel="external">fork数</a>和下载量最高的Spring应用（主框架除外）。2015年，<a href="https://www.google.com/trends/explore#q=spring%20boot%2C%20spring%20framework&amp;cmpt=q&amp;tz=Etc%2FGMT%2B5" target="_blank" rel="external">在谷歌上搜索Spring Boot的人数首次超过搜索Spring框架的人</a>。</p>
<h2 id="4-数据处理：Apache-Spark"><a href="#4-数据处理：Apache-Spark" class="headerlink" title="4. 数据处理：Apache Spark"></a>4. 数据处理：Apache Spark</h2><p>很久很久以前（2004年），谷歌研发出一种编程模型（<a href="http://ayende.com/blog/4435/map-reduce-a-visual-explanation" target="_blank" rel="external">MapReduce</a>），将分布式批处理任务通用化了，并撰写了一篇<a href="http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf" target="_blank" rel="external">著名的论文</a>。之后，Yahoo的工程师用Java编写了一个框架（<a href="https://hadoop.apache.org/" target="_blank" rel="external">Hadoop</a>）实现了MapReduce，以及一个<a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html" target="_blank" rel="external">分布式文件系统</a>，使得MapReduce任务能够更简便地读写数据。</p>
<p>将近十年的时间，Hadoop主宰了大数据处理框架的生态系统，即使批处理只能解决有限的问题——也许大多数企业和科学工作者已经习惯了大数据量的批处理分析吧。然而，并不是所有大型数据集都适合使用批处理。特别地，流式数据（如传感器数据）和迭代式数据分析（机器学习算法中最为常见）都不适合使用批处理。因此，大数据领域诞生了很多<a href="https://www.linkedin.com/pulse/100-open-source-big-data-architecture-papers-anil-madan" target="_blank" rel="external">新的编程模型、应用架构</a>、以及各类<a href="http://www.journalofbigdata.com/content/2/1/18" target="_blank" rel="external">数据存储</a>也逐渐流行开来（甚至还从MapReduce中分离出了一种<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="external">新的集群管理系统</a>）。</p>
<p>在这些新兴的系统之中，<a href="http://spark.apache.org/research.html" target="_blank" rel="external">伯克利AMPLab</a>研发的<a href="http://spark.apache.org/" target="_blank" rel="external">Apache Spark</a>在2015年脱颖而出。各类调研和报告（<a href="https://dzone.com/guides/big-data-business-intelligence-and-analytics-2015" target="_blank" rel="external">DZone</a>、<a href="http://cdn2.hubspot.net/hubfs/438089/DataBricks_Surveys_-_Content/Spark-Survey-2015-Infographic.pdf" target="_blank" rel="external">Databricks</a>、<a href="https://info.typesafe.com/COLL-20XX-Spark-Survey-Report_LP.html?lst=PR&amp;lsd=COLL-20XX-Spark-Survey-Trends-Adoption-Report" target="_blank" rel="external">Typesafe</a>）都显示Spark的成长速度非常之快。GitHub提交数从2013年开始就呈<a href="https://github.com/apache/spark/graphs/contributors" target="_blank" rel="external">线性增长</a>，而谷歌趋势则在2015年呈现出<a href="https://www.google.com/trends/explore#q=%2Fm%2F0ndhxqz" target="_blank" rel="external">指数级的增长</a>。</p>
<p>Spark如此流行，它究竟是做什么的呢？答案很简单，非常快速的批处理，不过这点是构建在Spark的一个杀手级特性之上的，能够应用到比Hadoop多得多的编程模型中。Spark将数据表达为弹性分布式数据集（RDD），处理结果保存在多个节点的内存中，不进行复制，只是记录数据的计算过程（这点可以和<a href="http://martinfowler.com/bliki/CQRS.html" target="_blank" rel="external">CQRS</a>、<a href="http://plato.stanford.edu/entries/peirce/" target="_blank" rel="external">实用主义</a>、<a href="http://people.cs.uchicago.edu/~fortnow/papers/kaikoura.pdf" target="_blank" rel="external">Kolmogorov复杂度</a>相较）。这一特点可以让迭代算法无需从底层（较慢的）分布式存储层读取数据。同时也意味着批处理流程无需再背负Nathan Marz在<a href="http://lambda-architecture.net/" target="_blank" rel="external">Lambda架构</a>中所描述的“数据卡顿”之恶名。RDD还能让Spark模拟实时流数据处理，通过将数据切分成小块，降低延迟时间，达到大部分应用对“准实时”的要求。</p>
<h2 id="5-软件交付：Docker"><a href="#5-软件交付：Docker" class="headerlink" title="5. 软件交付：Docker"></a>5. 软件交付：Docker</h2><p>严格意义上说，<a href="https://www.docker.com/" target="_blank" rel="external">Docker</a>并不是符合“框架”的定义：代码库，通用性好，使用一系列特殊约定来解决大型重复性问题。但是，如果框架指的是能够让程序员在一种舒适的抽象层之上进行编码，那Docker将是框架中的佼佼者（我们可以称它为“外壳型框架”，多制造一些命名上的混乱吧）。而且，如果将本文的标题改为“开发人员必知的5样东西”，又不把Docker包含进来，就显得太奇怪了。</p>
<p>为什么Docker很出色？首先我们应该问什么容器很受欢迎（FreeBSD Jail, Solaries Zones, OpenVZ, LXC）？很简单：无需使用一个完整的操作系统来实现隔离；或者说，在获得安全性和便利性的提升时，无需承担额外的开销。但是，隔离也有很多种形式（比如最先想到的<code>chroot</code>，以及各种虚拟内存技术），而且可以非常方便地用<code>systemd-nspawn</code>来启动程序，而不使用Docker。然而，仅仅隔离进程还不够，那Docker有什么<a href="http://techapostle.blogspot.com/2015/04/the-3-reasons-why-docker-got-it-right.html" target="_blank" rel="external">过人之处</a>呢？</p>
<p>两个原因：Dockefile（新型的tar包）增加了便携性；它的格式成为了现行的标准。第一个原因使得应用程序交付变得容易（之前人们是通过创建轻型虚拟机来实现的）。第二个原因则让容器更容易分享（不单是<a href="https://hub.docker.com/" target="_blank" rel="external">DockerHub</a>）。我可以很容易地尝试你编写的应用程序，而不是先要做些不相关的事（想想<code>apt-get</code>给你的体验）。</p>
<h2 id="关于作者"><a href="#关于作者" class="headerlink" title="关于作者"></a>关于作者</h2><p><img src="https://opensource.com/sites/default/files/styles/profile_pictures/public/pictures/john-esposito.jpg?itok=xPVFPzr2" alt="John Esposito"></p>
<p>John Esposito是DZone的主编，最近刚刚完成古典学博士学位的学习，养了两只猫。之前他是VBA和Force.com的开发者、DBA、网络管理员。（但说真的，Lisp是最棒的！）</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：&lt;a href=&quot;https://opensource.com/business/15/12/top-5-frameworks&quot;&gt;John Esposito&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;软件侵吞着世界&lt;a href=&quot;http://www.wsj.com/articles/SB10001424053111903480904576512250915629460&quot;&gt;已经四年多了&lt;/a&gt;，但开发人员看待软件的方式稍有不同。我们一直在致力于&lt;a href=&quot;http://www.dougengelbart.org/pubs/augment-3906.html&quot;&gt;解决实际问题&lt;/a&gt;，而&lt;a href=&quot;http://worrydream.com/refs/Brooks-NoSilverBullet.pdf&quot;&gt;很少思考软件开发的基石&lt;/a&gt;。当问题变得更庞大、解决方案更复杂时，一些实用的、不怎么&lt;a href=&quot;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&quot;&gt;产生泄漏&lt;/a&gt;的抽象工具就显得越来越重要。&lt;/p&gt;
&lt;p&gt;简单地来说，在那些追求生产效率的开发者眼中，&lt;em&gt;框架&lt;/em&gt;正在吞食着世界。那究竟是哪些框架、各自又在吞食着哪一部分呢？&lt;/p&gt;
&lt;p&gt;开源界的开发框架实在太多了，多到近乎疯狂的地步。我从2015年各种领域的榜单中选取了最受欢迎的5种框架。对于前端框架（我所擅长的领域），我只选取那些真正的客户端框架，这是因为现今的浏览器和移动设备已经具备非常好的性能，越来越多的单页应用（SPA）正在避免和服务端交换数据。&lt;/p&gt;
&lt;h2 id=&quot;1-展现层：Bootstrap&quot;&gt;&lt;a href=&quot;#1-展现层：Bootstrap&quot; class=&quot;headerlink&quot; title=&quot;1. 展现层：Bootstrap&quot;&gt;&lt;/a&gt;1. 展现层：Bootstrap&lt;/h2&gt;&lt;p&gt;我们从技术栈的顶端开始看——展现层，这一开发者和普通用户都会接触到的技术。展现层的赢家毫无疑问仍是&lt;a href=&quot;http://getbootstrap.com/&quot;&gt;Bootstrap&lt;/a&gt;。Bootstrap的&lt;a href=&quot;https://www.google.com/trends/explore#q=%2Fm%2F0j671ln&quot;&gt;流行度&lt;/a&gt;非常之惊人，&lt;a href=&quot;https://www.google.com/trends/explore#q=%2Fm%2F0j671ln%2C%20%2Fm%2F0ll4n18%2C%20Material%20Design%20Lite&amp;amp;cmpt=q&amp;amp;tz=Etc%2FGMT%2B5&quot;&gt;远远甩开&lt;/a&gt;了它的老对手&lt;a href=&quot;http://foundation.zurb.com/&quot;&gt;Foundation&lt;/a&gt;，以及新星&lt;a href=&quot;http://www.getmdl.io/&quot;&gt;Material Design Lite&lt;/a&gt;。在&lt;a href=&quot;http://trends.builtwith.com/docinfo/Twitter-Bootstrap&quot;&gt;BuiltWith&lt;/a&gt;上，Bootstrap占据主导地位；而在GitHub上则长期保持&lt;a href=&quot;https://github.com/search?q=stars:%3E1&amp;amp;s=stars&amp;amp;type=Repositories&quot;&gt;Star数&lt;/a&gt;和&lt;a href=&quot;https://github.com/search?o=desc&amp;amp;q=stars:%3E1&amp;amp;s=forks&amp;amp;type=Repositories&quot;&gt;Fork数&lt;/a&gt;最多的记录。&lt;/p&gt;
&lt;p&gt;如今，Bootstrap仍然有着非常活跃的开发社区。8月，Bootstrap发布了&lt;a href=&quot;http://v4-alpha.getbootstrap.com/&quot;&gt;v4&lt;/a&gt;&lt;a href=&quot;http://blog.getbootstrap.com/2015/08/19/bootstrap-4-alpha/&quot;&gt;内测版&lt;/a&gt;，庆祝它的四岁生日。这个版本是对现有功能的&lt;a href=&quot;http://v4-alpha.getbootstrap.com/migration/&quot;&gt;简化和扩充&lt;/a&gt;，主要包括：增强可编程性；从Less迁移至Sass；将所有HTML重置代码集中到一个模块；大量自定义样式可直接通过Sass变量指定；所有JavaScript插件都改用ES6重写等。开发团队还开设了&lt;a href=&quot;http://themes.getbootstrap.com/&quot;&gt;官方主题市场&lt;/a&gt;，进一步扩充现有的&lt;a href=&quot;https://www.google.com/search?q=bootstrap+theme+sites&quot;&gt;主题生态&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;2-网页MVC：AngularJS&quot;&gt;&lt;a href=&quot;#2-网页MVC：AngularJS&quot; class=&quot;headerlink&quot; title=&quot;2. 网页MVC：AngularJS&quot;&gt;&lt;/a&gt;2. 网页MVC：AngularJS&lt;/h2&gt;&lt;p&gt;随着网页平台技术越来越&lt;a href=&quot;https://www.w3.org/blog/news/&quot;&gt;成熟&lt;/a&gt;，开发者们可以远离仍在使用标记语言进行着色的DOM对象，转而面对日渐完善的抽象层进行开发。这一趋势始于现代单页应用（SPA）对XMLHttpRequest的高度依赖，而其中&lt;a href=&quot;https://www.google.com/trends/explore#q=%2Fm%2F0j45p7w%2C%20EmberJS%2C%20MeteorJS%2C%20BackboneJS&amp;amp;cmpt=q&amp;amp;tz=Etc%2FGMT%2B5&quot;&gt;最&lt;/a&gt;&lt;a href=&quot;https://www.pluralsight.com/browse#tab-courses-popular&quot;&gt;流行&lt;/a&gt;的SPA框架当属&lt;a href=&quot;https://angularjs.org/&quot;&gt;AngularJS&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;AngularJS有什么特别之处呢？一个词：指令（&lt;a href=&quot;https://docs.angularjs.org/guide/directive&quot;&gt;directive&lt;/a&gt;）。一个简单的&lt;code&gt;ng-&lt;/code&gt;就能让标签“起死回生”（从静态的标记到动态的JS代码）。依赖注入也是很重要的功能，许多Angular特性都致力于简化维护成本，并进一步从DOM中抽象出来。其基本原则就是将声明式的展现层代码和命令式的领域逻辑充分隔离开来，这种做法对于使用过POM或ORM的人尤为熟悉（我们之中还有人体验过XAML）。这一思想令人振奋，解放了开发者，甚至让人第一眼看上去有些奇怪——因为它赋予了HTML所不该拥有的能力。&lt;/p&gt;
&lt;p&gt;有些遗憾的是，AngualrJS的“杀手锏”双向绑定（让视图和模型数据保持一致）将在&lt;a href=&quot;https://www.quora.com/Why-is-the-two-way-data-binding-being-dropped-in-Angular-2&quot;&gt;Angular2&lt;/a&gt;中移除，已经&lt;a href=&quot;http://angularjs.blogspot.com/2015/11/highlights-from-angularconnect-2015.html&quot;&gt;临近公测&lt;/a&gt;。虽然这一魔法般的特性即将消失，却带来了极大的性能提升，并降低了调试的难度（可以想象一下在悬崖边行走的感觉）。随着单页应用越来越庞大和复杂，这种权衡会变得更有价值。&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="translation" scheme="http://shzhangji.com/tags/translation/"/>
    
  </entry>
  
  <entry>
    <title>使用Spring AOP向领域模型注入依赖</title>
    <link href="http://shzhangji.com/blog/2015/09/12/model-dependency-injection-with-spring-aop/"/>
    <id>http://shzhangji.com/blog/2015/09/12/model-dependency-injection-with-spring-aop/</id>
    <published>2015-09-12T14:03:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="http://shzhangji.com/blog/2015/09/05/anemic-domain-model/">贫血领域模型</a>这篇译文中，Martin阐述了这种“反模式”的症状和问题，并引用了领域驱动设计中的话来说明领域模型和分层设计之间的关系。对于Spring项目的开发人员来说，贫血领域模型十分常见：模型（或实体）仅仅包含对数据表的映射，通常是一组私有属性和公有getter/setter，所有的业务逻辑都写在服务层中，领域模型仅仅用来传递数据。为了编写真正的领域模型，我们需要将业务逻辑移至模型对象中，这就引出另一个问题：业务逻辑通常需要调用其他服务或模型，而使用<code>new</code>关键字或由JPA创建的对象是不受Spring托管的，也就无法进行依赖注入。解决这个问题的方法有很多，比较之后我选择使用面向切面编程来实现。</p>
<h2 id="面向切面编程"><a href="#面向切面编程" class="headerlink" title="面向切面编程"></a>面向切面编程</h2><p>面向切面编程，或<a href="https://en.wikipedia.org/wiki/Aspect-oriented_programming" target="_blank" rel="external">AOP</a>，是一种编程范式，和面向对象编程（<a href="https://en.wikipedia.org/wiki/Object-oriented_programming" target="_blank" rel="external">OOP</a>）互为补充。简单来说，AOP可以在不修改既有代码的情况下改变代码的行为。开发者通过定义一组规则，在特定的类方法前后增加逻辑，如记录日志、性能监控、事务管理等。这些逻辑称为切面（Aspect），规则称为切点（Pointcut），在调用前还是调用后执行称为通知（Before advice, After advice）。最后，我们可以选择在编译期将这些逻辑写入类文件，或是在运行时动态加载这些逻辑，这是两种不同的织入方式（Compile-time weaving, Load-time weaving）。</p>
<p>对于领域模型的依赖注入，我们要做的就是使用AOP在对象创建后调用Spring框架来注入依赖。幸运的是，<a href="http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop" target="_blank" rel="external">Spring AOP</a>已经提供了<code>@Configurable</code>注解来帮助我们实现这一需求。</p>
<a id="more"></a>
<h2 id="Configurable注解"><a href="#Configurable注解" class="headerlink" title="Configurable注解"></a>Configurable注解</h2><p>Spring应用程序会定义一个上下文容器，在该容器内创建的对象会由Spring负责注入依赖。对于容器外创建的对象，我们可以使用<code>@Configurable</code>来修饰类，告知Spring对这些类的实例也进行依赖注入。</p>
<p>假设有一个<code>Report</code>类（领域模型），其中一个方法需要解析JSON，我们可以使用<code>@Configurable</code>将容器内的<code>ObjectMapper</code>对象注入到类的实例中：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Entity</span></div><div class="line"><span class="meta">@Configurable</span>(autowire = Autowire.BY_TYPE)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Report</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Id</span> <span class="meta">@GeneratedValue</span></div><div class="line">    <span class="keyword">private</span> Integer id;</div><div class="line">    </div><div class="line">    <span class="meta">@Autowired</span> <span class="meta">@Transient</span></div><div class="line">    <span class="keyword">private</span> ObjectMapper mapper;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">render</span><span class="params">()</span> </span>&#123;</div><div class="line">        mapper.readValue(...);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li><code>autowire</code>参数默认是<code>NO</code>，因此需要显式打开，否则只能使用XML定义依赖。<code>@Autowired</code>是目前比较推荐的注入方式。</li>
<li><code>@Transient</code>用于告知JPA该属性不需要进行持久化。你也可以使用<code>transient</code>关键字来声明，效果相同。</li>
<li>项目依赖中需要包含<code>spring-aspects</code>。如果已经使用了<code>spring-boot-starter-data-jpa</code>，则无需配置。</li>
<li>应用程序配置中需要加入<code>@EnableSpringConfigured</code>：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@SpringBootApplication</span></div><div class="line"><span class="meta">@EnableTransactionManagement</span>(mode = AdviceMode.ASPECTJ)</div><div class="line"><span class="meta">@EnableSpringConfigured</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        SpringApplication.run(Application.class, args);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>在<code>src/main/resources</code>目录下，新建<code>META-INF/aop.xml</code>文件，用来限定哪些包会用到AOP。否则，AOP的织入操作会作用于所有的类（包括第三方类库），产生不必要的的报错信息。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&lt;!DOCTYPE aspectj PUBLIC "-//AspectJ//DTD//EN" "http://www.eclipse.org/aspectj/dtd/aspectj.dtd"&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">aspectj</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">weaver</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">include</span> <span class="attr">within</span>=<span class="string">"com.foobar..*"</span>/&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">weaver</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">aspectj</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="运行时织入（Load-Time-Weaving-LTW）"><a href="#运行时织入（Load-Time-Weaving-LTW）" class="headerlink" title="运行时织入（Load-Time Weaving, LTW）"></a>运行时织入（Load-Time Weaving, LTW）</h2><p>除了项目依赖和应用程序配置，我们还需要选择一种织入方式来使AOP生效。Spring AOP推荐的方式是运行时织入，并提供了一个专用的Jar包。运行时织入的原理是：当类加载器在读取类文件时，动态修改类的字节码。这一机制是从<a href="http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/instrument/package-summary.html" target="_blank" rel="external">JDK1.5</a>开始提供的，需要使用<code>-javaagent</code>参数开启，如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -javaagent:/path/to/spring-instrument.jar -jar app.jar</div></pre></td></tr></table></figure>
<p>在测试时发现，Spring AOP提供的这一Jar包对普通的类是有效果的，但对于使用<code>@Entity</code>修饰的类就没有作用了。因此，我们改用AspectJ提供的Jar包（可到<a href="http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.aspectj%22%20AND%20a%3A%22aspectjweaver%22" target="_blank" rel="external">Maven中央仓库</a>下载）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -javaagent:/path/to/aspectjweaver.jar -jar app.jar</div></pre></td></tr></table></figure>
<p>对于<a href="http://projects.spring.io/spring-boot/" target="_blank" rel="external">Spring Boot</a>应用程序，可以在Maven命令中加入以下参数：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ mvn spring-boot:run -Drun.agent=/path/to/aspectjweaver.jar</div></pre></td></tr></table></figure>
<p>此外，在使用AspectJ作为LTW的提供方后，会影响到Spring的事务管理，因此需要在应用程序配置中加入：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@EnableTransactionManagement</span>(mode = AdviceMode.ASPECTJ)</div></pre></td></tr></table></figure>
<h2 id="AnnotationBeanConfigurerAspect"><a href="#AnnotationBeanConfigurerAspect" class="headerlink" title="AnnotationBeanConfigurerAspect"></a>AnnotationBeanConfigurerAspect</h2><p>到这里我们已经通过简单配置完成了领域模型的依赖注入，这背后都是Spring中的<code>AnnotationBeanConfigurerAspect</code>在做工作。我们不妨浏览一下精简后的源码：</p>
<p><a href="https://github.com/spring-projects/spring-framework/blob/master/spring-aspects/src/main/java/org/springframework/beans/factory/aspectj/AnnotationBeanConfigurerAspect.aj" target="_blank" rel="external">AnnotationBeanConfigurerAspect.aj</a></p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">aspect</span> <span class="title">AnnotationBeanConfigurerAspect</span> <span class="keyword">implements</span> <span class="title">BeanFactoryAware</span> </span>&#123;</div><div class="line"></div><div class="line">	<span class="keyword">private</span> BeanConfigurerSupport beanConfigurerSupport = <span class="keyword">new</span> BeanConfigurerSupport();</div><div class="line"></div><div class="line">	<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">setBeanFactory</span><span class="params">(BeanFactory beanFactory)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.beanConfigurerSupport.setBeanFactory(beanFactory);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">configureBean</span><span class="params">(Object bean)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.beanConfigurerSupport.configureBean(bean);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">public</span> <span class="keyword">pointcut</span> <span class="title">inConfigurableBean</span>() : @<span class="keyword">this</span>(Configurable);</div><div class="line"></div><div class="line">	<span class="keyword">declare</span> <span class="keyword">parents</span>: @Configurable * <span class="keyword">implements</span> ConfigurableObject;</div><div class="line">	</div><div class="line">	<span class="keyword">public</span> <span class="keyword">pointcut</span> <span class="title">beanConstruction</span>(Object bean) :</div><div class="line">			<span class="keyword">initialization</span>(ConfigurableObject+.<span class="keyword">new</span>(..)) &amp;&amp; <span class="keyword">this</span>(bean);</div><div class="line"></div><div class="line">	<span class="keyword">after</span>(Object bean) <span class="keyword">returning</span> :</div><div class="line">		<span class="title">beanConstruction</span>(bean) &amp;&amp; inConfigurableBean() &#123;</div><div class="line">		configureBean(bean);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li><code>.aj</code>文件是AspectJ定义的语言，增加了pointcut、after等关键字，用来定义切点、通知等；</li>
<li><code>inConfigurationBean</code>切点用于匹配使用<code>Configurable</code>修饰的类型；</li>
<li><code>declare parents</code>将这些类型声明为<code>ConfigurableObject</code>接口，从而匹配<code>beanConstruction</code>切点；</li>
<li><code>ConfigurableObject+.new(..)</code>表示匹配该类型所有的构造函数；</li>
<li><code>after</code>定义一个通知，表示对象创建完成后执行<code>configureBean</code>方法；</li>
<li>该方法会调用<code>BeanConfigurerSupport</code>来对新实例进行依赖注入。</li>
</ul>
<h2 id="其它方案"><a href="#其它方案" class="headerlink" title="其它方案"></a>其它方案</h2><ol>
<li>将依赖作为参数传入。比如上文中的<code>render</code>方法可以定义为<code>render(ObjectMapper mapper)</code>。</li>
<li>将<code>ApplicationContext</code>作为某个类的静态成员，领域模型通过这个引用来获取依赖。</li>
<li>编写一个工厂方法，所有新建对象都要通过这个方法生成，进行依赖注入。</li>
<li>如果领域模型大多从数据库获得，并且JPA的提供方是Hibernate，则可以使用它的拦截器功能进行依赖注入。</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop-atconfigurable" target="_blank" rel="external">http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop-atconfigurable</a></li>
<li><a href="http://blog.igorstoyanov.com/2005/12/dependency-injection-or-service.html" target="_blank" rel="external">http://blog.igorstoyanov.com/2005/12/dependency-injection-or-service.html</a></li>
<li><a href="http://jblewitt.com/blog/?p=129" target="_blank" rel="external">http://jblewitt.com/blog/?p=129</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;http://shzhangji.com/blog/2015/09/05/anemic-domain-model/&quot;&gt;贫血领域模型&lt;/a&gt;这篇译文中，Martin阐述了这种“反模式”的症状和问题，并引用了领域驱动设计中的话来说明领域模型和分层设计之间的关系。对于Spring项目的开发人员来说，贫血领域模型十分常见：模型（或实体）仅仅包含对数据表的映射，通常是一组私有属性和公有getter/setter，所有的业务逻辑都写在服务层中，领域模型仅仅用来传递数据。为了编写真正的领域模型，我们需要将业务逻辑移至模型对象中，这就引出另一个问题：业务逻辑通常需要调用其他服务或模型，而使用&lt;code&gt;new&lt;/code&gt;关键字或由JPA创建的对象是不受Spring托管的，也就无法进行依赖注入。解决这个问题的方法有很多，比较之后我选择使用面向切面编程来实现。&lt;/p&gt;
&lt;h2 id=&quot;面向切面编程&quot;&gt;&lt;a href=&quot;#面向切面编程&quot; class=&quot;headerlink&quot; title=&quot;面向切面编程&quot;&gt;&lt;/a&gt;面向切面编程&lt;/h2&gt;&lt;p&gt;面向切面编程，或&lt;a href=&quot;https://en.wikipedia.org/wiki/Aspect-oriented_programming&quot;&gt;AOP&lt;/a&gt;，是一种编程范式，和面向对象编程（&lt;a href=&quot;https://en.wikipedia.org/wiki/Object-oriented_programming&quot;&gt;OOP&lt;/a&gt;）互为补充。简单来说，AOP可以在不修改既有代码的情况下改变代码的行为。开发者通过定义一组规则，在特定的类方法前后增加逻辑，如记录日志、性能监控、事务管理等。这些逻辑称为切面（Aspect），规则称为切点（Pointcut），在调用前还是调用后执行称为通知（Before advice, After advice）。最后，我们可以选择在编译期将这些逻辑写入类文件，或是在运行时动态加载这些逻辑，这是两种不同的织入方式（Compile-time weaving, Load-time weaving）。&lt;/p&gt;
&lt;p&gt;对于领域模型的依赖注入，我们要做的就是使用AOP在对象创建后调用Spring框架来注入依赖。幸运的是，&lt;a href=&quot;http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#aop&quot;&gt;Spring AOP&lt;/a&gt;已经提供了&lt;code&gt;@Configurable&lt;/code&gt;注解来帮助我们实现这一需求。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>贫血领域模型</title>
    <link href="http://shzhangji.com/blog/2015/09/05/anemic-domain-model/"/>
    <id>http://shzhangji.com/blog/2015/09/05/anemic-domain-model/</id>
    <published>2015-09-05T11:02:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://www.martinfowler.com/bliki/AnemicDomainModel.html" target="_blank" rel="external">http://www.martinfowler.com/bliki/AnemicDomainModel.html</a></p>
<p>贫血领域模型是一个存在已久的反模式，目前仍有许多拥趸者。一次我和Eric Evans聊天谈到它时，都觉得这个模型似乎越来越流行了。作为<a href="http://martinfowler.com/eaaCatalog/domainModel.html" target="_blank" rel="external">领域模型</a>的推广者，我们觉得这不是一件好事。</p>
<p>贫血领域模型的最初症状是：它第一眼看起来还真像这么回事儿。项目中有许多对象，它们的命名都是根据领域来的。对象之间有着丰富的连接方式，和真正的领域模型非常相似。但当你检视这些对象的行为时，会发现它们基本上没有任何行为，仅仅是一堆getter和setter的集合。其实这些对象在设计之初就被定义为只能包含数据，不能加入领域逻辑。这些逻辑要全部写入一组叫Service的对象中。这些Service构建在领域模型之上，使用这些模型来传递数据。</p>
<p>这种反模式的恐怖之处在于，它完全是和面向对象设计背道而驰。面向对象设计主张将数据和行为绑定在一起，而贫血领域模型则更像是一种面向过程设计，我和Eric在Smalltalk时就极力反对这种做法。更糟糕的时，很多人认为这些贫血领域对象是真正的对象，从而彻底误解了面向对象设计的涵义。</p>
<a id="more"></a>
<p>如今，面向对象的概念已经传播得很广泛了，而要反对这种贫血领域模型的做法，我还需要更多论据。贫血领域模型的根本问题在于，它引入了领域模型设计的所有成本，却没有带来任何好处。最主要的成本是将对象映射到数据库中，从而产生了一个对象关系映射层。只有当你充分使用了面向对象设计来组织复杂的业务逻辑后，这一成本才能够被抵消。如果将所有行为都写入到Service对象，那最终你会得到一组<a href="http://martinfowler.com/eaaCatalog/transactionScript.html" target="_blank" rel="external">事务处理脚本</a>，从而错过了领域模型带来的好处。正如我在<a href="http://martinfowler.com/books/eaa.html" target="_blank" rel="external">企业应用架构模式</a>一书中说到的，领域模型并不一定是最好的工具。</p>
<p>还需要强调的是，将行为放入领域模型，这点和分层设计（领域层、持久化层、展现层等）并不冲突。因为领域模型中放入的是和领域相关的逻辑——验证、计算、商业规则等。如果你要讨论能否将数据访问和展现逻辑放入到领域模型中，这就不在本文论述范围之内了。</p>
<p>一些面向对象专家的观点有时会让人产生疑惑，他们认为的确应该有一个面向过程的<a href="http://martinfowler.com/eaaCatalog/serviceLayer.html" target="_blank" rel="external">服务层</a>。但是，这并不意味着领域模型就不应该包含行为。事实上，服务层需要和一组富含行为的领域模型结合起来使用。</p>
<p>Eric Evans的<a href="http://domaindrivendesign.org/books/" target="_blank" rel="external">领域驱动设计</a>一书中有关于分层的论述：</p>
<blockquote>
<p>应用层（也就是上文中的服务层）：用来描述应用程序所要做的工作，并调度丰富的领域模型来完成它。这个层次的任务是描述业务逻辑，或和其它项目的应用层做交互。这个层次很薄，它不包含任何业务规则或知识，仅用于调度和派发任务给下一层的领域模型。这个层次没有业务状态，但可以为用户或程序提供任务状态。</p>
<p>领域层（或者叫模型层）：用于表示业务逻辑、业务场景和规则。这个层次会控制和使用业务状态，即使这些状态最终会交由持久化层来存储。总之，这个层次是软件的核心。</p>
</blockquote>
<p>关键点在于服务层是很薄的——所有重要的业务逻辑都写在领域层。他在服务模式中复述了这一观点：</p>
<blockquote>
<p>如今人们常犯的错误是不愿花时间将业务逻辑放置到合适的领域模型中，从而逐渐形成面向过程的程序设计。</p>
</blockquote>
<p>我不清楚为什么这种反模式会那么常见。我怀疑是因为大多数人并没有使用过一个设计良好的领域模型，特别是那些以数据为中心的开发人员。此外，有些技术也会推动这种反模式，比如J2EE的Entity Bean，这会让我更倾向于使用<a href="http://www.martinfowler.com/bliki/POJO.html" target="_blank" rel="external">POJO</a>领域模型。</p>
<p>总之，如果你将大部分行为都放置在服务层，那么你就会失去领域模型带来的好处。如果你将所有行为都放在服务层，那就无可救药了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://www.martinfowler.com/bliki/AnemicDomainModel.html&quot;&gt;http://www.martinfowler.com/bliki/AnemicDomainModel.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;贫血领域模型是一个存在已久的反模式，目前仍有许多拥趸者。一次我和Eric Evans聊天谈到它时，都觉得这个模型似乎越来越流行了。作为&lt;a href=&quot;http://martinfowler.com/eaaCatalog/domainModel.html&quot;&gt;领域模型&lt;/a&gt;的推广者，我们觉得这不是一件好事。&lt;/p&gt;
&lt;p&gt;贫血领域模型的最初症状是：它第一眼看起来还真像这么回事儿。项目中有许多对象，它们的命名都是根据领域来的。对象之间有着丰富的连接方式，和真正的领域模型非常相似。但当你检视这些对象的行为时，会发现它们基本上没有任何行为，仅仅是一堆getter和setter的集合。其实这些对象在设计之初就被定义为只能包含数据，不能加入领域逻辑。这些逻辑要全部写入一组叫Service的对象中。这些Service构建在领域模型之上，使用这些模型来传递数据。&lt;/p&gt;
&lt;p&gt;这种反模式的恐怖之处在于，它完全是和面向对象设计背道而驰。面向对象设计主张将数据和行为绑定在一起，而贫血领域模型则更像是一种面向过程设计，我和Eric在Smalltalk时就极力反对这种做法。更糟糕的时，很多人认为这些贫血领域对象是真正的对象，从而彻底误解了面向对象设计的涵义。&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
    
  </entry>
  
  <entry>
    <title>View Spark Source in Eclipse</title>
    <link href="http://shzhangji.com/blog/2015/09/01/view-spark-source-in-eclipse/"/>
    <id>http://shzhangji.com/blog/2015/09/01/view-spark-source-in-eclipse/</id>
    <published>2015-09-01T10:38:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>Reading source code is a great way to learn opensource projects. I used to read Java projects’ source code on <a href="http://grepcode.com/" target="_blank" rel="external">GrepCode</a> for it is online and has very nice cross reference features. As for Scala projects such as <a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a>, though its source code can be found on <a href="https://github.com/apache/spark/" target="_blank" rel="external">GitHub</a>, it’s quite necessary to setup an IDE to view the code more efficiently. Here’s a howto of viewing Spark source code in Eclipse.</p>
<h2 id="Install-Eclipse-and-Scala-IDE-Plugin"><a href="#Install-Eclipse-and-Scala-IDE-Plugin" class="headerlink" title="Install Eclipse and Scala IDE Plugin"></a>Install Eclipse and Scala IDE Plugin</h2><p>One can download Eclipse from <a href="http://www.eclipse.org/downloads/" target="_blank" rel="external">here</a>. I recommend the “Eclipse IDE for Java EE Developers”, which contains a lot of daily-used features.</p>
<p><img src="/images/scala-ide.png" alt=""></p>
<p>Then go to Scala IDE’s <a href="http://scala-ide.org/download/current.html" target="_blank" rel="external">official site</a> and install the plugin through update site or zip archive.</p>
<h2 id="Generate-Project-File-with-Maven"><a href="#Generate-Project-File-with-Maven" class="headerlink" title="Generate Project File with Maven"></a>Generate Project File with Maven</h2><p>Spark is mainly built with Maven, so make sure you have Maven installed on your box, and download the latest Spark source code from <a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">here</a>, unarchive it, and execute the following command:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ mvn -am -pl core dependency:resolve eclipse:eclipse</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>This command does a bunch of things. First, it indicates what modules should be built. Spark is a large project with multiple modules. Currently we’re only interested in its core module, so <code>-pl</code> or <code>--projects</code> is used. <code>-am</code> or <code>--also-make</code> tells Maven to build core module’s dependencies as well. We can see the module list in output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[INFO] Scanning for projects...</div><div class="line">[INFO] ------------------------------------------------------------------------</div><div class="line">[INFO] Reactor Build Order:</div><div class="line">[INFO]</div><div class="line">[INFO] Spark Project Parent POM</div><div class="line">[INFO] Spark Launcher Project</div><div class="line">[INFO] Spark Project Networking</div><div class="line">[INFO] Spark Project Shuffle Streaming Service</div><div class="line">[INFO] Spark Project Unsafe</div><div class="line">[INFO] Spark Project Core</div></pre></td></tr></table></figure>
<p><code>dependency:resolve</code> tells Maven to download all dependencies. <code>eclipse:eclipse</code> will generate the <code>.project</code> and <code>.classpath</code> files for Eclipse. But the result is not perfect, both files need some fixes.</p>
<p>Edit <code>core/.classpath</code>, change the following two lines:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">classpathentry</span> <span class="attr">kind</span>=<span class="string">"src"</span> <span class="attr">path</span>=<span class="string">"src/main/scala"</span> <span class="attr">including</span>=<span class="string">"**/*.java"</span>/&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">classpathentry</span> <span class="attr">kind</span>=<span class="string">"src"</span> <span class="attr">path</span>=<span class="string">"src/test/scala"</span> <span class="attr">output</span>=<span class="string">"target/scala-2.10/test-classes"</span> <span class="attr">including</span>=<span class="string">"**/*.java"</span>/&gt;</span></div></pre></td></tr></table></figure>
<p>to</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">classpathentry</span> <span class="attr">kind</span>=<span class="string">"src"</span> <span class="attr">path</span>=<span class="string">"src/main/scala"</span> <span class="attr">including</span>=<span class="string">"**/*.java|**/*.scala"</span>/&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">classpathentry</span> <span class="attr">kind</span>=<span class="string">"src"</span> <span class="attr">path</span>=<span class="string">"src/test/scala"</span> <span class="attr">output</span>=<span class="string">"target/scala-2.10/test-classes"</span> <span class="attr">including</span>=<span class="string">"**/*.java|**/*.scala"</span>/&gt;</span></div></pre></td></tr></table></figure>
<p>Edit <code>core/.project</code>, make it looks like this:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">buildSpec</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">buildCommand</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>org.scala-ide.sdt.core.scalabuilder<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">buildCommand</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">buildSpec</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">natures</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">nature</span>&gt;</span>org.scala-ide.sdt.core.scalanature<span class="tag">&lt;/<span class="name">nature</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">nature</span>&gt;</span>org.eclipse.jdt.core.javanature<span class="tag">&lt;/<span class="name">nature</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">natures</span>&gt;</span></div></pre></td></tr></table></figure>
<p>Now you can import “Existing Projects into Workspace”, including <code>core</code>, <code>launcher</code>, <code>network</code>, and <code>unsafe</code>.</p>
<h2 id="Miscellaneous"><a href="#Miscellaneous" class="headerlink" title="Miscellaneous"></a>Miscellaneous</h2><h3 id="Access-restriction-The-type-‘Unsafe’-is-not-API"><a href="#Access-restriction-The-type-‘Unsafe’-is-not-API" class="headerlink" title="Access restriction: The type ‘Unsafe’ is not API"></a>Access restriction: The type ‘Unsafe’ is not API</h3><p>For module <code>spark-unsafe</code>, Eclipse will report an error “Access restriction: The type ‘Unsafe’ is not API (restriction on required library /path/to/jre/lib/rt.jar”. To fix this, right click the “JRE System Library” entry in Package Explorer, change it to “Workspace default JRE”.</p>
<h3 id="Download-Sources-and-Javadocs"><a href="#Download-Sources-and-Javadocs" class="headerlink" title="Download Sources and Javadocs"></a>Download Sources and Javadocs</h3><p>Add the following entry into pom’s project / build / plugins:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-eclipse-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">downloadSources</span>&gt;</span>true<span class="tag">&lt;/<span class="name">downloadSources</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">downloadJavadocs</span>&gt;</span>true<span class="tag">&lt;/<span class="name">downloadJavadocs</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="build-helper-maven-plugin"><a href="#build-helper-maven-plugin" class="headerlink" title="build-helper-maven-plugin"></a>build-helper-maven-plugin</h3><p>Since Spark is a mixture of Java and Scala code, and the maven-eclipse-plugin only knows about Java source files, so we need to use build-helper-maven-plugin to include the Scala sources, as is described <a href="http://docs.scala-lang.org/tutorials/scala-with-maven.html#integration-with-eclipse-scala-ide24" target="_blank" rel="external">here</a>. Fortunately, Spark’s pom.xml has already included this setting.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="http://docs.scala-lang.org/tutorials/scala-with-maven.html" target="_blank" rel="external">http://docs.scala-lang.org/tutorials/scala-with-maven.html</a></li>
<li><a href="https://wiki.scala-lang.org/display/SIW/ScalaEclipseMaven" target="_blank" rel="external">https://wiki.scala-lang.org/display/SIW/ScalaEclipseMaven</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Reading source code is a great way to learn opensource projects. I used to read Java projects’ source code on &lt;a href=&quot;http://grepcode.com/&quot;&gt;GrepCode&lt;/a&gt; for it is online and has very nice cross reference features. As for Scala projects such as &lt;a href=&quot;http://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;, though its source code can be found on &lt;a href=&quot;https://github.com/apache/spark/&quot;&gt;GitHub&lt;/a&gt;, it’s quite necessary to setup an IDE to view the code more efficiently. Here’s a howto of viewing Spark source code in Eclipse.&lt;/p&gt;
&lt;h2 id=&quot;Install-Eclipse-and-Scala-IDE-Plugin&quot;&gt;&lt;a href=&quot;#Install-Eclipse-and-Scala-IDE-Plugin&quot; class=&quot;headerlink&quot; title=&quot;Install Eclipse and Scala IDE Plugin&quot;&gt;&lt;/a&gt;Install Eclipse and Scala IDE Plugin&lt;/h2&gt;&lt;p&gt;One can download Eclipse from &lt;a href=&quot;http://www.eclipse.org/downloads/&quot;&gt;here&lt;/a&gt;. I recommend the “Eclipse IDE for Java EE Developers”, which contains a lot of daily-used features.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/scala-ide.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Then go to Scala IDE’s &lt;a href=&quot;http://scala-ide.org/download/current.html&quot;&gt;official site&lt;/a&gt; and install the plugin through update site or zip archive.&lt;/p&gt;
&lt;h2 id=&quot;Generate-Project-File-with-Maven&quot;&gt;&lt;a href=&quot;#Generate-Project-File-with-Maven&quot; class=&quot;headerlink&quot; title=&quot;Generate Project File with Maven&quot;&gt;&lt;/a&gt;Generate Project File with Maven&lt;/h2&gt;&lt;p&gt;Spark is mainly built with Maven, so make sure you have Maven installed on your box, and download the latest Spark source code from &lt;a href=&quot;http://spark.apache.org/downloads.html&quot;&gt;here&lt;/a&gt;, unarchive it, and execute the following command:&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;$ mvn -am -pl core dependency:resolve eclipse:eclipse&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="english" scheme="http://shzhangji.com/tags/english/"/>
    
      <category term="spark" scheme="http://shzhangji.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>HotSpot JVM中的对象指针压缩</title>
    <link href="http://shzhangji.com/blog/2015/06/25/compressed-oops-in-the-hotspot-jvm/"/>
    <id>http://shzhangji.com/blog/2015/06/25/compressed-oops-in-the-hotspot-jvm/</id>
    <published>2015-06-25T09:41:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://wiki.openjdk.java.net/display/HotSpot/CompressedOops" target="_blank" rel="external">https://wiki.openjdk.java.net/display/HotSpot/CompressedOops</a></p>
<h2 id="什么是一般对象指针？"><a href="#什么是一般对象指针？" class="headerlink" title="什么是一般对象指针？"></a>什么是一般对象指针？</h2><p>一般对象指针（oop, ordinary object pointer）是HotSpot虚拟机的一个术语，表示受托管的对象指针。它的大小通常和本地指针是一样的。Java应用程序和GC子系统会非常小心地跟踪这些受托管的指针，以便在销毁对象时回收内存空间，或是在对空间进行整理时移动（复制）对象。</p>
<p>在一些从Smalltalk和Self演变而来的虚拟机实现中都有一般对象指针这个术语，包括：</p>
<ul>
<li><a href="https://github.com/russellallen/self/blob/master/vm/src/any/objects/oop.hh" target="_blank" rel="external">Self</a>：一门基于原型的语言，是Smalltalk的近亲</li>
<li><a href="http://code.google.com/p/strongtalk/wiki/VMTypesForSmalltalkObjects" target="_blank" rel="external">Strongtalk</a>：Smalltalk的一种实现</li>
<li><a href="http://hg.openjdk.java.net/hsx/hotspot-main/hotspot/file/0/src/share/vm/oops/oop.hpp" target="_blank" rel="external">Hotspot</a></li>
<li><a href="http://code.google.com/p/v8/source/browse/trunk/src/objects.h" target="_blank" rel="external">V8</a></li>
</ul>
<p>部分系统中会使用小整型（smi, small integers）这个名称，表示一个指向30位整型的虚拟指针。这个术语在Smalltalk的V8实现中也可以看到。</p>
<h2 id="为什么需要压缩？"><a href="#为什么需要压缩？" class="headerlink" title="为什么需要压缩？"></a>为什么需要压缩？</h2><p>在<a href="http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html" target="_blank" rel="external">LP64</a>系统中，指针需要使用64位来表示；<a href="http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html" target="_blank" rel="external">ILP32</a>系统中则只需要32位。在ILP32系统中，堆内存的大小只能支持到4Gb，这对很多应用程序来说是不够的。在LP64系统中，所有应用程序运行时占用的空间都会比ILP32大1.5倍左右，这是因为指针占用的空间增加了。虽然内存是比较廉价的，但网络带宽和缓存容量是紧张的。所以，为了解决4Gb的限制而增加堆内存的占用空间，就有些得不偿失了。</p>
<p>在x86芯片中，ILP32模式可用的寄存器数量是LP64模式的一半。SPARC没有此限制；RISC芯片本来就提供了很多寄存器，LP64模式下会提供更多。</p>
<p>压缩后的一般对象指针在使用时需要将32位整型按因数8进行扩展，并加到一个64位的基础地址上，从而找到所指向的对象。这种方法可以表示四十亿个对象，相当于32Gb的堆内存。同时，使用此法压缩数据结构也能达到和ILP32系统相近的效果。</p>
<p>我们使用<em>解码</em>来表示从32位对象指针转换成64位地址的过程，其反过程则称为<em>编码</em>。</p>
<a id="more"></a>
<h2 id="什么情况下会进行压缩？"><a href="#什么情况下会进行压缩？" class="headerlink" title="什么情况下会进行压缩？"></a>什么情况下会进行压缩？</h2><p>运行在ILP32模式下的Java虚拟机，或在运行时将<code>UseCompressedOops</code>标志位关闭，则所有的对象指针都不会被压缩。</p>
<p>如果<code>UseCompressedOops</code>是打开的，则以下对象的指针会被压缩：</p>
<ul>
<li>所有对象的<a href="http://stackoverflow.com/questions/16721021/what-is-klass-klassklass" target="_blank" rel="external">klass</a>属性</li>
<li>所有<a href="http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/sun/jvm/hotspot/oops/Oop.java#Oop" target="_blank" rel="external">对象指针实例</a>的属性</li>
<li>所有对象指针数组的元素（objArray）</li>
</ul>
<p>HotSpot VM中，用于表示Java类的数据结构是不会压缩的，这部分数据都存放在永久代（PermGen）中。</p>
<p>在解释器中，一般对象指针也是不压缩的，包括JVM本地变量和栈内元素、调用参数、返回值等。解释器会在读取堆内对象时解码对象指针，并在存入时进行编码。</p>
<p>同样，方法调用序列（method calling sequence），无论是解释执行还是编译执行，都不会使用对象指针压缩。</p>
<p>在编译后的代码中，对象指针是否压缩取决于不同的优化结果。优化后的代码可能会将压缩后的对象指针直接从一处搬往另一处，而不进行编解码操作。如果芯片（如x86）支持解码，那在使用对象指针时就不需要自行解码了。</p>
<p>所以，以下数据结构在编译后的代码中既可以是压缩后的对象指针，也可能是本地地址：</p>
<ul>
<li>寄存器或溢出槽（spill slot）中的数据</li>
<li>对象指针映射表（GC映射表）</li>
<li>调试信息</li>
<li>嵌套在机器码中的对象指针（在非RISC芯片中支持，如x86）</li>
<li><a href="http://openjdk.java.net/groups/hotspot/docs/HotSpotGlossary.html#nmethod" target="_blank" rel="external">nmethod</a>常量区（包括那些影响到机器码的重定位操作）</li>
</ul>
<p>在HotSpot JVM的C++代码部分，对象指针压缩与否反映在C++的静态类型系统中。通常情况下，对象指针是不压缩的。具体来说，C++的成员函数在操作本地代码传递过来的指针时（如<em>this</em>），其执行过程不会有什么不同。JVM中的部分方法则提供了重载，能够处理压缩和不压缩的对象指针。</p>
<p>重要的C++数据不会被压缩：</p>
<ul>
<li>C++对象指针（<em>this</em>）</li>
<li>受托管指针的句柄（Handle类型等）</li>
<li>JNI句柄（jobject类型）</li>
</ul>
<p>C++在使用对象指针压缩时（加载和存储等），会以<code>narrowOop</code>作为标记。</p>
<h2 id="使用压缩寻址"><a href="#使用压缩寻址" class="headerlink" title="使用压缩寻址"></a>使用压缩寻址</h2><p>以下是使用对象指针压缩的x86指令示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">! int R8; oop[] R9;  // R9是64位</div><div class="line">! oop R10 = R9[R8];  // R10是32位</div><div class="line">! 从原始基址指针加载压缩对象指针：</div><div class="line">movl R10, [R9 + R8&lt;&lt;3 + 16]</div><div class="line">! klassOop R11 = R10._klass;  // R11是32位</div><div class="line">! void* const R12 = GetHeapBase();</div><div class="line">! 从压缩基址指针加载klass指针：</div><div class="line">movl R11, [R12 + R10&lt;&lt;3 + 8]</div></pre></td></tr></table></figure>
<p>以下sparc指令用于解压对象指针（可为空）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">! java.lang.Thread::getThreadGroup@1 (line 1072)</div><div class="line">! L1 = L7.group</div><div class="line">ld  [ %l7 + 0x44 ], %l1</div><div class="line">! L3 = decode(L1)</div><div class="line">cmp  %l1, 0</div><div class="line">sllx  %l1, 3, %l3</div><div class="line">brnz,a   %l3, .+8</div><div class="line">add  %l3, %g6, %l3  ! %g6是常量堆基址</div></pre></td></tr></table></figure>
<p><em>输出中的注解来自<a href="https://wiki.openjdk.java.net/display/HotSpot/PrintAssembly" target="_blank" rel="external">PrintAssembly插件</a>。</em></p>
<h2 id="空值处理"><a href="#空值处理" class="headerlink" title="空值处理"></a>空值处理</h2><p>32位零值会被解压为64位空值，这就需要在解码逻辑中加入一段特殊的逻辑。或者说可以默认某些压缩对象指针肯定不会空（如klass的属性），这样就能使用简单一些的编解码逻辑了。</p>
<p>隐式空值检测对JVM的性能至关重要，包括解释执行和编译执行的字节码。对于一个偏移量较小的对象指针，如果基址指针为空，那很有可能造成系统崩溃，因为虚拟地址空间的前几页通常是没有映射的。</p>
<p>对于压缩对象指针，我们可以用一种类似的技巧来欺骗它：将堆内存前几页的映射去除，如果解压出的指针为空（相对于基址指针），仍可以用它来做加载和存储的操作，隐式空值检测也能照常运行。</p>
<h2 id="对象头信息"><a href="#对象头信息" class="headerlink" title="对象头信息"></a>对象头信息</h2><p>对象头信息通常包含几个部分：固定长度的标志位；klass信息；如果对象是数组，则包含一个32位的信息，并可能追加一个32位的空隙进行对齐；零个或多个实例属性，数组元素，元信息等。（有趣的是，Klass的对象头信息包含了一个C++的<a href="https://en.wikipedia.org/wiki/Virtual_method_table" target="_blank" rel="external">虚拟方法表</a>）</p>
<p>上述追加的32位空隙通常也可用于存储属性信息。</p>
<p>如果<code>UseCompressedOops</code>关闭，标志位和klass都是正常长度。对于数组，32位空隙在LP64系统中总是存在；而ILP32系统中，只有当数组元素是64位数据时才存在这个空隙。</p>
<p>如果<code>UseCompressedOops</code>打开，则klass是32位的。非数组对象在klass后会追加一个空隙，而数组对象则直接开始存储元素信息。</p>
<h2 id="零基压缩技术"><a href="#零基压缩技术" class="headerlink" title="零基压缩技术"></a>零基压缩技术</h2><p>压缩对象指针（narrow-oop）是基于某个地址的偏移量，这个基础地址（narrow-oop-base）是由Java堆内存基址减去一个内存页的大小得来的，从而支持隐式空值检测。所以一个属性字段的地址可以这样得到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;narrow-oop-base&gt; + (&lt;narrow-oop&gt; &lt;&lt; 3) + &lt;field-offset&gt;.</div></pre></td></tr></table></figure>
<p>如果基础地址可以是0（Java堆内存不一定要从0偏移量开始），那么公式就可以简化为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(&lt;narrow-oop &lt;&lt; 3) + &lt;field-offset&gt;</div></pre></td></tr></table></figure>
<p>理论上说，这一步可以省去一次寄存器上的加和操作。而且使用零基压缩技术后，空值检测也就不需要了。</p>
<p>之前的解压代码是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">if (&lt;narrow-oop&gt; == NULL)</div><div class="line">    &lt;wide_oop&gt; = NULL</div><div class="line">else</div><div class="line">    &lt;wide_oop&gt; = &lt;narrow-oop-base&gt; + (&lt;narrow-oop&gt; &lt;&lt; 3)</div></pre></td></tr></table></figure>
<p>使用零基压缩后，只需使用移位操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;wide_oop&gt; = &lt;narrow-oop&gt; &lt;&lt; 3</div></pre></td></tr></table></figure>
<p>零基压缩技术会根据堆内存的大小以及平台特性来选择不同的策略：</p>
<ol>
<li>堆内存小于4Gb，直接使用压缩对象指针进行寻址，无需压缩和解压；</li>
<li>堆内存大于4Gb，则尝试分配小于32Gb的堆内存，并使用零基压缩技术；</li>
<li>如果仍然失败，则使用普通的对象指针压缩技术，即<code>narrow-oop-base</code>。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;https://wiki.openjdk.java.net/display/HotSpot/CompressedOops&quot;&gt;https://wiki.openjdk.java.net/display/HotSpot/CompressedOops&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;什么是一般对象指针？&quot;&gt;&lt;a href=&quot;#什么是一般对象指针？&quot; class=&quot;headerlink&quot; title=&quot;什么是一般对象指针？&quot;&gt;&lt;/a&gt;什么是一般对象指针？&lt;/h2&gt;&lt;p&gt;一般对象指针（oop, ordinary object pointer）是HotSpot虚拟机的一个术语，表示受托管的对象指针。它的大小通常和本地指针是一样的。Java应用程序和GC子系统会非常小心地跟踪这些受托管的指针，以便在销毁对象时回收内存空间，或是在对空间进行整理时移动（复制）对象。&lt;/p&gt;
&lt;p&gt;在一些从Smalltalk和Self演变而来的虚拟机实现中都有一般对象指针这个术语，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/russellallen/self/blob/master/vm/src/any/objects/oop.hh&quot;&gt;Self&lt;/a&gt;：一门基于原型的语言，是Smalltalk的近亲&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://code.google.com/p/strongtalk/wiki/VMTypesForSmalltalkObjects&quot;&gt;Strongtalk&lt;/a&gt;：Smalltalk的一种实现&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://hg.openjdk.java.net/hsx/hotspot-main/hotspot/file/0/src/share/vm/oops/oop.hpp&quot;&gt;Hotspot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://code.google.com/p/v8/source/browse/trunk/src/objects.h&quot;&gt;V8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;部分系统中会使用小整型（smi, small integers）这个名称，表示一个指向30位整型的虚拟指针。这个术语在Smalltalk的V8实现中也可以看到。&lt;/p&gt;
&lt;h2 id=&quot;为什么需要压缩？&quot;&gt;&lt;a href=&quot;#为什么需要压缩？&quot; class=&quot;headerlink&quot; title=&quot;为什么需要压缩？&quot;&gt;&lt;/a&gt;为什么需要压缩？&lt;/h2&gt;&lt;p&gt;在&lt;a href=&quot;http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html&quot;&gt;LP64&lt;/a&gt;系统中，指针需要使用64位来表示；&lt;a href=&quot;http://docs.oracle.com/cd/E19620-01/805-3024/lp64-1/index.html&quot;&gt;ILP32&lt;/a&gt;系统中则只需要32位。在ILP32系统中，堆内存的大小只能支持到4Gb，这对很多应用程序来说是不够的。在LP64系统中，所有应用程序运行时占用的空间都会比ILP32大1.5倍左右，这是因为指针占用的空间增加了。虽然内存是比较廉价的，但网络带宽和缓存容量是紧张的。所以，为了解决4Gb的限制而增加堆内存的占用空间，就有些得不偿失了。&lt;/p&gt;
&lt;p&gt;在x86芯片中，ILP32模式可用的寄存器数量是LP64模式的一半。SPARC没有此限制；RISC芯片本来就提供了很多寄存器，LP64模式下会提供更多。&lt;/p&gt;
&lt;p&gt;压缩后的一般对象指针在使用时需要将32位整型按因数8进行扩展，并加到一个64位的基础地址上，从而找到所指向的对象。这种方法可以表示四十亿个对象，相当于32Gb的堆内存。同时，使用此法压缩数据结构也能达到和ILP32系统相近的效果。&lt;/p&gt;
&lt;p&gt;我们使用&lt;em&gt;解码&lt;/em&gt;来表示从32位对象指针转换成64位地址的过程，其反过程则称为&lt;em&gt;编码&lt;/em&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark Streaming Logging Configuration</title>
    <link href="http://shzhangji.com/blog/2015/05/31/spark-streaming-logging-configuration/"/>
    <id>http://shzhangji.com/blog/2015/05/31/spark-streaming-logging-configuration/</id>
    <published>2015-05-31T10:18:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>Spark Streaming applications tend to run forever, so their log files should be properly handled, to avoid exploding server hard drives. This article will give some practical advices of dealing with these log files, on both Spark on YARN and standalone mode.</p>
<h2 id="Log4j’s-RollingFileAppender"><a href="#Log4j’s-RollingFileAppender" class="headerlink" title="Log4j’s RollingFileAppender"></a>Log4j’s RollingFileAppender</h2><p>Spark uses log4j as logging facility. The default configuraiton is to write all logs into standard error, which is fine for batch jobs. But for streaming jobs, we’d better use rolling-file appender, to cut log files by size and keep only several recent files. Here’s an example:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">log4j.rootLogger=INFO, rolling</div><div class="line"></div><div class="line">log4j.appender.rolling=org.apache.log4j.RollingFileAppender</div><div class="line">log4j.appender.rolling.layout=org.apache.log4j.PatternLayout</div><div class="line">log4j.appender.rolling.layout.conversionPattern=[%d] %p %m (%c)%n</div><div class="line">log4j.appender.rolling.maxFileSize=50MB</div><div class="line">log4j.appender.rolling.maxBackupIndex=5</div><div class="line">log4j.appender.rolling.file=/var/log/spark/$&#123;dm.logging.name&#125;.log</div><div class="line">log4j.appender.rolling.encoding=UTF-8</div><div class="line"></div><div class="line">log4j.logger.org.apache.spark=WARN</div><div class="line">log4j.logger.org.eclipse.jetty=WARN</div><div class="line"></div><div class="line">log4j.logger.com.anjuke.dm=$&#123;dm.logging.level&#125;</div></pre></td></tr></table></figure>
<p>This means log4j will roll the log file by 50MB and keep only 5 recent files. These files are saved in <code>/var/log/spark</code> directory, with filename picked from system property <code>dm.logging.name</code>. We also set the logging level of our package <code>com.anjuke.dm</code> according to <code>dm.logging.level</code> property. Another thing to mention is that we set <code>org.apache.spark</code> to level <code>WARN</code>, so as to ignore verbose logs from spark.</p>
<a id="more"></a>
<h2 id="Standalone-Mode"><a href="#Standalone-Mode" class="headerlink" title="Standalone Mode"></a>Standalone Mode</h2><p>In standalone mode, Spark Streaming driver is running on the machine where you submit the job, and each Spark worker node will run an executor for this job. So you need to setup log4j for both driver and executor. </p>
<p>For driver, since it’s a long-running application, we tend to use some process management tools like <a href="http://supervisord.org/" target="_blank" rel="external">supervisor</a> to monitor it. And supervisor itself provides the facility of rolling log files, so we can safely write all logs into standard output when setting up driver’s log4j.</p>
<p>For executor, there’re two approaches. One is using <code>spark.executor.logs.rolling.strategy</code> provided by Spark 1.1 and above. It has both time-based and size-based rolling methods. These log files are stored in Spark’s work directory. You can find more details in the <a href="https://spark.apache.org/docs/1.1.0/configuration.html" target="_blank" rel="external">documentation</a>. </p>
<p>The other approach is to setup log4j manually, when you’re using a legacy version, or want to gain more control on the logging process. Here are the steps:</p>
<ol>
<li>Make sure the logging directory exists on all worker nodes. You can use some provisioning tools like <a href="https://github.com/ansible/ansible" target="_blank" rel="external">ansbile</a> to create them.</li>
<li>Create driver’s and executor’s log4j configuration files, and distribute the executor’s to all worker nodes.</li>
<li>Use the above two files in <code>spark-submit</code> command:</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">spark-submit</div><div class="line">  --master spark://127.0.0.1:7077</div><div class="line">  --driver-java-options &quot;-Dlog4j.configuration=file:/path/to/log4j-driver.properties -Ddm.logging.level=DEBUG&quot;</div><div class="line">  --conf &quot;spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/path/to/log4j-executor.properties -Ddm.logging.name=myapp -Ddm.logging.level=DEBUG&quot;</div><div class="line">  ...</div></pre></td></tr></table></figure>
<h2 id="Spark-on-YARN"><a href="#Spark-on-YARN" class="headerlink" title="Spark on YARN"></a>Spark on YARN</h2><p><a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/index.html" target="_blank" rel="external">YARN</a> is a <strong>resource manager</strong> introduced by Hadoop2. Now we can run differenct computational frameworks on the same cluster, like MapReduce, Spark, Storm, etc. The basic unit of YARN is called container, which represents a certain amount of resource (currently memory and virtual CPU cores). Every container has its working directory, and all related files such as application command (jars) and log files are stored in this directory.</p>
<p>When running Spark on YARN, there is a system property <code>spark.yarn.app.container.log.dir</code> indicating the container’s log directory. We only need to replace one line of the above log4j config:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">log4j.appender.rolling.file=$&#123;spark.yarn.app.container.log.dir&#125;/spark.log</div></pre></td></tr></table></figure>
<p>And these log files can be viewed on YARN’s web UI:</p>
<p><img src="/images/spark/yarn-logs.png" alt=""></p>
<p>The <code>spark-submit</code> command is as following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">spark-submit</div><div class="line">  --master yarn-cluster</div><div class="line">  --files /path/to/log4j-spark.properties</div><div class="line">  --conf &quot;spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j-spark.properties&quot;</div><div class="line">  --conf &quot;spark.executor.extraJavaOptions=-Dlog4j.configuration=log4j-spark.properties&quot;</div><div class="line">  ...</div></pre></td></tr></table></figure>
<p>As you can see, both driver and executor use the same configuration file. That is because in <code>yarn-cluster</code> mode, driver is also run as a container in YARN. In fact, the <code>spark-submit</code> command will just quit after job submission.</p>
<p>If YARN’s <a href="http://zh.hortonworks.com/blog/simplifying-user-logs-management-and-access-in-yarn/" target="_blank" rel="external">log aggregation</a> is enabled, application logs will be saved in HDFS after the job is done. One can use <code>yarn logs</code> command to view the files or browse directly into HDFS directory indicated by <code>yarn.nodemanager.log-dirs</code>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark Streaming applications tend to run forever, so their log files should be properly handled, to avoid exploding server hard drives. This article will give some practical advices of dealing with these log files, on both Spark on YARN and standalone mode.&lt;/p&gt;
&lt;h2 id=&quot;Log4j’s-RollingFileAppender&quot;&gt;&lt;a href=&quot;#Log4j’s-RollingFileAppender&quot; class=&quot;headerlink&quot; title=&quot;Log4j’s RollingFileAppender&quot;&gt;&lt;/a&gt;Log4j’s RollingFileAppender&lt;/h2&gt;&lt;p&gt;Spark uses log4j as logging facility. The default configuraiton is to write all logs into standard error, which is fine for batch jobs. But for streaming jobs, we’d better use rolling-file appender, to cut log files by size and keep only several recent files. Here’s an example:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;log4j.rootLogger=INFO, rolling&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling=org.apache.log4j.RollingFileAppender&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.layout=org.apache.log4j.PatternLayout&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.layout.conversionPattern=[%d] %p %m (%c)%n&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.maxFileSize=50MB&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.maxBackupIndex=5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.file=/var/log/spark/$&amp;#123;dm.logging.name&amp;#125;.log&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.appender.rolling.encoding=UTF-8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.logger.org.apache.spark=WARN&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.logger.org.eclipse.jetty=WARN&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;log4j.logger.com.anjuke.dm=$&amp;#123;dm.logging.level&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;This means log4j will roll the log file by 50MB and keep only 5 recent files. These files are saved in &lt;code&gt;/var/log/spark&lt;/code&gt; directory, with filename picked from system property &lt;code&gt;dm.logging.name&lt;/code&gt;. We also set the logging level of our package &lt;code&gt;com.anjuke.dm&lt;/code&gt; according to &lt;code&gt;dm.logging.level&lt;/code&gt; property. Another thing to mention is that we set &lt;code&gt;org.apache.spark&lt;/code&gt; to level &lt;code&gt;WARN&lt;/code&gt;, so as to ignore verbose logs from spark.&lt;/p&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
      <category term="english" scheme="http://shzhangji.com/tags/english/"/>
    
      <category term="spark" scheme="http://shzhangji.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch Performance Tips</title>
    <link href="http://shzhangji.com/blog/2015/04/28/elasticsearch-performance-tips/"/>
    <id>http://shzhangji.com/blog/2015/04/28/elasticsearch-performance-tips/</id>
    <published>2015-04-28T15:08:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>Recently we’re using ElasticSearch as a data backend of our recommendation API, to serve both offline and online computed data to users. Thanks to ElasticSearch’s rich and out-of-the-box functionality, it doesn’t take much trouble to setup the cluster. However, we still encounter some misuse and unwise configurations. So here’s a list of ElasticSearch performance tips that we learned from practice.</p>
<h2 id="Tip-1-Set-Num-of-shards-to-Num-of-nodes"><a href="#Tip-1-Set-Num-of-shards-to-Num-of-nodes" class="headerlink" title="Tip 1 Set Num-of-shards to Num-of-nodes"></a>Tip 1 Set Num-of-shards to Num-of-nodes</h2><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard" target="_blank" rel="external">Shard</a> is the foundation of ElasticSearch’s distribution capability. Every index is splitted into several shards (default 5) and are distributed across cluster nodes. But this capability does not come free. Since data being queried reside in all shards (this behaviour can be changed by <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-routing" target="_blank" rel="external">routing</a>), ElasticSearch has to run this query on every shard, fetch the result, and merge them, like a map-reduce process. So if there’re too many shards, more than the number of cluter nodes, the query will be executed more than once on the same node, and it’ll also impact the merge phase. On the other hand, too few shards will also reduce the performance, for not all nodes are being utilized.</p>
<p>Shards have two roles, primary shard and replica shard. Replica shard serves as a backup to the primary shard. When primary goes down, the replica takes its job. It also helps improving the search and get performance, for these requests can be executed on either primary or replica shard.</p>
<p>Shards can be visualized by <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard" target="_blank" rel="external">elasticsearch-head</a> plugin:</p>
<p><img src="/images/elasticsearch/shards-head.png" alt=""></p>
<p>The <code>cu_docs</code> index has two shards <code>0</code> and <code>1</code>, with <code>number_of_replicas</code> set to 1. Primary shard <code>0</code> (bold bordered) resides in server <code>Leon</code>, and its replica in <code>Pris</code>. They are green becuase all primary shards have enough repicas sitting in different servers, so the cluster is healthy.</p>
<p>Since <code>number_of_shards</code> of an index cannot be changed after creation (while <code>number_of_replicas</code> can), one should choose this config wisely. Here are some suggestions:</p>
<ol>
<li>How many nodes do you have, now and future? If you’re sure you’ll only have 3 nodes, set number of shards to 2 and replicas to 1, so there’ll be 4 shards across 3 nodes. If you’ll add some servers in the future, you can set number of shards to 3, so when the cluster grows to 5 nodes, there’ll be 6 distributed shards.</li>
<li>How big is your index? If it’s small, one shard with one replica will due.</li>
<li>How is the read and write frequency, respectively? If it’s search heavy, setup more relicas. </li>
</ol>
<a id="more"></a>
<h2 id="Tip-2-Tuning-Memory-Usage"><a href="#Tip-2-Tuning-Memory-Usage" class="headerlink" title="Tip 2 Tuning Memory Usage"></a>Tip 2 Tuning Memory Usage</h2><p>ElasticSearch and its backend <a href="http://lucene.apache.org/" target="_blank" rel="external">Lucene</a> are both Java application. There’re various memory tuning settings related to heap and native memory.</p>
<h3 id="Set-Max-Heap-Size-to-Half-of-Total-Memory"><a href="#Set-Max-Heap-Size-to-Half-of-Total-Memory" class="headerlink" title="Set Max Heap Size to Half of Total Memory"></a>Set Max Heap Size to Half of Total Memory</h3><p>Generally speaking, more heap memory leads to better performance. But in ElasticSearch’s case, Lucene also requires a lot of native memory (or off-heap memory), to store index segments and provide fast search performance. But it does not load the files by itself. Instead, it relies on the operating system to cache the segement files in memory.</p>
<p>Say we have 16G memory and set -Xmx to 8G, it doesn’t mean the remaining 8G is wasted. Except for the memory OS preserves for itself, it will cache the frequently accessed disk files in memory automatically, which results in a huge performance gain.</p>
<p>Do not set heap size over 32G though, even you have more than 64G memory. The reason is described in <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops" target="_blank" rel="external">this link</a>.</p>
<p>Also, you should probably set -Xms to 8G as well, to avoid the overhead of heap memory growth.</p>
<h3 id="Disable-Swapping"><a href="#Disable-Swapping" class="headerlink" title="Disable Swapping"></a>Disable Swapping</h3><p>Swapping is a way to move unused program code and data to disk so as to provide more space for running applications and file caching. It also provides a buffer for the system to recover from memory exhaustion. But for critical application like ElasticSearch, being swapped is definitely a performance killer.</p>
<p>There’re several ways to disable swapping, and our choice is setting <code>bootstrap.mlockall</code> to true. This tells ElasticSearch to lock its memory space in RAM so that OS will not swap it out. One can confirm this setting via <code>http://localhost:9200/_nodes/process?pretty</code>.</p>
<p>If ElasticSearch is not started as root (and it probably shouldn’t), this setting may not take effect. For Ubuntu server, one needs to add <code>&lt;user&gt; hard memlock unlimited</code> to <code>/etc/security/limits.conf</code>, and run <code>ulimit -l unlimited</code> before starting ElasticSearch process.</p>
<h3 id="Increase-mmap-Counts"><a href="#Increase-mmap-Counts" class="headerlink" title="Increase mmap Counts"></a>Increase <code>mmap</code> Counts</h3><p>ElasticSearch uses memory mapped files, and the default <code>mmap</code> counts is low. Add <code>vm.max_map_count=262144</code> to <code>/etc/sysctl.conf</code>, run <code>sysctl -p /etc/sysctl.conf</code> as root, and then restart ElasticSearch.</p>
<h2 id="Tip-3-Setup-a-Cluster-with-Unicast"><a href="#Tip-3-Setup-a-Cluster-with-Unicast" class="headerlink" title="Tip 3 Setup a Cluster with Unicast"></a>Tip 3 Setup a Cluster with Unicast</h2><p>ElasticSearch has two options to form a cluster, multicast and unicast. The former is suitable when you have a large group of servers and a well configured network. But we found unicast more concise and less error-prone.</p>
<p>Here’s an example of using unicast:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">node.name: &quot;NODE-1&quot;</div><div class="line">discovery.zen.ping.multicast.enabled: false</div><div class="line">discovery.zen.ping.unicast.hosts: [&quot;node-1.example.com&quot;, &quot;node-2.example.com&quot;, &quot;node-3.example.com&quot;]</div><div class="line">discovery.zen.minimum_master_nodes: 2</div></pre></td></tr></table></figure>
<p>The <code>discovery.zen.minimum_master_nodes</code> setting is a way to prevent split-brain symptom, i.e. more than one node thinks itself the master of the cluster. And for this setting to work, you should have an odd number of nodes, and set this config to <code>ceil(num_of_nodes / 2)</code>. In the above cluster, you can lose at most one node. It’s much like a quorum in <a href="http://zookeeper.apache.org" target="_blank" rel="external">Zookeeper</a>.</p>
<h2 id="Tip-4-Disable-Unnecessary-Features"><a href="#Tip-4-Disable-Unnecessary-Features" class="headerlink" title="Tip 4 Disable Unnecessary Features"></a>Tip 4 Disable Unnecessary Features</h2><p>ElasticSearch is a full-featured search engine, but you should always tailor it to your own needs. Here’s a brief list:</p>
<ul>
<li>Use corrent index type. There’re <code>index</code>, <code>not_analyzed</code>, and <code>no</code>. If you don’t need to search the field, set it to <code>no</code>; if you only search for full match, use <code>not_analyzed</code>.</li>
<li>For search-only fields, set <code>store</code> to false.</li>
<li>Disable <code>_all</code> field, if you always know which field to search.</li>
<li>Disable <code>_source</code> fields, if documents are big and you don’t need the update capability.</li>
<li>If you have a document key, set this field in <code>_id</code> - <code>path</code>, instead of index the field twice.</li>
<li>Set <code>index.refresh_interval</code> to a larger number (default 1s), if you don’t need near-realtime search. It’s also an important option in bulk-load operation described below.</li>
</ul>
<h2 id="Tip-5-Use-Bulk-Operations"><a href="#Tip-5-Use-Bulk-Operations" class="headerlink" title="Tip 5 Use Bulk Operations"></a>Tip 5 Use Bulk Operations</h2><p><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/bulk.html" target="_blank" rel="external">Bulk is cheaper</a></p>
<ul>
<li>Bulk Read<ul>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html" target="_blank" rel="external">Multi Get</a> to retrieve multiple documents by a list of ids. </li>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html" target="_blank" rel="external">Scroll</a> to search a large number of documents.</li>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/1.4/msearch.html" target="_blank" rel="external">MultiSearch api</a> to run search requests in parallel. </li>
</ul>
</li>
<li>Bulk Write<ul>
<li>Use <a href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/1.4/bulk.html" target="_blank" rel="external">Bulk API</a> to index, update, delete multiple documents.</li>
<li>Alter <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html" target="_blank" rel="external">index aliases</a> simultaneously.</li>
</ul>
</li>
<li>Bulk Load: when initially building a large index, do the following,<ul>
<li>Set <code>number_of_relicas</code> to 0, so no relicas will be created;</li>
<li>Set <code>index.refresh_interval</code> to -1, disabling nrt search;</li>
<li>Bulk build the documents;</li>
<li>Call <code>optimize</code> on the index, so newly built docs are available for search;</li>
<li>Reset replicas and refresh interval, let ES cluster recover to green.</li>
</ul>
</li>
</ul>
<h2 id="Miscellaneous"><a href="#Miscellaneous" class="headerlink" title="Miscellaneous"></a>Miscellaneous</h2><ul>
<li>File descriptors: system default is too small for ES, set it to 64K will be OK. If <code>ulimit -n 64000</code> does not work, you need to add <code>&lt;user&gt; hard nofile 64000</code> to <code>/etc/security/limits.conf</code>, just like the <code>memlock</code> setting mentioned above.</li>
<li>When using ES client library, it will create a lot of worker threads according to the number of processors. Sometimes it’s not necessary. This behaviour can be changed by setting <code>processors</code> to a lower value like 2:</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> settings = <span class="type">ImmutableSettings</span>.settingsBuilder()</div><div class="line">    .put(<span class="string">"cluster.name"</span>, <span class="string">"elasticsearch"</span>)</div><div class="line">    .put(<span class="string">"processors"</span>, <span class="number">2</span>)</div><div class="line">    .build()</div><div class="line"><span class="keyword">val</span> uri = <span class="type">ElasticsearchClientUri</span>(<span class="string">"elasticsearch://127.0.0.1:9300"</span>)</div><div class="line"><span class="type">ElasticClient</span>.remote(settings, uri)</div></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html</a></li>
<li><a href="http://cpratt.co/how-many-shards-should-elasticsearch-indexes-have/" target="_blank" rel="external">http://cpratt.co/how-many-shards-should-elasticsearch-indexes-have/</a></li>
<li><a href="https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing" target="_blank" rel="external">https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing</a></li>
<li><a href="https://www.loggly.com/blog/nine-tips-configuring-elasticsearch-for-high-performance/" target="_blank" rel="external">https://www.loggly.com/blog/nine-tips-configuring-elasticsearch-for-high-performance/</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Recently we’re using ElasticSearch as a data backend of our recommendation API, to serve both offline and online computed data to users. Thanks to ElasticSearch’s rich and out-of-the-box functionality, it doesn’t take much trouble to setup the cluster. However, we still encounter some misuse and unwise configurations. So here’s a list of ElasticSearch performance tips that we learned from practice.&lt;/p&gt;
&lt;h2 id=&quot;Tip-1-Set-Num-of-shards-to-Num-of-nodes&quot;&gt;&lt;a href=&quot;#Tip-1-Set-Num-of-shards-to-Num-of-nodes&quot; class=&quot;headerlink&quot; title=&quot;Tip 1 Set Num-of-shards to Num-of-nodes&quot;&gt;&lt;/a&gt;Tip 1 Set Num-of-shards to Num-of-nodes&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard&quot;&gt;Shard&lt;/a&gt; is the foundation of ElasticSearch’s distribution capability. Every index is splitted into several shards (default 5) and are distributed across cluster nodes. But this capability does not come free. Since data being queried reside in all shards (this behaviour can be changed by &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-routing&quot;&gt;routing&lt;/a&gt;), ElasticSearch has to run this query on every shard, fetch the result, and merge them, like a map-reduce process. So if there’re too many shards, more than the number of cluter nodes, the query will be executed more than once on the same node, and it’ll also impact the merge phase. On the other hand, too few shards will also reduce the performance, for not all nodes are being utilized.&lt;/p&gt;
&lt;p&gt;Shards have two roles, primary shard and replica shard. Replica shard serves as a backup to the primary shard. When primary goes down, the replica takes its job. It also helps improving the search and get performance, for these requests can be executed on either primary or replica shard.&lt;/p&gt;
&lt;p&gt;Shards can be visualized by &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary-shard&quot;&gt;elasticsearch-head&lt;/a&gt; plugin:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/elasticsearch/shards-head.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;cu_docs&lt;/code&gt; index has two shards &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, with &lt;code&gt;number_of_replicas&lt;/code&gt; set to 1. Primary shard &lt;code&gt;0&lt;/code&gt; (bold bordered) resides in server &lt;code&gt;Leon&lt;/code&gt;, and its replica in &lt;code&gt;Pris&lt;/code&gt;. They are green becuase all primary shards have enough repicas sitting in different servers, so the cluster is healthy.&lt;/p&gt;
&lt;p&gt;Since &lt;code&gt;number_of_shards&lt;/code&gt; of an index cannot be changed after creation (while &lt;code&gt;number_of_replicas&lt;/code&gt; can), one should choose this config wisely. Here are some suggestions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How many nodes do you have, now and future? If you’re sure you’ll only have 3 nodes, set number of shards to 2 and replicas to 1, so there’ll be 4 shards across 3 nodes. If you’ll add some servers in the future, you can set number of shards to 3, so when the cluster grows to 5 nodes, there’ll be 6 distributed shards.&lt;/li&gt;
&lt;li&gt;How big is your index? If it’s small, one shard with one replica will due.&lt;/li&gt;
&lt;li&gt;How is the read and write frequency, respectively? If it’s search heavy, setup more relicas. &lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="english" scheme="http://shzhangji.com/tags/english/"/>
    
      <category term="elasticsearch" scheme="http://shzhangji.com/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Apache HBase的适用场景</title>
    <link href="http://shzhangji.com/blog/2015/03/08/hbase-dos-and-donts/"/>
    <id>http://shzhangji.com/blog/2015/03/08/hbase-dos-and-donts/</id>
    <published>2015-03-08T00:03:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/" target="_blank" rel="external">http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/</a></p>
<p>最近我在<a href="http://www.meetup.com/LA-HUG/" target="_blank" rel="external">洛杉矶Hadoop用户组</a>做了一次关于<a href="http://www.meetup.com/LA-HUG/pages/Video_from_April_13th_HBASE_DO%27S_and_DON%27TS/" target="_blank" rel="external">HBase适用场景</a>的分享。在场的听众水平都很高，给到了我很多值得深思的反馈。主办方是来自Shopzilla的Jody，我非常感谢他能给我一个在60多位Hadoop使用者面前演讲的机会。可能一些朋友没有机会来洛杉矶参加这次会议，我将分享中的主要内容做了一个整理。如果你没有时间阅读全文，以下是一些摘要：</p>
<ul>
<li>HBase很棒，但不是关系型数据库或HDFS的替代者；</li>
<li>配置得当才能运行良好；</li>
<li>监控，监控，监控，重要的事情要说三遍。</li>
</ul>
<p>Cloudera是HBase的铁杆粉丝。我们热爱这项技术，热爱这个社区，发现它能适用于非常多的应用场景。HBase如今已经有很多<a href="#use-cases">成功案例</a>，所以很多公司也在考虑如何将其应用到自己的架构中。我做这次分享以及写这篇文章的动因就是希望能列举出HBase的适用场景，并提醒各位哪些场景是不适用的，以及如何做好HBase的部署。</p>
<a id="more"></a>
<h2 id="何时使用HBase"><a href="#何时使用HBase" class="headerlink" title="何时使用HBase"></a>何时使用HBase</h2><p>虽然HBase是一种绝佳的工具，但我们一定要记住，它并非银弹。HBase并不擅长传统的事务处理程序或关联分析，它也不能完全替代MapReduce过程中使用到的HDFS。从文末的<a href="#use-cases">成功案例</a>中你可以大致了解HBase适用于怎样的应用场景。如果你还有疑问，可以到<a href="http://www.cloudera.com/community/" target="_blank" rel="external">社区</a>中提问，我说过这是一个非常棒的社区。</p>
<p>除去上述限制之外，你为何要选择HBase呢？如果你的应用程序中，数据表每一行的结构是有差别的，那就可以考虑使用HBase，比如在标准化建模的过程中使用它；如果你需要经常追加字段，且大部分字段是NULL值的，那可以考虑HBase；如果你的数据（包括元数据、消息、二进制数据等）都有着同一个主键，那就可以使用HBase；如果你需要通过键来访问和修改数据，使用HBase吧。</p>
<h2 id="后台服务"><a href="#后台服务" class="headerlink" title="后台服务"></a>后台服务</h2><p>如果你已决定尝试一下HBase，那以下是一些部署过程中的提示。HBase会用到一些后台服务，这些服务非常关键。如果你之前没有了解过ZooKeeper，那现在是个好时候。HBase使用ZooKeeper作为它的分布式协调服务，用于选举Master等。随着HBase的发展，ZooKeeper发挥的作用越来越重要。另外，你需要搭建合适的网络基础设施，如NTP和DNS。HBase要求集群内的所有服务器时间一致，并且能正确地访问其它服务器。正确配置NTP和DNS可以杜绝一些奇怪的问题，如服务器A认为当前是明天，B认为当前是昨天；再如Master要求服务器C开启新的Region，而C不知道自己的机器名，从而无法响应。NTP和DNS服务器可以让你减少很多麻烦。</p>
<p>我前面提到过，在考虑是否使用HBase时，需要针对你自己的应用场景来进行判别。而在真正使用HBase时，监控则成了第一要务。和大多数分布式服务一样，HBase服务器宕机会有多米诺骨牌效应。如果一台服务器因内存不足开始swap数据，它会失去和Master的联系，这时Master会命令其他服务器接过这部分请求，可能会导致第二台服务器也发生宕机。所以，你需要密切监控服务器的CPU、I/O以及网络延迟，确保每台HBase服务器都在良好地工作。监控对于维护HBase集群的健康至关重要。</p>
<h2 id="HBase架构最佳实践"><a href="#HBase架构最佳实践" class="headerlink" title="HBase架构最佳实践"></a>HBase架构最佳实践</h2><p>当你找到了适用场景，并搭建起一个健康的HBase集群后，我们来看一些使用过程中的最佳实践。键的前缀要有良好的分布性。如果你使用时间戳或其他类似的递增量作为前缀，那就会让单个Region承载所有请求，而不是分布到各个Region上。此外，你需要根据Memstore和内存的大小来控制Region的数量。RegionServer的JVM堆内存应该控制在12G以内，从而避免过长的GC停顿。举个例子，在一台内存为36G的服务器上部署RegionServer，同时还运行着DataNode，那大约可以提供100个48M大小的Region。这样的配置对HDFS、HBase、以及Linux本身的文件缓存都是有利的。</p>
<p>其他一些设置包括禁用自动合并机制（默认的合并操作会在HBase启动后每隔24小时进行），改为手动的方式在低峰期间执行。你还应该配置数据文件压缩（如LZO），并将正确的配置文件加入HBase的CLASSPATH中。</p>
<h2 id="非适用场景"><a href="#非适用场景" class="headerlink" title="非适用场景"></a>非适用场景</h2><p>上文讲述了HBase的适用场景和最佳实践，以下则是一些需要规避的问题。比如，不要期许HBase可以完全替代关系型数据库——虽然它在许多方面都表现优秀。它不支持SQL，也没有优化器，更不能支持跨越多条记录的事务或关联查询。如果你用不到这些特性，那HBase将是你的不二选择。</p>
<p>在复用HBase的服务器时有一些注意事项。如果你需要保证HBase的服务器质量，同时又想在HBase上运行批处理脚本（如使用Pig从HBase中获取数据进行处理），建议还是另搭一套集群。HBase在处理大量顺序I/O操作时（如MapReduce），其CPU和内存资源将会十分紧张。将这两类应用放置在同一集群上会造成不可预估的服务延迟。此外，共享集群时还需要调低任务槽（task slot）的数量，至少要留一半的CPU核数给HBase。密切关注内存，因为一旦发生swap，HBase很可能会停止心跳，从而被集群判为无效，最终产生一系列宕机。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后要提的一点是，在加载数据到HBase时，应该使用MapReduce+HFileOutputFormat来实现。如果仅使用客户端API，不仅速度慢，也没有充分利用HBase的分布式特性。</p>
<p>用一句话概述，HBase可以让你用键来存储和搜索数据，且无需定义表结构。</p>
<h2 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a><a id="use-cases"></a>使用案例</h2><ul>
<li>Apache HBase: <a href="http://wiki.apache.org/hadoop/Hbase/PoweredBy" target="_blank" rel="external">Powered By HBase Wiki</a></li>
<li>Mozilla: <a href="http://blog.mozilla.com/webdev/2010/07/26/moving-socorro-to-hbase/" target="_blank" rel="external">Moving Socorro to HBase</a></li>
<li>Facebook: <a href="http://highscalability.com/blog/2010/11/16/facebooks-new-real-time-messaging-system-hbase-to-store-135.html" target="_blank" rel="external">Facebook’s New Real-Time Messaging System: HBase</a></li>
<li>StumbleUpon: <a href="http://www.stumbleupon.com/devblog/hbase_at_stumbleupon/" target="_blank" rel="external">HBase at StumbleUpon</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/&quot;&gt;http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最近我在&lt;a href=&quot;http://www.meetup.com/LA-HUG/&quot;&gt;洛杉矶Hadoop用户组&lt;/a&gt;做了一次关于&lt;a href=&quot;http://www.meetup.com/LA-HUG/pages/Video_from_April_13th_HBASE_DO%27S_and_DON%27TS/&quot;&gt;HBase适用场景&lt;/a&gt;的分享。在场的听众水平都很高，给到了我很多值得深思的反馈。主办方是来自Shopzilla的Jody，我非常感谢他能给我一个在60多位Hadoop使用者面前演讲的机会。可能一些朋友没有机会来洛杉矶参加这次会议，我将分享中的主要内容做了一个整理。如果你没有时间阅读全文，以下是一些摘要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HBase很棒，但不是关系型数据库或HDFS的替代者；&lt;/li&gt;
&lt;li&gt;配置得当才能运行良好；&lt;/li&gt;
&lt;li&gt;监控，监控，监控，重要的事情要说三遍。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cloudera是HBase的铁杆粉丝。我们热爱这项技术，热爱这个社区，发现它能适用于非常多的应用场景。HBase如今已经有很多&lt;a href=&quot;#use-cases&quot;&gt;成功案例&lt;/a&gt;，所以很多公司也在考虑如何将其应用到自己的架构中。我做这次分享以及写这篇文章的动因就是希望能列举出HBase的适用场景，并提醒各位哪些场景是不适用的，以及如何做好HBase的部署。&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Translation/Big-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>深入理解Reduce-side Join</title>
    <link href="http://shzhangji.com/blog/2015/01/13/understand-reduce-side-join/"/>
    <id>http://shzhangji.com/blog/2015/01/13/understand-reduce-side-join/</id>
    <published>2015-01-13T06:20:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>在《<a href="http://www.amazon.com/MapReduce-Design-Patterns-Effective-Algorithms/dp/1449327176" target="_blank" rel="external">MapReduce Design Patterns</a>》一书中，作者给出了Reduce-side Join的实现方法，大致步骤如下：</p>
<p><img src="/images/reduce-side-join/reduce-side-join.png" alt=""></p>
<ol>
<li>使用<a href="https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/MultipleInputs.html" target="_blank" rel="external">MultipleInputs</a>指定不同的来源表和相应的Mapper类；</li>
<li>Mapper输出的Key为Join的字段内容，Value为打了来源表标签的记录；</li>
<li>Reducer在接收到同一个Key的记录后，执行以下两步：<ol>
<li>遍历Values，根据标签将来源表的记录分别放到两个List中；</li>
<li>遍历两个List，输出Join结果。</li>
</ol>
</li>
</ol>
<p>具体实现可以参考<a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/InnerJoinJob.java" target="_blank" rel="external">这段代码</a>。但是这种实现方法有一个问题：如果同一个Key的记录数过多，存放在List中就会占用很多内存，严重的会造成内存溢出（Out of Memory, OOM）。这种方法在一对一的情况下没有问题，而一对多、多对多的情况就会有隐患。那么，Hive在做Reduce-side Join时是如何避免OOM的呢？两个关键点：</p>
<ol>
<li>Reducer在遍历Values时，会将前面的表缓存在内存中，对于最后一张表则边扫描边输出；</li>
<li>如果前面几张表内存中放不下，就写入磁盘。</li>
</ol>
<a id="more"></a>
<p>按照我们的实现，Mapper输出的Key是<code>product_id</code>，Values是打了标签的产品表（Product）和订单表（Order）的记录。从数据量来看，应该缓存产品表，扫描订单表。这就要求两表记录到达Reducer时是有序的，产品表在前，边扫描边放入内存；订单表在后，边扫描边结合产品表的记录进行输出。要让Hadoop在Shuffle&amp;Sort阶段先按<code>product_id</code>排序、再按表的标签排序，就需要用到二次排序。</p>
<p>二次排序的概念很简单，将Mapper输出的Key由单一的<code>product_id</code>修改为<code>product_id+tag</code>的复合Key就可以了，但需通过以下几步实现：</p>
<h3 id="自定义Key类型"><a href="#自定义Key类型" class="headerlink" title="自定义Key类型"></a>自定义Key类型</h3><p>原来<code>product_id</code>是Text类型，我们的复合Key则要包含<code>product_id</code>和<code>tag</code>两个数据，并实现<code>WritableComparable</code>接口：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaggedKey</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">TaggedKey</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> Text joinKey = <span class="keyword">new</span> Text();</div><div class="line">    <span class="keyword">private</span> IntWritable tag = <span class="keyword">new</span> IntWritable();</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(TaggedKey taggedKey)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> compareValue = joinKey.compareTo(taggedKey.getJoinKey());</div><div class="line">        <span class="keyword">if</span> (compareValue == <span class="number">0</span>) &#123;</div><div class="line">            compareValue = tag.compareTo(taggedKey.getTag());</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> compareValue;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="comment">// 此处省略部分代码</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可以看到，在比较两个TaggedKey时，会先比较joinKey（即<code>product_id</code>），再比较<code>tag</code>。</p>
<h3 id="自定义分区方法"><a href="#自定义分区方法" class="headerlink" title="自定义分区方法"></a>自定义分区方法</h3><p>默认情况下，Hadoop会对Key进行哈希，以保证相同的Key会分配到同一个Reducer中。由于我们改变了Key的结构，因此需要重新编 写分区函数：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaggedJoiningPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">TaggedKey</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(TaggedKey taggedKey, Text text, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> taggedKey.getJoinKey().hashCode() % numPartitions;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="自定义分组方法"><a href="#自定义分组方法" class="headerlink" title="自定义分组方法"></a>自定义分组方法</h3><p>同理，调用reduce函数需要传入同一个Key的所有记录，这就需要重新定义分组函数：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaggedJoiningGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TaggedJoiningGroupingComparator</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">super</span>(TaggedKey.class, <span class="keyword">true</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@SuppressWarnings</span>(<span class="string">"rawtypes"</span>)</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</div><div class="line">        TaggedKey taggedKey1 = (TaggedKey) a;</div><div class="line">        TaggedKey taggedKey2 = (TaggedKey) b;</div><div class="line">        <span class="keyword">return</span> taggedKey1.getJoinKey().compareTo(taggedKey2.getJoinKey());</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="配置Job"><a href="#配置Job" class="headerlink" title="配置Job"></a>配置Job</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">job.setMapOutputKeyClass(TaggedKey.class);</div><div class="line">job.setMapOutputValueClass(Text.class);</div><div class="line"></div><div class="line">job.setPartitionerClass(TaggedJoiningPartitioner.class);</div><div class="line">job.setGroupingComparatorClass(TaggedJoiningGroupingComparator.class);</div></pre></td></tr></table></figure>
<h3 id="MapReduce过程"><a href="#MapReduce过程" class="headerlink" title="MapReduce过程"></a>MapReduce过程</h3><p>最后，我们在Mapper阶段使用TaggedKey，在Reducer阶段按照tag进行不同的操作就可以了：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(TaggedKey key, Iterable&lt;Text&gt; values, Context context)</span></span></div><div class="line">        <span class="keyword">throws</span> IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">    List&lt;String&gt; products = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line"></div><div class="line">    <span class="keyword">for</span> (Text value : values) &#123;</div><div class="line">        <span class="keyword">switch</span> (key.getTag().get()) &#123;</div><div class="line">        <span class="keyword">case</span> <span class="number">1</span>: <span class="comment">// Product</span></div><div class="line">            products.add(value.toString());</div><div class="line">            <span class="keyword">break</span>;</div><div class="line"></div><div class="line">        <span class="keyword">case</span> <span class="number">2</span>: <span class="comment">// Order</span></div><div class="line">            String[] order = value.toString().split(<span class="string">","</span>);</div><div class="line">            <span class="keyword">for</span> (String productString : products) &#123;</div><div class="line">                String[] product = productString.split(<span class="string">","</span>);</div><div class="line">                List&lt;String&gt; output = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line">                output.add(order[<span class="number">0</span>]);</div><div class="line">                <span class="comment">// ...</span></div><div class="line">                context.write(NullWritable.get(), <span class="keyword">new</span> Text(StringUtils.join(output, <span class="string">","</span>)));</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">break</span>;</div><div class="line"></div><div class="line">        <span class="keyword">default</span>:</div><div class="line">            <span class="keyword">assert</span> <span class="keyword">false</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>遍历values时，开始都是tag=1的记录，之后都是tag=2的记录。以上代码可以<a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/ReduceSideJoinJob.java" target="_blank" rel="external">在这里</a>查看。</p>
<p>对于第二个问题，超过缓存大小的记录（默认25000条）就会存入临时文件，由Hive的RowContainer类实现，具体可以看<a href="http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hive/hive-exec/0.10.0-cdh4.5.0/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java#RowContainer.add%28java.util.List%29" target="_blank" rel="external">这个链接</a>。</p>
<p>需要注意的是，Hive默认是按SQL中表的书写顺序来决定排序的，因此应该将大表放在最后。如果要人工改变顺序，可以使用STREAMTABLE配置：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ STREAMTABLE(a) */</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key = b.key1) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key = b.key1)</div></pre></td></tr></table></figure>
<p>但不要将这点和Map-side Join混淆，在配置了<code>hive.auto.convert.join=true</code>后，是不需要注意表的顺序的，Hive会自动将小表缓存在Mapper的内存中。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="http://codingjunkie.net/mapreduce-reduce-joins/" target="_blank" rel="external">http://codingjunkie.net/mapreduce-reduce-joins/</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在《&lt;a href=&quot;http://www.amazon.com/MapReduce-Design-Patterns-Effective-Algorithms/dp/1449327176&quot;&gt;MapReduce Design Patterns&lt;/a&gt;》一书中，作者给出了Reduce-side Join的实现方法，大致步骤如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/reduce-side-join/reduce-side-join.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用&lt;a href=&quot;https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/MultipleInputs.html&quot;&gt;MultipleInputs&lt;/a&gt;指定不同的来源表和相应的Mapper类；&lt;/li&gt;
&lt;li&gt;Mapper输出的Key为Join的字段内容，Value为打了来源表标签的记录；&lt;/li&gt;
&lt;li&gt;Reducer在接收到同一个Key的记录后，执行以下两步：&lt;ol&gt;
&lt;li&gt;遍历Values，根据标签将来源表的记录分别放到两个List中；&lt;/li&gt;
&lt;li&gt;遍历两个List，输出Join结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体实现可以参考&lt;a href=&quot;https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/join/InnerJoinJob.java&quot;&gt;这段代码&lt;/a&gt;。但是这种实现方法有一个问题：如果同一个Key的记录数过多，存放在List中就会占用很多内存，严重的会造成内存溢出（Out of Memory, OOM）。这种方法在一对一的情况下没有问题，而一对多、多对多的情况就会有隐患。那么，Hive在做Reduce-side Join时是如何避免OOM的呢？两个关键点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reducer在遍历Values时，会将前面的表缓存在内存中，对于最后一张表则边扫描边输出；&lt;/li&gt;
&lt;li&gt;如果前面几张表内存中放不下，就写入磁盘。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Big-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>使用git rebase让历史变得清晰</title>
    <link href="http://shzhangji.com/blog/2014/12/23/use-git-rebase-to-clarify-history/"/>
    <id>http://shzhangji.com/blog/2014/12/23/use-git-rebase-to-clarify-history/</id>
    <published>2014-12-23T08:10:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>当多人协作开发一个分支时，历史记录通常如下方左图所示，比较凌乱。如果希望能像右图那样呈线性提交，就需要学习git rebase的用法。</p>
<p><img src="/images/git-rebase/rebase-result.png" alt=""></p>
<h2 id="“Merge-branch”提交的产生"><a href="#“Merge-branch”提交的产生" class="headerlink" title="“Merge branch”提交的产生"></a>“Merge branch”提交的产生</h2><p>我们的工作流程是：修改代码→提交到本地仓库→拉取远程改动→推送。正是在git pull这一步产生的Merge branch提交。事实上，git pull等效于get fetch origin和get merge origin/master这两条命令，前者是拉取远程仓库到本地临时库，后者是将临时库中的改动合并到本地分支中。</p>
<p>要避免Merge branch提交也有一个“土法”：先pull、再commit、最后push。不过万一commit和push之间远程又发生了改动，还需要再pull一次，就又会产生Merge branch提交。</p>
<h2 id="使用git-pull-–rebase"><a href="#使用git-pull-–rebase" class="headerlink" title="使用git pull –rebase"></a>使用git pull –rebase</h2><p>修改代码→commit→git pull –rebase→git push。也就是将get merge origin/master替换成了git rebase origin/master，它的过程是先将HEAD指向origin/master，然后逐一应用本地的修改，这样就不会产生Merge branch提交了。具体过程见下文扩展阅读。</p>
<a id="more"></a>
<p>使用git rebase是有条件的，你的本地仓库要“足够干净”。可以用git status命令查看当前改动：：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ git status</div><div class="line">On branch master</div><div class="line">Your branch is up-to-date with &apos;origin/master&apos;.</div><div class="line">nothing to commit, working directory clean</div></pre></td></tr></table></figure>
<p>本地没有任何未提交的改动，这是最“干净”的。稍差一些的是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ git status</div><div class="line">On branch master</div><div class="line">Your branch is up-to-date with &apos;origin/master&apos;.</div><div class="line">Untracked files:</div><div class="line">  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</div><div class="line">    test.txt</div><div class="line">nothing added to commit but untracked files present (use &quot;git add&quot; to track)</div></pre></td></tr></table></figure>
<p>即本地只有新增文件未提交，没有改动文件。我们应该尽量保持本地仓库的“整洁”，这样才能顺利使用git rebase。特殊情况下也可以用git stash来解决问题，有兴趣的可自行搜索。</p>
<h2 id="修改git-pull的默认行为"><a href="#修改git-pull的默认行为" class="headerlink" title="修改git pull的默认行为"></a>修改git pull的默认行为</h2><p>每次都加–rebase似乎有些麻烦，我们可以指定某个分支在执行git pull时默认采用rebase方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git config branch.master.rebase true</div></pre></td></tr></table></figure>
<p>如果你觉得所有的分支都应该用rebase，那就设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git config --global branch.autosetuprebase always</div></pre></td></tr></table></figure>
<p>这样对于新建的分支都会设定上面的rebase=true了。已经创建好的分支还是需要手动配置的。</p>
<h2 id="扩展阅读-1-：git-rebase工作原理"><a href="#扩展阅读-1-：git-rebase工作原理" class="headerlink" title="扩展阅读[1]：git rebase工作原理"></a>扩展阅读[1]：git rebase工作原理</h2><p>先看看git merge的示意图：</p>
<p><img src="/images/git-rebase/merge.png" alt=""></p>
<p><a href="https://www.atlassian.com/ja/git/tutorial/git-branches" target="_blank" rel="external">图片来源</a></p>
<p>可以看到Some Feature分支的两个提交通过一个新的提交（蓝色）和master连接起来了。</p>
<p>再来看git rebase的示意图：</p>
<p><img src="/images/git-rebase/rebase-1.png" alt=""></p>
<p><img src="/images/git-rebase/rebase-2.png" alt=""></p>
<p>Feature分支中的两个提交被“嫁接”到了Master分支的头部，或者说Feature分支的“基”（base）变成了 Master，rebase也因此得名。</p>
<h2 id="扩展阅读-2-：git-merge-–no-ff"><a href="#扩展阅读-2-：git-merge-–no-ff" class="headerlink" title="扩展阅读[2]：git merge –no-ff"></a>扩展阅读[2]：git merge –no-ff</h2><p>在做项目开发时会用到分支，合并时采用以下步骤：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ git checkout feature-branch</div><div class="line">$ git rebase master</div><div class="line">$ git checkout master</div><div class="line">$ git merge --no-ff feature-branch</div><div class="line">$ git push origin master</div></pre></td></tr></table></figure>
<p>历史就成了这样：</p>
<p><img src="/images/git-rebase/no-ff.png" alt=""></p>
<p>可以看到，Merge branch ‘feature-branch’那段可以很好的展现出这些提交是属于某一特性的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当多人协作开发一个分支时，历史记录通常如下方左图所示，比较凌乱。如果希望能像右图那样呈线性提交，就需要学习git rebase的用法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/git-rebase/rebase-result.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;“Merge-branch”提交的产生&quot;&gt;&lt;a href=&quot;#“Merge-branch”提交的产生&quot; class=&quot;headerlink&quot; title=&quot;“Merge branch”提交的产生&quot;&gt;&lt;/a&gt;“Merge branch”提交的产生&lt;/h2&gt;&lt;p&gt;我们的工作流程是：修改代码→提交到本地仓库→拉取远程改动→推送。正是在git pull这一步产生的Merge branch提交。事实上，git pull等效于get fetch origin和get merge origin/master这两条命令，前者是拉取远程仓库到本地临时库，后者是将临时库中的改动合并到本地分支中。&lt;/p&gt;
&lt;p&gt;要避免Merge branch提交也有一个“土法”：先pull、再commit、最后push。不过万一commit和push之间远程又发生了改动，还需要再pull一次，就又会产生Merge branch提交。&lt;/p&gt;
&lt;h2 id=&quot;使用git-pull-–rebase&quot;&gt;&lt;a href=&quot;#使用git-pull-–rebase&quot; class=&quot;headerlink&quot; title=&quot;使用git pull –rebase&quot;&gt;&lt;/a&gt;使用git pull –rebase&lt;/h2&gt;&lt;p&gt;修改代码→commit→git pull –rebase→git push。也就是将get merge origin/master替换成了git rebase origin/master，它的过程是先将HEAD指向origin/master，然后逐一应用本地的修改，这样就不会产生Merge branch提交了。具体过程见下文扩展阅读。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark快速入门</title>
    <link href="http://shzhangji.com/blog/2014/12/16/spark-quick-start/"/>
    <id>http://shzhangji.com/blog/2014/12/16/spark-quick-start/</id>
    <published>2014-12-16T07:59:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://spark.apache.org/images/spark-logo.png" alt=""></p>
<p><a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a>是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：</p>
<ul>
<li><strong>通用计算引擎</strong> 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；</li>
<li><strong>基于内存</strong> 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；</li>
<li><strong>与Hadoop集成</strong> 能够直接读写HDFS中的数据，并能运行在YARN之上。</li>
</ul>
<p>Spark是用<a href="http://www.scala-lang.org/" target="_blank" rel="external">Scala语言</a>编写的，所提供的API也很好地利用了这门语言的特性。它也可以使用Java和Python编写应用。本文将用Scala进行讲解。</p>
<h2 id="安装Spark和SBT"><a href="#安装Spark和SBT" class="headerlink" title="安装Spark和SBT"></a>安装Spark和SBT</h2><ul>
<li>从<a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">官网</a>上下载编译好的压缩包，解压到一个文件夹中。下载时需注意对应的Hadoop版本，如要读写CDH4 HDFS中的数据，则应下载Pre-built for CDH4这个版本。</li>
<li>为了方便起见，可以将spark/bin添加到$PATH环境变量中：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> SPARK_HOME=/path/to/spark</div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</div></pre></td></tr></table></figure>
<ul>
<li>在练习例子时，我们还会用到<a href="http://www.scala-sbt.org/" target="_blank" rel="external">SBT</a>这个工具，它是用来编译打包Scala项目的。Linux下的安装过程比较简单：<ul>
<li>下载<a href="https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.7/sbt-launch.jar" target="_blank" rel="external">sbt-launch.jar</a>到$HOME/bin目录；</li>
<li>新建$HOME/bin/sbt文件，权限设置为755，内容如下：</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">SBT_OPTS=<span class="string">"-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M"</span></div><div class="line">java <span class="variable">$SBT_OPTS</span> -jar `dirname <span class="variable">$0</span>`/sbt-launch.jar <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="日志分析示例"><a href="#日志分析示例" class="headerlink" title="日志分析示例"></a>日志分析示例</h2><p>假设我们有如下格式的日志文件，保存在/tmp/logs.txt文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">2014-12-11 18:33:52	INFO	Java	some message</div><div class="line">2014-12-11 18:34:33	INFO	MySQL	some message</div><div class="line">2014-12-11 18:34:54	WARN	Java	some message</div><div class="line">2014-12-11 18:35:25	WARN	Nginx	some message</div><div class="line">2014-12-11 18:36:09	INFO	Java	some message</div></pre></td></tr></table></figure>
<p>每条记录有四个字段，即时间、级别、应用、信息，使用制表符分隔。</p>
<p>Spark提供了一个交互式的命令行工具，可以直接执行Spark查询：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ spark-shell</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0</div><div class="line">      /_/</div><div class="line">Spark context available as sc.</div><div class="line">scala&gt;</div></pre></td></tr></table></figure>
<h3 id="加载并预览数据"><a href="#加载并预览数据" class="headerlink" title="加载并预览数据"></a>加载并预览数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> lines = sc.textFile(<span class="string">"/tmp/logs.txt"</span>)</div><div class="line">lines: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = /tmp/logs.txt <span class="type">MappedRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">12</span></div><div class="line"></div><div class="line">scala&gt; lines.first()</div><div class="line">res0: <span class="type">String</span> = <span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">33</span>:<span class="number">52</span>	<span class="type">INFO</span>	<span class="type">Java</span>	some message</div></pre></td></tr></table></figure>
<ul>
<li>sc是一个SparkContext类型的变量，可以认为是Spark的入口，这个对象在spark-shell中已经自动创建了。</li>
<li>sc.textFile()用于生成一个RDD，并声明该RDD指向的是/tmp/logs.txt文件。RDD可以暂时认为是一个列表，列表中的元素是一行行日志（因此是String类型）。这里的路径也可以是HDFS上的文件，如hdfs://127.0.0.1:8020/user/hadoop/logs.txt。</li>
<li>lines.first()表示调用RDD提供的一个方法：first()，返回第一行数据。</li>
</ul>
<h3 id="解析日志"><a href="#解析日志" class="headerlink" title="解析日志"></a>解析日志</h3><p>为了能对日志进行筛选，如只处理级别为ERROR的日志，我们需要将每行日志按制表符进行分割：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> logs = lines.map(line =&gt; line.split(<span class="string">"\t"</span>))</div><div class="line">logs: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">MappedRDD</span>[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">14</span></div><div class="line"></div><div class="line">scala&gt; logs.first()</div><div class="line">res1: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">33</span>:<span class="number">52</span>, <span class="type">INFO</span>, <span class="type">Java</span>, some message)</div></pre></td></tr></table></figure>
<ul>
<li>lines.map(f)表示对RDD中的每一个元素使用f函数来处理，并返回一个新的RDD。</li>
<li>line =&gt; line.split(“\t”)是一个匿名函数，又称为Lambda表达式、闭包等。它的作用和普通的函数是一样的，如这个匿名函数的参数是line（String类型），返回值是Array数组类型，因为String.split()函数返回的是数组。</li>
<li>同样使用first()方法来看这个RDD的首条记录，可以发现日志已经被拆分成四个元素了。</li>
</ul>
<h3 id="过滤并计数"><a href="#过滤并计数" class="headerlink" title="过滤并计数"></a>过滤并计数</h3><p>我们想要统计错误日志的数量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> errors = logs.filter(log =&gt; log(<span class="number">1</span>) == <span class="string">"ERROR"</span>)</div><div class="line">errors: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">FilteredRDD</span>[<span class="number">3</span>] at filter at &lt;console&gt;:<span class="number">16</span></div><div class="line"></div><div class="line">scala&gt; errors.first()</div><div class="line">res2: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>, <span class="type">ERROR</span>, <span class="type">Java</span>, some message)</div><div class="line"></div><div class="line">scala&gt; errors.count()</div><div class="line">res3: <span class="type">Long</span> = <span class="number">158</span></div></pre></td></tr></table></figure>
<ul>
<li>logs.filter(f)表示筛选出满足函数f的记录，其中函数f需要返回一个布尔值。</li>
<li>log(1) == “ERROR”表示获取每行日志的第二个元素（即日志级别），并判断是否等于ERROR。</li>
<li>errors.count()用于返回该RDD中的记录。</li>
</ul>
<h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>由于我们还会对错误日志做一些处理，为了加快速度，可以将错误日志缓存到内存中，从而省去解析和过滤的过程：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scala&gt; errors.cache()</div></pre></td></tr></table></figure>
<p>errors.cache()函数会告知Spark计算完成后将结果保存在内存中。所以说Spark是否缓存结果是需要用户手动触发的。在实际应用中，我们需要迭代处理的往往只是一部分数据，因此很适合放到内存里。</p>
<p>需要注意的是，cache函数并不会立刻执行缓存操作，事实上map、filter等函数都不会立刻执行，而是在用户执行了一些特定操作后才会触发，比如first、count、reduce等。这两类操作分别称为Transformations和Actions。</p>
<h3 id="显示前10条记录"><a href="#显示前10条记录" class="headerlink" title="显示前10条记录"></a>显示前10条记录</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> firstTenErrors = errors.take(<span class="number">10</span>)</div><div class="line">firstTenErrors: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>, <span class="type">ERROR</span>, <span class="type">Java</span>, some message), <span class="type">Array</span>(<span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">40</span>:<span class="number">23</span>, <span class="type">ERROR</span>, <span class="type">Nginx</span>, some message), ...)</div><div class="line"></div><div class="line">scala&gt; firstTenErrors.map(log =&gt; log.mkString(<span class="string">"\t"</span>)).foreach(line =&gt; println(line))</div><div class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">39</span>:<span class="number">42</span>	<span class="type">ERROR</span>	<span class="type">Java</span>	some message</div><div class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-11</span> <span class="number">18</span>:<span class="number">40</span>:<span class="number">23</span>	<span class="type">ERROR</span>	<span class="type">Nginx</span>	some message</div><div class="line">...</div></pre></td></tr></table></figure>
<p>errors.take(n)方法可用于返回RDD前N条记录，它的返回值是一个数组。之后对firstTenErrors的处理使用的是Scala集合类库中的方法，如map、foreach，和RDD提供的接口基本一致。所以说用Scala编写Spark程序是最自然的。</p>
<h3 id="按应用进行统计"><a href="#按应用进行统计" class="headerlink" title="按应用进行统计"></a>按应用进行统计</h3><p>我们想要知道错误日志中有几条Java、几条Nginx，这和常见的Wordcount思路是一样的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> apps = errors.map(log =&gt; (log(<span class="number">2</span>), <span class="number">1</span>))</div><div class="line">apps: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MappedRDD</span>[<span class="number">15</span>] at map at &lt;console&gt;:<span class="number">18</span></div><div class="line"></div><div class="line">scala&gt; apps.first()</div><div class="line">res20: (<span class="type">String</span>, <span class="type">Int</span>) = (<span class="type">Java</span>,<span class="number">1</span>)</div><div class="line"></div><div class="line">scala&gt; <span class="keyword">val</span> counts = apps.reduceByKey((a, b) =&gt; a + b)</div><div class="line">counts: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">17</span>] at reduceByKey at &lt;console&gt;:<span class="number">20</span></div><div class="line"></div><div class="line">scala&gt; counts.foreach(t =&gt; println(t))</div><div class="line">(<span class="type">Java</span>,<span class="number">58</span>)</div><div class="line">(<span class="type">Nginx</span>,<span class="number">53</span>)</div><div class="line">(<span class="type">MySQL</span>,<span class="number">47</span>)</div></pre></td></tr></table></figure>
<p>errors.map(log =&gt; (log(2), 1))用于将每条日志转换为键值对，键是应用（Java、Nginx等），值是1，如<code>(&quot;Java&quot;, 1)</code>，这种数据结构在Scala中称为元组（Tuple），这里它有两个元素，因此称为二元组。</p>
<p>对于数据类型是二元组的RDD，Spark提供了额外的方法，reduceByKey(f)就是其中之一。它的作用是按键进行分组，然后对同一个键下的所有值使用f函数进行归约（reduce）。归约的过程是：使用列表中第一、第二个元素进行计算，然后用结果和第三元素进行计算，直至列表耗尽。如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>).reduce((a, b) =&gt; a + b)</div><div class="line">res23: <span class="type">Int</span> = <span class="number">10</span></div></pre></td></tr></table></figure>
<p>上述代码的计算过程即<code>((1 + 2) + 3) + 4</code>。</p>
<p>counts.foreach(f)表示遍历RDD中的每条记录，并应用f函数。这里的f函数是一条打印语句（println）。</p>
<h2 id="打包应用程序"><a href="#打包应用程序" class="headerlink" title="打包应用程序"></a>打包应用程序</h2><p>为了让我们的日志分析程序能够在集群上运行，我们需要创建一个Scala项目。项目的大致结构是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">spark-sandbox</div><div class="line">├── build.sbt</div><div class="line">├── project</div><div class="line">│   ├── build.properties</div><div class="line">│   └── plugins.sbt</div><div class="line">└── src</div><div class="line">    └── main</div><div class="line">        └── scala</div><div class="line">            └── LogMining.scala</div></pre></td></tr></table></figure>
<p>你可以直接使用<a href="https://github.com/jizhang/spark-sandbox" target="_blank" rel="external">这个项目</a>作为模板。下面说明一些关键部分：</p>
<h3 id="配置依赖"><a href="#配置依赖" class="headerlink" title="配置依赖"></a>配置依赖</h3><p><code>build.sbt</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">libraryDependencies += <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-core"</span> % <span class="string">"1.1.1"</span></div></pre></td></tr></table></figure>
<h3 id="程序内容"><a href="#程序内容" class="headerlink" title="程序内容"></a>程序内容</h3><p><code>src/main/scala/LogMining.scala</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogMining</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</div><div class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"LogMining"</span>)</div><div class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">  <span class="keyword">val</span> inputFile = args(<span class="number">0</span>)</div><div class="line">  <span class="keyword">val</span> lines = sc.textFile(inputFile)</div><div class="line">  <span class="comment">// 解析日志</span></div><div class="line">  <span class="keyword">val</span> logs = lines.map(_.split(<span class="string">"\t"</span>))</div><div class="line">  <span class="keyword">val</span> errors = logs.filter(_(<span class="number">1</span>) == <span class="string">"ERROR"</span>)</div><div class="line">  <span class="comment">// 缓存错误日志</span></div><div class="line">  errors.cache()</div><div class="line">  <span class="comment">// 统计错误日志记录数</span></div><div class="line">  println(errors.count())</div><div class="line">  <span class="comment">// 获取前10条MySQL的错误日志</span></div><div class="line">  <span class="keyword">val</span> mysqlErrors = errors.filter(_(<span class="number">2</span>) == <span class="string">"MySQL"</span>)</div><div class="line">  mysqlErrors.take(<span class="number">10</span>).map(_ mkString <span class="string">"\t"</span>).foreach(println)</div><div class="line">  <span class="comment">// 统计每个应用的错误日志数</span></div><div class="line">  <span class="keyword">val</span> errorApps = errors.map(_(<span class="number">2</span>) -&gt; <span class="number">1</span>)</div><div class="line">  errorApps.countByKey().foreach(println)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="打包运行"><a href="#打包运行" class="headerlink" title="打包运行"></a>打包运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> spark-sandbox</div><div class="line">$ sbt package</div><div class="line">$ spark-submit --class LogMining --master <span class="built_in">local</span> target/scala-2.10/spark-sandbox_2.10-0.1.0.jar data/logs.txt</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">Spark Programming Guide</a></li>
<li><a href="http://www.slideshare.net/cloudera/spark-devwebinarslides-final" target="_blank" rel="external">Introduction to Spark Developer Training</a></li>
<li><a href="http://www.slideshare.net/liancheng/dtcc-14-spark-runtime-internals" target="_blank" rel="external">Spark Runtime Internals</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://spark.apache.org/images/spark-logo.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;是新兴的一种快速通用的大规模数据处理引擎。它的优势有三个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;通用计算引擎&lt;/strong&gt; 能够运行MapReduce、数据挖掘、图运算、流式计算、SQL等多种框架；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于内存&lt;/strong&gt; 数据可缓存在内存中，特别适用于需要迭代多次运算的场景；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与Hadoop集成&lt;/strong&gt; 能够直接读写HDFS中的数据，并能运行在YARN之上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Spark是用&lt;a href=&quot;http://www.scala-lang.org/&quot;&gt;Scala语言&lt;/a&gt;编写的，所提供的API也很好地利用了这门语言的特性。它也可以使用Java和Python编写应用。本文将用Scala进行讲解。&lt;/p&gt;
&lt;h2 id=&quot;安装Spark和SBT&quot;&gt;&lt;a href=&quot;#安装Spark和SBT&quot; class=&quot;headerlink&quot; title=&quot;安装Spark和SBT&quot;&gt;&lt;/a&gt;安装Spark和SBT&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;从&lt;a href=&quot;http://spark.apache.org/downloads.html&quot;&gt;官网&lt;/a&gt;上下载编译好的压缩包，解压到一个文件夹中。下载时需注意对应的Hadoop版本，如要读写CDH4 HDFS中的数据，则应下载Pre-built for CDH4这个版本。&lt;/li&gt;
&lt;li&gt;为了方便起见，可以将spark/bin添加到$PATH环境变量中：&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; SPARK_HOME=/path/to/spark&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; PATH=&lt;span class=&quot;variable&quot;&gt;$PATH&lt;/span&gt;:&lt;span class=&quot;variable&quot;&gt;$SPARK_HOME&lt;/span&gt;/bin&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;在练习例子时，我们还会用到&lt;a href=&quot;http://www.scala-sbt.org/&quot;&gt;SBT&lt;/a&gt;这个工具，它是用来编译打包Scala项目的。Linux下的安装过程比较简单：&lt;ul&gt;
&lt;li&gt;下载&lt;a href=&quot;https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.7/sbt-launch.jar&quot;&gt;sbt-launch.jar&lt;/a&gt;到$HOME/bin目录；&lt;/li&gt;
&lt;li&gt;新建$HOME/bin/sbt文件，权限设置为755，内容如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;SBT_OPTS=&lt;span class=&quot;string&quot;&gt;&quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;java &lt;span class=&quot;variable&quot;&gt;$SBT_OPTS&lt;/span&gt; -jar `dirname &lt;span class=&quot;variable&quot;&gt;$0&lt;/span&gt;`/sbt-launch.jar &lt;span class=&quot;string&quot;&gt;&quot;&lt;span class=&quot;variable&quot;&gt;$@&lt;/span&gt;&quot;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Tutorial" scheme="http://shzhangji.com/categories/Tutorial/"/>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Tutorial/Big-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>离线环境下构建sbt项目</title>
    <link href="http://shzhangji.com/blog/2014/11/07/sbt-offline/"/>
    <id>http://shzhangji.com/blog/2014/11/07/sbt-offline/</id>
    <published>2014-11-07T07:02:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>在公司网络中使用<a href="http://www.scala-sbt.org/" target="_blank" rel="external">sbt</a>、<a href="http://maven.apache.org/" target="_blank" rel="external">Maven</a>等项目构建工具时，我们通常会搭建一个公用的<a href="http://www.sonatype.org/nexus/" target="_blank" rel="external">Nexus</a>镜像服务，原因有以下几个：</p>
<ul>
<li>避免重复下载依赖，节省公司带宽；</li>
<li>国内网络环境不理想，下载速度慢；</li>
<li>IDC服务器没有外网访问权限；</li>
<li>用于发布内部模块。</li>
</ul>
<p>sbt的依赖管理基于<a href="http://ant.apache.org/ivy/" target="_blank" rel="external">Ivy</a>，虽然它能直接使用<a href="http://search.maven.org/" target="_blank" rel="external">Maven中央仓库</a>中的Jar包，在配置时还是有一些注意事项的。</p>
<a id="more"></a>
<h2 id="配置Nexus镜像"><a href="#配置Nexus镜像" class="headerlink" title="配置Nexus镜像"></a>配置Nexus镜像</h2><p>根据这篇<a href="http://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html" target="_blank" rel="external">官方文档</a>的描述，Ivy和Maven在依赖管理方面有些许差异，因此不能直接将两者的镜像仓库配置成一个，而需分别建立两个虚拟镜像组。</p>
<p><img src="http://www.scala-sbt.org/0.13/docs/files/proxy-ivy-mvn-setup.png" alt=""></p>
<p>安装Nexus后默认会有一个Public Repositories组，可以将其作为Maven的镜像组，并添加一些常用的第三方镜像：</p>
<ul>
<li>cloudera: <a href="https://repository.cloudera.com/artifactory/cloudera-repos/" target="_blank" rel="external">https://repository.cloudera.com/artifactory/cloudera-repos/</a></li>
<li>spring: <a href="http://repo.springsource.org/libs-release-remote/" target="_blank" rel="external">http://repo.springsource.org/libs-release-remote/</a></li>
<li>scala-tools: <a href="https://oss.sonatype.org/content/groups/scala-tools/" target="_blank" rel="external">https://oss.sonatype.org/content/groups/scala-tools/</a></li>
</ul>
<p>对于Ivy镜像，我们创建一个新的虚拟组：ivy-releases，并添加以下两个镜像：</p>
<ul>
<li>type-safe: <a href="http://repo.typesafe.com/typesafe/ivy-releases/" target="_blank" rel="external">http://repo.typesafe.com/typesafe/ivy-releases/</a></li>
<li>sbt-plugin: <a href="http://dl.bintray.com/sbt/sbt-plugin-releases/" target="_blank" rel="external">http://dl.bintray.com/sbt/sbt-plugin-releases/</a></li>
</ul>
<p>对于sbt-plugin，由于一些原因，Nexus会将其置为Automatically Blocked状态，因此要在配置中将这个选项关闭，否则将无法下载远程的依赖包。</p>
<h2 id="配置sbt"><a href="#配置sbt" class="headerlink" title="配置sbt"></a>配置sbt</h2><p>为了让sbt使用Nexus镜像，需要创建一个~/.sbt/repositories文件，内容为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[repositories]</div><div class="line">  local</div><div class="line">  my-ivy-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]</div><div class="line">  my-maven-proxy-releases: http://10.x.x.x:8081/nexus/content/groups/public/</div></pre></td></tr></table></figure>
<p>这样配置对大部分项目来说是足够了。但是有些项目会在构建描述文件中添加其它仓库，我们需要覆盖这种行为，方法是：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sbt -Dsbt.override.build.repos=<span class="literal">true</span></div></pre></td></tr></table></figure>
<p>你也可以通过设置SBT_OPTS环境变量来进行全局配置。</p>
<p>经过以上步骤，sbt执行过程中就不需要访问外网了，因此速度会有很大提升。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在公司网络中使用&lt;a href=&quot;http://www.scala-sbt.org/&quot;&gt;sbt&lt;/a&gt;、&lt;a href=&quot;http://maven.apache.org/&quot;&gt;Maven&lt;/a&gt;等项目构建工具时，我们通常会搭建一个公用的&lt;a href=&quot;http://www.sonatype.org/nexus/&quot;&gt;Nexus&lt;/a&gt;镜像服务，原因有以下几个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;避免重复下载依赖，节省公司带宽；&lt;/li&gt;
&lt;li&gt;国内网络环境不理想，下载速度慢；&lt;/li&gt;
&lt;li&gt;IDC服务器没有外网访问权限；&lt;/li&gt;
&lt;li&gt;用于发布内部模块。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;sbt的依赖管理基于&lt;a href=&quot;http://ant.apache.org/ivy/&quot;&gt;Ivy&lt;/a&gt;，虽然它能直接使用&lt;a href=&quot;http://search.maven.org/&quot;&gt;Maven中央仓库&lt;/a&gt;中的Jar包，在配置时还是有一些注意事项的。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>MySQL异常UTF-8字符的处理</title>
    <link href="http://shzhangji.com/blog/2014/10/14/mysql-incorrent-utf8-value/"/>
    <id>http://shzhangji.com/blog/2014/10/14/mysql-incorrent-utf8-value/</id>
    <published>2014-10-14T05:16:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>ETL流程中，我们会将Hive中的数据导入MySQL——先用Hive命令行将数据保存为文本文件，然后用MySQL的LOAD DATA语句进行加载。最近有一张表在加载到MySQL时会报以下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Incorrect string value: &apos;\xF0\x9D\x8C\x86&apos; for column ...</div></pre></td></tr></table></figure>
<p>经查，这个字段中保存的是用户聊天记录，因此会有一些表情符号。这些符号在UTF-8编码下需要使用4个字节来记录，而MySQL中的utf8编码只支持3个字节，因此无法导入。</p>
<p>根据UTF-8的编码规范，3个字节支持的Unicode字符范围是U+0000–U+FFFF，因此可以在Hive中对数据做一下清洗：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> REGEXP_REPLACE(<span class="keyword">content</span>, <span class="string">'[^\\u0000-\\uFFFF]'</span>, <span class="string">''</span>) <span class="keyword">FROM</span> ...</div></pre></td></tr></table></figure>
<p>这样就能排除那些需要使用3个以上字节来记录的字符了，从而成功导入MySQL。</p>
<p>以下是一些详细说明和参考资料。</p>
<a id="more"></a>
<h2 id="Unicode字符集和UTF编码"><a href="#Unicode字符集和UTF编码" class="headerlink" title="Unicode字符集和UTF编码"></a>Unicode字符集和UTF编码</h2><p><a href="http://en.wikipedia.org/wiki/Unicode" target="_blank" rel="external">Unicode字符集</a>是一种将全球所有文字都囊括在内的字符集，从而实现跨语言、跨平台的文字信息交换。它由<a href="http://en.wikipedia.org/wiki/Plane_\(Unicode\" target="_blank" rel="external">基本多语平面（BMP）</a>#Basic_Multilingual_Plane)和多个扩展平面（non-BMP）组成。前者的编码范围是U+0000-U+FFFF，包括了绝大多数现代语言文字，因此最为常用。</p>
<p><a href="http://en.wikipedia.org/wiki/Unicode#Unicode_Transformation_Format_and_Universal_Character_Set" target="_blank" rel="external">UTF</a>则是一种编码格式，负责将Unicode字符对应的编号转换为计算机可以识别的二进制数据，进行保存和读取。</p>
<p>比如，磁盘上记录了以下二进制数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">1101000 1100101 1101100 1101100 1101111</div></pre></td></tr></table></figure>
<p>读取它的程序知道这是以UTF-8编码保存的字符串，因此将其解析为以下编号：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">104 101 108 108 111</div></pre></td></tr></table></figure>
<p>又因为UTF-8编码对应的字符集是Unicode，所以上面这五个编号对应的字符便是“hello”。</p>
<p>很多人会将Unicode和UTF混淆，但两者并不具可比性，它们完成的功能是不同的。</p>
<h2 id="UTF-8编码"><a href="#UTF-8编码" class="headerlink" title="UTF-8编码"></a>UTF-8编码</h2><p>UTF编码家族也有很多成员，其中<a href="http://en.wikipedia.org/wiki/UTF-8" target="_blank" rel="external">UTF-8</a>最为常用。它是一种变长的编码格式，对于ASCII码中的字符使用1个字节进行编码，对于中文等则使用3个字节。这样做的优点是在存储西方语言文字时不会造成空间浪费，不像UTF-16和UTF-32，分别使用两个字节和四个字节对所有字符进行编码。</p>
<p>UTF-8编码的字节数上限并不是3个。对于U+0000-U+FFFF范围内的字符，使用3个字节可以表示完全；对于non-BMP中的字符，则会使用4-6个字节来表示。同样，UTF-16编码也会使用四个字节来表示non-BMP中的字符。</p>
<h2 id="MySQL的UTF-8编码"><a href="#MySQL的UTF-8编码" class="headerlink" title="MySQL的UTF-8编码"></a>MySQL的UTF-8编码</h2><p>根据MySQL的<a href="http://dev.mysql.com/doc/refman/5.5/en/charset-unicode.html" target="_blank" rel="external">官方文档</a>，它的UTF-8编码支持是不完全的，最多使用3个字符，这也是导入数据时报错的原因。</p>
<p>MySQL5.5开始支持utf8mb4编码，至多使用4个字节，因此能包含到non-BMP字符。只是我们的MySQL版本仍是5.1，因此选择丢弃这些字符。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8" target="_blank" rel="external">http://stackoverflow.com/questions/3951722/whats-the-difference-between-unicode-and-utf8</a></li>
<li><a href="http://www.joelonsoftware.com/articles/Unicode.html" target="_blank" rel="external">http://www.joelonsoftware.com/articles/Unicode.html</a></li>
<li><a href="http://apps.timwhitlock.info/emoji/tables/unicode" target="_blank" rel="external">http://apps.timwhitlock.info/emoji/tables/unicode</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ETL流程中，我们会将Hive中的数据导入MySQL——先用Hive命令行将数据保存为文本文件，然后用MySQL的LOAD DATA语句进行加载。最近有一张表在加载到MySQL时会报以下错误：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;Incorrect string value: &amp;apos;\xF0\x9D\x8C\x86&amp;apos; for column ...&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;经查，这个字段中保存的是用户聊天记录，因此会有一些表情符号。这些符号在UTF-8编码下需要使用4个字节来记录，而MySQL中的utf8编码只支持3个字节，因此无法导入。&lt;/p&gt;
&lt;p&gt;根据UTF-8的编码规范，3个字节支持的Unicode字符范围是U+0000–U+FFFF，因此可以在Hive中对数据做一下清洗：&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;SELECT&lt;/span&gt; REGEXP_REPLACE(&lt;span class=&quot;keyword&quot;&gt;content&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;[^\\u0000-\\uFFFF]&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;&#39;&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;FROM&lt;/span&gt; ...&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这样就能排除那些需要使用3个以上字节来记录的字符了，从而成功导入MySQL。&lt;/p&gt;
&lt;p&gt;以下是一些详细说明和参考资料。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
    
  </entry>
  
  <entry>
    <title>在CDH 4.5上安装Shark 0.9</title>
    <link href="http://shzhangji.com/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5/"/>
    <id>http://shzhangji.com/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5/</id>
    <published>2014-07-05T09:16:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://spark.apache.org" target="_blank" rel="external">Spark</a>是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的<a href="http://hadoop.apache.org" target="_blank" rel="external">Hadoop</a>生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于<a href="http://hive.apache.org" target="_blank" rel="external">Hive</a>，Spark也有相应的替代项目——<a href="http://shark.cs.berkeley.edu/" target="_blank" rel="external">Shark</a>，能做到 <strong>drop-in replacement</strong> ，直接构建在现有集群之上。本文就将简要阐述如何在CDH4.5上搭建Shark0.9集群。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ul>
<li>安装方式：Spark使用CDH提供的Parcel，以Standalone模式启动</li>
<li>软件版本<ul>
<li>Cloudera Manager 4.8.2</li>
<li>CDH 4.5</li>
<li>Spark 0.9.0 Parcel</li>
<li><a href="http://cloudera.rst.im/shark/" target="_blank" rel="external">Shark 0.9.1 Binary</a></li>
</ul>
</li>
<li>服务器基础配置<ul>
<li>可用的软件源（如<a href="http://mirrors.ustc.edu.cn/" target="_blank" rel="external">中科大的源</a>）</li>
<li>配置主节点至子节点的root账户SSH无密码登录。</li>
<li>在<code>/etc/hosts</code>中写死IP和主机名，或者DNS做好正反解析。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h2><ul>
<li>使用CM安装Parcel，不需要重启服务。</li>
<li>修改<code>/etc/spark/conf/spark-env.sh</code>：（其中one-843是主节点的域名）</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">STANDALONE_SPARK_MASTER_HOST=one-843</div><div class="line">DEFAULT_HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</div><div class="line"><span class="built_in">export</span> SPARK_CLASSPATH=<span class="variable">$SPARK_CLASSPATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*</div><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native</div></pre></td></tr></table></figure>
<ul>
<li>修改<code>/etc/spark/conf/slaves</code>，添加各节点主机名。</li>
<li>将<code>/etc/spark/conf</code>目录同步至所有节点。</li>
<li>启动Spark服务（即Standalone模式）：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-master.sh</div><div class="line">$ /opt/cloudera/parcels/SPARK/lib/spark/sbin/start-slaves.sh</div></pre></td></tr></table></figure>
<ul>
<li>测试<code>spark-shell</code>是否可用：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.textFile(<span class="string">"hdfs://one-843:8020/user/jizhang/zj_people.txt.lzo"</span>).count</div></pre></td></tr></table></figure>
<h2 id="安装Shark"><a href="#安装Shark" class="headerlink" title="安装Shark"></a>安装Shark</h2><ul>
<li>安装Oracle JDK 1.7 Update 45至<code>/usr/lib/jvm/jdk1.7.0_45</code>。</li>
<li>下载别人编译好的二进制包：<a href="http://cloudera.rst.im/shark/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz" target="_blank" rel="external">shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz</a></li>
<li>解压至<code>/opt</code>目录，修改<code>conf/shark-env.sh</code>：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.7.0_45</div><div class="line"><span class="built_in">export</span> SCALA_HOME=/opt/cloudera/parcels/SPARK/lib/spark</div><div class="line"><span class="built_in">export</span> SHARK_HOME=/root/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0</div><div class="line"></div><div class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/etc/hive/conf</div><div class="line"></div><div class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</div><div class="line"><span class="built_in">export</span> SPARK_HOME=/opt/cloudera/parcels/SPARK/lib/spark</div><div class="line"><span class="built_in">export</span> MASTER=spark://one-843:7077</div><div class="line"></div><div class="line"><span class="built_in">export</span> SPARK_CLASSPATH=<span class="variable">$SPARK_CLASSPATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*</div><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native</div></pre></td></tr></table></figure>
<ul>
<li>开启SharkServer2，使用Supervisord管理：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[program:sharkserver2]</div><div class="line">command = /opt/shark/bin/shark --service sharkserver2</div><div class="line">autostart = true</div><div class="line">autorestart = true</div><div class="line">stdout_logfile = /var/log/sharkserver2.log</div><div class="line">redirect_stderr = true</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ supervisorctl start sharkserver2</div></pre></td></tr></table></figure>
<ul>
<li>测试</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ /opt/shark/bin/beeline -u jdbc:hive2://one-843:10000 -n root</div></pre></td></tr></table></figure>
<h2 id="版本问题"><a href="#版本问题" class="headerlink" title="版本问题"></a>版本问题</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h4 id="CDH"><a href="#CDH" class="headerlink" title="CDH"></a>CDH</h4><p>CDH是对Hadoop生态链各组件的打包，每个CDH版本都会对应一组Hadoop组件的版本，如<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH-Version-and-Packaging-Information/cdhvd_topic_3.html" target="_blank" rel="external">CDH4.5</a>的部分对应关系如下：</p>
<ul>
<li>Apache Hadoop: hadoop-2.0.0+1518</li>
<li>Apache Hive: hive-0.10.0+214</li>
<li>Hue: hue-2.5.0+182</li>
</ul>
<p>可以看到，CDH4.5对应的Hive版本是0.10.0，因此它的<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH4-Installation-Guide/cdh4ig_hive_metastore_configure.html" target="_blank" rel="external">Metastore Server</a>使用的也是0.10.0版本的API。</p>
<h4 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h4><p>Spark目前最高版本是0.9.1，CDH前不久推出了0.9.0的Parcel，使得安装过程变的简单得多。CDH5中对Spark做了深度集成，即可以用CM来直接控制Spark的服务，且支持Spark on YARN架构。</p>
<h4 id="Shark"><a href="#Shark" class="headerlink" title="Shark"></a>Shark</h4><p>Shark是基于Spark的一款应用，可以简单地认为是将Hive的MapReduce引擎替换为了Spark。</p>
<p>Shark的一个特点的是需要使用特定的Hive版本——<a href="https://github.com/amplab/hive" target="_blank" rel="external">AMPLab patched Hive</a>：</p>
<ul>
<li>Shark 0.8.x: AMPLab Hive 0.9.0</li>
<li>Shark 0.9.x: AMPLab Hive 0.11.0</li>
</ul>
<p>在0.9.0以前，我们需要手动下载AMPLab Hive的二进制包，并在Shark的环境变量中设置$HIVE_HOME。在0.9.1以后，AMPLab将该版本的Hive包上传至了Maven，可以直接打进Shark的二进制包中。但是，这个Jar是用JDK7编译的，因此运行Shark需要使用Oracle JDK7。CDH建议使用Update 45这个小版本。</p>
<h4 id="Shark与Hive的并存"><a href="#Shark与Hive的并存" class="headerlink" title="Shark与Hive的并存"></a>Shark与Hive的并存</h4><p>Shark的一个卖点是和Hive的<a href="5">高度兼容</a>，也就是说它可以直接操作Hive的metastore db，或是和metastore server通信。当然，前提是两者的Hive版本需要一致，这也是目前遇到的最大问题。</p>
<h3 id="目前发现的不兼容SQL"><a href="#目前发现的不兼容SQL" class="headerlink" title="目前发现的不兼容SQL"></a>目前发现的不兼容SQL</h3><ul>
<li>DROP TABLE …</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">FAILED: Error in metadata: org.apache.thrift.TApplicationException: Invalid method name: &apos;drop_table_with_environment_context&apos;</div></pre></td></tr></table></figure>
<ul>
<li>INSERT OVERWRITE TABLE … PARTITION (…) SELECT …</li>
<li>LOAD DATA INPATH ‘…’ OVERWRITE INTO TABLE … PARTITION (…)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Failed with exception org.apache.thrift.TApplicationException: Invalid method name: &apos;partition_name_has_valid_characters&apos;</div></pre></td></tr></table></figure>
<p>也就是说上述两个方法名是0.11.0接口中定义的，在0.10.0的定义中并不存在，所以出现上述问题。</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="对存在问题的SQL使用Hive命令去调"><a href="#对存在问题的SQL使用Hive命令去调" class="headerlink" title="对存在问题的SQL使用Hive命令去调"></a>对存在问题的SQL使用Hive命令去调</h4><p>因为Shark初期是想给分析师使用的，他们对分区表并不是很在意，而DROP TABLE可以在客户端做判断，转而使用Hive来执行。</p>
<p>这个方案的优点是可以在现有集群上立刻用起来，但缺点是需要做一些额外的开发，而且API不一致的问题可能还会有其他坑在里面。</p>
<h4 id="升级到CDH5"><a href="#升级到CDH5" class="headerlink" title="升级到CDH5"></a>升级到CDH5</h4><p>CDH5中Hive的版本是0.12.0，所以不排除同样存在API不兼容的问题。不过网上也有人尝试跳过AMPLab Hive，让Shark直接调用CDH中的Hive，其可行性还需要我们自己测试。</p>
<p>对于这个问题，我只在<a href="https://groups.google.com/forum/#!starred/shark-users/x_Dh5-3isIc" target="_blank" rel="external">Google Groups</a>上看到一篇相关的帖子，不过并没有给出解决方案。</p>
<p>目前我们实施的是 <strong>第一种方案</strong>，即在客户端和Shark之间添加一层，不支持的SQL语句直接降级用Hive执行，效果不错。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://spark.apache.org&quot;&gt;Spark&lt;/a&gt;是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的&lt;a href=&quot;http://hadoop.apache.org&quot;&gt;Hadoop&lt;/a&gt;生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于&lt;a href=&quot;http://hive.apache.org&quot;&gt;Hive&lt;/a&gt;，Spark也有相应的替代项目——&lt;a href=&quot;http://shark.cs.berkeley.edu/&quot;&gt;Shark&lt;/a&gt;，能做到 &lt;strong&gt;drop-in replacement&lt;/strong&gt; ，直接构建在现有集群之上。本文就将简要阐述如何在CDH4.5上搭建Shark0.9集群。&lt;/p&gt;
&lt;h2 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;安装方式：Spark使用CDH提供的Parcel，以Standalone模式启动&lt;/li&gt;
&lt;li&gt;软件版本&lt;ul&gt;
&lt;li&gt;Cloudera Manager 4.8.2&lt;/li&gt;
&lt;li&gt;CDH 4.5&lt;/li&gt;
&lt;li&gt;Spark 0.9.0 Parcel&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cloudera.rst.im/shark/&quot;&gt;Shark 0.9.1 Binary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;服务器基础配置&lt;ul&gt;
&lt;li&gt;可用的软件源（如&lt;a href=&quot;http://mirrors.ustc.edu.cn/&quot;&gt;中科大的源&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;配置主节点至子节点的root账户SSH无密码登录。&lt;/li&gt;
&lt;li&gt;在&lt;code&gt;/etc/hosts&lt;/code&gt;中写死IP和主机名，或者DNS做好正反解析。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Notes/Big-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>Use WebJars in Scalatra Project</title>
    <link href="http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project/"/>
    <id>http://shzhangji.com/blog/2014/05/27/use-webjars-in-scalatra-project/</id>
    <published>2014-05-27T09:44:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>As I’m working with my first <a href="http://www.scalatra.org/" target="_blank" rel="external">Scalatra</a> project, I automatically think of using <a href="http://www.webjars.org/" target="_blank" rel="external">WebJars</a> to manage Javascript library dependencies, since it’s more convenient and seems like a good practice. Though there’s no <a href="http://www.webjars.org/documentation" target="_blank" rel="external">official support</a> for Scalatra framework, the installation process is not very complex. But this doesn’t mean I didn’t spend much time on this. I’m still a newbie to Scala, and there’s only a few materials on this subject.</p>
<h2 id="Add-WebJars-Dependency-in-SBT-Build-File"><a href="#Add-WebJars-Dependency-in-SBT-Build-File" class="headerlink" title="Add WebJars Dependency in SBT Build File"></a>Add WebJars Dependency in SBT Build File</h2><p>Scalatra uses <code>.scala</code> configuration file instead of <code>.sbt</code>, so let’s add dependency into <code>project/build.scala</code>. Take <a href="http://dojotoolkit.org/" target="_blank" rel="external">Dojo</a> for example.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">DwExplorerBuild</span> <span class="keyword">extends</span> <span class="title">Build</span> </span>&#123;</div><div class="line">  ...</div><div class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> project = <span class="type">Project</span> (</div><div class="line">    ...</div><div class="line">    settings = <span class="type">Defaults</span>.defaultSettings ++ <span class="type">ScalatraPlugin</span>.scalatraWithJRebel ++ scalateSettings ++ <span class="type">Seq</span>(</div><div class="line">      ...</div><div class="line">      libraryDependencies ++= <span class="type">Seq</span>(</div><div class="line">        ...</div><div class="line">        <span class="string">"org.webjars"</span> % <span class="string">"dojo"</span> % <span class="string">"1.9.3"</span></div><div class="line">      ),</div><div class="line">      ...</div><div class="line">    )</div><div class="line">  )</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>To view this dependency in Eclipse, you need to run <code>sbt eclipse</code> again. In the <em>Referenced Libraries</em> section, you can see a <code>dojo-1.9.3.jar</code>, and the library lies in <code>META-INF/resources/webjars/</code>.</p>
<a id="more"></a>
<h2 id="Add-a-Route-for-WebJars-Resources"><a href="#Add-a-Route-for-WebJars-Resources" class="headerlink" title="Add a Route for WebJars Resources"></a>Add a Route for WebJars Resources</h2><p>Find the <code>ProjectNameStack.scala</code> file and add the following lines at the bottom of the trait:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">trait</span> <span class="title">ProjectNameStack</span> <span class="keyword">extends</span> <span class="title">ScalatraServlet</span> <span class="keyword">with</span> <span class="title">ScalateSupport</span> </span>&#123;</div><div class="line">  ...</div><div class="line">  get(<span class="string">"/webjars/*"</span>) &#123;</div><div class="line">    <span class="keyword">val</span> resourcePath = <span class="string">"/META-INF/resources/webjars/"</span> + params(<span class="string">"splat"</span>)</div><div class="line">    <span class="type">Option</span>(getClass.getResourceAsStream(resourcePath)) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(inputStream) =&gt; &#123;</div><div class="line">        contentType = servletContext.getMimeType(resourcePath)</div><div class="line">        <span class="type">IOUtil</span>.loadBytes(inputStream)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; resourceNotFound()</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>That’s it!</strong> Now you can refer to the WebJars resources in views, like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#set (title)</div><div class="line">Hello, Dojo!</div><div class="line">#end</div><div class="line"></div><div class="line">&lt;div id=&quot;greeting&quot;&gt;&lt;/div&gt;</div><div class="line"></div><div class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;$&#123;uri(&quot;/webjars/dojo/1.9.3/dojo/dojo.js&quot;)&#125;&quot; data-dojo-config=&quot;async: true&quot;&gt;&lt;/script&gt;</div><div class="line">&lt;script type=&quot;text/javascript&quot;&gt;</div><div class="line">require([</div><div class="line">    &apos;dojo/dom&apos;,</div><div class="line">    &apos;dojo/dom-construct&apos;</div><div class="line">], function (dom, domConstruct) &#123;</div><div class="line">    var greetingNode = dom.byId(&apos;greeting&apos;);</div><div class="line">    domConstruct.place(&apos;&lt;i&gt;Dojo!&lt;/i&gt;&apos;, greetingNode);</div><div class="line">&#125;);</div><div class="line">&lt;/script&gt;</div></pre></td></tr></table></figure>
<h3 id="Some-Explanations-on-This-Route"><a href="#Some-Explanations-on-This-Route" class="headerlink" title="Some Explanations on This Route"></a>Some Explanations on This Route</h3><ul>
<li><code>/webjars/*</code> is a <a href="http://www.scalatra.org/2.2/guides/http/routes.html#toc_233" target="_blank" rel="external">Wildcards</a> and <code>params(&quot;splat&quot;)</code> is to extract the asterisk part.</li>
<li><code>resourcePath</code> points to the WebJars resources in the jar file, as we saw in Eclipse. It is then fetched as an <code>InputStream</code> with <code>getResourceAsStream()</code>.</li>
<li><code>servletContext.getMimeType()</code> is a handy method to determine the content type of the requested resource, instead of parsing it by ourselves. I find this in SpringMVC’s <a href="http://grepcode.com/file/repo1.maven.org/maven2/org.springframework/spring-webmvc/3.2.7.RELEASE/org/springframework/web/servlet/resource/ResourceHttpRequestHandler.java#ResourceHttpRequestHandler.handleRequest%28javax.servlet.http.HttpServletRequest%2Cjavax.servlet.http.HttpServletResponse%29" target="_blank" rel="external">ResourceHttpRequestHandler</a>.</li>
<li><code>IOUtil</code> is a utiliy class that comes with <a href="http://scalate.fusesource.org/" target="_blank" rel="external">Scalate</a>, so don’t forget to import it first.</li>
</ul>
<p>At first I tried to figure out whether Scalatra provides a conveniet way to serve static files in classpath, I failed. So I decided to serve them by my own, and <a href="https://gist.github.com/laurilehmijoki/4483113" target="_blank" rel="external">this gist</a> was very helpful.</p>
<p>Anyway, I’ve spent more than half a day to solve this problem, and it turned out to be a very challenging yet interesting way to learn a new language, new framework, and new tools. Keep moving!</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;As I’m working with my first &lt;a href=&quot;http://www.scalatra.org/&quot;&gt;Scalatra&lt;/a&gt; project, I automatically think of using &lt;a href=&quot;http://www.webjars.org/&quot;&gt;WebJars&lt;/a&gt; to manage Javascript library dependencies, since it’s more convenient and seems like a good practice. Though there’s no &lt;a href=&quot;http://www.webjars.org/documentation&quot;&gt;official support&lt;/a&gt; for Scalatra framework, the installation process is not very complex. But this doesn’t mean I didn’t spend much time on this. I’m still a newbie to Scala, and there’s only a few materials on this subject.&lt;/p&gt;
&lt;h2 id=&quot;Add-WebJars-Dependency-in-SBT-Build-File&quot;&gt;&lt;a href=&quot;#Add-WebJars-Dependency-in-SBT-Build-File&quot; class=&quot;headerlink&quot; title=&quot;Add WebJars Dependency in SBT Build File&quot;&gt;&lt;/a&gt;Add WebJars Dependency in SBT Build File&lt;/h2&gt;&lt;p&gt;Scalatra uses &lt;code&gt;.scala&lt;/code&gt; configuration file instead of &lt;code&gt;.sbt&lt;/code&gt;, so let’s add dependency into &lt;code&gt;project/build.scala&lt;/code&gt;. Take &lt;a href=&quot;http://dojotoolkit.org/&quot;&gt;Dojo&lt;/a&gt; for example.&lt;/p&gt;
&lt;figure class=&quot;highlight scala&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;DwExplorerBuild&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Build&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;lazy&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;val&lt;/span&gt; project = &lt;span class=&quot;type&quot;&gt;Project&lt;/span&gt; (&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    settings = &lt;span class=&quot;type&quot;&gt;Defaults&lt;/span&gt;.defaultSettings ++ &lt;span class=&quot;type&quot;&gt;ScalatraPlugin&lt;/span&gt;.scalatraWithJRebel ++ scalateSettings ++ &lt;span class=&quot;type&quot;&gt;Seq&lt;/span&gt;(&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      libraryDependencies ++= &lt;span class=&quot;type&quot;&gt;Seq&lt;/span&gt;(&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;string&quot;&gt;&quot;org.webjars&quot;&lt;/span&gt; % &lt;span class=&quot;string&quot;&gt;&quot;dojo&quot;&lt;/span&gt; % &lt;span class=&quot;string&quot;&gt;&quot;1.9.3&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      ),&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      ...&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    )&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  )&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;To view this dependency in Eclipse, you need to run &lt;code&gt;sbt eclipse&lt;/code&gt; again. In the &lt;em&gt;Referenced Libraries&lt;/em&gt; section, you can see a &lt;code&gt;dojo-1.9.3.jar&lt;/code&gt;, and the library lies in &lt;code&gt;META-INF/resources/webjars/&lt;/code&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://shzhangji.com/categories/Programming/"/>
    
    
      <category term="scala" scheme="http://shzhangji.com/tags/scala/"/>
    
      <category term="scalatra" scheme="http://shzhangji.com/tags/scalatra/"/>
    
      <category term="webjars" scheme="http://shzhangji.com/tags/webjars/"/>
    
      <category term="english" scheme="http://shzhangji.com/tags/english/"/>
    
  </entry>
  
  <entry>
    <title>Hive小文件问题的处理</title>
    <link href="http://shzhangji.com/blog/2014/04/07/hive-small-files/"/>
    <id>http://shzhangji.com/blog/2014/04/07/hive-small-files/</id>
    <published>2014-04-07T09:09:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hive的后端存储是HDFS，它对大文件的处理是非常高效的，如果合理配置文件系统的块大小，NameNode可以支持很大的数据量。但是在数据仓库中，越是上层的表其汇总程度就越高，数据量也就越小。而且这些表通常会按日期进行分区，随着时间的推移，HDFS的文件数目就会逐渐增加。</p>
<h2 id="小文件带来的问题"><a href="#小文件带来的问题" class="headerlink" title="小文件带来的问题"></a>小文件带来的问题</h2><p>关于这个问题的阐述可以读一读Cloudera的<a href="http://blog.cloudera.com/blog/2009/02/the-small-files-problem/" target="_blank" rel="external">这篇文章</a>。简单来说，HDFS的文件元信息，包括位置、大小、分块信息等，都是保存在NameNode的内存中的。每个对象大约占用150个字节，因此一千万个文件及分块就会占用约3G的内存空间，一旦接近这个量级，NameNode的性能就会开始下降了。</p>
<p>此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，每个脚本只处理很少的数据，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决。</p>
<a id="more"></a>
<h2 id="Hive小文件产生的原因"><a href="#Hive小文件产生的原因" class="headerlink" title="Hive小文件产生的原因"></a>Hive小文件产生的原因</h2><p>前面已经提到，汇总后的数据量通常比源数据要少得多。而为了提升运算速度，我们会增加Reducer的数量，Hive本身也会做类似优化——Reducer数量等于源数据的量除以hive.exec.reducers.bytes.per.reducer所配置的量（默认1G）。Reducer数量的增加也即意味着结果文件的增加，从而产生小文件的问题。</p>
<h2 id="配置Hive结果合并"><a href="#配置Hive结果合并" class="headerlink" title="配置Hive结果合并"></a>配置Hive结果合并</h2><p>我们可以通过一些配置项来使Hive在执行结束后对结果文件进行合并：</p>
<ul>
<li><code>hive.merge.mapfiles</code> 在map-only job后合并文件，默认<code>true</code></li>
<li><code>hive.merge.mapredfiles</code> 在map-reduce job后合并文件，默认<code>false</code></li>
<li><code>hive.merge.size.per.task</code> 合并后每个文件的大小，默认<code>256000000</code></li>
<li><code>hive.merge.smallfiles.avgsize</code> 平均文件大小，是决定是否执行合并操作的阈值，默认<code>16000000</code></li>
</ul>
<p>Hive在对结果文件进行合并时会执行一个额外的map-only脚本，mapper的数量是文件总大小除以size.per.task参数所得的值，触发合并的条件是：</p>
<ol>
<li>根据查询类型不同，相应的mapfiles/mapredfiles参数需要打开；</li>
<li>结果文件的平均大小需要大于avgsize参数的值。</li>
</ol>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">-- map-red job，5个reducer，产生5个60K的文件。</span></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> paid, <span class="keyword">count</span>(*)</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">group</span> <span class="keyword">by</span> paid;</div><div class="line"></div><div class="line"><span class="comment">-- 执行额外的map-only job，一个mapper，产生一个300K的文件。</span></div><div class="line"><span class="keyword">set</span> hive.merge.mapredfiles=<span class="literal">true</span>;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> paid, <span class="keyword">count</span>(*)</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">group</span> <span class="keyword">by</span> paid;</div><div class="line"></div><div class="line"><span class="comment">-- map-only job，45个mapper，产生45个25M左右的文件。</span></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> *</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">and</span> paid <span class="keyword">like</span> <span class="string">'%baidu%'</span>;</div><div class="line"></div><div class="line"><span class="comment">-- 执行额外的map-only job，4个mapper，产生4个250M左右的文件。</span></div><div class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize=<span class="number">100000000</span>;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small <span class="keyword">as</span></div><div class="line"><span class="keyword">select</span> *</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">and</span> paid <span class="keyword">like</span> <span class="string">'%baidu%'</span>;</div></pre></td></tr></table></figure>
<h3 id="压缩文件的处理"><a href="#压缩文件的处理" class="headerlink" title="压缩文件的处理"></a>压缩文件的处理</h3><p>如果结果表使用了压缩格式，则必须配合SequenceFile来存储，否则无法进行合并，以下是示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">set</span> mapred.output.compression.type=<span class="keyword">BLOCK</span>;</div><div class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</div><div class="line"><span class="keyword">set</span> mapred.output.compression.codec=org.apache.hadoop.io.compress.LzoCodec;</div><div class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize=<span class="number">100000000</span>;</div><div class="line"></div><div class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> dw_stage.zj_small;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dw_stage.zj_small</div><div class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE</div><div class="line"><span class="keyword">as</span> <span class="keyword">select</span> *</div><div class="line"><span class="keyword">from</span> dw_db.dw_soj_imp_dtl</div><div class="line"><span class="keyword">where</span> log_dt = <span class="string">'2014-04-14'</span></div><div class="line"><span class="keyword">and</span> paid <span class="keyword">like</span> <span class="string">'%baidu%'</span>;</div></pre></td></tr></table></figure>
<h2 id="使用HAR归档文件"><a href="#使用HAR归档文件" class="headerlink" title="使用HAR归档文件"></a>使用HAR归档文件</h2><p>Hadoop的<a href="http://hadoop.apache.org/docs/stable1/hadoop_archives.html" target="_blank" rel="external">归档文件</a>格式也是解决小文件问题的方式之一。而且Hive提供了<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Archiving" target="_blank" rel="external">原生支持</a>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">set hive.archive.enabled=true;</div><div class="line">set hive.archive.har.parentdir.settable=true;</div><div class="line">set har.partfile.size=1099511627776;</div><div class="line"></div><div class="line">ALTER TABLE srcpart ARCHIVE PARTITION(ds=&apos;2008-04-08&apos;, hr=&apos;12&apos;);</div><div class="line"></div><div class="line">ALTER TABLE srcpart UNARCHIVE PARTITION(ds=&apos;2008-04-08&apos;, hr=&apos;12&apos;);</div></pre></td></tr></table></figure>
<p>如果使用的不是分区表，则可创建成外部表，并使用<code>har://</code>协议来指定路径。</p>
<h2 id="HDFS-Federation"><a href="#HDFS-Federation" class="headerlink" title="HDFS Federation"></a>HDFS Federation</h2><p>Hadoop V2引入了HDFS Federation的概念：</p>
<p><img src="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/images/federation.gif" alt=""></p>
<p>实则是将NameNode做了拆分，从而增强了它的扩展性，小文件的问题也能够得到缓解。</p>
<h2 id="其他工具"><a href="#其他工具" class="headerlink" title="其他工具"></a>其他工具</h2><p>对于通常的应用，使用Hive结果合并就能达到很好的效果。如果不想因此增加运行时间，可以自行编写一些脚本，在系统空闲时对分区内的文件进行合并，也能达到目的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive的后端存储是HDFS，它对大文件的处理是非常高效的，如果合理配置文件系统的块大小，NameNode可以支持很大的数据量。但是在数据仓库中，越是上层的表其汇总程度就越高，数据量也就越小。而且这些表通常会按日期进行分区，随着时间的推移，HDFS的文件数目就会逐渐增加。&lt;/p&gt;
&lt;h2 id=&quot;小文件带来的问题&quot;&gt;&lt;a href=&quot;#小文件带来的问题&quot; class=&quot;headerlink&quot; title=&quot;小文件带来的问题&quot;&gt;&lt;/a&gt;小文件带来的问题&lt;/h2&gt;&lt;p&gt;关于这个问题的阐述可以读一读Cloudera的&lt;a href=&quot;http://blog.cloudera.com/blog/2009/02/the-small-files-problem/&quot;&gt;这篇文章&lt;/a&gt;。简单来说，HDFS的文件元信息，包括位置、大小、分块信息等，都是保存在NameNode的内存中的。每个对象大约占用150个字节，因此一千万个文件及分块就会占用约3G的内存空间，一旦接近这个量级，NameNode的性能就会开始下降了。&lt;/p&gt;
&lt;p&gt;此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，每个脚本只处理很少的数据，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决。&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Notes/Big-Data/"/>
    
    
  </entry>
  
  <entry>
    <title>Java反射机制</title>
    <link href="http://shzhangji.com/blog/2014/01/25/java-reflection-tutorial/"/>
    <id>http://shzhangji.com/blog/2014/01/25/java-reflection-tutorial/</id>
    <published>2014-01-25T01:42:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://www.programcreek.com/2013/09/java-reflection-tutorial/" target="_blank" rel="external">http://www.programcreek.com/2013/09/java-reflection-tutorial/</a></p>
<p>什么是反射？它有何用处？</p>
<h2 id="1-什么是反射？"><a href="#1-什么是反射？" class="headerlink" title="1. 什么是反射？"></a>1. 什么是反射？</h2><p>“反射（Reflection）能够让运行于JVM中的程序检测和修改运行时的行为。”这个概念常常会和内省（Introspection）混淆，以下是这两个术语在Wikipedia中的解释：</p>
<ol>
<li>内省用于在运行时检测某个对象的类型和其包含的属性；</li>
<li>反射用于在运行时检测和修改某个对象的结构及其行为。</li>
</ol>
<p>从他们的定义可以看出，内省是反射的一个子集。有些语言支持内省，但并不支持反射，如C++。</p>
<p><img src="http://www.programcreek.com/wp-content/uploads/2013/09/reflection-introspection-650x222.png" alt="反射和内省"></p>
<a id="more"></a>
<p>内省示例：<code>instanceof</code>运算符用于检测某个对象是否属于特定的类。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (obj <span class="keyword">instanceof</span> Dog) &#123;</div><div class="line">    Dog d = (Dog) obj;</div><div class="line">    d.bark();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>反射示例：<code>Class.forName()</code>方法可以通过类或接口的名称（一个字符串或完全限定名）来获取对应的<code>Class</code>对象。<code>forName</code>方法会触发类的初始化。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 使用反射</span></div><div class="line">Class&lt;?&gt; c = Class.forName(<span class="string">"classpath.and.classname"</span>);</div><div class="line">Object dog = c.newInstance();</div><div class="line">Method m = c.getDeclaredMethod(<span class="string">"bark"</span>, <span class="keyword">new</span> Class&lt;?&gt;[<span class="number">0</span>]);</div><div class="line">m.invoke(dog);</div></pre></td></tr></table></figure>
<p>在Java中，反射更接近于内省，因为你无法改变一个对象的结构。虽然一些API可以用来修改方法和属性的可见性，但并不能修改结构。</p>
<h2 id="2-我们为何需要反射？"><a href="#2-我们为何需要反射？" class="headerlink" title="2. 我们为何需要反射？"></a>2. 我们为何需要反射？</h2><p>反射能够让我们：</p>
<ul>
<li>在运行时检测对象的类型；</li>
<li>动态构造某个类的对象；</li>
<li>检测类的属性和方法；</li>
<li>任意调用对象的方法；</li>
<li>修改构造函数、方法、属性的可见性；</li>
<li>以及其他</li>
</ul>
<p>反射是框架中常用的方法。</p>
<p>例如，<a href="http://www.programcreek.com/2012/02/junit-tutorial-2-annotations/" target="_blank" rel="external">JUnit</a>通过反射来遍历包含 <em>@Test</em> 注解的方法，并在运行单元测试时调用它们。（<a href="http://www.programcreek.com/2012/02/junit-tutorial-2-annotations/" target="_blank" rel="external">这个连接</a>中包含了一些JUnit的使用案例）</p>
<p>对于Web框架，开发人员在配置文件中定义他们对各种接口和类的实现。通过反射机制，框架能够快速地动态初始化所需要的类。</p>
<p>例如，Spring框架使用如下的配置文件：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"someID"</span> <span class="attr">class</span>=<span class="string">"com.programcreek.Foo"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"someField"</span> <span class="attr">value</span>=<span class="string">"someValue"</span> /&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></div></pre></td></tr></table></figure>
<p>当Spring容器处理&lt;bean&gt;元素时，会使用<code>Class.forName(&quot;com.programcreek.Foo&quot;)</code>来初始化这个类，并再次使用反射获取&lt;property&gt;元素对应的<code>setter</code>方法，为对象的属性赋值。</p>
<p>Servlet也会使用相同的机制：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">servlet</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">servlet-name</span>&gt;</span>someServlet<span class="tag">&lt;/<span class="name">servlet-name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">servlet-class</span>&gt;</span>com.programcreek.WhyReflectionServlet<span class="tag">&lt;/<span class="name">servlet-class</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">servlet</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="3-如何使用反射？"><a href="#3-如何使用反射？" class="headerlink" title="3. 如何使用反射？"></a>3. 如何使用反射？</h2><p>让我们通过几个典型的案例来学习如何使用反射。</p>
<p>示例1：获取对象的类型名称。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> myreflection;</div><div class="line"><span class="keyword">import</span> java.lang.reflect.Method;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReflectionHelloWorld</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</div><div class="line">        Foo f = <span class="keyword">new</span> Foo();</div><div class="line">        System.out.println(f.getClass().getName());			</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</div><div class="line">        System.out.println(<span class="string">"abc"</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">myreflection.Foo</div></pre></td></tr></table></figure>
<p>示例2：调用未知对象的方法。</p>
<p>在下列代码中，设想对象的类型是未知的。通过反射，我们可以判断它是否包含<code>print</code>方法，并调用它。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> myreflection;</div><div class="line"><span class="keyword">import</span> java.lang.reflect.Method;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReflectionHelloWorld</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</div><div class="line">        Foo f = <span class="keyword">new</span> Foo();</div><div class="line"> </div><div class="line">        Method method;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            method = f.getClass().getMethod(<span class="string">"print"</span>, <span class="keyword">new</span> Class&lt;?&gt;[<span class="number">0</span>]);</div><div class="line">            method.invoke(f);</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;           </div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</div><div class="line">        System.out.println(<span class="string">"abc"</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">abc</div></pre></td></tr></table></figure>
<p>示例3：创建对象</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> myreflection;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReflectionHelloWorld</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</div><div class="line">        <span class="comment">// 创建Class实例</span></div><div class="line">        Class&lt;?&gt; c = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">try</span>&#123;</div><div class="line">            c=Class.forName(<span class="string">"myreflection.Foo"</span>);</div><div class="line">        &#125;<span class="keyword">catch</span>(Exception e)&#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line"> </div><div class="line">        <span class="comment">// 创建Foo实例</span></div><div class="line">        Foo f = <span class="keyword">null</span>;</div><div class="line"> </div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            f = (Foo) c.newInstance();</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;   </div><div class="line"> </div><div class="line">        f.print();</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</div><div class="line">        System.out.println(<span class="string">"abc"</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>示例4：获取构造函数，并创建对象。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> myreflection;</div><div class="line"> </div><div class="line"><span class="keyword">import</span> java.lang.reflect.Constructor;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReflectionHelloWorld</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</div><div class="line">        <span class="comment">// 创建Class实例</span></div><div class="line">        Class&lt;?&gt; c = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">try</span>&#123;</div><div class="line">            c=Class.forName(<span class="string">"myreflection.Foo"</span>);</div><div class="line">        &#125;<span class="keyword">catch</span>(Exception e)&#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line"> </div><div class="line">        <span class="comment">// 创建Foo实例</span></div><div class="line">        Foo f1 = <span class="keyword">null</span>;</div><div class="line">        Foo f2 = <span class="keyword">null</span>;</div><div class="line"> </div><div class="line">        <span class="comment">// 获取所有的构造函数</span></div><div class="line">        Constructor&lt;?&gt; cons[] = c.getConstructors();</div><div class="line"> </div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            f1 = (Foo) cons[<span class="number">0</span>].newInstance();</div><div class="line">            f2 = (Foo) cons[<span class="number">1</span>].newInstance(<span class="string">"abc"</span>);</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;   </div><div class="line"> </div><div class="line">        f1.print();</div><div class="line">        f2.print();</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">    String s; </div><div class="line"> </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Foo</span><span class="params">()</span></span>&#123;&#125;</div><div class="line"> </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Foo</span><span class="params">(String s)</span></span>&#123;</div><div class="line">        <span class="keyword">this</span>.s=s;</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</div><div class="line">        System.out.println(s);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">null</div><div class="line">abc</div></pre></td></tr></table></figure>
<p>此外，你可以通过<code>Class</code>实例来获取该类实现的接口、父类、声明的属性等。</p>
<p>示例5：通过反射来修改数组的大小。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> myreflection;</div><div class="line"> </div><div class="line"><span class="keyword">import</span> java.lang.reflect.Array;</div><div class="line"> </div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReflectionHelloWorld</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span>[] intArray = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> &#125;;</div><div class="line">        <span class="keyword">int</span>[] newIntArray = (<span class="keyword">int</span>[]) changeArraySize(intArray, <span class="number">10</span>);</div><div class="line">        print(newIntArray);</div><div class="line"> </div><div class="line">        String[] atr = &#123; <span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span> &#125;;</div><div class="line">        String[] str1 = (String[]) changeArraySize(atr, <span class="number">10</span>);</div><div class="line">        print(str1);</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    <span class="comment">// 修改数组的大小</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Object <span class="title">changeArraySize</span><span class="params">(Object obj, <span class="keyword">int</span> len)</span> </span>&#123;</div><div class="line">        Class&lt;?&gt; arr = obj.getClass().getComponentType();</div><div class="line">        Object newArray = Array.newInstance(arr, len);</div><div class="line"> </div><div class="line">        <span class="comment">// 复制数组</span></div><div class="line">        <span class="keyword">int</span> co = Array.getLength(obj);</div><div class="line">        System.arraycopy(obj, <span class="number">0</span>, newArray, <span class="number">0</span>, co);</div><div class="line">        <span class="keyword">return</span> newArray;</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    <span class="comment">// 打印</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">(Object obj)</span> </span>&#123;</div><div class="line">        Class&lt;?&gt; c = obj.getClass();</div><div class="line">        <span class="keyword">if</span> (!c.isArray()) &#123;</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line"> </div><div class="line">        System.out.println(<span class="string">"\nArray length: "</span> + Array.getLength(obj));</div><div class="line"> </div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Array.getLength(obj); i++) &#123;</div><div class="line">            System.out.print(Array.get(obj, i) + <span class="string">" "</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Array length: 10</div><div class="line">1 2 3 4 5 0 0 0 0 0 </div><div class="line">Array length: 10</div><div class="line">a b c d e null null null null null</div></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上述示例代码仅仅展现了Java反射机制很小一部分的功能。如果你觉得意犹未尽，可以前去阅读<a href="http://docs.oracle.com/javase/tutorial/reflect/" target="_blank" rel="external">官方文档</a>。</p>
<p>参考资料：</p>
<ol>
<li><a href="http://en.wikipedia.org/wiki/Reflection_(computer_programming" target="_blank" rel="external">http://en.wikipedia.org/wiki/Reflection_(computer_programming</a>)</li>
<li><a href="http://docs.oracle.com/javase/tutorial/reflect/" target="_blank" rel="external">http://docs.oracle.com/javase/tutorial/reflect/</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://www.programcreek.com/2013/09/java-reflection-tutorial/&quot;&gt;http://www.programcreek.com/2013/09/java-reflection-tutorial/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;什么是反射？它有何用处？&lt;/p&gt;
&lt;h2 id=&quot;1-什么是反射？&quot;&gt;&lt;a href=&quot;#1-什么是反射？&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是反射？&quot;&gt;&lt;/a&gt;1. 什么是反射？&lt;/h2&gt;&lt;p&gt;“反射（Reflection）能够让运行于JVM中的程序检测和修改运行时的行为。”这个概念常常会和内省（Introspection）混淆，以下是这两个术语在Wikipedia中的解释：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;内省用于在运行时检测某个对象的类型和其包含的属性；&lt;/li&gt;
&lt;li&gt;反射用于在运行时检测和修改某个对象的结构及其行为。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;从他们的定义可以看出，内省是反射的一个子集。有些语言支持内省，但并不支持反射，如C++。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://www.programcreek.com/wp-content/uploads/2013/09/reflection-introspection-650x222.png&quot; alt=&quot;反射和内省&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
      <category term="Tutorial" scheme="http://shzhangji.com/categories/Translation/Tutorial/"/>
    
    
  </entry>
  
  <entry>
    <title>抽象泄漏定律</title>
    <link href="http://shzhangji.com/blog/2013/12/17/the-law-of-leaky-abstractions/"/>
    <id>http://shzhangji.com/blog/2013/12/17/the-law-of-leaky-abstractions/</id>
    <published>2013-12-17T05:05:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html" target="_blank" rel="external">http://www.joelonsoftware.com/articles/LeakyAbstractions.html</a></p>
<p>TCP协议是互联网的基石，我们每天都需要依靠它来构建各类互联网应用。也正是在这一协议中，时刻发生着一件近乎神奇的事情。</p>
<p>TCP是一种 <em>可靠的</em> 数据传输协议，也就是说，当你通过TCP协议在网络上传输一条消息时，它一定会到达目的地，而且不会失真或毁坏。</p>
<p>我们可以使用TCP来做很多事情，从浏览网页信息到收发邮件。TCP的可靠性使得东非贪污受贿的新闻能够一字一句地传递到世界各地。真是太棒了！</p>
<p>和TCP协议相比，IP协议也是一种传输协议，但它是 <em>不可靠的</em> 。没有人可以保证你的数据一定会到达目的地，或者在它到达前就已经被破坏了。如果你发送了一组消息，不要惊讶为何只有一半的消息到达，有些消息的顺序会不正确，甚至消息的内容被替换成了黑猩猩宝宝的图片，或是一堆无法阅读的垃圾数据，像极了台湾人的邮件标题。</p>
<p>这就是TCP协议神奇的地方：它是构建在IP协议之上的。换句话说，TCP协议能够 <em>使用一个不可靠的工具来可靠地传输数据</em> 。</p>
<a id="more"></a>
<p>为了更好地说明这有多么神奇，让我们设想下面的场景。虽然有些荒诞，但本质上是相同的。</p>
<p>假设我们用一辆辆汽车将百老汇的演员们运送到好莱坞，这是一条横跨美国的漫长线路。其中一些汽车出了交通事故，车上的演员在事故中死亡。有些演员则在车上酗酒嗑药，兴奋之余将自己的头发剃了，或是纹上了丑陋的纹身，这样一来就失去了他们原先的样貌，无法在好莱坞演出。更普遍的情况是，演员们没有按照出发的顺序到达目的地，因为他们走的都是不同的线路。现在再让我们设想有一个名为“好莱坞快线”的运输服务，在运送这些演员时能够保证三点：他们都能够到达；到达顺序和出发顺序一致；并且都完好无损。神奇的是，好莱坞快线除了用汽车来运输这些演员之外，没有任何其他的方法。所以它能做的就是检查每一个到达目的地的演员，看他们是否和原先的相貌一致。如果有所差别，它就立刻通知百老汇的办公室，派出该演员的双胞胎兄妹，重新发送过来。如果演员到达的顺序不同，好莱坞快线会负责重新排序。如果有一架UFO在飞往51区的途中不慎坠毁在内华达州，造成高速公路阻塞，这时所有打算从这条路经过的演员会绕道亚利桑那州。好莱坞快线不会告诉加利福尼亚州的导演路上发生了什么，只是这些演员到的比较迟而已。</p>
<p>这大致上就是TCP协议的神奇之处，计算机科学家们通常会将其称作为“抽象”：将复杂的问题用简单的方式表现出来。事实上，很多计算机编程工作都是在进行抽象。字符串库做了什么？它能让我们觉得计算机可以像处理数字那样处理文字。文件系统是什么？它让硬盘不再是一组高速旋转的磁性盘块，而是一个有着目录层级结构、能够按字节存储字符信息的设备。</p>
<p>我们继续说TCP。刚才我打了一个比方，有些人可能觉得那很疯狂。但是，当我说TCP协议可以保证消息一定能够到达，事实上并非如此。如果你的宠物蛇把网线给咬坏了，那即便是TCP协议也无法传输数据；如果你和网络管理员闹了矛盾，他将你的网口接到了一台负载很高的交换机上，那即便你的数据包可以传输，速度也会奇慢无比。</p>
<p>这就是我所说的“抽象泄漏”。TCP协议试图提供一个完整的抽象，将底层不可靠的数据传输包装起来，但是，底层的传输有时也会发生问题，即便是TCP协议也无法解决，这时你会发现，它也不是万能的。TCP协议就是“抽象泄漏定律”的示例之一，其实，几乎所有的抽象都是泄漏的。这种泄漏有时很小，有时会很严重。下面再举一些例子：</p>
<ul>
<li><p>对于一个简单的操作，如循环遍历一个二维数组，当遍历的方式不同（横向或纵向），也会对性能造成很大影响，这主要取决于数组中数据的分布——按某个方向遍历时可能会产生更多的页缺失（page fault），而页缺失往往是非常消耗性能的。即使是汇编程序员，他们在编写代码时也会假设程序的内存空间是连续的，这是系统底层的虚拟内存机制提供的抽象，而这一机制在遇到页缺失时就会消耗更多时间。</p>
</li>
<li><p>SQL语言意图将过程式的数据库访问操作封装起来，你只需要告诉操作系统你想要的数据，系统会自动生成各个步骤并加以执行。但在有些情况下，某些SQL查询会比其逻辑等同的查询语句要慢得多。一个著名的示例是，对大多数SQL服务器，指定“WHERE a = b AND b = c AND a = c”要比单纯指定“WHERE a = b AND b = c”快的多，即便它们的结果集是一致的。在使用SQL时，我们不需要思考过程，只需关注定义。但有时，这种抽象会造成性能上的大幅下降，你需要去了解SQL语法分析器的工作原理，找出问题的原因，并想出应对措施，让自己的查询运行得更快。</p>
</li>
<li><p>即便有NFS、SMB这样的协议可以让你像在处理本地文件一样处理远程文件，如果网络传输很慢，或是完全中断了，程序员就需要手动处理这种情况。所以，这种“远程文件即本地文件”的抽象机制是存在<a href="http://www.joelonsoftware.com/articles/fog0000000041.html" target="_blank" rel="external">泄漏</a>的。这里举一个现实的例子：如果你将用户的home目录加载到NFS上（一次抽象），你的用户创建了.forward文件，用来转发他所有的电子邮件（二次抽象），当NFS服务器宕机，.forward文件会找不到，这样就无法转发邮件了，造成丢失。</p>
</li>
<li><p>C++的字符串处理类库相当于增加了一种基础数据类型：字符串，将<a href="http://www.joelonsoftware.com/articles/fog0000000319.html" target="_blank" rel="external">各种操作细节</a>封装起来，让程序员可以方便地使用它。几乎所有的C++字符串类都会重载+操作符，这样你就能用 <em>s + “bar”</em> 来拼接字符串了。但是，无论哪种类库都无法实现 <em>“foo” + “bar”</em> 这种语句，因为在C++中，字符串字面量（string literal）都是char *类型的。这就是一种泄漏。（有趣的是，C++语言的发展历程很大一部分是在争论字符串是否应该在语言层面支持。我个人并不太能理解这为何需要争论。）</p>
</li>
<li><p>当你在雨天开车，虽然你坐在车里，前窗有雨刷，车内有空调，这些措施将“天气”给抽象走了。但是，你还是要小心雨天的轮胎打滑，有时这雨下得太大，可见度很糟，所以你还是得慢行。也就是说，“天气”因素并没有被完全抽象走，它也是存在泄漏的。</p>
</li>
</ul>
<p>抽象泄漏引发的麻烦之一是，它并没有完全简化我们的工作。当我指导别人学习C++时，我当然希望可以跳过char *和指针运算，直接讲解STL字符串类库的使用。但是，当某一天他写出了 <em>“foo” + “bar”</em> 这样的代码，并询问我为什么编译错误时，我还是需要告诉它char *的存在。或者说，当他需要调用一个Windows API，需要指定OUT LPTSTR参数，这时他就必须学习char *、指针、Unicode、wchar_t、TCHAR头文件等一系列知识，这些都是抽象泄漏。</p>
<p>在指导COM编程时，我希望可以直接让大家如何使用Visual Studio的代码生成向导。但将来如果出现问题，学员面对这些生成的代码会不知所从，这时还是要回过头来学习IUnknown、CLSID、ProgIDS等等。天呐！</p>
<p>在指导ASP.NET编程时，我希望可以直接告诉大家双击页面上的控件，在弹出的代码框中输入点击响应事件。的确，ASP.NET将处理点击的HTML代码抽象掉了，但问题在于，ASP.NET的设计者需要动用JS来模拟表单的提交，因为HTML中的&lt;a/&gt;标签是没有这一功能的。这样一来，如果终端用户将JS禁止了，这个程序将无法运行。初学者会不知所措，直至他了解ASP.NET的运作方式，了解它究竟将什么样的工作封装起来了，才能进一步排查。</p>
<p>由于抽象定律的存在，每当有人说自己发现了一款新的代码生成工具，能够大大提高我们的编程效率时，你会听很多人说“先学习手工编写，再去用工具生成”。代码生成工具是一种抽象，同样也会泄漏，唯一的解决方法是学习它的实现原理，即它抽象了什么。所以说抽象只是用于提高我们的工作效率的，而不会节省我们的学习时间。</p>
<p>这就形成了一个悖论：当我们拥有越来越高级的开发工具，越来越好的“抽象”，要成为一个高水平的程序员反而越来越困难了。</p>
<p>我在微软实习的第一年，是为Macintosh编写字符串处理类库。很普通的一个任务：编写 <em>strcat</em> 函数，返回一个指针，指向新字符串的尾部。几行C语言代码就能实现了，这些都是从K&amp;R这本C语言编程书上学习到的。</p>
<p>如今，我在CityDesk供职，需要使用Visual Basic、COM、ATL、C++、InnoSetup、Internet Explorer原理、正则表达式、DOM、HTML、CSS、XML等等，这些相对于古老的K&amp;R来说都是非常高级的工具，但是我仍然需要用到K&amp;R的相关知识，否则会困难重重。</p>
<p>十年前，我们会想象未来能够出现各种新式的编程范型，简化我们的工作。的确，这些年我们创造的各类抽象使得开发复杂的大型软件变得比十五年前要简单得多，就像GUI和网络编程。现代的面向对象编程语言让我们的工作变得高效快速。但突然有一天，这种抽象泄漏出一个问题，解决它需要耗费两星期。如果你需要招录一个VB程序员，那不是一个好主意，因为当他碰到VB语言泄漏的问题时，他会变得寸步难行。</p>
<p>抽象泄漏定律正在阻碍我们前进。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&quot;&gt;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TCP协议是互联网的基石，我们每天都需要依靠它来构建各类互联网应用。也正是在这一协议中，时刻发生着一件近乎神奇的事情。&lt;/p&gt;
&lt;p&gt;TCP是一种 &lt;em&gt;可靠的&lt;/em&gt; 数据传输协议，也就是说，当你通过TCP协议在网络上传输一条消息时，它一定会到达目的地，而且不会失真或毁坏。&lt;/p&gt;
&lt;p&gt;我们可以使用TCP来做很多事情，从浏览网页信息到收发邮件。TCP的可靠性使得东非贪污受贿的新闻能够一字一句地传递到世界各地。真是太棒了！&lt;/p&gt;
&lt;p&gt;和TCP协议相比，IP协议也是一种传输协议，但它是 &lt;em&gt;不可靠的&lt;/em&gt; 。没有人可以保证你的数据一定会到达目的地，或者在它到达前就已经被破坏了。如果你发送了一组消息，不要惊讶为何只有一半的消息到达，有些消息的顺序会不正确，甚至消息的内容被替换成了黑猩猩宝宝的图片，或是一堆无法阅读的垃圾数据，像极了台湾人的邮件标题。&lt;/p&gt;
&lt;p&gt;这就是TCP协议神奇的地方：它是构建在IP协议之上的。换句话说，TCP协议能够 &lt;em&gt;使用一个不可靠的工具来可靠地传输数据&lt;/em&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="Translation" scheme="http://shzhangji.com/categories/Translation/"/>
    
    
  </entry>
  
  <entry>
    <title>Generate Auto-increment Id in Map-reduce Job</title>
    <link href="http://shzhangji.com/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job/"/>
    <id>http://shzhangji.com/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job/</id>
    <published>2013-10-31T01:35:00.000Z</published>
    <updated>2017-01-03T12:40:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>In DBMS world, it’s easy to generate a unique, auto-increment id, using MySQL’s <a href="http://dev.mysql.com/doc/refman/5.1/en/example-auto-increment.html" target="_blank" rel="external">AUTO_INCREMENT attribute</a> on a primary key or MongoDB’s <a href="http://docs.mongodb.org/manual/tutorial/create-an-auto-incrementing-field/" target="_blank" rel="external">Counters Collection</a> pattern. But when it comes to a distributed, parallel processing framework, like Hadoop Map-reduce, it is not that straight forward. The best solution to identify every record in such framework is to use UUID. But when an integer id is required, it’ll take some steps.</p>
<h2 id="Solution-A-Single-Reducer"><a href="#Solution-A-Single-Reducer" class="headerlink" title="Solution A: Single Reducer"></a>Solution A: Single Reducer</h2><p>This is the most obvious and simple one, just use the following code to specify reducer numbers to 1:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">job.setNumReduceTasks(<span class="number">1</span>);</div></pre></td></tr></table></figure>
<p>And also obvious, there are several demerits:</p>
<ol>
<li>All mappers output will be copied to one task tracker.</li>
<li>Only one process is working on shuffel &amp; sort.</li>
<li>When producing output, there’s also only one process.</li>
</ol>
<p>The above is not a problem for small data sets, or at least small mapper outputs. And it is also the approach that Pig and Hive use when they need to perform a total sort. But when hitting a certain threshold, the sort and copy phase will become very slow and unacceptable.</p>
<a id="more"></a>
<h2 id="Solution-B-Increment-by-Number-of-Tasks"><a href="#Solution-B-Increment-by-Number-of-Tasks" class="headerlink" title="Solution B: Increment by Number of Tasks"></a>Solution B: Increment by Number of Tasks</h2><p>Inspired by a <a href="http://mail-archives.apache.org/mod_mbox/hadoop-common-user/200904.mbox/%3C49E13557.7090504@domaintools.com%3E" target="_blank" rel="external">mailing list</a> that is quite hard to find, which is inspired by MySQL master-master setup (with auto_increment_increment and auto_increment_offset), there’s a brilliant way to generate a globally unique integer id across mappers or reducers. Let’s take mapper for example:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">long</span> id;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">int</span> increment;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException,</span></div><div class="line">            InterruptedException &#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>.setup(context);</div><div class="line"></div><div class="line">        id = context.getTaskAttemptID().getTaskID().getId();</div><div class="line">        increment = context.getConfiguration().getInt(<span class="string">"mapred.map.tasks"</span>, <span class="number">0</span>);</div><div class="line">        <span class="keyword">if</span> (increment == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"mapred.map.tasks is zero"</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></div><div class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">        id += increment;</div><div class="line">        context.write(<span class="keyword">new</span> LongWritable(id),</div><div class="line">                <span class="keyword">new</span> Text(String.format(<span class="string">"%d, %s"</span>, key.get(), value.toString())));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>The basic idea is simple:</p>
<ol>
<li>Set the initial id to current tasks’s id.</li>
<li>When mapping each row, increment the id by the number of tasks.</li>
</ol>
<p>It’s also applicable to reducers.</p>
<h2 id="Solution-C-Sorted-Auto-increment-Id"><a href="#Solution-C-Sorted-Auto-increment-Id" class="headerlink" title="Solution C: Sorted Auto-increment Id"></a>Solution C: Sorted Auto-increment Id</h2><p>Here’s a real senario: we have several log files pulled from different machines, and we want to identify each row by an auto-increment id, and they should be in time sequence order.</p>
<p>We know Hadoop has a sort phase, so we can use timestamp as the mapper output key, and the framework will do the trick. But the sorting thing happends in one reducer (partition, in fact), so when using multiple reducer tasks, the result is not in total order. To achieve this, we can use the <a href="http://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.html" target="_blank" rel="external">TotalOrderPartitioner</a>.</p>
<p>How about the incremental id? Even though the outputs are in total order, Solution B is not applicable here. So we take another approach: seperate the job in two phases, use the reducer to do sorting <em>and</em> counting, then use the second mapper to generate the id.</p>
<p>Here’s what we gonna do:</p>
<ol>
<li>Use TotalOrderPartitioner, and generate the partition file.</li>
<li>Parse logs in mapper A, use time as the output key.</li>
<li>Let the framework do partitioning and sorting.</li>
<li>Count records in reducer, write it with <a href="http://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.html" target="_blank" rel="external">MultipleOutput</a>.</li>
<li>In mapper B, use count as offset, and increment by 1.</li>
</ol>
<p>To simplify the situation, we assume to have the following inputs and outputs:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"> Input       Output</div><div class="line"> </div><div class="line">11:00 a     1 11:00 a</div><div class="line">12:00 b     2 11:01 aa</div><div class="line">13:00 c     3 11:02 aaa</div><div class="line"></div><div class="line">11:01 aa    4 12:00 b</div><div class="line">12:01 bb    5 12:01 bb</div><div class="line">13:01 cc    6 12:02 bbb</div><div class="line"></div><div class="line">11:02 aaa   7 13:00 c</div><div class="line">12:02 bbb   8 13:01 cc</div><div class="line">13:02 ccc   9 13:02 ccc</div></pre></td></tr></table></figure>
<h3 id="Generate-Partition-File"><a href="#Generate-Partition-File" class="headerlink" title="Generate Partition File"></a>Generate Partition File</h3><p>To use TotalOrderpartitioner, we need a partition file (i.e. boundaries) to tell the partitioner how to partition the mapper outputs. Usually we’ll use <a href="https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapreduce/lib/partition/InputSampler.RandomSampler.html" target="_blank" rel="external">InputSampler.RandomSampler</a> class, but this time let’s use a manual partition file.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">SequenceFile.Writer writer = <span class="keyword">new</span> SequenceFile.Writer(fs, getConf(), partition,</div><div class="line">        Text.class, NullWritable.class);</div><div class="line">Text key = <span class="keyword">new</span> Text();</div><div class="line">NullWritable value = NullWritable.get();</div><div class="line">key.set(<span class="string">"12:00"</span>);</div><div class="line">writer.append(key, value);</div><div class="line">key.set(<span class="string">"13:00"</span>);</div><div class="line">writer.append(key, value);</div><div class="line">writer.close();</div></pre></td></tr></table></figure>
<p>So basically, the partitioner will partition the mapper outputs into three parts, the first part will be less than “12:00”, seceond part [“12:00”, “13:00”), thrid [“13:00”, ).</p>
<p>And then, indicate the job to use this partition file:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">job.setPartitionerClass(TotalOrderPartitioner.class);</div><div class="line">otalOrderPartitioner.setPartitionFile(job.getConfiguration(), partition);</div><div class="line"></div><div class="line"><span class="comment">// The number of reducers should equal the number of partitions.</span></div><div class="line">job.setNumReduceTasks(<span class="number">3</span>);</div></pre></td></tr></table></figure>
<h3 id="Use-MutipleOutputs"><a href="#Use-MutipleOutputs" class="headerlink" title="Use MutipleOutputs"></a>Use MutipleOutputs</h3><p>In the reducer, we need to note down the row count of this partition, to do that, we’ll need the MultipleOutputs class, which let use output multiple result files apart from the default “part-r-xxxxx”. The reducer’s code is as following:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JobReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> MultipleOutputs&lt;NullWritable, Text&gt; mos;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">long</span> count;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span></span></div><div class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>.setup(context);</div><div class="line">        mos = <span class="keyword">new</span> MultipleOutputs&lt;NullWritable, Text&gt;(context);</div><div class="line">        count = <span class="number">0</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span></span></div><div class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">        <span class="keyword">for</span> (Text value : values) &#123;</div><div class="line">            context.write(NullWritable.get(), value);</div><div class="line">            ++count;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span></span></div><div class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>.cleanup(context);</div><div class="line">        mos.write(<span class="string">"count"</span>, NullWritable.get(), <span class="keyword">new</span> LongWritable(count));</div><div class="line">        mos.close();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>There’re several things to pay attention to:</p>
<ol>
<li>MultipleOutputs is declared as class member, defined in Reducer#setup method, and must be closed at Reducer#cleanup (otherwise the file will be empty).</li>
<li>When instantiating MultipleOutputs class, the generic type needs to be the same as reducer’s output key/value class.</li>
<li>In order to use a different output key/value class, additional setup needs to be done at job definition:</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Job job = <span class="keyword">new</span> Job(getConf());</div><div class="line">MultipleOutputs.addNamedOutput(job, <span class="string">"count"</span>, SequenceFileOutputFormat.class,</div><div class="line">    NullWritable.class, LongWritable.class);</div></pre></td></tr></table></figure>
<p>For example, if the output folder is “/tmp/total-sort/“, there’ll be the following files when job is done:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">/tmp/total-sort/count-r-00001</div><div class="line">/tmp/total-sort/count-r-00002</div><div class="line">/tmp/total-sort/count-r-00003</div><div class="line">/tmp/total-sort/part-r-00001</div><div class="line">/tmp/total-sort/part-r-00002</div><div class="line">/tmp/total-sort/part-r-00003</div></pre></td></tr></table></figure>
<h3 id="Pass-Start-Ids-to-Mapper"><a href="#Pass-Start-Ids-to-Mapper" class="headerlink" title="Pass Start Ids to Mapper"></a>Pass Start Ids to Mapper</h3><p>When the second mapper processes the inputs, we want them to know the initial id of its partition, which can be calculated from the “count-*” files we produce before. To pass this information, we can use the job’s Configuration object.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Read and calculate the start id from those row-count files.</span></div><div class="line">Map&lt;String, Long&gt; startIds = <span class="keyword">new</span> HashMap&lt;String, Long&gt;();</div><div class="line"><span class="keyword">long</span> startId = <span class="number">1</span>;</div><div class="line">FileSystem fs = FileSystem.get(getConf());</div><div class="line"><span class="keyword">for</span> (FileStatus file : fs.listStatus(countPath)) &#123;</div><div class="line"></div><div class="line">    Path path = file.getPath();</div><div class="line">    String name = path.getName();</div><div class="line">    <span class="keyword">if</span> (!name.startsWith(<span class="string">"count-"</span>)) &#123;</div><div class="line">        <span class="keyword">continue</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    startIds.put(name.substring(name.length() - <span class="number">5</span>), startId);</div><div class="line"></div><div class="line">    SequenceFile.Reader reader = <span class="keyword">new</span> SequenceFile.Reader(fs, path, getConf());</div><div class="line">    NullWritable key = NullWritable.get();</div><div class="line">    LongWritable value = <span class="keyword">new</span> LongWritable();</div><div class="line">    <span class="keyword">if</span> (!reader.next(key, value)) &#123;</div><div class="line">        <span class="keyword">continue</span>;</div><div class="line">    &#125;</div><div class="line">    startId += value.get();</div><div class="line">    reader.close();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Serialize the map and pass it to Configuration.</span></div><div class="line">job.getConfiguration().set(<span class="string">"startIds"</span>, Base64.encodeBase64String(</div><div class="line">        SerializationUtils.serialize((Serializable) startIds)));</div><div class="line">        </div><div class="line"><span class="comment">// Recieve it in Mapper#setup</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMapperB</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> Map&lt;String, Long&gt; startIds;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">long</span> startId;</div><div class="line"></div><div class="line">    <span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span></span></div><div class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>.setup(context);</div><div class="line">        startIds = (Map&lt;String, Long&gt;) SerializationUtils.deserialize(</div><div class="line">                Base64.decodeBase64(context.getConfiguration().get(<span class="string">"startIds"</span>)));</div><div class="line">        String name = ((FileSplit) context.getInputSplit()).getPath().getName();</div><div class="line">        startId = startIds.get(name.substring(name.length() - <span class="number">5</span>));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(NullWritable key, Text value, Context context)</span></span></div><div class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">        context.write(<span class="keyword">new</span> LongWritable(startId++), value);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Set-the-Input-Non-splitable"><a href="#Set-the-Input-Non-splitable" class="headerlink" title="Set the Input Non-splitable"></a>Set the Input Non-splitable</h3><p>When the file is bigger than a block or so (depending on some configuration entries), Hadoop will split it, which is not good for us. So let’s define a new InputFormat class to disable the splitting behaviour:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">NonSplitableSequence</span> <span class="keyword">extends</span> <span class="title">SequenceFileInputFormat</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isSplitable</span><span class="params">(JobContext context, Path filename)</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// use it</span></div><div class="line">job.setInputFormatClass(NonSplitableSequence.class);</div></pre></td></tr></table></figure>
<p>And that’s it, we are able to generate a unique, auto-increment id for a sorted collection, with Hadoop’s parallel computing capability. The process is rather complicated, which requires several techniques about Hadoop. It’s worthwhile to dig.</p>
<p>A workable example can be found in my <a href="https://github.com/jizhang/mapred-sandbox/blob/master/src/main/java/com/shzhangji/mapred_sandbox/AutoIncrementId2Job.java" target="_blank" rel="external">Github repository</a>. If you have some more straight-forward approach, please do let me know.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In DBMS world, it’s easy to generate a unique, auto-increment id, using MySQL’s &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.1/en/example-auto-increment.html&quot;&gt;AUTO_INCREMENT attribute&lt;/a&gt; on a primary key or MongoDB’s &lt;a href=&quot;http://docs.mongodb.org/manual/tutorial/create-an-auto-incrementing-field/&quot;&gt;Counters Collection&lt;/a&gt; pattern. But when it comes to a distributed, parallel processing framework, like Hadoop Map-reduce, it is not that straight forward. The best solution to identify every record in such framework is to use UUID. But when an integer id is required, it’ll take some steps.&lt;/p&gt;
&lt;h2 id=&quot;Solution-A-Single-Reducer&quot;&gt;&lt;a href=&quot;#Solution-A-Single-Reducer&quot; class=&quot;headerlink&quot; title=&quot;Solution A: Single Reducer&quot;&gt;&lt;/a&gt;Solution A: Single Reducer&lt;/h2&gt;&lt;p&gt;This is the most obvious and simple one, just use the following code to specify reducer numbers to 1:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;job.setNumReduceTasks(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;);&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;And also obvious, there are several demerits:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;All mappers output will be copied to one task tracker.&lt;/li&gt;
&lt;li&gt;Only one process is working on shuffel &amp;amp; sort.&lt;/li&gt;
&lt;li&gt;When producing output, there’s also only one process.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The above is not a problem for small data sets, or at least small mapper outputs. And it is also the approach that Pig and Hive use when they need to perform a total sort. But when hitting a certain threshold, the sort and copy phase will become very slow and unacceptable.&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://shzhangji.com/categories/Notes/"/>
    
      <category term="Big Data" scheme="http://shzhangji.com/categories/Notes/Big-Data/"/>
    
    
  </entry>
  
</feed>
