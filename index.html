
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Ji ZHANG's Blog</title>
  <meta name="author" content="Ji ZHANG">

  
  <meta name="description" content="Spark是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的Hadoop生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于Hive，Spark也有相应的替代项目——Shark，能做到 drop-in &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://shzhangji.com">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Ji ZHANG's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/libs/jquery.min.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>-->

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-37223379-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">Ji ZHANG's Blog</a></h1>
  
    <h2>If I rest, I rust.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:shzhangji.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/categories/tutorial">Tutorial</a></li>
  <li><a href="/blog/categories/translation">Translation</a></li>
  <li><a href="/blog/categories/notes">Notes</a></li>
  <li><a href="/blog/categories/big-data">Big Data</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5/">在CDH 4.5上安装Shark 0.9</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-05T17:16:00+08:00" pubdate data-updated="true">Jul 5<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://spark.apache.org">Spark</a>是一个新兴的大数据计算平台，它的优势之一是内存型计算，因此对于需要多次迭代的算法尤为适用。同时，它又能够很好地融合到现有的<a href="http://hadoop.apache.org">Hadoop</a>生态环境中，包括直接存取HDFS上的文件，以及运行于YARN之上。对于<a href="http://hive.apache.org">Hive</a>，Spark也有相应的替代项目——<a href="http://shark.cs.berkeley.edu/">Shark</a>，能做到 <strong>drop-in replacement</strong> ，直接构建在现有集群之上。本文就将简要阐述如何在CDH4.5上搭建Shark0.9集群。</p>

<h2>准备工作</h2>

<ul>
<li>安装方式：Spark使用CDH提供的Parcel，以Standalone模式启动</li>
<li>软件版本

<ul>
<li>Cloudera Manager 4.8.2</li>
<li>CDH 4.5</li>
<li>Spark 0.9.0 Parcel</li>
<li><a href="http://cloudera.rst.im/shark/">Shark 0.9.1 Binary</a></li>
</ul>
</li>
<li>服务器基础配置

<ul>
<li>可用的软件源（如<a href="http://mirrors.ustc.edu.cn/">中科大的源</a>）</li>
<li>配置主节点至子节点的root账户SSH无密码登录。</li>
<li>在<code>/etc/hosts</code>中写死IP和主机名，或者DNS做好正反解析。</li>
</ul>
</li>
</ul>


<h2>安装Spark</h2>

<ul>
<li>使用CM安装Parcel，不需要重启服务。</li>
<li>修改<code>/etc/spark/conf/spark-env.sh</code>：（其中one-843是主节点的域名）</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">STANDALONE_SPARK_MASTER_HOST</span><span class="o">=</span>one-843
</span><span class='line'><span class="nv">DEFAULT_HADOOP_HOME</span><span class="o">=</span>/opt/cloudera/parcels/CDH/lib/hadoop
</span><span class='line'><span class="nb">export </span><span class="nv">SPARK_CLASSPATH</span><span class="o">=</span><span class="nv">$SPARK_CLASSPATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*
</span><span class='line'><span class="nb">export </span><span class="nv">SPARK_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$SPARK_LIBRARY_PATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>修改<code>/etc/spark/conf/slaves</code>，添加各节点主机名。</li>
<li>将<code>/etc/spark/conf</code>目录同步至所有节点。</li>
<li>启动Spark服务（即Standalone模式）：</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>/opt/cloudera/parcels/SPARK/lib/spark/sbin/start-master.sh
</span><span class='line'><span class="nv">$ </span>/opt/cloudera/parcels/SPARK/lib/spark/sbin/start-slaves.sh
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>测试<code>spark-shell</code>是否可用：</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;hdfs://one-843:8020/user/jizhang/zj_people.txt.lzo&quot;</span><span class="o">).</span><span class="n">count</span>
</span></code></pre></td></tr></table></div></figure>


<h2>安装Shark</h2>

<ul>
<li>安装Oracle JDK 1.7 Update 45至<code>/usr/lib/jvm/jdk1.7.0_45</code>。</li>
<li>下载别人编译好的二进制包：<a href="http://cloudera.rst.im/shark/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz">shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0.tar.gz</a></li>
<li>解压至<code>/opt</code>目录，修改<code>conf/shark-env.sh</code>：</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/usr/lib/jvm/jdk1.7.0_45
</span><span class='line'><span class="nb">export </span><span class="nv">SCALA_HOME</span><span class="o">=</span>/opt/cloudera/parcels/SPARK/lib/spark
</span><span class='line'><span class="nb">export </span><span class="nv">SHARK_HOME</span><span class="o">=</span>/root/shark-0.9.1-bin-2.0.0-mr1-cdh-4.6.0
</span><span class='line'>
</span><span class='line'><span class="nb">export </span><span class="nv">HIVE_CONF_DIR</span><span class="o">=</span>/etc/hive/conf
</span><span class='line'>
</span><span class='line'><span class="nb">export </span><span class="nv">HADOOP_HOME</span><span class="o">=</span>/opt/cloudera/parcels/CDH/lib/hadoop
</span><span class='line'><span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span>/opt/cloudera/parcels/SPARK/lib/spark
</span><span class='line'><span class="nb">export </span><span class="nv">MASTER</span><span class="o">=</span>spark://one-843:7077
</span><span class='line'>
</span><span class='line'><span class="nb">export </span><span class="nv">SPARK_CLASSPATH</span><span class="o">=</span><span class="nv">$SPARK_CLASSPATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/*
</span><span class='line'><span class="nb">export </span><span class="nv">SPARK_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$SPARK_LIBRARY_PATH</span>:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>开启SharkServer2，使用Supervisord管理：</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="o">[</span>program:sharkserver2<span class="o">]</span>
</span><span class='line'><span class="nb">command</span> <span class="o">=</span> /opt/shark/bin/shark --service sharkserver2
</span><span class='line'><span class="nv">autostart</span> <span class="o">=</span> <span class="nb">true</span>
</span><span class='line'><span class="nv">autorestart</span> <span class="o">=</span> <span class="nb">true</span>
</span><span class='line'><span class="nv">stdout_logfile</span> <span class="o">=</span> /var/log/sharkserver2.log
</span><span class='line'><span class="nv">redirect_stderr</span> <span class="o">=</span> <span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>supervisorctl start sharkserver2
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>测试</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>/opt/shark/bin/beeline -u jdbc:hive2://one-843:10000 -n root
</span></code></pre></td></tr></table></div></figure>


<h2>版本问题</h2>

<h3>背景</h3>

<h4>CDH</h4>

<p>CDH是对Hadoop生态链各组件的打包，每个CDH版本都会对应一组Hadoop组件的版本，如<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH-Version-and-Packaging-Information/cdhvd_topic_3.html">CDH4.5</a>的部分对应关系如下：</p>

<ul>
<li>Apache Hadoop: hadoop-2.0.0+1518</li>
<li>Apache Hive: hive-0.10.0+214</li>
<li>Hue: hue-2.5.0+182</li>
</ul>


<p>可以看到，CDH4.5对应的Hive版本是0.10.0，因此它的<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.5.0/CDH4-Installation-Guide/cdh4ig_hive_metastore_configure.html">Metastore Server</a>使用的也是0.10.0版本的API。</p>

<h4>Spark</h4>

<p>Spark目前最高版本是0.9.1，CDH前不久推出了0.9.0的Parcel，使得安装过程变的简单得多。CDH5中对Spark做了深度集成，即可以用CM来直接控制Spark的服务，且支持Spark on YARN架构。</p>

<h4>Shark</h4>

<p>Shark是基于Spark的一款应用，可以简单地认为是将Hive的MapReduce引擎替换为了Spark。</p>

<p>Shark的一个特点的是需要使用特定的Hive版本——<a href="https://github.com/amplab/hive">AMPLab patched Hive</a>：</p>

<ul>
<li>Shark 0.8.x: AMPLab Hive 0.9.0</li>
<li>Shark 0.9.x: AMPLab Hive 0.11.0</li>
</ul>


<p>在0.9.0以前，我们需要手动下载AMPLab Hive的二进制包，并在Shark的环境变量中设置$HIVE_HOME。在0.9.1以后，AMPLab将该版本的Hive包上传至了Maven，可以直接打进Shark的二进制包中。但是，这个Jar是用JDK7编译的，因此运行Shark需要使用Oracle JDK7。CDH建议使用Update 45这个小版本。</p>

<h4>Shark与Hive的并存</h4>

<p>Shark的一个卖点是和Hive的<a href="5">高度兼容</a>，也就是说它可以直接操作Hive的metastore db，或是和metastore server通信。当然，前提是两者的Hive版本需要一致，这也是目前遇到的最大问题。</p>

<h3>目前发现的不兼容SQL</h3>

<ul>
<li>DROP TABLE &hellip;</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>FAILED: Error in metadata: org.apache.thrift.TApplicationException: Invalid method name: <span class="s1">&#39;drop_table_with_environment_context&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>INSERT OVERWRITE TABLE &hellip; PARTITION (&hellip;) SELECT &hellip;</li>
<li>LOAD DATA INPATH &lsquo;&hellip;&rsquo; OVERWRITE INTO TABLE &hellip; PARTITION (&hellip;)</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>Failed with exception org.apache.thrift.TApplicationException: Invalid method name: <span class="s1">&#39;partition_name_has_valid_characters&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<p>也就是说上述两个方法名是0.11.0接口中定义的，在0.10.0的定义中并不存在，所以出现上述问题。</p>

<h3>解决方案</h3>

<h4>对存在问题的SQL使用Hive命令去调</h4>

<p>因为Shark初期是想给分析师使用的，他们对分区表并不是很在意，而DROP TABLE可以在客户端做判断，转而使用Hive来执行。</p>

<p>这个方案的优点是可以在现有集群上立刻用起来，但缺点是需要做一些额外的开发，而且API不一致的问题可能还会有其他坑在里面。</p>

<h4>升级到CDH5</h4>

<p>CDH5中Hive的版本是0.12.0，所以不排除同样存在API不兼容的问题。不过网上也有人尝试跳过AMPLab Hive，让Shark直接调用CDH中的Hive，其可行性还需要我们自己测试。</p>

<p>对于这个问题，我只在<a href="https://groups.google.com/forum/#!starred/shark-users/x_Dh5-3isIc">Google Groups</a>上看到一篇相关的帖子，不过并没有给出解决方案。</p>

<p>目前我们实施的是 <strong>第一种方案</strong>，即在客户端和Shark之间添加一层，不支持的SQL语句直接降级用Hive执行，效果不错。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/05/27/use-webjars-in-scalatra-project/">Use WebJars in Scalatra Project</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-05-27T17:44:00+08:00" pubdate data-updated="true">May 27<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>As I&rsquo;m working with my first <a href="http://www.scalatra.org/">Scalatra</a> project, I automatically think of using <a href="http://www.webjars.org/">WebJars</a> to manage Javascript library dependencies, since it&rsquo;s more convenient and seems like a good practice. Though there&rsquo;s no <a href="http://www.webjars.org/documentation">official support</a> for Scalatra framework, the installation process is not very complex. But this doesn&rsquo;t mean I didn&rsquo;t spend much time on this. I&rsquo;m still a newbie to Scala, and there&rsquo;s only a few materials on this subject.</p>

<h2>Add WebJars Dependency in SBT Build File</h2>

<p>Scalatra uses <code>.scala</code> configuration file instead of <code>.sbt</code>, so let&rsquo;s add dependency into <code>project/build.scala</code>. Take <a href="http://dojotoolkit.org/">Dojo</a> for example.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">object</span> <span class="nc">DwExplorerBuild</span> <span class="k">extends</span> <span class="nc">Build</span> <span class="o">{</span>
</span><span class='line'>  <span class="o">...</span>
</span><span class='line'>  <span class="k">lazy</span> <span class="k">val</span> <span class="n">project</span> <span class="k">=</span> <span class="nc">Project</span> <span class="o">(</span>
</span><span class='line'>    <span class="o">...</span>
</span><span class='line'>    <span class="n">settings</span> <span class="k">=</span> <span class="nc">Defaults</span><span class="o">.</span><span class="n">defaultSettings</span> <span class="o">++</span> <span class="nc">ScalatraPlugin</span><span class="o">.</span><span class="n">scalatraWithJRebel</span> <span class="o">++</span> <span class="n">scalateSettings</span> <span class="o">++</span> <span class="nc">Seq</span><span class="o">(</span>
</span><span class='line'>      <span class="o">...</span>
</span><span class='line'>      <span class="n">libraryDependencies</span> <span class="o">++=</span> <span class="nc">Seq</span><span class="o">(</span>
</span><span class='line'>        <span class="o">...</span>
</span><span class='line'>        <span class="s">&quot;org.webjars&quot;</span> <span class="o">%</span> <span class="s">&quot;dojo&quot;</span> <span class="o">%</span> <span class="s">&quot;1.9.3&quot;</span>
</span><span class='line'>      <span class="o">),</span>
</span><span class='line'>      <span class="o">...</span>
</span><span class='line'>    <span class="o">)</span>
</span><span class='line'>  <span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To view this dependency in Eclipse, you need to run <code>sbt eclipse</code> again. In the <em>Referenced Libraries</em> section, you can see a <code>dojo-1.9.3.jar</code>, and the library lies in <code>META-INF/resources/webjars/</code>.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/05/27/use-webjars-in-scalatra-project/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/01/25/java-reflection-tutorial/">Java反射机制</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-01-25T09:42:00+08:00" pubdate data-updated="true">Jan 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>原文：<a href="http://www.programcreek.com/2013/09/java-reflection-tutorial/">http://www.programcreek.com/2013/09/java-reflection-tutorial/</a></p>

<p>什么是反射？它有何用处？</p>

<h2>1. 什么是反射？</h2>

<p>“反射（Reflection）能够让运行于JVM中的程序检测和修改运行时的行为。”这个概念常常会和内省（Introspection）混淆，以下是这两个术语在Wikipedia中的解释：</p>

<ol>
<li>内省用于在运行时检测某个对象的类型和其包含的属性；</li>
<li>反射用于在运行时检测和修改某个对象的结构及其行为。</li>
</ol>


<p>从他们的定义可以看出，内省是反射的一个子集。有些语言支持内省，但并不支持反射，如C++。</p>

<p><img src="http://www.programcreek.com/wp-content/uploads/2013/09/reflection-introspection-650x222.png" alt="反射和内省" /></p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/01/25/java-reflection-tutorial/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/12/17/the-law-of-leaky-abstractions/">抽象泄漏定律</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-12-17T13:05:00+08:00" pubdate data-updated="true">Dec 17<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>原文：<a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html">http://www.joelonsoftware.com/articles/LeakyAbstractions.html</a></p>

<p>TCP协议是互联网的基石，我们每天都需要依靠它来构建各类互联网应用。也正是在这一协议中，时刻发生着一件近乎神奇的事情。</p>

<p>TCP是一种 <em>可靠的</em> 数据传输协议，也就是说，当你通过TCP协议在网络上传输一条消息时，它一定会到达目的地，而且不会失真或毁坏。</p>

<p>我们可以使用TCP来做很多事情，从浏览网页信息到收发邮件。TCP的可靠性使得东非贪污受贿的新闻能够一字一句地传递到世界各地。真是太棒了！</p>

<p>和TCP协议相比，IP协议也是一种传输协议，但它是 <em>不可靠的</em> 。没有人可以保证你的数据一定会到达目的地，或者在它到达前就已经被破坏了。如果你发送了一组消息，不要惊讶为何只有一半的消息到达，有些消息的顺序会不正确，甚至消息的内容被替换成了黑猩猩宝宝的图片，或是一堆无法阅读的垃圾数据，像极了台湾人的邮件标题。</p>

<p>这就是TCP协议神奇的地方：它是构建在IP协议之上的。换句话说，TCP协议能够 <em>使用一个不可靠的工具来可靠地传输数据</em> 。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/12/17/the-law-of-leaky-abstractions/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job/">Generate Auto-increment Id in Map-reduce Job</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-10-31T09:35:00+08:00" pubdate data-updated="true">Oct 31<span>st</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In DBMS world, it&rsquo;s easy to generate a unique, auto-increment id, using MySQL&rsquo;s <a href="http://dev.mysql.com/doc/refman/5.1/en/example-auto-increment.html">AUTO_INCREMENT attribute</a> on a primary key or MongoDB&rsquo;s <a href="http://docs.mongodb.org/manual/tutorial/create-an-auto-incrementing-field/">Counters Collection</a> pattern. But when it comes to a distributed, parallel processing framework, like Hadoop Map-reduce, it is not that straight forward. The best solution to identify every record in such framework is to use UUID. But when an integer id is required, it&rsquo;ll take some steps.</p>

<h2>Solution A: Single Reducer</h2>

<p>This is the most obvious and simple one, just use the following code to specify reducer numbers to 1:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">job</span><span class="o">.</span><span class="na">setNumReduceTasks</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>And also obvious, there are several demerits:</p>

<ol>
<li>All mappers output will be copied to one task tracker.</li>
<li>Only one process is working on shuffel &amp; sort.</li>
<li>When producing output, there&rsquo;s also only one process.</li>
</ol>


<p>The above is not a problem for small data sets, or at least small mapper outputs. And it is also the approach that Pig and Hive use when they need to perform a total sort. But when hitting a certain threshold, the sort and copy phase will become very slow and unacceptable.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/09/06/hive-deleteme-error/">Hive并发情况下报DELETEME表不存在的异常</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-09-06T11:20:00+08:00" pubdate data-updated="true">Sep 6<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>在每天运行的Hive脚本中，偶尔会抛出以下错误：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2013-09-03 01:39:00,973 ERROR parse.SemanticAnalyzer (SemanticAnalyzer.java:getMetaData(1128)) - org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table dw_xxx_xxx
</span><span class='line'>        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:896)
</span><span class='line'>        ...
</span><span class='line'>Caused by: javax.jdo.JDODataStoreException: Exception thrown obtaining schema column information from datastore
</span><span class='line'>NestedThrowables:
</span><span class='line'>com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'hive.DELETEME1378143540925' doesn't exist
</span><span class='line'>        at org.datanucleus.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:313)
</span><span class='line'>        ...</span></code></pre></td></tr></table></div></figure>


<p>查阅了网上的资料，是DataNucleus的问题。</p>

<p>背景1：我们知道MySQL中的库表信息是存放在information_schema库中的，Hive也有类似的机制，它会将库表信息存放在一个第三方的RDBMS中，目前我们线上配置的是本机MySQL，即：</p>

<p>$ mysql -uhive -ppassword hive</p>

<p><img src="/images/hive-deleteme-error/1.png" alt="1.png" /></p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/09/06/hive-deleteme-error/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/11/ansible-faq/">Ansible FAQ</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-11T21:18:00+08:00" pubdate data-updated="true">Jun 11<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>本文是从原Ansible官网的FAQ页面翻译而来，网站改版后该页面已无法访问，但可以从<a href="https://github.com/ansible/ansible.github.com/blob/4a2bf7f60a020f0d0a7b042056fc3dd8716588f2/faq.html">Github历史提交</a>中获得。翻译这篇原始FAQ文档是因为它陈述了Ansible这款工具诞生的原因，设计思路和特性，以及与Puppet、Fabric等同类软件的比较，可以让我们对Ansible有一个整体的了解，所以值得使用者一读。</p>

<h2>目录</h2>

<ul>
<li>为什么命名为“Ansible”？</li>
<li>Ansible受到了谁的启发？</li>
<li>与同类软件比较

<ul>
<li>Func？</li>
<li>Puppet？</li>
<li>Chef？</li>
<li>Capistrano/Fabric？</li>
</ul>
</li>
<li>其它问题

<ul>
<li>Ansible的安全性如何？</li>
<li>Ansible如何扩展？</li>
<li>是否支持SSH以外的协议？</li>
<li>Ansible的适用场景有哪些？</li>
</ul>
</li>
</ul>


<h2>为什么命名为“Ansible”？</h2>

<p>我最喜爱的书籍之一是奥森·斯科特·卡特的《安德的游戏》。在这本书中，“Ansible”是一种能够跨越时空的即时通讯工具。强烈推荐这本书！</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/06/11/ansible-faq/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/05/25/apache-hadoop-yarn-background-and-an-overview/">Apache Hadoop YARN - 项目背景与简介</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-25T10:57:00+08:00" pubdate data-updated="true">May 25<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>原文：<a href="http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/">http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/</a></p>

<p>日前，Apache Hadoop YARN已被提升为Apache软件基金会的子项目，这是一个值得庆祝的里程碑。这里我们也第一时间为各位献上Apache Hadoop YARN项目的系列介绍文章。YARN是一个普适的、分布式的应用管理框架，运行于Hadoop集群之上，用以替代传统的Apache Hadoop MapReduce框架。</p>

<h2>MapReduce 模式</h2>

<p>本质上来说，MapReduce模型包含两个部分：一是Map过程，将数据拆分成若干份，分别处理，彼此之间没有依赖关系；二是Reduce过程，将中间结果汇总计算成最终结果。这是一种简单而又条件苛刻的模型，但也促使它成为高效和极易扩展的并行计算方式。</p>

<p>Apache Hadoop MapReduce是当下最流行的开源MapReduce模型。</p>

<p>特别地，当MapReduce配合分布式文件系统，类似Apache Hadoop HDFS，就能在大集群上提供高吞吐量的计算，这一经济效应是Hadoop得以流行的重要原因。</p>

<p>这一模式成功的原因之一是，它使用的是“移动计算能力至数据节点”而非通过网络“移动数据至计算节点”的方式。具体来说，一个MapReduce任务会被调度到输入数据所在的HDFS节点执行，这会极大地减少I/O支出，因为大部分I/O会发生在本地磁盘或是同一机架中——这是核心优势。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/05/25/apache-hadoop-yarn-background-and-an-overview/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/05/01/introducing-cascalog-a-clojure-based-query-language-for-hado/">Cascalog：基于Clojure的Hadoop查询语言</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-01T18:01:00+08:00" pubdate data-updated="true">May 1<span>st</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>原文：<a href="http://nathanmarz.com/blog/introducing-cascalog-a-clojure-based-query-language-for-hado.html">http://nathanmarz.com/blog/introducing-cascalog-a-clojure-based-query-language-for-hado.html</a></p>

<p>我非常兴奋地告诉大家，<a href="http://github.com/nathanmarz/cascalog">Cascalog</a>开源了！Cascalog受<a href="http://en.wikipedia.org/wiki/Datalog">Datalog</a>启发，是一种基于Clojure、运行于Hadoop平台上的查询语言。</p>

<h2>特点</h2>

<ul>
<li><strong>简单</strong> &ndash; 使用相同的语法编写函数、过滤规则、聚合运算；数据联合（join）变得简单而自然。</li>
<li><strong>表达能力强</strong> &ndash; 强大的逻辑组合条件，你可以在查询语句中任意编写Clojure函数。</li>
<li><strong>交互性</strong> &ndash; 可以在Clojure REPL中执行查询语句。</li>
<li><strong>可扩展</strong> &ndash; Cascalog的查询语句是一组MapReduce脚本。</li>
<li><strong>任意数据源</strong> &ndash; HDFS、数据库、本地数据、以及任何能够使用Cascading的<code>Tap</code>读取的数据。</li>
<li><strong>正确处理空值</strong> &ndash; 空值往往让事情变得棘手。Cascalog提供了内置的“非空变量”来自动过滤空值。</li>
<li><strong>与Cascading结合</strong> &ndash; 使用Cascalog定义的流程可以在Cascading中直接使用，反之亦然。</li>
<li><strong>与Clojure结合</strong> &ndash; 能够使用普通的Clojure函数来编写操作流程、过滤规则，又因为Cascalog是一种Clojure DSL，因此也能在其他Clojure代码中使用。</li>
</ul>


</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/05/01/introducing-cascalog-a-clojure-based-query-language-for-hado/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/04/30/manage-leiningen-project-configuration/">Manage Leiningen Project Configuration</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-30T16:16:00+08:00" pubdate data-updated="true">Apr 30<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In Maven projects, we tend to use <code>.properties</code> files to store various configurations, and use Maven profiles to switch between development and production environments. Like the following example:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'># database.properties
</span><span class='line'>mydb.jdbcUrl=${mydb.jdbcUrl}
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="c">&lt;!-- pom.xml --&gt;</span>
</span><span class='line'><span class="nt">&lt;profiles&gt;</span>
</span><span class='line'>    <span class="nt">&lt;profile&gt;</span>
</span><span class='line'>        <span class="nt">&lt;id&gt;</span>development<span class="nt">&lt;/id&gt;</span>
</span><span class='line'>        <span class="nt">&lt;activation&gt;&lt;activeByDefault&gt;</span>true<span class="nt">&lt;/activeByDefault&gt;&lt;/activation&gt;</span>
</span><span class='line'>        <span class="nt">&lt;properties&gt;</span>
</span><span class='line'>            <span class="nt">&lt;mydb.jdbcUrl&gt;</span>jdbc:mysql://127.0.0.1:3306/mydb<span class="nt">&lt;/mydb.jdbcUrl&gt;</span>
</span><span class='line'>        <span class="nt">&lt;/properties&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/profile&gt;</span>
</span><span class='line'>    <span class="nt">&lt;profile&gt;</span>
</span><span class='line'>        <span class="nt">&lt;id&gt;</span>production<span class="nt">&lt;/id&gt;</span>
</span><span class='line'>        <span class="c">&lt;!-- This profile could be moved to ~/.m2/settings.xml to increase security. --&gt;</span>
</span><span class='line'>        <span class="nt">&lt;properties&gt;</span>
</span><span class='line'>            <span class="nt">&lt;mydb.jdbcUrl&gt;</span>jdbc:mysql://10.0.2.15:3306/mydb<span class="nt">&lt;/mydb.jdbcUrl&gt;</span>
</span><span class='line'>        <span class="nt">&lt;/properties&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/profile&gt;</span>
</span><span class='line'><span class="nt">&lt;/profiles&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>As for Leiningen projects, there&rsquo;s no variable substitution in profile facility, and although in profiles we could use <code>:resources</code> to compact production-wise files into Jar, these files are actually replacing the original ones, instead of being merged. One solution is to strictly seperate environment specific configs from the others, so the replacement will be ok. But here I take another approach, to manually load files from difference locations, and then merge them.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/04/30/manage-leiningen-project-configuration/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/07/05/deploy-shark-0.9-with-cdh-4.5/">在CDH 4.5上安装Shark 0.9</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/05/27/use-webjars-in-scalatra-project/">Use WebJars in Scalatra Project</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/01/25/java-reflection-tutorial/">Java反射机制</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/17/the-law-of-leaky-abstractions/">抽象泄漏定律</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/10/31/generate-auto-increment-id-in-map-reduce-job/">Generate Auto-increment Id in Map-reduce Job</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/jizhang">@jizhang</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'jizhang',
            count: 3,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
<a href="http://stackoverflow.com/users/1030720/jerry">
<img src="http://stackoverflow.com/users/flair/1030720.png?theme=clean" width="208" height="58" alt="profile for Jerry at Stack Overflow, Q&amp;A for professional and enthusiast programmers" title="profile for Jerry at Stack Overflow, Q&amp;A for professional and enthusiast programmers">
</a>
</section>

<section>
  <h1>On Delicious</h1>
  <div id="delicious"></div>
  <script type="text/javascript" src="http://feeds.delicious.com/v2/json/zjerryj?count=3&amp;sort=date&amp;callback=renderDeliciousLinks"></script>
  <p><a href="http://delicious.com/zjerryj">My Delicious Bookmarks &raquo;</a></p>
</section>


<section class="googleplus">
  <h1>
    <a href="https://plus.google.com/zhangji87@gmail.com?rel=author">
      <img src="http://www.google.com/images/icons/ui/gprofile_button-32.png" width="32" height="32">
      Google+
    </a>
  </h1>
</section>



  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Ji ZHANG -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'jizhang';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>






<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
